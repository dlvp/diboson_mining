{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=10, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=False)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.calculate_expectation(\n",
    "        x_filename=sample_dir + 'validation{}/x_validation.npy'.format(cut_label)\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:56  \n",
      "13:56  ------------------------------------------------------------\n",
      "13:56  |                                                          |\n",
      "13:56  |  MadMiner v2018.11.12                                    |\n",
      "13:56  |                                                          |\n",
      "13:56  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "13:56  |                                                          |\n",
      "13:56  ------------------------------------------------------------\n",
      "13:56  \n",
      "13:56  Training 10 estimators in ensemble\n",
      "13:56  Training estimator 1 / 10 in ensemble\n",
      "13:56  Starting training\n",
      "13:56    Method:                 sally\n",
      "13:56    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "13:56                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "13:56    Features:               all\n",
      "13:56    Method:                 sally\n",
      "13:56    Hidden layers:          (100, 100, 100, 100)\n",
      "13:56    Activation function:    tanh\n",
      "13:56    Batch size:             128\n",
      "13:56    Trainer:                amsgrad\n",
      "13:56    Epochs:                 50\n",
      "13:56    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "13:56    Validation split:       None\n",
      "13:56    Early stopping:         True\n",
      "13:56    Scale inputs:           True\n",
      "13:56    Regularization:         None\n",
      "13:56  Loading training data\n",
      "13:56  Found 1000000 samples with 2 parameters and 27 observables\n",
      "13:56  Rescaling inputs\n",
      "13:56  Creating model for method sally\n",
      "13:56  Training model\n",
      "14:03    Epoch 5: train loss 4.7448 (mse_score: 4.7448)\n",
      "14:08    Epoch 10: train loss 4.7232 (mse_score: 4.7232)\n",
      "14:14    Epoch 15: train loss 4.6860 (mse_score: 4.6860)\n",
      "14:20    Epoch 20: train loss 4.6144 (mse_score: 4.6144)\n",
      "14:26    Epoch 25: train loss 4.5432 (mse_score: 4.5432)\n",
      "14:32    Epoch 30: train loss 4.4741 (mse_score: 4.4741)\n",
      "14:37    Epoch 35: train loss 4.4185 (mse_score: 4.4185)\n",
      "14:41    Epoch 40: train loss 4.3801 (mse_score: 4.3801)\n",
      "14:47    Epoch 45: train loss 4.3597 (mse_score: 4.3597)\n",
      "14:52    Epoch 50: train loss 4.3482 (mse_score: 4.3482)\n",
      "14:52  Finished training\n",
      "14:52  Training estimator 2 / 10 in ensemble\n",
      "14:52  Starting training\n",
      "14:52    Method:                 sally\n",
      "14:52    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "14:52                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "14:52    Features:               all\n",
      "14:52    Method:                 sally\n",
      "14:52    Hidden layers:          (100, 100, 100, 100)\n",
      "14:52    Activation function:    tanh\n",
      "14:52    Batch size:             128\n",
      "14:52    Trainer:                amsgrad\n",
      "14:52    Epochs:                 50\n",
      "14:52    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "14:52    Validation split:       None\n",
      "14:52    Early stopping:         True\n",
      "14:52    Scale inputs:           True\n",
      "14:52    Regularization:         None\n",
      "14:52  Loading training data\n",
      "14:52  Found 1000000 samples with 2 parameters and 27 observables\n",
      "14:52  Rescaling inputs\n",
      "14:52  Creating model for method sally\n",
      "14:52  Training model\n",
      "14:59    Epoch 5: train loss 4.8606 (mse_score: 4.8606)\n",
      "15:05    Epoch 10: train loss 4.7892 (mse_score: 4.7892)\n",
      "15:11    Epoch 15: train loss 4.7214 (mse_score: 4.7214)\n",
      "15:17    Epoch 20: train loss 4.6242 (mse_score: 4.6242)\n",
      "15:23    Epoch 25: train loss 4.5202 (mse_score: 4.5202)\n",
      "15:30    Epoch 30: train loss 4.4330 (mse_score: 4.4330)\n",
      "15:36    Epoch 35: train loss 4.3545 (mse_score: 4.3545)\n",
      "15:43    Epoch 40: train loss 4.3118 (mse_score: 4.3118)\n",
      "15:50    Epoch 45: train loss 4.2857 (mse_score: 4.2857)\n",
      "15:54    Epoch 50: train loss 4.2724 (mse_score: 4.2724)\n",
      "15:54  Finished training\n",
      "15:54  Training estimator 3 / 10 in ensemble\n",
      "15:54  Starting training\n",
      "15:54    Method:                 sally\n",
      "15:54    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "15:54                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "15:54    Features:               all\n",
      "15:54    Method:                 sally\n",
      "15:54    Hidden layers:          (100, 100, 100, 100)\n",
      "15:54    Activation function:    tanh\n",
      "15:54    Batch size:             128\n",
      "15:54    Trainer:                amsgrad\n",
      "15:54    Epochs:                 50\n",
      "15:54    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "15:54    Validation split:       None\n",
      "15:54    Early stopping:         True\n",
      "15:54    Scale inputs:           True\n",
      "15:54    Regularization:         None\n",
      "15:54  Loading training data\n",
      "15:54  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:54  Rescaling inputs\n",
      "15:54  Creating model for method sally\n",
      "15:54  Training model\n",
      "15:58    Epoch 5: train loss 4.0787 (mse_score: 4.0787)\n",
      "16:02    Epoch 10: train loss 4.0073 (mse_score: 4.0073)\n",
      "16:06    Epoch 15: train loss 3.9213 (mse_score: 3.9213)\n",
      "16:09    Epoch 20: train loss 3.8271 (mse_score: 3.8271)\n",
      "16:12    Epoch 25: train loss 3.7472 (mse_score: 3.7472)\n",
      "17:08    Epoch 30: train loss 3.6775 (mse_score: 3.6775)\n",
      "17:14    Epoch 35: train loss 3.6329 (mse_score: 3.6329)\n",
      "17:19    Epoch 40: train loss 3.6033 (mse_score: 3.6033)\n",
      "17:24    Epoch 45: train loss 3.5888 (mse_score: 3.5888)\n",
      "17:29    Epoch 50: train loss 3.5807 (mse_score: 3.5807)\n",
      "17:29  Finished training\n",
      "17:29  Training estimator 4 / 10 in ensemble\n",
      "17:29  Starting training\n",
      "17:29    Method:                 sally\n",
      "17:29    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "17:29                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "17:29    Features:               all\n",
      "17:29    Method:                 sally\n",
      "17:29    Hidden layers:          (100, 100, 100, 100)\n",
      "17:29    Activation function:    tanh\n",
      "17:29    Batch size:             128\n",
      "17:29    Trainer:                amsgrad\n",
      "17:29    Epochs:                 50\n",
      "17:29    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "17:29    Validation split:       None\n",
      "17:29    Early stopping:         True\n",
      "17:29    Scale inputs:           True\n",
      "17:29    Regularization:         None\n",
      "17:29  Loading training data\n",
      "17:29  Found 1000000 samples with 2 parameters and 27 observables\n",
      "17:29  Rescaling inputs\n",
      "17:29  Creating model for method sally\n",
      "17:29  Training model\n",
      "17:34    Epoch 5: train loss 4.2239 (mse_score: 4.2239)\n",
      "17:38    Epoch 10: train loss 4.1041 (mse_score: 4.1041)\n",
      "17:43    Epoch 15: train loss 4.0155 (mse_score: 4.0155)\n",
      "17:48    Epoch 20: train loss 3.9159 (mse_score: 3.9159)\n",
      "17:53    Epoch 25: train loss 3.8115 (mse_score: 3.8115)\n",
      "17:58    Epoch 30: train loss 3.7375 (mse_score: 3.7375)\n",
      "18:03    Epoch 35: train loss 3.6837 (mse_score: 3.6837)\n",
      "18:08    Epoch 40: train loss 3.6541 (mse_score: 3.6541)\n",
      "18:13    Epoch 45: train loss 3.6397 (mse_score: 3.6397)\n",
      "18:18    Epoch 50: train loss 3.6311 (mse_score: 3.6311)\n",
      "18:18  Finished training\n",
      "18:18  Training estimator 5 / 10 in ensemble\n",
      "18:18  Starting training\n",
      "18:18    Method:                 sally\n",
      "18:18    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "18:18                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "18:18    Features:               all\n",
      "18:18    Method:                 sally\n",
      "18:18    Hidden layers:          (100, 100, 100, 100)\n",
      "18:18    Activation function:    tanh\n",
      "18:18    Batch size:             128\n",
      "18:18    Trainer:                amsgrad\n",
      "18:18    Epochs:                 50\n",
      "18:18    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "18:18    Validation split:       None\n",
      "18:18    Early stopping:         True\n",
      "18:18    Scale inputs:           True\n",
      "18:18    Regularization:         None\n",
      "18:18  Loading training data\n",
      "18:18  Found 1000000 samples with 2 parameters and 27 observables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:18  Rescaling inputs\n",
      "18:18  Creating model for method sally\n",
      "18:18  Training model\n",
      "18:24    Epoch 5: train loss 3.8643 (mse_score: 3.8643)\n",
      "18:32    Epoch 10: train loss 3.7167 (mse_score: 3.7167)\n",
      "18:42    Epoch 15: train loss 3.6556 (mse_score: 3.6556)\n",
      "18:46    Epoch 20: train loss 3.5386 (mse_score: 3.5386)\n",
      "18:51    Epoch 25: train loss 3.4401 (mse_score: 3.4401)\n",
      "18:56    Epoch 30: train loss 3.3023 (mse_score: 3.3023)\n",
      "19:01    Epoch 35: train loss 3.2202 (mse_score: 3.2202)\n",
      "19:06    Epoch 40: train loss 3.1628 (mse_score: 3.1628)\n",
      "19:10    Epoch 45: train loss 3.1367 (mse_score: 3.1367)\n",
      "19:15    Epoch 50: train loss 3.1219 (mse_score: 3.1219)\n",
      "19:15  Finished training\n",
      "19:15  Training estimator 6 / 10 in ensemble\n",
      "19:15  Starting training\n",
      "19:15    Method:                 sally\n",
      "19:15    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "19:15                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "19:15    Features:               all\n",
      "19:15    Method:                 sally\n",
      "19:15    Hidden layers:          (100, 100, 100, 100)\n",
      "19:15    Activation function:    tanh\n",
      "19:15    Batch size:             128\n",
      "19:15    Trainer:                amsgrad\n",
      "19:15    Epochs:                 50\n",
      "19:15    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "19:15    Validation split:       None\n",
      "19:15    Early stopping:         True\n",
      "19:15    Scale inputs:           True\n",
      "19:15    Regularization:         None\n",
      "19:15  Loading training data\n",
      "19:15  Found 1000000 samples with 2 parameters and 27 observables\n",
      "19:15  Rescaling inputs\n",
      "19:15  Creating model for method sally\n",
      "19:15  Training model\n",
      "19:21    Epoch 5: train loss 4.5089 (mse_score: 4.5089)\n",
      "19:26    Epoch 10: train loss 4.3839 (mse_score: 4.3839)\n",
      "19:31    Epoch 15: train loss 4.2500 (mse_score: 4.2500)\n",
      "19:37    Epoch 20: train loss 4.1902 (mse_score: 4.1902)\n",
      "19:42    Epoch 25: train loss 4.0977 (mse_score: 4.0977)\n",
      "19:47    Epoch 30: train loss 3.9950 (mse_score: 3.9950)\n",
      "19:52    Epoch 35: train loss 3.9271 (mse_score: 3.9271)\n",
      "19:57    Epoch 40: train loss 3.8889 (mse_score: 3.8889)\n",
      "20:03    Epoch 45: train loss 3.8650 (mse_score: 3.8650)\n",
      "20:08    Epoch 50: train loss 3.8535 (mse_score: 3.8535)\n",
      "20:08  Finished training\n",
      "20:08  Training estimator 7 / 10 in ensemble\n",
      "20:08  Starting training\n",
      "20:08    Method:                 sally\n",
      "20:08    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "20:08                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "20:08    Features:               all\n",
      "20:08    Method:                 sally\n",
      "20:08    Hidden layers:          (100, 100, 100, 100)\n",
      "20:08    Activation function:    tanh\n",
      "20:08    Batch size:             128\n",
      "20:08    Trainer:                amsgrad\n",
      "20:08    Epochs:                 50\n",
      "20:08    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "20:08    Validation split:       None\n",
      "20:08    Early stopping:         True\n",
      "20:08    Scale inputs:           True\n",
      "20:08    Regularization:         None\n",
      "20:08  Loading training data\n",
      "20:08  Found 1000000 samples with 2 parameters and 27 observables\n",
      "20:08  Rescaling inputs\n",
      "20:08  Creating model for method sally\n",
      "20:08  Training model\n",
      "20:13    Epoch 5: train loss 7.8039 (mse_score: 7.8039)\n",
      "20:19    Epoch 10: train loss 7.7599 (mse_score: 7.7599)\n",
      "20:24    Epoch 15: train loss 7.6160 (mse_score: 7.6160)\n",
      "20:32    Epoch 20: train loss 7.5195 (mse_score: 7.5195)\n",
      "20:42    Epoch 25: train loss 7.4080 (mse_score: 7.4080)\n",
      "20:50    Epoch 30: train loss 7.3072 (mse_score: 7.3072)\n",
      "20:59    Epoch 35: train loss 7.2110 (mse_score: 7.2110)\n",
      "21:07    Epoch 40: train loss 7.1588 (mse_score: 7.1588)\n",
      "21:14    Epoch 45: train loss 7.1299 (mse_score: 7.1299)\n",
      "21:21    Epoch 50: train loss 7.1142 (mse_score: 7.1142)\n",
      "21:21  Finished training\n",
      "21:21  Training estimator 8 / 10 in ensemble\n",
      "21:21  Starting training\n",
      "21:21    Method:                 sally\n",
      "21:21    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "21:21                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "21:21    Features:               all\n",
      "21:21    Method:                 sally\n",
      "21:21    Hidden layers:          (100, 100, 100, 100)\n",
      "21:21    Activation function:    tanh\n",
      "21:21    Batch size:             128\n",
      "21:21    Trainer:                amsgrad\n",
      "21:21    Epochs:                 50\n",
      "21:21    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "21:21    Validation split:       None\n",
      "21:21    Early stopping:         True\n",
      "21:21    Scale inputs:           True\n",
      "21:21    Regularization:         None\n",
      "21:21  Loading training data\n",
      "21:21  Found 1000000 samples with 2 parameters and 27 observables\n",
      "21:21  Rescaling inputs\n",
      "21:22  Creating model for method sally\n",
      "21:22  Training model\n",
      "21:28    Epoch 5: train loss 4.0485 (mse_score: 4.0485)\n",
      "21:34    Epoch 10: train loss 3.9147 (mse_score: 3.9147)\n",
      "21:39    Epoch 15: train loss 3.7619 (mse_score: 3.7619)\n",
      "21:45    Epoch 20: train loss 3.7174 (mse_score: 3.7174)\n",
      "21:51    Epoch 25: train loss 3.6124 (mse_score: 3.6124)\n",
      "21:57    Epoch 30: train loss 3.5265 (mse_score: 3.5265)\n",
      "22:02    Epoch 35: train loss 3.4705 (mse_score: 3.4705)\n",
      "22:09    Epoch 40: train loss 3.4455 (mse_score: 3.4455)\n",
      "22:15    Epoch 45: train loss 3.4285 (mse_score: 3.4285)\n",
      "22:20    Epoch 50: train loss 3.4206 (mse_score: 3.4206)\n",
      "22:20  Finished training\n",
      "22:20  Training estimator 9 / 10 in ensemble\n",
      "22:20  Starting training\n",
      "22:20    Method:                 sally\n",
      "22:20    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "22:20                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "22:20    Features:               all\n",
      "22:20    Method:                 sally\n",
      "22:20    Hidden layers:          (100, 100, 100, 100)\n",
      "22:20    Activation function:    tanh\n",
      "22:20    Batch size:             128\n",
      "22:20    Trainer:                amsgrad\n",
      "22:20    Epochs:                 50\n",
      "22:20    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "22:20    Validation split:       None\n",
      "22:20    Early stopping:         True\n",
      "22:20    Scale inputs:           True\n",
      "22:20    Regularization:         None\n",
      "22:20  Loading training data\n",
      "22:20  Found 1000000 samples with 2 parameters and 27 observables\n",
      "22:20  Rescaling inputs\n",
      "22:20  Creating model for method sally\n",
      "22:20  Training model\n",
      "22:25    Epoch 5: train loss 4.1621 (mse_score: 4.1621)\n",
      "22:31    Epoch 10: train loss 4.0600 (mse_score: 4.0600)\n",
      "22:37    Epoch 15: train loss 3.9850 (mse_score: 3.9850)\n",
      "22:43    Epoch 20: train loss 3.9259 (mse_score: 3.9259)\n",
      "22:49    Epoch 25: train loss 3.8571 (mse_score: 3.8571)\n",
      "22:55    Epoch 30: train loss 3.7789 (mse_score: 3.7789)\n",
      "23:01    Epoch 35: train loss 3.7384 (mse_score: 3.7384)\n",
      "23:07    Epoch 40: train loss 3.7133 (mse_score: 3.7133)\n",
      "23:14    Epoch 45: train loss 3.7030 (mse_score: 3.7030)\n",
      "23:20    Epoch 50: train loss 3.6962 (mse_score: 3.6962)\n",
      "23:20  Finished training\n",
      "23:20  Training estimator 10 / 10 in ensemble\n",
      "23:20  Starting training\n",
      "23:20    Method:                 sally\n",
      "23:20    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "23:20                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "23:20    Features:               all\n",
      "23:20    Method:                 sally\n",
      "23:20    Hidden layers:          (100, 100, 100, 100)\n",
      "23:20    Activation function:    tanh\n",
      "23:20    Batch size:             128\n",
      "23:20    Trainer:                amsgrad\n",
      "23:20    Epochs:                 50\n",
      "23:20    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "23:20    Validation split:       None\n",
      "23:20    Early stopping:         True\n",
      "23:20    Scale inputs:           True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:20    Regularization:         None\n",
      "23:20  Loading training data\n",
      "23:20  Found 1000000 samples with 2 parameters and 27 observables\n",
      "23:20  Rescaling inputs\n",
      "23:20  Creating model for method sally\n",
      "23:20  Training model\n",
      "23:26    Epoch 5: train loss 7.2573 (mse_score: 7.2573)\n",
      "23:32    Epoch 10: train loss 7.1772 (mse_score: 7.1772)\n",
      "23:38    Epoch 15: train loss 7.1024 (mse_score: 7.1024)\n",
      "23:44    Epoch 20: train loss 7.0042 (mse_score: 7.0042)\n",
      "23:50    Epoch 25: train loss 6.9463 (mse_score: 6.9463)\n",
      "23:56    Epoch 30: train loss 6.8851 (mse_score: 6.8851)\n",
      "00:01    Epoch 35: train loss 6.8331 (mse_score: 6.8331)\n",
      "00:05    Epoch 40: train loss 6.8053 (mse_score: 6.8053)\n",
      "00:09    Epoch 45: train loss 6.7862 (mse_score: 6.7862)\n",
      "00:14    Epoch 50: train loss 6.7776 (mse_score: 6.7776)\n",
      "00:14  Finished training\n",
      "00:14  Calculating expectation for 10 estimators in ensemble\n",
      "00:14  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "00:14  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "00:14  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "00:15  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "00:15  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "00:15  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "00:16  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "00:16  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "00:17  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "00:17  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble('all', use_tight_cuts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:17  Training 10 estimators in ensemble\n",
      "00:17  Training estimator 1 / 10 in ensemble\n",
      "00:17  Starting training\n",
      "00:17    Method:                 sally\n",
      "00:17    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "00:17                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "00:17    Features:               all\n",
      "00:17    Method:                 sally\n",
      "00:17    Hidden layers:          (100, 100, 100, 100)\n",
      "00:17    Activation function:    tanh\n",
      "00:17    Batch size:             128\n",
      "00:17    Trainer:                amsgrad\n",
      "00:17    Epochs:                 50\n",
      "00:17    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "00:17    Validation split:       None\n",
      "00:17    Early stopping:         True\n",
      "00:17    Scale inputs:           True\n",
      "00:17    Regularization:         None\n",
      "00:17  Loading training data\n",
      "00:17  Found 1000000 samples with 2 parameters and 27 observables\n",
      "00:17  Rescaling inputs\n",
      "00:17  Creating model for method sally\n",
      "00:17  Training model\n",
      "00:22    Epoch 5: train loss 432.1569 (mse_score: 432.1569)\n",
      "00:27    Epoch 10: train loss 422.7990 (mse_score: 422.7990)\n",
      "00:31    Epoch 15: train loss 415.7542 (mse_score: 415.7542)\n",
      "00:36    Epoch 20: train loss 409.9214 (mse_score: 409.9214)\n",
      "00:41    Epoch 25: train loss 402.7649 (mse_score: 402.7649)\n",
      "00:45    Epoch 30: train loss 399.3643 (mse_score: 399.3643)\n",
      "00:50    Epoch 35: train loss 397.1985 (mse_score: 397.1985)\n",
      "00:54    Epoch 40: train loss 396.3251 (mse_score: 396.3251)\n",
      "00:59    Epoch 45: train loss 395.8852 (mse_score: 395.8852)\n",
      "01:04    Epoch 50: train loss 395.6541 (mse_score: 395.6541)\n",
      "01:04  Finished training\n",
      "01:04  Training estimator 2 / 10 in ensemble\n",
      "01:04  Starting training\n",
      "01:04    Method:                 sally\n",
      "01:04    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "01:04                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "01:04    Features:               all\n",
      "01:04    Method:                 sally\n",
      "01:04    Hidden layers:          (100, 100, 100, 100)\n",
      "01:04    Activation function:    tanh\n",
      "01:04    Batch size:             128\n",
      "01:04    Trainer:                amsgrad\n",
      "01:04    Epochs:                 50\n",
      "01:04    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "01:04    Validation split:       None\n",
      "01:04    Early stopping:         True\n",
      "01:04    Scale inputs:           True\n",
      "01:04    Regularization:         None\n",
      "01:04  Loading training data\n",
      "01:04  Found 1000000 samples with 2 parameters and 27 observables\n",
      "01:04  Rescaling inputs\n",
      "01:04  Creating model for method sally\n",
      "01:04  Training model\n",
      "01:09    Epoch 5: train loss 393.0605 (mse_score: 393.0605)\n",
      "01:13    Epoch 10: train loss 386.2113 (mse_score: 386.2113)\n",
      "01:18    Epoch 15: train loss 378.3964 (mse_score: 378.3964)\n",
      "01:22    Epoch 20: train loss 372.3651 (mse_score: 372.3651)\n",
      "01:27    Epoch 25: train loss 366.2961 (mse_score: 366.2961)\n",
      "01:31    Epoch 30: train loss 360.4167 (mse_score: 360.4167)\n",
      "01:36    Epoch 35: train loss 356.4636 (mse_score: 356.4636)\n",
      "01:41    Epoch 40: train loss 353.9802 (mse_score: 353.9802)\n",
      "01:45    Epoch 45: train loss 352.7204 (mse_score: 352.7204)\n",
      "01:50    Epoch 50: train loss 352.0430 (mse_score: 352.0430)\n",
      "01:50  Finished training\n",
      "01:50  Training estimator 3 / 10 in ensemble\n",
      "01:50  Starting training\n",
      "01:50    Method:                 sally\n",
      "01:50    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "01:50                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "01:50    Features:               all\n",
      "01:50    Method:                 sally\n",
      "01:50    Hidden layers:          (100, 100, 100, 100)\n",
      "01:50    Activation function:    tanh\n",
      "01:50    Batch size:             128\n",
      "01:50    Trainer:                amsgrad\n",
      "01:50    Epochs:                 50\n",
      "01:50    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "01:50    Validation split:       None\n",
      "01:50    Early stopping:         True\n",
      "01:50    Scale inputs:           True\n",
      "01:50    Regularization:         None\n",
      "01:50  Loading training data\n",
      "01:50  Found 1000000 samples with 2 parameters and 27 observables\n",
      "01:50  Rescaling inputs\n",
      "01:50  Creating model for method sally\n",
      "01:50  Training model\n",
      "01:55    Epoch 5: train loss 456.0539 (mse_score: 456.0539)\n",
      "01:59    Epoch 10: train loss 448.9539 (mse_score: 448.9539)\n",
      "02:04    Epoch 15: train loss 442.1719 (mse_score: 442.1719)\n",
      "02:08    Epoch 20: train loss 435.4803 (mse_score: 435.4803)\n",
      "02:13    Epoch 25: train loss 428.0730 (mse_score: 428.0730)\n",
      "02:17    Epoch 30: train loss 421.4284 (mse_score: 421.4284)\n",
      "02:22    Epoch 35: train loss 417.1359 (mse_score: 417.1359)\n",
      "02:26    Epoch 40: train loss 414.6142 (mse_score: 414.6142)\n",
      "02:31    Epoch 45: train loss 413.3604 (mse_score: 413.3604)\n",
      "02:35    Epoch 50: train loss 412.6197 (mse_score: 412.6197)\n",
      "02:35  Finished training\n",
      "02:35  Training estimator 4 / 10 in ensemble\n",
      "02:35  Starting training\n",
      "02:35    Method:                 sally\n",
      "02:35    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_3.npy\n",
      "02:35                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_3.npy\n",
      "02:35    Features:               all\n",
      "02:35    Method:                 sally\n",
      "02:35    Hidden layers:          (100, 100, 100, 100)\n",
      "02:35    Activation function:    tanh\n",
      "02:35    Batch size:             128\n",
      "02:35    Trainer:                amsgrad\n",
      "02:35    Epochs:                 50\n",
      "02:35    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "02:35    Validation split:       None\n",
      "02:35    Early stopping:         True\n",
      "02:35    Scale inputs:           True\n",
      "02:35    Regularization:         None\n",
      "02:35  Loading training data\n",
      "02:35  Found 1000000 samples with 2 parameters and 27 observables\n",
      "02:35  Rescaling inputs\n",
      "02:35  Creating model for method sally\n",
      "02:35  Training model\n",
      "02:40    Epoch 5: train loss 404.7976 (mse_score: 404.7976)\n",
      "02:45    Epoch 10: train loss 396.1123 (mse_score: 396.1123)\n",
      "02:50    Epoch 15: train loss 391.0940 (mse_score: 391.0940)\n",
      "02:55    Epoch 20: train loss 384.8454 (mse_score: 384.8454)\n",
      "03:00    Epoch 25: train loss 380.4214 (mse_score: 380.4214)\n",
      "03:04    Epoch 30: train loss 377.5545 (mse_score: 377.5545)\n",
      "03:09    Epoch 35: train loss 375.4899 (mse_score: 375.4899)\n",
      "03:13    Epoch 40: train loss 374.3947 (mse_score: 374.3947)\n",
      "03:18    Epoch 45: train loss 373.8823 (mse_score: 373.8823)\n",
      "03:23    Epoch 50: train loss 373.6006 (mse_score: 373.6006)\n",
      "03:23  Finished training\n",
      "03:23  Training estimator 5 / 10 in ensemble\n",
      "03:23  Starting training\n",
      "03:23    Method:                 sally\n",
      "03:23    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_4.npy\n",
      "03:23                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_4.npy\n",
      "03:23    Features:               all\n",
      "03:23    Method:                 sally\n",
      "03:23    Hidden layers:          (100, 100, 100, 100)\n",
      "03:23    Activation function:    tanh\n",
      "03:23    Batch size:             128\n",
      "03:23    Trainer:                amsgrad\n",
      "03:23    Epochs:                 50\n",
      "03:23    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "03:23    Validation split:       None\n",
      "03:23    Early stopping:         True\n",
      "03:23    Scale inputs:           True\n",
      "03:23    Regularization:         None\n",
      "03:23  Loading training data\n",
      "03:23  Found 1000000 samples with 2 parameters and 27 observables\n",
      "03:23  Rescaling inputs\n",
      "03:23  Creating model for method sally\n",
      "03:23  Training model\n",
      "03:28    Epoch 5: train loss 374.0081 (mse_score: 374.0081)\n",
      "03:33    Epoch 10: train loss 364.1877 (mse_score: 364.1877)\n",
      "03:37    Epoch 15: train loss 357.3654 (mse_score: 357.3654)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:42    Epoch 20: train loss 352.5294 (mse_score: 352.5294)\n",
      "03:47    Epoch 25: train loss 348.0134 (mse_score: 348.0134)\n",
      "03:51    Epoch 30: train loss 344.3583 (mse_score: 344.3583)\n",
      "03:56    Epoch 35: train loss 342.1161 (mse_score: 342.1161)\n",
      "04:01    Epoch 40: train loss 341.0843 (mse_score: 341.0843)\n",
      "04:05    Epoch 45: train loss 340.6171 (mse_score: 340.6171)\n",
      "04:10    Epoch 50: train loss 340.4170 (mse_score: 340.4170)\n",
      "04:10  Finished training\n",
      "04:10  Training estimator 6 / 10 in ensemble\n",
      "04:10  Starting training\n",
      "04:10    Method:                 sally\n",
      "04:10    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_5.npy\n",
      "04:10                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_5.npy\n",
      "04:10    Features:               all\n",
      "04:10    Method:                 sally\n",
      "04:10    Hidden layers:          (100, 100, 100, 100)\n",
      "04:10    Activation function:    tanh\n",
      "04:10    Batch size:             128\n",
      "04:10    Trainer:                amsgrad\n",
      "04:10    Epochs:                 50\n",
      "04:10    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "04:10    Validation split:       None\n",
      "04:10    Early stopping:         True\n",
      "04:10    Scale inputs:           True\n",
      "04:10    Regularization:         None\n",
      "04:10  Loading training data\n",
      "04:10  Found 1000000 samples with 2 parameters and 27 observables\n",
      "04:10  Rescaling inputs\n",
      "04:10  Creating model for method sally\n",
      "04:10  Training model\n",
      "04:15    Epoch 5: train loss 426.6178 (mse_score: 426.6178)\n",
      "04:20    Epoch 10: train loss 417.6207 (mse_score: 417.6207)\n",
      "04:25    Epoch 15: train loss 411.3589 (mse_score: 411.3589)\n",
      "04:30    Epoch 20: train loss 406.2187 (mse_score: 406.2187)\n",
      "04:35    Epoch 25: train loss 401.5952 (mse_score: 401.5952)\n",
      "04:39    Epoch 30: train loss 398.3147 (mse_score: 398.3147)\n",
      "04:44    Epoch 35: train loss 396.6412 (mse_score: 396.6412)\n",
      "04:49    Epoch 40: train loss 395.6741 (mse_score: 395.6741)\n",
      "04:54    Epoch 45: train loss 395.2648 (mse_score: 395.2648)\n",
      "04:59    Epoch 50: train loss 395.0279 (mse_score: 395.0279)\n",
      "04:59  Finished training\n",
      "04:59  Training estimator 7 / 10 in ensemble\n",
      "04:59  Starting training\n",
      "04:59    Method:                 sally\n",
      "04:59    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_6.npy\n",
      "04:59                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_6.npy\n",
      "04:59    Features:               all\n",
      "04:59    Method:                 sally\n",
      "04:59    Hidden layers:          (100, 100, 100, 100)\n",
      "04:59    Activation function:    tanh\n",
      "04:59    Batch size:             128\n",
      "04:59    Trainer:                amsgrad\n",
      "04:59    Epochs:                 50\n",
      "04:59    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "04:59    Validation split:       None\n",
      "04:59    Early stopping:         True\n",
      "04:59    Scale inputs:           True\n",
      "04:59    Regularization:         None\n",
      "04:59  Loading training data\n",
      "04:59  Found 1000000 samples with 2 parameters and 27 observables\n",
      "04:59  Rescaling inputs\n",
      "04:59  Creating model for method sally\n",
      "04:59  Training model\n",
      "05:04    Epoch 5: train loss 418.9003 (mse_score: 418.9003)\n",
      "05:08    Epoch 10: train loss 408.5847 (mse_score: 408.5847)\n",
      "05:13    Epoch 15: train loss 401.5366 (mse_score: 401.5366)\n",
      "05:18    Epoch 20: train loss 394.0246 (mse_score: 394.0246)\n",
      "05:22    Epoch 25: train loss 388.9323 (mse_score: 388.9323)\n",
      "05:27    Epoch 30: train loss 383.8157 (mse_score: 383.8157)\n",
      "05:32    Epoch 35: train loss 381.0691 (mse_score: 381.0691)\n",
      "05:36    Epoch 40: train loss 379.8015 (mse_score: 379.8015)\n",
      "05:41    Epoch 45: train loss 379.1871 (mse_score: 379.1871)\n",
      "05:45    Epoch 50: train loss 378.8831 (mse_score: 378.8831)\n",
      "05:45  Finished training\n",
      "05:45  Training estimator 8 / 10 in ensemble\n",
      "05:45  Starting training\n",
      "05:45    Method:                 sally\n",
      "05:45    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_7.npy\n",
      "05:45                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_7.npy\n",
      "05:45    Features:               all\n",
      "05:45    Method:                 sally\n",
      "05:45    Hidden layers:          (100, 100, 100, 100)\n",
      "05:45    Activation function:    tanh\n",
      "05:45    Batch size:             128\n",
      "05:45    Trainer:                amsgrad\n",
      "05:45    Epochs:                 50\n",
      "05:45    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "05:45    Validation split:       None\n",
      "05:45    Early stopping:         True\n",
      "05:45    Scale inputs:           True\n",
      "05:45    Regularization:         None\n",
      "05:45  Loading training data\n",
      "05:45  Found 1000000 samples with 2 parameters and 27 observables\n",
      "05:45  Rescaling inputs\n",
      "05:46  Creating model for method sally\n",
      "05:46  Training model\n",
      "05:50    Epoch 5: train loss 408.8434 (mse_score: 408.8434)\n",
      "05:55    Epoch 10: train loss 402.4003 (mse_score: 402.4003)\n",
      "06:00    Epoch 15: train loss 394.8654 (mse_score: 394.8654)\n",
      "06:04    Epoch 20: train loss 389.4625 (mse_score: 389.4625)\n",
      "06:09    Epoch 25: train loss 384.3839 (mse_score: 384.3839)\n",
      "06:14    Epoch 30: train loss 381.3448 (mse_score: 381.3448)\n",
      "06:19    Epoch 35: train loss 379.7026 (mse_score: 379.7026)\n",
      "06:23    Epoch 40: train loss 379.0518 (mse_score: 379.0518)\n",
      "06:28    Epoch 45: train loss 378.6478 (mse_score: 378.6478)\n",
      "06:32    Epoch 50: train loss 378.4077 (mse_score: 378.4077)\n",
      "06:32  Finished training\n",
      "06:32  Training estimator 9 / 10 in ensemble\n",
      "06:32  Starting training\n",
      "06:32    Method:                 sally\n",
      "06:32    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_8.npy\n",
      "06:32                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_8.npy\n",
      "06:32    Features:               all\n",
      "06:32    Method:                 sally\n",
      "06:32    Hidden layers:          (100, 100, 100, 100)\n",
      "06:32    Activation function:    tanh\n",
      "06:32    Batch size:             128\n",
      "06:32    Trainer:                amsgrad\n",
      "06:32    Epochs:                 50\n",
      "06:32    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "06:32    Validation split:       None\n",
      "06:32    Early stopping:         True\n",
      "06:32    Scale inputs:           True\n",
      "06:32    Regularization:         None\n",
      "06:32  Loading training data\n",
      "06:33  Found 1000000 samples with 2 parameters and 27 observables\n",
      "06:33  Rescaling inputs\n",
      "06:33  Creating model for method sally\n",
      "06:33  Training model\n",
      "06:38    Epoch 5: train loss 393.0507 (mse_score: 393.0507)\n",
      "06:42    Epoch 10: train loss 386.1431 (mse_score: 386.1431)\n",
      "06:47    Epoch 15: train loss 381.7770 (mse_score: 381.7770)\n",
      "06:52    Epoch 20: train loss 377.0615 (mse_score: 377.0615)\n",
      "06:57    Epoch 25: train loss 372.5611 (mse_score: 372.5611)\n",
      "07:02    Epoch 30: train loss 368.9480 (mse_score: 368.9480)\n",
      "07:06    Epoch 35: train loss 366.8739 (mse_score: 366.8739)\n",
      "07:11    Epoch 40: train loss 365.9164 (mse_score: 365.9164)\n",
      "07:14    Epoch 45: train loss 365.3486 (mse_score: 365.3486)\n",
      "07:17    Epoch 50: train loss 365.0920 (mse_score: 365.0920)\n",
      "07:17  Finished training\n",
      "07:17  Training estimator 10 / 10 in ensemble\n",
      "07:17  Starting training\n",
      "07:17    Method:                 sally\n",
      "07:17    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_9.npy\n",
      "07:17                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_9.npy\n",
      "07:17    Features:               all\n",
      "07:17    Method:                 sally\n",
      "07:17    Hidden layers:          (100, 100, 100, 100)\n",
      "07:17    Activation function:    tanh\n",
      "07:17    Batch size:             128\n",
      "07:17    Trainer:                amsgrad\n",
      "07:17    Epochs:                 50\n",
      "07:17    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "07:17    Validation split:       None\n",
      "07:17    Early stopping:         True\n",
      "07:17    Scale inputs:           True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:17    Regularization:         None\n",
      "07:17  Loading training data\n",
      "07:17  Found 1000000 samples with 2 parameters and 27 observables\n",
      "07:17  Rescaling inputs\n",
      "07:17  Creating model for method sally\n",
      "07:17  Training model\n",
      "07:20    Epoch 5: train loss 452.5377 (mse_score: 452.5377)\n",
      "07:23    Epoch 10: train loss 444.4663 (mse_score: 444.4663)\n",
      "07:26    Epoch 15: train loss 436.2341 (mse_score: 436.2341)\n",
      "07:29    Epoch 20: train loss 429.0399 (mse_score: 429.0399)\n",
      "07:31    Epoch 25: train loss 421.1423 (mse_score: 421.1423)\n",
      "07:34    Epoch 30: train loss 415.0749 (mse_score: 415.0749)\n",
      "07:37    Epoch 35: train loss 410.7962 (mse_score: 410.7962)\n",
      "07:40    Epoch 40: train loss 408.3174 (mse_score: 408.3174)\n",
      "07:42    Epoch 45: train loss 407.0590 (mse_score: 407.0590)\n",
      "07:46    Epoch 50: train loss 406.3461 (mse_score: 406.3461)\n",
      "07:46  Finished training\n",
      "07:46  Calculating expectation for 10 estimators in ensemble\n",
      "07:46  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "07:46  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "07:47  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "07:47  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "07:57  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "07:57  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "07:58  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "07:58  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "07:58  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "07:59  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble('all_tight', use_tight_cuts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal observable basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal',\n",
    "    use_tight_cuts=False,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just resurrection phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:15  Training 10 estimators in ensemble\n",
      "08:15  Training estimator 1 / 10 in ensemble\n",
      "08:15  Starting training\n",
      "08:15    Method:                 sally\n",
      "08:15    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "08:15                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "08:15    Features:               [26]\n",
      "08:15    Method:                 sally\n",
      "08:15    Hidden layers:          (100, 100, 100, 100)\n",
      "08:15    Activation function:    tanh\n",
      "08:15    Batch size:             128\n",
      "08:15    Trainer:                amsgrad\n",
      "08:15    Epochs:                 50\n",
      "08:15    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "08:15    Validation split:       None\n",
      "08:15    Early stopping:         True\n",
      "08:15    Scale inputs:           True\n",
      "08:15    Regularization:         None\n",
      "08:15  Loading training data\n",
      "08:15  Found 1000000 samples with 2 parameters and 27 observables\n",
      "08:15  Rescaling inputs\n",
      "08:15  Only using 1 of 27 observables\n",
      "08:15  Creating model for method sally\n",
      "08:15  Training model\n",
      "08:38    Epoch 5: train loss 491.6405 (mse_score: 491.6405)\n",
      "08:41    Epoch 10: train loss 491.3770 (mse_score: 491.3770)\n",
      "08:44    Epoch 15: train loss 491.2615 (mse_score: 491.2615)\n",
      "08:48    Epoch 20: train loss 491.1608 (mse_score: 491.1608)\n",
      "08:51    Epoch 25: train loss 491.1362 (mse_score: 491.1362)\n",
      "08:54    Epoch 30: train loss 491.1005 (mse_score: 491.1005)\n",
      "08:58    Epoch 35: train loss 491.0769 (mse_score: 491.0769)\n",
      "09:17    Epoch 40: train loss 491.0589 (mse_score: 491.0589)\n",
      "09:22    Epoch 45: train loss 491.0548 (mse_score: 491.0548)\n",
      "09:26    Epoch 50: train loss 491.0696 (mse_score: 491.0696)\n",
      "09:26  Finished training\n",
      "09:26  Training estimator 2 / 10 in ensemble\n",
      "09:26  Starting training\n",
      "09:26    Method:                 sally\n",
      "09:26    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "09:26                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "09:26    Features:               [26]\n",
      "09:26    Method:                 sally\n",
      "09:26    Hidden layers:          (100, 100, 100, 100)\n",
      "09:26    Activation function:    tanh\n",
      "09:26    Batch size:             128\n",
      "09:26    Trainer:                amsgrad\n",
      "09:26    Epochs:                 50\n",
      "09:26    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "09:26    Validation split:       None\n",
      "09:26    Early stopping:         True\n",
      "09:26    Scale inputs:           True\n",
      "09:26    Regularization:         None\n",
      "09:26  Loading training data\n",
      "09:26  Found 1000000 samples with 2 parameters and 27 observables\n",
      "09:26  Rescaling inputs\n",
      "09:26  Only using 1 of 27 observables\n",
      "09:26  Creating model for method sally\n",
      "09:26  Training model\n",
      "09:40    Epoch 5: train loss 446.8149 (mse_score: 446.8149)\n",
      "09:50    Epoch 10: train loss 446.4876 (mse_score: 446.4876)\n",
      "09:54    Epoch 15: train loss 446.3354 (mse_score: 446.3354)\n",
      "09:57    Epoch 20: train loss 443.0635 (mse_score: 443.0635)\n",
      "10:01    Epoch 25: train loss 442.7790 (mse_score: 442.7790)\n",
      "10:04    Epoch 30: train loss 442.6753 (mse_score: 442.6753)\n",
      "10:08    Epoch 35: train loss 442.6401 (mse_score: 442.6401)\n",
      "10:11    Epoch 40: train loss 442.5989 (mse_score: 442.5989)\n",
      "10:15    Epoch 45: train loss 442.5973 (mse_score: 442.5973)\n",
      "10:18    Epoch 50: train loss 442.5665 (mse_score: 442.5665)\n",
      "10:18  Finished training\n",
      "10:18  Training estimator 3 / 10 in ensemble\n",
      "10:18  Starting training\n",
      "10:18    Method:                 sally\n",
      "10:18    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "10:18                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "10:18    Features:               [26]\n",
      "10:18    Method:                 sally\n",
      "10:18    Hidden layers:          (100, 100, 100, 100)\n",
      "10:18    Activation function:    tanh\n",
      "10:18    Batch size:             128\n",
      "10:18    Trainer:                amsgrad\n",
      "10:18    Epochs:                 50\n",
      "10:18    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "10:18    Validation split:       None\n",
      "10:18    Early stopping:         True\n",
      "10:18    Scale inputs:           True\n",
      "10:18    Regularization:         None\n",
      "10:18  Loading training data\n",
      "10:18  Found 1000000 samples with 2 parameters and 27 observables\n",
      "10:18  Rescaling inputs\n",
      "10:18  Only using 1 of 27 observables\n",
      "10:18  Creating model for method sally\n",
      "10:18  Training model\n",
      "10:21    Epoch 5: train loss 513.3918 (mse_score: 513.3918)\n",
      "10:25    Epoch 10: train loss 513.1949 (mse_score: 513.1949)\n",
      "10:27    Epoch 15: train loss 513.0836 (mse_score: 513.0836)\n",
      "10:30    Epoch 20: train loss 512.9581 (mse_score: 512.9581)\n",
      "10:34    Epoch 25: train loss 512.9239 (mse_score: 512.9239)\n",
      "10:37    Epoch 30: train loss 512.8800 (mse_score: 512.8800)\n",
      "10:40    Epoch 35: train loss 512.8630 (mse_score: 512.8630)\n",
      "10:43    Epoch 40: train loss 512.8606 (mse_score: 512.8606)\n",
      "10:46    Epoch 45: train loss 512.8538 (mse_score: 512.8538)\n",
      "10:49    Epoch 50: train loss 512.8336 (mse_score: 512.8336)\n",
      "10:49  Finished training\n",
      "10:49  Training estimator 4 / 10 in ensemble\n",
      "10:49  Starting training\n",
      "10:49    Method:                 sally\n",
      "10:49    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_3.npy\n",
      "10:49                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_3.npy\n",
      "10:49    Features:               [26]\n",
      "10:49    Method:                 sally\n",
      "10:49    Hidden layers:          (100, 100, 100, 100)\n",
      "10:49    Activation function:    tanh\n",
      "10:49    Batch size:             128\n",
      "10:49    Trainer:                amsgrad\n",
      "10:49    Epochs:                 50\n",
      "10:49    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "10:49    Validation split:       None\n",
      "10:49    Early stopping:         True\n",
      "10:49    Scale inputs:           True\n",
      "10:49    Regularization:         None\n",
      "10:49  Loading training data\n",
      "10:49  Found 1000000 samples with 2 parameters and 27 observables\n",
      "10:49  Rescaling inputs\n",
      "10:49  Only using 1 of 27 observables\n",
      "10:49  Creating model for method sally\n",
      "10:49  Training model\n",
      "10:53    Epoch 5: train loss 462.5733 (mse_score: 462.5733)\n",
      "10:56    Epoch 10: train loss 462.3806 (mse_score: 462.3806)\n",
      "10:59    Epoch 15: train loss 462.2288 (mse_score: 462.2288)\n",
      "11:02    Epoch 20: train loss 462.1835 (mse_score: 462.1835)\n",
      "11:04    Epoch 25: train loss 458.8388 (mse_score: 458.8388)\n",
      "11:07    Epoch 30: train loss 457.9009 (mse_score: 457.9009)\n",
      "11:09    Epoch 35: train loss 457.7782 (mse_score: 457.7782)\n",
      "11:12    Epoch 40: train loss 457.7300 (mse_score: 457.7300)\n",
      "11:15    Epoch 45: train loss 457.7065 (mse_score: 457.7065)\n",
      "11:18    Epoch 50: train loss 457.6828 (mse_score: 457.6828)\n",
      "11:18  Finished training\n",
      "11:18  Training estimator 5 / 10 in ensemble\n",
      "11:18  Starting training\n",
      "11:18    Method:                 sally\n",
      "11:18    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_4.npy\n",
      "11:18                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_4.npy\n",
      "11:18    Features:               [26]\n",
      "11:18    Method:                 sally\n",
      "11:18    Hidden layers:          (100, 100, 100, 100)\n",
      "11:18    Activation function:    tanh\n",
      "11:18    Batch size:             128\n",
      "11:18    Trainer:                amsgrad\n",
      "11:18    Epochs:                 50\n",
      "11:18    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "11:18    Validation split:       None\n",
      "11:18    Early stopping:         True\n",
      "11:18    Scale inputs:           True\n",
      "11:18    Regularization:         None\n",
      "11:18  Loading training data\n",
      "11:18  Found 1000000 samples with 2 parameters and 27 observables\n",
      "11:18  Rescaling inputs\n",
      "11:18  Only using 1 of 27 observables\n",
      "11:18  Creating model for method sally\n",
      "11:18  Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:22    Epoch 5: train loss 428.2557 (mse_score: 428.2557)\n",
      "11:25    Epoch 10: train loss 428.0868 (mse_score: 428.0868)\n",
      "11:28    Epoch 15: train loss 427.9306 (mse_score: 427.9306)\n",
      "11:32    Epoch 20: train loss 427.8273 (mse_score: 427.8273)\n",
      "11:35    Epoch 25: train loss 427.7919 (mse_score: 427.7919)\n",
      "11:39    Epoch 30: train loss 427.8116 (mse_score: 427.8116)\n",
      "11:42    Epoch 35: train loss 427.7349 (mse_score: 427.7349)\n",
      "11:46    Epoch 40: train loss 427.7345 (mse_score: 427.7345)\n",
      "11:49    Epoch 45: train loss 427.7359 (mse_score: 427.7359)\n",
      "11:53    Epoch 50: train loss 427.7105 (mse_score: 427.7105)\n",
      "11:53  Finished training\n",
      "11:53  Training estimator 6 / 10 in ensemble\n",
      "11:53  Starting training\n",
      "11:53    Method:                 sally\n",
      "11:53    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_5.npy\n",
      "11:53                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_5.npy\n",
      "11:53    Features:               [26]\n",
      "11:53    Method:                 sally\n",
      "11:53    Hidden layers:          (100, 100, 100, 100)\n",
      "11:53    Activation function:    tanh\n",
      "11:53    Batch size:             128\n",
      "11:53    Trainer:                amsgrad\n",
      "11:53    Epochs:                 50\n",
      "11:53    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "11:53    Validation split:       None\n",
      "11:53    Early stopping:         True\n",
      "11:53    Scale inputs:           True\n",
      "11:53    Regularization:         None\n",
      "11:53  Loading training data\n",
      "11:53  Found 1000000 samples with 2 parameters and 27 observables\n",
      "11:53  Rescaling inputs\n",
      "11:53  Only using 1 of 27 observables\n",
      "11:53  Creating model for method sally\n",
      "11:53  Training model\n",
      "11:57    Epoch 5: train loss 482.8944 (mse_score: 482.8944)\n",
      "12:01    Epoch 10: train loss 482.6136 (mse_score: 482.6136)\n",
      "12:05    Epoch 15: train loss 482.5178 (mse_score: 482.5178)\n",
      "12:09    Epoch 20: train loss 482.4185 (mse_score: 482.4185)\n",
      "12:13    Epoch 25: train loss 482.3456 (mse_score: 482.3456)\n",
      "12:17    Epoch 30: train loss 482.3186 (mse_score: 482.3186)\n",
      "12:21    Epoch 35: train loss 482.2971 (mse_score: 482.2971)\n",
      "12:25    Epoch 40: train loss 482.3448 (mse_score: 482.3448)\n",
      "12:29    Epoch 45: train loss 482.3046 (mse_score: 482.3046)\n",
      "12:33    Epoch 50: train loss 482.3127 (mse_score: 482.3127)\n",
      "12:33  Finished training\n",
      "12:33  Training estimator 7 / 10 in ensemble\n",
      "12:33  Starting training\n",
      "12:33    Method:                 sally\n",
      "12:33    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_6.npy\n",
      "12:33                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_6.npy\n",
      "12:33    Features:               [26]\n",
      "12:33    Method:                 sally\n",
      "12:33    Hidden layers:          (100, 100, 100, 100)\n",
      "12:33    Activation function:    tanh\n",
      "12:33    Batch size:             128\n",
      "12:33    Trainer:                amsgrad\n",
      "12:33    Epochs:                 50\n",
      "12:33    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "12:33    Validation split:       None\n",
      "12:33    Early stopping:         True\n",
      "12:33    Scale inputs:           True\n",
      "12:33    Regularization:         None\n",
      "12:33  Loading training data\n",
      "12:33  Found 1000000 samples with 2 parameters and 27 observables\n",
      "12:33  Rescaling inputs\n",
      "12:33  Only using 1 of 27 observables\n",
      "12:33  Creating model for method sally\n",
      "12:33  Training model\n",
      "12:36    Epoch 5: train loss 481.9564 (mse_score: 481.9564)\n",
      "12:40    Epoch 10: train loss 481.7037 (mse_score: 481.7037)\n",
      "12:43    Epoch 15: train loss 481.6414 (mse_score: 481.6414)\n",
      "12:46    Epoch 20: train loss 481.5357 (mse_score: 481.5357)\n",
      "12:50    Epoch 25: train loss 481.5429 (mse_score: 481.5429)\n",
      "12:53    Epoch 30: train loss 481.4411 (mse_score: 481.4411)\n",
      "14:32    Epoch 35: train loss 481.4297 (mse_score: 481.4297)\n",
      "14:35    Epoch 40: train loss 481.4304 (mse_score: 481.4304)\n",
      "14:38    Epoch 45: train loss 481.4065 (mse_score: 481.4065)\n",
      "14:41    Epoch 50: train loss 481.4056 (mse_score: 481.4056)\n",
      "14:41  Finished training\n",
      "14:41  Training estimator 8 / 10 in ensemble\n",
      "14:41  Starting training\n",
      "14:41    Method:                 sally\n",
      "14:41    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_7.npy\n",
      "14:41                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_7.npy\n",
      "14:41    Features:               [26]\n",
      "14:41    Method:                 sally\n",
      "14:41    Hidden layers:          (100, 100, 100, 100)\n",
      "14:41    Activation function:    tanh\n",
      "14:41    Batch size:             128\n",
      "14:41    Trainer:                amsgrad\n",
      "14:41    Epochs:                 50\n",
      "14:41    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "14:41    Validation split:       None\n",
      "14:41    Early stopping:         True\n",
      "14:41    Scale inputs:           True\n",
      "14:41    Regularization:         None\n",
      "14:41  Loading training data\n",
      "14:41  Found 1000000 samples with 2 parameters and 27 observables\n",
      "14:41  Rescaling inputs\n",
      "14:41  Only using 1 of 27 observables\n",
      "14:41  Creating model for method sally\n",
      "14:41  Training model\n",
      "14:44    Epoch 5: train loss 470.8090 (mse_score: 470.8090)\n",
      "14:48    Epoch 10: train loss 470.5801 (mse_score: 470.5801)\n",
      "14:52    Epoch 15: train loss 470.4860 (mse_score: 470.4860)\n",
      "14:55    Epoch 20: train loss 470.4546 (mse_score: 470.4546)\n",
      "14:59    Epoch 25: train loss 467.0653 (mse_score: 467.0653)\n",
      "15:03    Epoch 30: train loss 466.2586 (mse_score: 466.2586)\n",
      "15:07    Epoch 35: train loss 466.1209 (mse_score: 466.1209)\n",
      "15:11    Epoch 40: train loss 466.0477 (mse_score: 466.0477)\n",
      "15:14    Epoch 45: train loss 466.0053 (mse_score: 466.0053)\n",
      "15:17    Epoch 50: train loss 465.9860 (mse_score: 465.9860)\n",
      "15:17  Finished training\n",
      "15:17  Training estimator 9 / 10 in ensemble\n",
      "15:17  Starting training\n",
      "15:17    Method:                 sally\n",
      "15:17    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_8.npy\n",
      "15:17                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_8.npy\n",
      "15:17    Features:               [26]\n",
      "15:17    Method:                 sally\n",
      "15:17    Hidden layers:          (100, 100, 100, 100)\n",
      "15:17    Activation function:    tanh\n",
      "15:17    Batch size:             128\n",
      "15:17    Trainer:                amsgrad\n",
      "15:17    Epochs:                 50\n",
      "15:17    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "15:17    Validation split:       None\n",
      "15:17    Early stopping:         True\n",
      "15:17    Scale inputs:           True\n",
      "15:17    Regularization:         None\n",
      "15:17  Loading training data\n",
      "15:17  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:17  Rescaling inputs\n",
      "15:17  Only using 1 of 27 observables\n",
      "15:17  Creating model for method sally\n",
      "15:17  Training model\n",
      "15:21    Epoch 5: train loss 452.2129 (mse_score: 452.2129)\n",
      "15:24    Epoch 10: train loss 452.2929 (mse_score: 452.2929)\n",
      "15:27    Epoch 15: train loss 452.1668 (mse_score: 452.1668)\n",
      "15:31    Epoch 20: train loss 452.0954 (mse_score: 452.0954)\n",
      "15:36    Epoch 25: train loss 452.0559 (mse_score: 452.0559)\n",
      "15:41    Epoch 30: train loss 451.9980 (mse_score: 451.9980)\n",
      "15:45    Epoch 35: train loss 451.9841 (mse_score: 451.9841)\n",
      "15:49    Epoch 40: train loss 451.9717 (mse_score: 451.9717)\n",
      "15:53    Epoch 45: train loss 451.9781 (mse_score: 451.9781)\n",
      "15:57    Epoch 50: train loss 451.9521 (mse_score: 451.9521)\n",
      "15:57  Finished training\n",
      "15:57  Training estimator 10 / 10 in ensemble\n",
      "15:57  Starting training\n",
      "15:57    Method:                 sally\n",
      "15:57    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_9.npy\n",
      "15:57                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_9.npy\n",
      "15:57    Features:               [26]\n",
      "15:57    Method:                 sally\n",
      "15:57    Hidden layers:          (100, 100, 100, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:57    Activation function:    tanh\n",
      "15:57    Batch size:             128\n",
      "15:57    Trainer:                amsgrad\n",
      "15:57    Epochs:                 50\n",
      "15:57    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "15:57    Validation split:       None\n",
      "15:57    Early stopping:         True\n",
      "15:57    Scale inputs:           True\n",
      "15:57    Regularization:         None\n",
      "15:57  Loading training data\n",
      "15:57  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:57  Rescaling inputs\n",
      "15:57  Only using 1 of 27 observables\n",
      "15:57  Creating model for method sally\n",
      "15:57  Training model\n",
      "16:00    Epoch 5: train loss 507.3529 (mse_score: 507.3529)\n",
      "16:03    Epoch 10: train loss 507.0352 (mse_score: 507.0352)\n",
      "16:07    Epoch 15: train loss 506.7964 (mse_score: 506.7964)\n",
      "16:11    Epoch 20: train loss 506.6972 (mse_score: 506.6972)\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'resurrection',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[26] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
