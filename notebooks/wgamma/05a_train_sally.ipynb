{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Marco Farina, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=n_estimators, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=False)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.calculate_expectation(\n",
    "        x_filename=sample_dir + 'validation{}/x_validation.npy'.format(cut_label)\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:14  Training 5 estimators in ensemble\n",
      "15:14  Training estimator 1 / 5 in ensemble\n",
      "15:14  Starting training\n",
      "15:14    Method:                 sally\n",
      "15:14    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "15:14                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "15:14    Features:               all\n",
      "15:14    Method:                 sally\n",
      "15:14    Hidden layers:          (100, 100)\n",
      "15:14    Activation function:    tanh\n",
      "15:14    Batch size:             128\n",
      "15:14    Trainer:                amsgrad\n",
      "15:14    Epochs:                 50\n",
      "15:14    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:14    Validation split:       None\n",
      "15:14    Early stopping:         True\n",
      "15:14    Scale inputs:           True\n",
      "15:14    Shuffle labels          False\n",
      "15:14    Regularization:         None\n",
      "15:14  Loading training data\n",
      "15:14  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:14  Rescaling inputs\n",
      "15:14  Creating model for method sally\n",
      "15:14  Training model\n",
      "15:16    Epoch 5: train loss 3.9530 (mse_score: 3.9530)\n",
      "15:17    Epoch 10: train loss 3.7116 (mse_score: 3.7116)\n",
      "15:19    Epoch 15: train loss 3.5093 (mse_score: 3.5093)\n",
      "15:38    Epoch 20: train loss 3.3862 (mse_score: 3.3862)\n",
      "15:39    Epoch 25: train loss 3.2871 (mse_score: 3.2871)\n",
      "15:41    Epoch 30: train loss 3.2073 (mse_score: 3.2073)\n",
      "15:42    Epoch 35: train loss 3.1457 (mse_score: 3.1457)\n",
      "15:44    Epoch 40: train loss 3.0929 (mse_score: 3.0929)\n",
      "15:51    Epoch 45: train loss 3.0548 (mse_score: 3.0548)\n",
      "15:53    Epoch 50: train loss 3.0212 (mse_score: 3.0212)\n",
      "15:53  Finished training\n",
      "15:53  Training estimator 2 / 5 in ensemble\n",
      "15:53  Starting training\n",
      "15:53    Method:                 sally\n",
      "15:53    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "15:53                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "15:53    Features:               all\n",
      "15:53    Method:                 sally\n",
      "15:53    Hidden layers:          (100, 100)\n",
      "15:53    Activation function:    tanh\n",
      "15:53    Batch size:             128\n",
      "15:53    Trainer:                amsgrad\n",
      "15:53    Epochs:                 50\n",
      "15:53    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:53    Validation split:       None\n",
      "15:53    Early stopping:         True\n",
      "15:53    Scale inputs:           True\n",
      "15:53    Shuffle labels          False\n",
      "15:53    Regularization:         None\n",
      "15:53  Loading training data\n",
      "15:53  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:53  Rescaling inputs\n",
      "15:53  Creating model for method sally\n",
      "15:53  Training model\n",
      "15:54    Epoch 5: train loss 4.4108 (mse_score: 4.4108)\n",
      "15:56    Epoch 10: train loss 4.1493 (mse_score: 4.1493)\n",
      "15:57    Epoch 15: train loss 3.9504 (mse_score: 3.9504)\n",
      "15:59    Epoch 20: train loss 3.8105 (mse_score: 3.8105)\n",
      "16:00    Epoch 25: train loss 3.6788 (mse_score: 3.6788)\n",
      "16:02    Epoch 30: train loss 3.5714 (mse_score: 3.5714)\n",
      "16:03    Epoch 35: train loss 3.4958 (mse_score: 3.4958)\n",
      "16:05    Epoch 40: train loss 3.4342 (mse_score: 3.4342)\n",
      "16:06    Epoch 45: train loss 3.3819 (mse_score: 3.3819)\n",
      "16:07    Epoch 50: train loss 3.3435 (mse_score: 3.3435)\n",
      "16:07  Finished training\n",
      "16:07  Training estimator 3 / 5 in ensemble\n",
      "16:07  Starting training\n",
      "16:07    Method:                 sally\n",
      "16:07    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "16:07                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "16:07    Features:               all\n",
      "16:07    Method:                 sally\n",
      "16:07    Hidden layers:          (100, 100)\n",
      "16:07    Activation function:    tanh\n",
      "16:07    Batch size:             128\n",
      "16:07    Trainer:                amsgrad\n",
      "16:07    Epochs:                 50\n",
      "16:07    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:07    Validation split:       None\n",
      "16:07    Early stopping:         True\n",
      "16:07    Scale inputs:           True\n",
      "16:07    Shuffle labels          False\n",
      "16:07    Regularization:         None\n",
      "16:07  Loading training data\n",
      "16:07  Found 1000000 samples with 2 parameters and 27 observables\n",
      "16:07  Rescaling inputs\n",
      "16:08  Creating model for method sally\n",
      "16:08  Training model\n",
      "16:09    Epoch 5: train loss 3.6707 (mse_score: 3.6707)\n",
      "16:11    Epoch 10: train loss 3.4859 (mse_score: 3.4859)\n",
      "16:12    Epoch 15: train loss 3.3339 (mse_score: 3.3339)\n",
      "16:14    Epoch 20: train loss 3.1964 (mse_score: 3.1964)\n",
      "16:15    Epoch 25: train loss 3.1050 (mse_score: 3.1050)\n",
      "16:17    Epoch 30: train loss 3.0203 (mse_score: 3.0203)\n",
      "16:18    Epoch 35: train loss 2.9580 (mse_score: 2.9580)\n",
      "16:20    Epoch 40: train loss 2.9125 (mse_score: 2.9125)\n",
      "16:21    Epoch 45: train loss 2.8725 (mse_score: 2.8725)\n",
      "16:22    Epoch 50: train loss 2.8418 (mse_score: 2.8418)\n",
      "16:22  Finished training\n",
      "16:22  Training estimator 4 / 5 in ensemble\n",
      "16:22  Starting training\n",
      "16:22    Method:                 sally\n",
      "16:22    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "16:22                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "16:22    Features:               all\n",
      "16:22    Method:                 sally\n",
      "16:22    Hidden layers:          (100, 100)\n",
      "16:22    Activation function:    tanh\n",
      "16:22    Batch size:             128\n",
      "16:22    Trainer:                amsgrad\n",
      "16:22    Epochs:                 50\n",
      "16:22    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:22    Validation split:       None\n",
      "16:22    Early stopping:         True\n",
      "16:22    Scale inputs:           True\n",
      "16:22    Shuffle labels          False\n",
      "16:22    Regularization:         None\n",
      "16:22  Loading training data\n",
      "16:22  Found 1000000 samples with 2 parameters and 27 observables\n",
      "16:22  Rescaling inputs\n",
      "16:23  Creating model for method sally\n",
      "16:23  Training model\n",
      "16:24    Epoch 5: train loss 3.8012 (mse_score: 3.8012)\n",
      "16:26    Epoch 10: train loss 3.6163 (mse_score: 3.6163)\n",
      "16:27    Epoch 15: train loss 3.4843 (mse_score: 3.4843)\n",
      "16:29    Epoch 20: train loss 3.3854 (mse_score: 3.3854)\n",
      "16:30    Epoch 25: train loss 3.3014 (mse_score: 3.3014)\n",
      "16:31    Epoch 30: train loss 3.2304 (mse_score: 3.2304)\n",
      "16:33    Epoch 35: train loss 3.1767 (mse_score: 3.1767)\n",
      "16:34    Epoch 40: train loss 3.1341 (mse_score: 3.1341)\n",
      "16:36    Epoch 45: train loss 3.0964 (mse_score: 3.0964)\n",
      "16:38    Epoch 50: train loss 3.0645 (mse_score: 3.0645)\n",
      "16:38  Finished training\n",
      "16:38  Training estimator 5 / 5 in ensemble\n",
      "16:38  Starting training\n",
      "16:38    Method:                 sally\n",
      "16:38    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "16:38                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "16:38    Features:               all\n",
      "16:38    Method:                 sally\n",
      "16:38    Hidden layers:          (100, 100)\n",
      "16:38    Activation function:    tanh\n",
      "16:38    Batch size:             128\n",
      "16:38    Trainer:                amsgrad\n",
      "16:38    Epochs:                 50\n",
      "16:38    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:38    Validation split:       None\n",
      "16:38    Early stopping:         True\n",
      "16:38    Scale inputs:           True\n",
      "16:38    Shuffle labels          False\n",
      "16:38    Regularization:         None\n",
      "16:38  Loading training data\n",
      "16:38  Found 1000000 samples with 2 parameters and 27 observables\n",
      "16:38  Rescaling inputs\n",
      "16:38  Creating model for method sally\n",
      "16:38  Training model\n",
      "16:40    Epoch 5: train loss 3.3603 (mse_score: 3.3603)\n",
      "16:43    Epoch 10: train loss 3.1718 (mse_score: 3.1718)\n",
      "16:47    Epoch 15: train loss 3.0449 (mse_score: 3.0449)\n",
      "16:50    Epoch 20: train loss 2.9412 (mse_score: 2.9412)\n",
      "16:53    Epoch 25: train loss 2.8548 (mse_score: 2.8548)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:57    Epoch 30: train loss 2.7942 (mse_score: 2.7942)\n",
      "17:03    Epoch 35: train loss 2.7452 (mse_score: 2.7452)\n",
      "17:04    Epoch 40: train loss 2.7072 (mse_score: 2.7072)\n",
      "17:06    Epoch 45: train loss 2.6765 (mse_score: 2.6765)\n",
      "17:08    Epoch 50: train loss 2.6536 (mse_score: 2.6536)\n",
      "17:08  Finished training\n",
      "17:08  Calculating expectation for 5 estimators in ensemble\n",
      "17:08  Starting evaluation for estimator 1 / 5 in ensemble\n",
      "17:08  Starting evaluation for estimator 2 / 5 in ensemble\n",
      "17:08  Starting evaluation for estimator 3 / 5 in ensemble\n",
      "17:08  Starting evaluation for estimator 4 / 5 in ensemble\n",
      "17:08  Starting evaluation for estimator 5 / 5 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'all',\n",
    "    use_tight_cuts=False,\n",
    "    initial_lr=0.001,\n",
    "    n_hidden=(100,100,)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:08  Training 5 estimators in ensemble\n",
      "17:08  Training estimator 1 / 5 in ensemble\n",
      "17:08  Starting training\n",
      "17:08    Method:                 sally\n",
      "17:08    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "17:08                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "17:08    Features:               all\n",
      "17:08    Method:                 sally\n",
      "17:08    Hidden layers:          (100, 100)\n",
      "17:08    Activation function:    tanh\n",
      "17:08    Batch size:             128\n",
      "17:08    Trainer:                amsgrad\n",
      "17:08    Epochs:                 50\n",
      "17:08    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "17:08    Validation split:       None\n",
      "17:08    Early stopping:         True\n",
      "17:08    Scale inputs:           True\n",
      "17:08    Shuffle labels          False\n",
      "17:08    Regularization:         None\n",
      "17:08  Loading training data\n",
      "17:08  Found 1000000 samples with 2 parameters and 27 observables\n",
      "17:08  Rescaling inputs\n",
      "17:08  Creating model for method sally\n",
      "17:08  Training model\n",
      "17:10    Epoch 5: train loss 407.5110 (mse_score: 407.5110)\n",
      "17:12    Epoch 10: train loss 397.9935 (mse_score: 397.9935)\n",
      "17:14    Epoch 15: train loss 391.9205 (mse_score: 391.9205)\n",
      "17:15    Epoch 20: train loss 387.4767 (mse_score: 387.4767)\n",
      "17:17    Epoch 25: train loss 384.0162 (mse_score: 384.0162)\n",
      "17:19    Epoch 30: train loss 381.4017 (mse_score: 381.4017)\n",
      "17:20    Epoch 35: train loss 379.5331 (mse_score: 379.5331)\n",
      "17:22    Epoch 40: train loss 378.0503 (mse_score: 378.0503)\n",
      "17:23    Epoch 45: train loss 376.8940 (mse_score: 376.8940)\n",
      "17:25    Epoch 50: train loss 375.9752 (mse_score: 375.9752)\n",
      "17:25  Finished training\n",
      "17:25  Training estimator 2 / 5 in ensemble\n",
      "17:25  Starting training\n",
      "17:25    Method:                 sally\n",
      "17:25    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "17:25                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "17:25    Features:               all\n",
      "17:25    Method:                 sally\n",
      "17:25    Hidden layers:          (100, 100)\n",
      "17:25    Activation function:    tanh\n",
      "17:25    Batch size:             128\n",
      "17:25    Trainer:                amsgrad\n",
      "17:25    Epochs:                 50\n",
      "17:25    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "17:25    Validation split:       None\n",
      "17:25    Early stopping:         True\n",
      "17:25    Scale inputs:           True\n",
      "17:25    Shuffle labels          False\n",
      "17:25    Regularization:         None\n",
      "17:25  Loading training data\n",
      "17:25  Found 1000000 samples with 2 parameters and 27 observables\n",
      "17:25  Rescaling inputs\n",
      "17:25  Creating model for method sally\n",
      "17:25  Training model\n",
      "17:27    Epoch 5: train loss 366.0659 (mse_score: 366.0659)\n",
      "17:28    Epoch 10: train loss 358.4461 (mse_score: 358.4461)\n",
      "17:30    Epoch 15: train loss 352.9987 (mse_score: 352.9987)\n",
      "17:32    Epoch 20: train loss 347.9644 (mse_score: 347.9644)\n",
      "17:34    Epoch 25: train loss 344.7923 (mse_score: 344.7923)\n",
      "17:36    Epoch 30: train loss 342.3777 (mse_score: 342.3777)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b52ddd66eb58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0muse_tight_cuts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minitial_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-13-1ed76feb0721>\u001b[0m in \u001b[0;36mtrain_ensemble\u001b[0;34m(filename, use_tight_cuts, n_estimators, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mx_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train_local{}/x_train_{}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mt_xz0_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train_local{}/t_xz_train_{}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/projects/madminer/madminer/madminer/ml.py\u001b[0m in \u001b[0;36mtrain_all\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training estimator %s / %s in ensemble\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs_this_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta0_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/projects/madminer/madminer/madminer/ml.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, method, x_filename, y_filename, theta0_filename, theta1_filename, r_xz_filename, t_xz0_filename, t_xz1_filename, features, nde_type, n_hidden, activation, maf_n_mades, maf_batch_norm, maf_batch_norm_alpha, maf_mog_n_components, alpha, trainer, n_epochs, batch_size, initial_lr, final_lr, nesterov_momentum, validation_split, early_stopping, scale_inputs, shuffle_labels, grad_x_regularization)\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0mnesterov_momentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnesterov_momentum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"all\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"some\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m                 \u001b[0mgrad_x_regularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_x_regularization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m             )\n\u001b[1;32m    503\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"nde\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scandal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/projects/madminer/madminer/madminer/utils/ml/score_trainer.py\u001b[0m in \u001b[0;36mtrain_local_score_model\u001b[0;34m(model, loss_functions, xs, t_xzs, loss_weights, loss_labels, batch_size, trainer, initial_learning_rate, final_learning_rate, nesterov_momentum, n_epochs, clip_gradient, run_on_gpu, double_precision, validation_split, early_stopping, early_stopping_patience, grad_x_regularization, learning_curve_folder, learning_curve_filename, verbose)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;31m# Clip gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclip_gradient\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mindividual_train_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mnorm_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \"\"\"\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, memo, prefix)\u001b[0m\n\u001b[1;32m    758\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m             \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'all_tight',\n",
    "    use_tight_cuts=True,\n",
    "    initial_lr=0.001,\n",
    "    n_hidden=(100,100,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal observable basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_obs = [0,1] + list(range(4,12)) + list(range(16,27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:03  \n",
      "22:03  ------------------------------------------------------------\n",
      "22:03  |                                                          |\n",
      "22:03  |  MadMiner v2018.11.13                                    |\n",
      "22:03  |                                                          |\n",
      "22:03  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "22:03  |                                                          |\n",
      "22:03  ------------------------------------------------------------\n",
      "22:03  \n",
      "22:03  Training 10 estimators in ensemble\n",
      "22:03  Training estimator 1 / 10 in ensemble\n",
      "22:03  Starting training\n",
      "22:03    Method:                 sally\n",
      "22:03    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "22:03                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "22:03    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "22:03    Method:                 sally\n",
      "22:03    Hidden layers:          (100, 100, 100, 100)\n",
      "22:03    Activation function:    tanh\n",
      "22:03    Batch size:             128\n",
      "22:03    Trainer:                amsgrad\n",
      "22:03    Epochs:                 50\n",
      "22:03    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "22:03    Validation split:       None\n",
      "22:03    Early stopping:         True\n",
      "22:03    Scale inputs:           True\n",
      "22:03    Shuffle labels          False\n",
      "22:03    Regularization:         None\n",
      "22:03  Loading training data\n",
      "22:03  Found 1000000 samples with 2 parameters and 27 observables\n",
      "22:03  Rescaling inputs\n",
      "22:03  Only using 21 of 27 observables\n",
      "22:03  Creating model for method sally\n",
      "22:03  Training model\n",
      "22:05    Epoch 5: train loss 4.6577 (mse_score: 4.6577)\n",
      "22:07    Epoch 10: train loss 4.5168 (mse_score: 4.5168)\n",
      "22:09    Epoch 15: train loss 4.3999 (mse_score: 4.3999)\n",
      "22:11    Epoch 20: train loss 4.3157 (mse_score: 4.3157)\n",
      "22:13    Epoch 25: train loss 4.2210 (mse_score: 4.2210)\n",
      "22:15    Epoch 30: train loss 4.1038 (mse_score: 4.1038)\n",
      "22:18    Epoch 35: train loss 4.0033 (mse_score: 4.0033)\n",
      "22:20    Epoch 40: train loss 3.9469 (mse_score: 3.9469)\n",
      "22:22    Epoch 45: train loss 3.9147 (mse_score: 3.9147)\n",
      "22:24    Epoch 50: train loss 3.8949 (mse_score: 3.8949)\n",
      "22:24  Finished training\n",
      "22:24  Training estimator 2 / 10 in ensemble\n",
      "22:24  Starting training\n",
      "22:24    Method:                 sally\n",
      "22:24    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "22:24                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "22:24    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "22:24    Method:                 sally\n",
      "22:24    Hidden layers:          (100, 100, 100, 100)\n",
      "22:24    Activation function:    tanh\n",
      "22:24    Batch size:             128\n",
      "22:24    Trainer:                amsgrad\n",
      "22:24    Epochs:                 50\n",
      "22:24    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "22:24    Validation split:       None\n",
      "22:24    Early stopping:         True\n",
      "22:24    Scale inputs:           True\n",
      "22:24    Shuffle labels          False\n",
      "22:24    Regularization:         None\n",
      "22:24  Loading training data\n",
      "22:24  Found 1000000 samples with 2 parameters and 27 observables\n",
      "22:24  Rescaling inputs\n",
      "22:24  Only using 21 of 27 observables\n",
      "22:24  Creating model for method sally\n",
      "22:24  Training model\n",
      "22:27    Epoch 5: train loss 4.8665 (mse_score: 4.8665)\n",
      "22:29    Epoch 10: train loss 4.7707 (mse_score: 4.7707)\n",
      "22:32    Epoch 15: train loss 4.6365 (mse_score: 4.6365)\n",
      "22:35    Epoch 20: train loss 4.5461 (mse_score: 4.5461)\n",
      "22:37    Epoch 25: train loss 4.4275 (mse_score: 4.4275)\n",
      "22:40    Epoch 30: train loss 4.3512 (mse_score: 4.3512)\n",
      "22:42    Epoch 35: train loss 4.2594 (mse_score: 4.2594)\n",
      "22:45    Epoch 40: train loss 4.2202 (mse_score: 4.2202)\n",
      "22:48    Epoch 45: train loss 4.2005 (mse_score: 4.2005)\n",
      "22:51    Epoch 50: train loss 4.1900 (mse_score: 4.1900)\n",
      "22:51  Finished training\n",
      "22:51  Training estimator 3 / 10 in ensemble\n",
      "22:51  Starting training\n",
      "22:51    Method:                 sally\n",
      "22:51    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "22:51                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "22:51    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "22:51    Method:                 sally\n",
      "22:51    Hidden layers:          (100, 100, 100, 100)\n",
      "22:51    Activation function:    tanh\n",
      "22:51    Batch size:             128\n",
      "22:51    Trainer:                amsgrad\n",
      "22:51    Epochs:                 50\n",
      "22:51    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "22:51    Validation split:       None\n",
      "22:51    Early stopping:         True\n",
      "22:51    Scale inputs:           True\n",
      "22:51    Shuffle labels          False\n",
      "22:51    Regularization:         None\n",
      "22:51  Loading training data\n",
      "22:51  Found 1000000 samples with 2 parameters and 27 observables\n",
      "22:51  Rescaling inputs\n",
      "22:51  Only using 21 of 27 observables\n",
      "22:51  Creating model for method sally\n",
      "22:51  Training model\n",
      "22:53    Epoch 5: train loss 4.0631 (mse_score: 4.0631)\n",
      "22:55    Epoch 10: train loss 3.9261 (mse_score: 3.9261)\n",
      "22:58    Epoch 15: train loss 3.8739 (mse_score: 3.8739)\n",
      "23:00    Epoch 20: train loss 3.8046 (mse_score: 3.8046)\n",
      "23:02    Epoch 25: train loss 3.6875 (mse_score: 3.6875)\n",
      "23:05    Epoch 30: train loss 3.6040 (mse_score: 3.6040)\n",
      "23:07    Epoch 35: train loss 3.5414 (mse_score: 3.5414)\n",
      "23:10    Epoch 40: train loss 3.5047 (mse_score: 3.5047)\n",
      "23:12    Epoch 45: train loss 3.4864 (mse_score: 3.4864)\n",
      "23:15    Epoch 50: train loss 3.4756 (mse_score: 3.4756)\n",
      "23:15  Finished training\n",
      "23:15  Training estimator 4 / 10 in ensemble\n",
      "23:15  Starting training\n",
      "23:15    Method:                 sally\n",
      "23:15    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "23:15                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "23:15    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "23:15    Method:                 sally\n",
      "23:15    Hidden layers:          (100, 100, 100, 100)\n",
      "23:15    Activation function:    tanh\n",
      "23:15    Batch size:             128\n",
      "23:15    Trainer:                amsgrad\n",
      "23:15    Epochs:                 50\n",
      "23:15    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "23:15    Validation split:       None\n",
      "23:15    Early stopping:         True\n",
      "23:15    Scale inputs:           True\n",
      "23:15    Shuffle labels          False\n",
      "23:15    Regularization:         None\n",
      "23:15  Loading training data\n",
      "23:15  Found 1000000 samples with 2 parameters and 27 observables\n",
      "23:15  Rescaling inputs\n",
      "23:15  Only using 21 of 27 observables\n",
      "23:15  Creating model for method sally\n",
      "23:15  Training model\n",
      "23:17    Epoch 5: train loss 4.1754 (mse_score: 4.1754)\n",
      "23:20    Epoch 10: train loss 4.0804 (mse_score: 4.0804)\n",
      "23:22    Epoch 15: train loss 3.9563 (mse_score: 3.9563)\n",
      "23:25    Epoch 20: train loss 3.8971 (mse_score: 3.8971)\n",
      "23:27    Epoch 25: train loss 3.8333 (mse_score: 3.8333)\n",
      "23:30    Epoch 30: train loss 3.7585 (mse_score: 3.7585)\n",
      "23:32    Epoch 35: train loss 3.7038 (mse_score: 3.7038)\n",
      "23:35    Epoch 40: train loss 3.6747 (mse_score: 3.6747)\n",
      "23:37    Epoch 45: train loss 3.6597 (mse_score: 3.6597)\n",
      "23:40    Epoch 50: train loss 3.6509 (mse_score: 3.6509)\n",
      "23:40  Finished training\n",
      "23:40  Training estimator 5 / 10 in ensemble\n",
      "23:40  Starting training\n",
      "23:40    Method:                 sally\n",
      "23:40    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "23:40                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:40    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "23:40    Method:                 sally\n",
      "23:40    Hidden layers:          (100, 100, 100, 100)\n",
      "23:40    Activation function:    tanh\n",
      "23:40    Batch size:             128\n",
      "23:40    Trainer:                amsgrad\n",
      "23:40    Epochs:                 50\n",
      "23:40    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "23:40    Validation split:       None\n",
      "23:40    Early stopping:         True\n",
      "23:40    Scale inputs:           True\n",
      "23:40    Shuffle labels          False\n",
      "23:40    Regularization:         None\n",
      "23:40  Loading training data\n",
      "23:40  Found 1000000 samples with 2 parameters and 27 observables\n",
      "23:40  Rescaling inputs\n",
      "23:40  Only using 21 of 27 observables\n",
      "23:40  Creating model for method sally\n",
      "23:40  Training model\n",
      "23:42    Epoch 5: train loss 3.8729 (mse_score: 3.8729)\n",
      "23:45    Epoch 10: train loss 3.7855 (mse_score: 3.7855)\n",
      "23:47    Epoch 15: train loss 3.7326 (mse_score: 3.7326)\n",
      "23:50    Epoch 20: train loss 3.6172 (mse_score: 3.6172)\n",
      "23:52    Epoch 25: train loss 3.5577 (mse_score: 3.5577)\n",
      "23:55    Epoch 30: train loss 3.4964 (mse_score: 3.4964)\n",
      "23:57    Epoch 35: train loss 3.4526 (mse_score: 3.4526)\n",
      "00:00    Epoch 40: train loss 3.4288 (mse_score: 3.4288)\n",
      "00:02    Epoch 45: train loss 3.4159 (mse_score: 3.4159)\n",
      "00:05    Epoch 50: train loss 3.4086 (mse_score: 3.4086)\n",
      "00:05  Finished training\n",
      "00:05  Training estimator 6 / 10 in ensemble\n",
      "00:05  Starting training\n",
      "00:05    Method:                 sally\n",
      "00:05    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "00:05                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "00:05    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "00:05    Method:                 sally\n",
      "00:05    Hidden layers:          (100, 100, 100, 100)\n",
      "00:05    Activation function:    tanh\n",
      "00:05    Batch size:             128\n",
      "00:05    Trainer:                amsgrad\n",
      "00:05    Epochs:                 50\n",
      "00:05    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "00:05    Validation split:       None\n",
      "00:05    Early stopping:         True\n",
      "00:05    Scale inputs:           True\n",
      "00:05    Shuffle labels          False\n",
      "00:05    Regularization:         None\n",
      "00:05  Loading training data\n",
      "00:05  Found 1000000 samples with 2 parameters and 27 observables\n",
      "00:05  Rescaling inputs\n",
      "00:05  Only using 21 of 27 observables\n",
      "00:05  Creating model for method sally\n",
      "00:05  Training model\n",
      "00:07    Epoch 5: train loss 4.5141 (mse_score: 4.5141)\n",
      "00:10    Epoch 10: train loss 4.4046 (mse_score: 4.4046)\n",
      "00:12    Epoch 15: train loss 4.3272 (mse_score: 4.3272)\n",
      "00:15    Epoch 20: train loss 4.2608 (mse_score: 4.2608)\n",
      "00:17    Epoch 25: train loss 4.2055 (mse_score: 4.2055)\n",
      "00:20    Epoch 30: train loss 4.1164 (mse_score: 4.1164)\n",
      "00:22    Epoch 35: train loss 4.0631 (mse_score: 4.0631)\n",
      "00:24    Epoch 40: train loss 4.0389 (mse_score: 4.0389)\n",
      "00:27    Epoch 45: train loss 4.0258 (mse_score: 4.0258)\n",
      "00:29    Epoch 50: train loss 4.0188 (mse_score: 4.0188)\n",
      "00:29  Finished training\n",
      "00:29  Training estimator 7 / 10 in ensemble\n",
      "00:29  Starting training\n",
      "00:29    Method:                 sally\n",
      "00:29    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "00:29                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "00:29    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "00:29    Method:                 sally\n",
      "00:29    Hidden layers:          (100, 100, 100, 100)\n",
      "00:29    Activation function:    tanh\n",
      "00:29    Batch size:             128\n",
      "00:29    Trainer:                amsgrad\n",
      "00:29    Epochs:                 50\n",
      "00:29    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "00:29    Validation split:       None\n",
      "00:29    Early stopping:         True\n",
      "00:29    Scale inputs:           True\n",
      "00:29    Shuffle labels          False\n",
      "00:29    Regularization:         None\n",
      "00:29  Loading training data\n",
      "00:29  Found 1000000 samples with 2 parameters and 27 observables\n",
      "00:29  Rescaling inputs\n",
      "00:29  Only using 21 of 27 observables\n",
      "00:29  Creating model for method sally\n",
      "00:29  Training model\n",
      "00:32    Epoch 5: train loss 7.8268 (mse_score: 7.8268)\n",
      "00:35    Epoch 10: train loss 7.6838 (mse_score: 7.6838)\n",
      "00:37    Epoch 15: train loss 7.5206 (mse_score: 7.5206)\n",
      "00:39    Epoch 20: train loss 7.4156 (mse_score: 7.4156)\n",
      "00:42    Epoch 25: train loss 7.2516 (mse_score: 7.2516)\n",
      "00:44    Epoch 30: train loss 7.1202 (mse_score: 7.1202)\n",
      "00:47    Epoch 35: train loss 7.0435 (mse_score: 7.0435)\n",
      "00:49    Epoch 40: train loss 6.9922 (mse_score: 6.9922)\n",
      "00:52    Epoch 45: train loss 6.9679 (mse_score: 6.9679)\n",
      "00:54    Epoch 50: train loss 6.9485 (mse_score: 6.9485)\n",
      "00:54  Finished training\n",
      "00:54  Training estimator 8 / 10 in ensemble\n",
      "00:54  Starting training\n",
      "00:54    Method:                 sally\n",
      "00:54    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "00:54                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "00:54    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "00:54    Method:                 sally\n",
      "00:54    Hidden layers:          (100, 100, 100, 100)\n",
      "00:54    Activation function:    tanh\n",
      "00:54    Batch size:             128\n",
      "00:54    Trainer:                amsgrad\n",
      "00:54    Epochs:                 50\n",
      "00:54    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "00:54    Validation split:       None\n",
      "00:54    Early stopping:         True\n",
      "00:54    Scale inputs:           True\n",
      "00:54    Shuffle labels          False\n",
      "00:54    Regularization:         None\n",
      "00:54  Loading training data\n",
      "00:54  Found 1000000 samples with 2 parameters and 27 observables\n",
      "00:54  Rescaling inputs\n",
      "00:54  Only using 21 of 27 observables\n",
      "00:54  Creating model for method sally\n",
      "00:54  Training model\n",
      "00:57    Epoch 5: train loss 4.0411 (mse_score: 4.0411)\n",
      "00:59    Epoch 10: train loss 3.8336 (mse_score: 3.8336)\n",
      "01:02    Epoch 15: train loss 3.7774 (mse_score: 3.7774)\n",
      "01:04    Epoch 20: train loss 3.6510 (mse_score: 3.6510)\n",
      "01:07    Epoch 25: train loss 3.5379 (mse_score: 3.5379)\n",
      "01:09    Epoch 30: train loss 3.4177 (mse_score: 3.4177)\n",
      "01:11    Epoch 35: train loss 3.3508 (mse_score: 3.3508)\n",
      "01:14    Epoch 40: train loss 3.3153 (mse_score: 3.3153)\n",
      "01:16    Epoch 45: train loss 3.2985 (mse_score: 3.2985)\n",
      "01:19    Epoch 50: train loss 3.2890 (mse_score: 3.2890)\n",
      "01:19  Finished training\n",
      "01:19  Training estimator 9 / 10 in ensemble\n",
      "01:19  Starting training\n",
      "01:19    Method:                 sally\n",
      "01:19    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "01:19                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "01:19    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "01:19    Method:                 sally\n",
      "01:19    Hidden layers:          (100, 100, 100, 100)\n",
      "01:19    Activation function:    tanh\n",
      "01:19    Batch size:             128\n",
      "01:19    Trainer:                amsgrad\n",
      "01:19    Epochs:                 50\n",
      "01:19    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "01:19    Validation split:       None\n",
      "01:19    Early stopping:         True\n",
      "01:19    Scale inputs:           True\n",
      "01:19    Shuffle labels          False\n",
      "01:19    Regularization:         None\n",
      "01:19  Loading training data\n",
      "01:19  Found 1000000 samples with 2 parameters and 27 observables\n",
      "01:19  Rescaling inputs\n",
      "01:19  Only using 21 of 27 observables\n",
      "01:19  Creating model for method sally\n",
      "01:19  Training model\n",
      "01:22    Epoch 5: train loss 4.2204 (mse_score: 4.2204)\n",
      "01:24    Epoch 10: train loss 4.0986 (mse_score: 4.0986)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:26    Epoch 15: train loss 3.9553 (mse_score: 3.9553)\n",
      "01:29    Epoch 20: train loss 3.8798 (mse_score: 3.8798)\n",
      "01:31    Epoch 25: train loss 3.7901 (mse_score: 3.7901)\n",
      "01:34    Epoch 30: train loss 3.6920 (mse_score: 3.6920)\n",
      "01:36    Epoch 35: train loss 3.6289 (mse_score: 3.6289)\n",
      "01:39    Epoch 40: train loss 3.5943 (mse_score: 3.5943)\n",
      "01:41    Epoch 45: train loss 3.5787 (mse_score: 3.5787)\n",
      "01:44    Epoch 50: train loss 3.5697 (mse_score: 3.5697)\n",
      "01:44  Finished training\n",
      "01:44  Training estimator 10 / 10 in ensemble\n",
      "01:44  Starting training\n",
      "01:44    Method:                 sally\n",
      "01:44    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "01:44                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "01:44    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "01:44    Method:                 sally\n",
      "01:44    Hidden layers:          (100, 100, 100, 100)\n",
      "01:44    Activation function:    tanh\n",
      "01:44    Batch size:             128\n",
      "01:44    Trainer:                amsgrad\n",
      "01:44    Epochs:                 50\n",
      "01:44    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "01:44    Validation split:       None\n",
      "01:44    Early stopping:         True\n",
      "01:44    Scale inputs:           True\n",
      "01:44    Shuffle labels          False\n",
      "01:44    Regularization:         None\n",
      "01:44  Loading training data\n",
      "01:44  Found 1000000 samples with 2 parameters and 27 observables\n",
      "01:44  Rescaling inputs\n",
      "01:44  Only using 21 of 27 observables\n",
      "01:44  Creating model for method sally\n",
      "01:44  Training model\n",
      "01:46    Epoch 5: train loss 7.2942 (mse_score: 7.2942)\n",
      "01:49    Epoch 10: train loss 7.2715 (mse_score: 7.2715)\n",
      "01:51    Epoch 15: train loss 7.2240 (mse_score: 7.2240)\n",
      "01:54    Epoch 20: train loss 7.1745 (mse_score: 7.1745)\n",
      "01:56    Epoch 25: train loss 7.0954 (mse_score: 7.0954)\n",
      "01:59    Epoch 30: train loss 7.0343 (mse_score: 7.0343)\n",
      "02:01    Epoch 35: train loss 6.9956 (mse_score: 6.9956)\n",
      "02:04    Epoch 40: train loss 6.9733 (mse_score: 6.9733)\n",
      "02:06    Epoch 45: train loss 6.9602 (mse_score: 6.9602)\n",
      "02:09    Epoch 50: train loss 6.9525 (mse_score: 6.9525)\n",
      "02:09  Finished training\n",
      "02:09  Calculating expectation for 10 estimators in ensemble\n",
      "02:09  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "02:09  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "02:09  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "02:09  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "02:09  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "02:10  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "02:10  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "02:10  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "02:10  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "02:10  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'minimal',\n",
    "    use_tight_cuts=False,\n",
    "    features=[min_obs for _ in range(n_estimators)],\n",
    "    initial_lr=0.001,\n",
    "    n_hidden=(100,100,)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:11  Training 10 estimators in ensemble\n",
      "02:11  Training estimator 1 / 10 in ensemble\n",
      "02:11  Starting training\n",
      "02:11    Method:                 sally\n",
      "02:11    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "02:11                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "02:11    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "02:11    Method:                 sally\n",
      "02:11    Hidden layers:          (100, 100, 100, 100)\n",
      "02:11    Activation function:    tanh\n",
      "02:11    Batch size:             128\n",
      "02:11    Trainer:                amsgrad\n",
      "02:11    Epochs:                 50\n",
      "02:11    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "02:11    Validation split:       None\n",
      "02:11    Early stopping:         True\n",
      "02:11    Scale inputs:           True\n",
      "02:11    Shuffle labels          False\n",
      "02:11    Regularization:         None\n",
      "02:11  Loading training data\n",
      "02:11  Found 1000000 samples with 2 parameters and 27 observables\n",
      "02:11  Rescaling inputs\n",
      "02:11  Only using 21 of 27 observables\n",
      "02:11  Creating model for method sally\n",
      "02:11  Training model\n",
      "02:14    Epoch 5: train loss 432.8362 (mse_score: 432.8362)\n",
      "02:16    Epoch 10: train loss 425.5745 (mse_score: 425.5745)\n",
      "02:19    Epoch 15: train loss 419.5052 (mse_score: 419.5052)\n",
      "02:22    Epoch 20: train loss 414.3927 (mse_score: 414.3927)\n",
      "02:25    Epoch 25: train loss 410.4976 (mse_score: 410.4976)\n",
      "02:27    Epoch 30: train loss 407.1200 (mse_score: 407.1200)\n",
      "02:30    Epoch 35: train loss 405.2789 (mse_score: 405.2789)\n",
      "02:33    Epoch 40: train loss 404.4702 (mse_score: 404.4702)\n",
      "02:35    Epoch 45: train loss 404.1107 (mse_score: 404.1107)\n",
      "02:38    Epoch 50: train loss 403.8606 (mse_score: 403.8606)\n",
      "02:38  Finished training\n",
      "02:38  Training estimator 2 / 10 in ensemble\n",
      "02:38  Starting training\n",
      "02:38    Method:                 sally\n",
      "02:38    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "02:38                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "02:38    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "02:38    Method:                 sally\n",
      "02:38    Hidden layers:          (100, 100, 100, 100)\n",
      "02:38    Activation function:    tanh\n",
      "02:38    Batch size:             128\n",
      "02:38    Trainer:                amsgrad\n",
      "02:38    Epochs:                 50\n",
      "02:38    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "02:38    Validation split:       None\n",
      "02:38    Early stopping:         True\n",
      "02:38    Scale inputs:           True\n",
      "02:38    Shuffle labels          False\n",
      "02:38    Regularization:         None\n",
      "02:38  Loading training data\n",
      "02:38  Found 1000000 samples with 2 parameters and 27 observables\n",
      "02:38  Rescaling inputs\n",
      "02:38  Only using 21 of 27 observables\n",
      "02:38  Creating model for method sally\n",
      "02:38  Training model\n",
      "02:41    Epoch 5: train loss 389.6176 (mse_score: 389.6176)\n",
      "02:43    Epoch 10: train loss 382.8253 (mse_score: 382.8253)\n",
      "02:46    Epoch 15: train loss 377.4740 (mse_score: 377.4740)\n",
      "02:48    Epoch 20: train loss 374.1825 (mse_score: 374.1825)\n",
      "02:51    Epoch 25: train loss 370.3904 (mse_score: 370.3904)\n",
      "02:53    Epoch 30: train loss 366.5221 (mse_score: 366.5221)\n",
      "02:56    Epoch 35: train loss 364.8479 (mse_score: 364.8479)\n",
      "02:58    Epoch 40: train loss 363.7749 (mse_score: 363.7749)\n",
      "03:01    Epoch 45: train loss 363.3099 (mse_score: 363.3099)\n",
      "03:04    Epoch 50: train loss 363.0700 (mse_score: 363.0700)\n",
      "03:04  Finished training\n",
      "03:04  Training estimator 3 / 10 in ensemble\n",
      "03:04  Starting training\n",
      "03:04    Method:                 sally\n",
      "03:04    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "03:04                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "03:04    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "03:04    Method:                 sally\n",
      "03:04    Hidden layers:          (100, 100, 100, 100)\n",
      "03:04    Activation function:    tanh\n",
      "03:04    Batch size:             128\n",
      "03:04    Trainer:                amsgrad\n",
      "03:04    Epochs:                 50\n",
      "03:04    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "03:04    Validation split:       None\n",
      "03:04    Early stopping:         True\n",
      "03:04    Scale inputs:           True\n",
      "03:04    Shuffle labels          False\n",
      "03:04    Regularization:         None\n",
      "03:04  Loading training data\n",
      "03:04  Found 1000000 samples with 2 parameters and 27 observables\n",
      "03:04  Rescaling inputs\n",
      "03:04  Only using 21 of 27 observables\n",
      "03:04  Creating model for method sally\n",
      "03:04  Training model\n",
      "03:06    Epoch 5: train loss 455.4485 (mse_score: 455.4485)\n",
      "03:09    Epoch 10: train loss 446.7343 (mse_score: 446.7343)\n",
      "03:11    Epoch 15: train loss 439.5474 (mse_score: 439.5474)\n",
      "03:14    Epoch 20: train loss 432.6682 (mse_score: 432.6682)\n",
      "03:16    Epoch 25: train loss 425.6648 (mse_score: 425.6648)\n",
      "03:19    Epoch 30: train loss 419.5236 (mse_score: 419.5236)\n",
      "03:21    Epoch 35: train loss 415.8800 (mse_score: 415.8800)\n",
      "03:24    Epoch 40: train loss 413.7503 (mse_score: 413.7503)\n",
      "03:26    Epoch 45: train loss 412.6480 (mse_score: 412.6480)\n",
      "03:29    Epoch 50: train loss 412.0398 (mse_score: 412.0398)\n",
      "03:29  Finished training\n",
      "03:29  Training estimator 4 / 10 in ensemble\n",
      "03:29  Starting training\n",
      "03:29    Method:                 sally\n",
      "03:29    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_3.npy\n",
      "03:29                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_3.npy\n",
      "03:29    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "03:29    Method:                 sally\n",
      "03:29    Hidden layers:          (100, 100, 100, 100)\n",
      "03:29    Activation function:    tanh\n",
      "03:29    Batch size:             128\n",
      "03:29    Trainer:                amsgrad\n",
      "03:29    Epochs:                 50\n",
      "03:29    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "03:29    Validation split:       None\n",
      "03:29    Early stopping:         True\n",
      "03:29    Scale inputs:           True\n",
      "03:29    Shuffle labels          False\n",
      "03:29    Regularization:         None\n",
      "03:29  Loading training data\n",
      "03:29  Found 1000000 samples with 2 parameters and 27 observables\n",
      "03:29  Rescaling inputs\n",
      "03:29  Only using 21 of 27 observables\n",
      "03:29  Creating model for method sally\n",
      "03:29  Training model\n",
      "03:31    Epoch 5: train loss 402.2154 (mse_score: 402.2154)\n",
      "03:34    Epoch 10: train loss 395.8856 (mse_score: 395.8856)\n",
      "03:36    Epoch 15: train loss 390.7708 (mse_score: 390.7708)\n",
      "03:39    Epoch 20: train loss 385.1878 (mse_score: 385.1878)\n",
      "03:41    Epoch 25: train loss 381.1567 (mse_score: 381.1567)\n",
      "03:44    Epoch 30: train loss 377.5245 (mse_score: 377.5245)\n",
      "03:47    Epoch 35: train loss 375.5759 (mse_score: 375.5759)\n",
      "03:49    Epoch 40: train loss 374.4858 (mse_score: 374.4858)\n",
      "03:52    Epoch 45: train loss 373.9443 (mse_score: 373.9443)\n",
      "03:54    Epoch 50: train loss 373.6667 (mse_score: 373.6667)\n",
      "03:54  Finished training\n",
      "03:54  Training estimator 5 / 10 in ensemble\n",
      "03:54  Starting training\n",
      "03:54    Method:                 sally\n",
      "03:54    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_4.npy\n",
      "03:54                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_4.npy\n",
      "03:54    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "03:54    Method:                 sally\n",
      "03:54    Hidden layers:          (100, 100, 100, 100)\n",
      "03:54    Activation function:    tanh\n",
      "03:54    Batch size:             128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:54    Trainer:                amsgrad\n",
      "03:54    Epochs:                 50\n",
      "03:54    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "03:54    Validation split:       None\n",
      "03:54    Early stopping:         True\n",
      "03:54    Scale inputs:           True\n",
      "03:54    Shuffle labels          False\n",
      "03:54    Regularization:         None\n",
      "03:54  Loading training data\n",
      "03:54  Found 1000000 samples with 2 parameters and 27 observables\n",
      "03:54  Rescaling inputs\n",
      "03:54  Only using 21 of 27 observables\n",
      "03:54  Creating model for method sally\n",
      "03:54  Training model\n",
      "03:57    Epoch 5: train loss 374.5579 (mse_score: 374.5579)\n",
      "03:59    Epoch 10: train loss 368.1496 (mse_score: 368.1496)\n",
      "04:02    Epoch 15: train loss 362.3705 (mse_score: 362.3705)\n",
      "04:05    Epoch 20: train loss 356.7275 (mse_score: 356.7275)\n",
      "04:07    Epoch 25: train loss 351.1099 (mse_score: 351.1099)\n",
      "04:10    Epoch 30: train loss 345.7920 (mse_score: 345.7920)\n",
      "04:12    Epoch 35: train loss 341.7555 (mse_score: 341.7555)\n",
      "04:15    Epoch 40: train loss 339.4545 (mse_score: 339.4545)\n",
      "04:17    Epoch 45: train loss 338.2420 (mse_score: 338.2420)\n",
      "04:20    Epoch 50: train loss 337.5653 (mse_score: 337.5653)\n",
      "04:20  Finished training\n",
      "04:20  Training estimator 6 / 10 in ensemble\n",
      "04:20  Starting training\n",
      "04:20    Method:                 sally\n",
      "04:20    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_5.npy\n",
      "04:20                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_5.npy\n",
      "04:20    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "04:20    Method:                 sally\n",
      "04:20    Hidden layers:          (100, 100, 100, 100)\n",
      "04:20    Activation function:    tanh\n",
      "04:20    Batch size:             128\n",
      "04:20    Trainer:                amsgrad\n",
      "04:20    Epochs:                 50\n",
      "04:20    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "04:20    Validation split:       None\n",
      "04:20    Early stopping:         True\n",
      "04:20    Scale inputs:           True\n",
      "04:20    Shuffle labels          False\n",
      "04:20    Regularization:         None\n",
      "04:20  Loading training data\n",
      "04:20  Found 1000000 samples with 2 parameters and 27 observables\n",
      "04:20  Rescaling inputs\n",
      "04:20  Only using 21 of 27 observables\n",
      "04:20  Creating model for method sally\n",
      "04:20  Training model\n",
      "04:22    Epoch 5: train loss 423.2420 (mse_score: 423.2420)\n",
      "04:25    Epoch 10: train loss 417.2247 (mse_score: 417.2247)\n",
      "04:27    Epoch 15: train loss 409.6575 (mse_score: 409.6575)\n",
      "04:30    Epoch 20: train loss 402.8814 (mse_score: 402.8814)\n",
      "04:32    Epoch 25: train loss 398.4294 (mse_score: 398.4294)\n",
      "04:35    Epoch 30: train loss 394.4551 (mse_score: 394.4551)\n",
      "04:37    Epoch 35: train loss 392.0481 (mse_score: 392.0481)\n",
      "04:40    Epoch 40: train loss 390.9757 (mse_score: 390.9757)\n",
      "04:42    Epoch 45: train loss 390.3703 (mse_score: 390.3703)\n",
      "04:45    Epoch 50: train loss 390.0839 (mse_score: 390.0839)\n",
      "04:45  Finished training\n",
      "04:45  Training estimator 7 / 10 in ensemble\n",
      "04:45  Starting training\n",
      "04:45    Method:                 sally\n",
      "04:45    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_6.npy\n",
      "04:45                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_6.npy\n",
      "04:45    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "04:45    Method:                 sally\n",
      "04:45    Hidden layers:          (100, 100, 100, 100)\n",
      "04:45    Activation function:    tanh\n",
      "04:45    Batch size:             128\n",
      "04:45    Trainer:                amsgrad\n",
      "04:45    Epochs:                 50\n",
      "04:45    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "04:45    Validation split:       None\n",
      "04:45    Early stopping:         True\n",
      "04:45    Scale inputs:           True\n",
      "04:45    Shuffle labels          False\n",
      "04:45    Regularization:         None\n",
      "04:45  Loading training data\n",
      "04:45  Found 1000000 samples with 2 parameters and 27 observables\n",
      "04:45  Rescaling inputs\n",
      "04:45  Only using 21 of 27 observables\n",
      "04:45  Creating model for method sally\n",
      "04:45  Training model\n",
      "04:48    Epoch 5: train loss 424.3122 (mse_score: 424.3122)\n",
      "04:50    Epoch 10: train loss 417.6060 (mse_score: 417.6060)\n",
      "04:53    Epoch 15: train loss 410.6281 (mse_score: 410.6281)\n",
      "04:55    Epoch 20: train loss 403.6621 (mse_score: 403.6621)\n",
      "04:58    Epoch 25: train loss 397.1273 (mse_score: 397.1273)\n",
      "05:00    Epoch 30: train loss 390.4911 (mse_score: 390.4911)\n",
      "05:03    Epoch 35: train loss 385.3106 (mse_score: 385.3106)\n",
      "05:05    Epoch 40: train loss 382.4805 (mse_score: 382.4805)\n",
      "05:08    Epoch 45: train loss 380.9013 (mse_score: 380.9013)\n",
      "05:10    Epoch 50: train loss 380.0957 (mse_score: 380.0957)\n",
      "05:10  Finished training\n",
      "05:10  Training estimator 8 / 10 in ensemble\n",
      "05:10  Starting training\n",
      "05:10    Method:                 sally\n",
      "05:10    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_7.npy\n",
      "05:10                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_7.npy\n",
      "05:10    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "05:10    Method:                 sally\n",
      "05:10    Hidden layers:          (100, 100, 100, 100)\n",
      "05:10    Activation function:    tanh\n",
      "05:10    Batch size:             128\n",
      "05:10    Trainer:                amsgrad\n",
      "05:10    Epochs:                 50\n",
      "05:10    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "05:10    Validation split:       None\n",
      "05:10    Early stopping:         True\n",
      "05:10    Scale inputs:           True\n",
      "05:10    Shuffle labels          False\n",
      "05:10    Regularization:         None\n",
      "05:10  Loading training data\n",
      "05:10  Found 1000000 samples with 2 parameters and 27 observables\n",
      "05:10  Rescaling inputs\n",
      "05:10  Only using 21 of 27 observables\n",
      "05:10  Creating model for method sally\n",
      "05:10  Training model\n",
      "05:13    Epoch 5: train loss 411.9814 (mse_score: 411.9814)\n",
      "05:16    Epoch 10: train loss 404.8700 (mse_score: 404.8700)\n",
      "05:18    Epoch 15: train loss 397.5437 (mse_score: 397.5437)\n",
      "05:21    Epoch 20: train loss 393.3843 (mse_score: 393.3843)\n",
      "05:23    Epoch 25: train loss 389.0558 (mse_score: 389.0558)\n",
      "05:26    Epoch 30: train loss 385.2099 (mse_score: 385.2099)\n",
      "05:28    Epoch 35: train loss 383.0303 (mse_score: 383.0303)\n",
      "05:31    Epoch 40: train loss 382.1426 (mse_score: 382.1426)\n",
      "05:33    Epoch 45: train loss 381.7051 (mse_score: 381.7051)\n",
      "05:36    Epoch 50: train loss 381.4131 (mse_score: 381.4131)\n",
      "05:36  Finished training\n",
      "05:36  Training estimator 9 / 10 in ensemble\n",
      "05:36  Starting training\n",
      "05:36    Method:                 sally\n",
      "05:36    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_8.npy\n",
      "05:36                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_8.npy\n",
      "05:36    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "05:36    Method:                 sally\n",
      "05:36    Hidden layers:          (100, 100, 100, 100)\n",
      "05:36    Activation function:    tanh\n",
      "05:36    Batch size:             128\n",
      "05:36    Trainer:                amsgrad\n",
      "05:36    Epochs:                 50\n",
      "05:36    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "05:36    Validation split:       None\n",
      "05:36    Early stopping:         True\n",
      "05:36    Scale inputs:           True\n",
      "05:36    Shuffle labels          False\n",
      "05:36    Regularization:         None\n",
      "05:36  Loading training data\n",
      "05:36  Found 1000000 samples with 2 parameters and 27 observables\n",
      "05:36  Rescaling inputs\n",
      "05:36  Only using 21 of 27 observables\n",
      "05:36  Creating model for method sally\n",
      "05:36  Training model\n",
      "05:39    Epoch 5: train loss 394.2514 (mse_score: 394.2514)\n",
      "05:41    Epoch 10: train loss 387.1092 (mse_score: 387.1092)\n",
      "05:44    Epoch 15: train loss 380.2111 (mse_score: 380.2111)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:46    Epoch 20: train loss 374.2649 (mse_score: 374.2649)\n",
      "05:49    Epoch 25: train loss 369.4312 (mse_score: 369.4312)\n",
      "05:51    Epoch 30: train loss 365.5496 (mse_score: 365.5496)\n",
      "05:54    Epoch 35: train loss 362.2990 (mse_score: 362.2990)\n",
      "05:56    Epoch 40: train loss 360.8422 (mse_score: 360.8422)\n",
      "05:59    Epoch 45: train loss 360.4164 (mse_score: 360.4164)\n",
      "06:01    Epoch 50: train loss 359.6699 (mse_score: 359.6699)\n",
      "06:01  Finished training\n",
      "06:01  Training estimator 10 / 10 in ensemble\n",
      "06:01  Starting training\n",
      "06:01    Method:                 sally\n",
      "06:01    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_9.npy\n",
      "06:01                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_9.npy\n",
      "06:01    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "06:01    Method:                 sally\n",
      "06:01    Hidden layers:          (100, 100, 100, 100)\n",
      "06:01    Activation function:    tanh\n",
      "06:01    Batch size:             128\n",
      "06:01    Trainer:                amsgrad\n",
      "06:01    Epochs:                 50\n",
      "06:01    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "06:01    Validation split:       None\n",
      "06:01    Early stopping:         True\n",
      "06:01    Scale inputs:           True\n",
      "06:01    Shuffle labels          False\n",
      "06:01    Regularization:         None\n",
      "06:01  Loading training data\n",
      "06:01  Found 1000000 samples with 2 parameters and 27 observables\n",
      "06:01  Rescaling inputs\n",
      "06:01  Only using 21 of 27 observables\n",
      "06:01  Creating model for method sally\n",
      "06:01  Training model\n",
      "06:04    Epoch 5: train loss 444.3964 (mse_score: 444.3964)\n",
      "06:07    Epoch 10: train loss 434.8489 (mse_score: 434.8489)\n",
      "06:09    Epoch 15: train loss 429.1036 (mse_score: 429.1036)\n",
      "06:12    Epoch 20: train loss 419.9471 (mse_score: 419.9471)\n",
      "06:14    Epoch 25: train loss 414.2990 (mse_score: 414.2990)\n",
      "06:17    Epoch 30: train loss 410.3746 (mse_score: 410.3746)\n",
      "06:19    Epoch 35: train loss 408.5437 (mse_score: 408.5437)\n",
      "06:22    Epoch 40: train loss 407.6825 (mse_score: 407.6825)\n",
      "06:24    Epoch 45: train loss 407.2495 (mse_score: 407.2495)\n",
      "06:27    Epoch 50: train loss 406.9893 (mse_score: 406.9893)\n",
      "06:27  Finished training\n",
      "06:27  Calculating expectation for 10 estimators in ensemble\n",
      "06:27  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "06:27  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "06:27  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "06:28  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "06:28  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "06:28  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "06:28  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "06:29  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "06:29  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "06:29  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[min_obs for _ in range(10)],\n",
    "    initial_lr=0.001,\n",
    "    n_hidden=(100,100,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just resurrection phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:15  Training 10 estimators in ensemble\n",
      "08:15  Training estimator 1 / 10 in ensemble\n",
      "08:15  Starting training\n",
      "08:15    Method:                 sally\n",
      "08:15    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "08:15                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "08:15    Features:               [26]\n",
      "08:15    Method:                 sally\n",
      "08:15    Hidden layers:          (100, 100, 100, 100)\n",
      "08:15    Activation function:    tanh\n",
      "08:15    Batch size:             128\n",
      "08:15    Trainer:                amsgrad\n",
      "08:15    Epochs:                 50\n",
      "08:15    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "08:15    Validation split:       None\n",
      "08:15    Early stopping:         True\n",
      "08:15    Scale inputs:           True\n",
      "08:15    Regularization:         None\n",
      "08:15  Loading training data\n",
      "08:15  Found 1000000 samples with 2 parameters and 27 observables\n",
      "08:15  Rescaling inputs\n",
      "08:15  Only using 1 of 27 observables\n",
      "08:15  Creating model for method sally\n",
      "08:15  Training model\n",
      "08:38    Epoch 5: train loss 491.6405 (mse_score: 491.6405)\n",
      "08:41    Epoch 10: train loss 491.3770 (mse_score: 491.3770)\n",
      "08:44    Epoch 15: train loss 491.2615 (mse_score: 491.2615)\n",
      "08:48    Epoch 20: train loss 491.1608 (mse_score: 491.1608)\n",
      "08:51    Epoch 25: train loss 491.1362 (mse_score: 491.1362)\n",
      "08:54    Epoch 30: train loss 491.1005 (mse_score: 491.1005)\n",
      "08:58    Epoch 35: train loss 491.0769 (mse_score: 491.0769)\n",
      "09:17    Epoch 40: train loss 491.0589 (mse_score: 491.0589)\n",
      "09:22    Epoch 45: train loss 491.0548 (mse_score: 491.0548)\n",
      "09:26    Epoch 50: train loss 491.0696 (mse_score: 491.0696)\n",
      "09:26  Finished training\n",
      "09:26  Training estimator 2 / 10 in ensemble\n",
      "09:26  Starting training\n",
      "09:26    Method:                 sally\n",
      "09:26    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "09:26                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "09:26    Features:               [26]\n",
      "09:26    Method:                 sally\n",
      "09:26    Hidden layers:          (100, 100, 100, 100)\n",
      "09:26    Activation function:    tanh\n",
      "09:26    Batch size:             128\n",
      "09:26    Trainer:                amsgrad\n",
      "09:26    Epochs:                 50\n",
      "09:26    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "09:26    Validation split:       None\n",
      "09:26    Early stopping:         True\n",
      "09:26    Scale inputs:           True\n",
      "09:26    Regularization:         None\n",
      "09:26  Loading training data\n",
      "09:26  Found 1000000 samples with 2 parameters and 27 observables\n",
      "09:26  Rescaling inputs\n",
      "09:26  Only using 1 of 27 observables\n",
      "09:26  Creating model for method sally\n",
      "09:26  Training model\n",
      "09:40    Epoch 5: train loss 446.8149 (mse_score: 446.8149)\n",
      "09:50    Epoch 10: train loss 446.4876 (mse_score: 446.4876)\n",
      "09:54    Epoch 15: train loss 446.3354 (mse_score: 446.3354)\n",
      "09:57    Epoch 20: train loss 443.0635 (mse_score: 443.0635)\n",
      "10:01    Epoch 25: train loss 442.7790 (mse_score: 442.7790)\n",
      "10:04    Epoch 30: train loss 442.6753 (mse_score: 442.6753)\n",
      "10:08    Epoch 35: train loss 442.6401 (mse_score: 442.6401)\n",
      "10:11    Epoch 40: train loss 442.5989 (mse_score: 442.5989)\n",
      "10:15    Epoch 45: train loss 442.5973 (mse_score: 442.5973)\n",
      "10:18    Epoch 50: train loss 442.5665 (mse_score: 442.5665)\n",
      "10:18  Finished training\n",
      "10:18  Training estimator 3 / 10 in ensemble\n",
      "10:18  Starting training\n",
      "10:18    Method:                 sally\n",
      "10:18    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "10:18                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "10:18    Features:               [26]\n",
      "10:18    Method:                 sally\n",
      "10:18    Hidden layers:          (100, 100, 100, 100)\n",
      "10:18    Activation function:    tanh\n",
      "10:18    Batch size:             128\n",
      "10:18    Trainer:                amsgrad\n",
      "10:18    Epochs:                 50\n",
      "10:18    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "10:18    Validation split:       None\n",
      "10:18    Early stopping:         True\n",
      "10:18    Scale inputs:           True\n",
      "10:18    Regularization:         None\n",
      "10:18  Loading training data\n",
      "10:18  Found 1000000 samples with 2 parameters and 27 observables\n",
      "10:18  Rescaling inputs\n",
      "10:18  Only using 1 of 27 observables\n",
      "10:18  Creating model for method sally\n",
      "10:18  Training model\n",
      "10:21    Epoch 5: train loss 513.3918 (mse_score: 513.3918)\n",
      "10:25    Epoch 10: train loss 513.1949 (mse_score: 513.1949)\n",
      "10:27    Epoch 15: train loss 513.0836 (mse_score: 513.0836)\n",
      "10:30    Epoch 20: train loss 512.9581 (mse_score: 512.9581)\n",
      "10:34    Epoch 25: train loss 512.9239 (mse_score: 512.9239)\n",
      "10:37    Epoch 30: train loss 512.8800 (mse_score: 512.8800)\n",
      "10:40    Epoch 35: train loss 512.8630 (mse_score: 512.8630)\n",
      "10:43    Epoch 40: train loss 512.8606 (mse_score: 512.8606)\n",
      "10:46    Epoch 45: train loss 512.8538 (mse_score: 512.8538)\n",
      "10:49    Epoch 50: train loss 512.8336 (mse_score: 512.8336)\n",
      "10:49  Finished training\n",
      "10:49  Training estimator 4 / 10 in ensemble\n",
      "10:49  Starting training\n",
      "10:49    Method:                 sally\n",
      "10:49    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_3.npy\n",
      "10:49                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_3.npy\n",
      "10:49    Features:               [26]\n",
      "10:49    Method:                 sally\n",
      "10:49    Hidden layers:          (100, 100, 100, 100)\n",
      "10:49    Activation function:    tanh\n",
      "10:49    Batch size:             128\n",
      "10:49    Trainer:                amsgrad\n",
      "10:49    Epochs:                 50\n",
      "10:49    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "10:49    Validation split:       None\n",
      "10:49    Early stopping:         True\n",
      "10:49    Scale inputs:           True\n",
      "10:49    Regularization:         None\n",
      "10:49  Loading training data\n",
      "10:49  Found 1000000 samples with 2 parameters and 27 observables\n",
      "10:49  Rescaling inputs\n",
      "10:49  Only using 1 of 27 observables\n",
      "10:49  Creating model for method sally\n",
      "10:49  Training model\n",
      "10:53    Epoch 5: train loss 462.5733 (mse_score: 462.5733)\n",
      "10:56    Epoch 10: train loss 462.3806 (mse_score: 462.3806)\n",
      "10:59    Epoch 15: train loss 462.2288 (mse_score: 462.2288)\n",
      "11:02    Epoch 20: train loss 462.1835 (mse_score: 462.1835)\n",
      "11:04    Epoch 25: train loss 458.8388 (mse_score: 458.8388)\n",
      "11:07    Epoch 30: train loss 457.9009 (mse_score: 457.9009)\n",
      "11:09    Epoch 35: train loss 457.7782 (mse_score: 457.7782)\n",
      "11:12    Epoch 40: train loss 457.7300 (mse_score: 457.7300)\n",
      "11:15    Epoch 45: train loss 457.7065 (mse_score: 457.7065)\n",
      "11:18    Epoch 50: train loss 457.6828 (mse_score: 457.6828)\n",
      "11:18  Finished training\n",
      "11:18  Training estimator 5 / 10 in ensemble\n",
      "11:18  Starting training\n",
      "11:18    Method:                 sally\n",
      "11:18    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_4.npy\n",
      "11:18                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_4.npy\n",
      "11:18    Features:               [26]\n",
      "11:18    Method:                 sally\n",
      "11:18    Hidden layers:          (100, 100, 100, 100)\n",
      "11:18    Activation function:    tanh\n",
      "11:18    Batch size:             128\n",
      "11:18    Trainer:                amsgrad\n",
      "11:18    Epochs:                 50\n",
      "11:18    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "11:18    Validation split:       None\n",
      "11:18    Early stopping:         True\n",
      "11:18    Scale inputs:           True\n",
      "11:18    Regularization:         None\n",
      "11:18  Loading training data\n",
      "11:18  Found 1000000 samples with 2 parameters and 27 observables\n",
      "11:18  Rescaling inputs\n",
      "11:18  Only using 1 of 27 observables\n",
      "11:18  Creating model for method sally\n",
      "11:18  Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:22    Epoch 5: train loss 428.2557 (mse_score: 428.2557)\n",
      "11:25    Epoch 10: train loss 428.0868 (mse_score: 428.0868)\n",
      "11:28    Epoch 15: train loss 427.9306 (mse_score: 427.9306)\n",
      "11:32    Epoch 20: train loss 427.8273 (mse_score: 427.8273)\n",
      "11:35    Epoch 25: train loss 427.7919 (mse_score: 427.7919)\n",
      "11:39    Epoch 30: train loss 427.8116 (mse_score: 427.8116)\n",
      "11:42    Epoch 35: train loss 427.7349 (mse_score: 427.7349)\n",
      "11:46    Epoch 40: train loss 427.7345 (mse_score: 427.7345)\n",
      "11:49    Epoch 45: train loss 427.7359 (mse_score: 427.7359)\n",
      "11:53    Epoch 50: train loss 427.7105 (mse_score: 427.7105)\n",
      "11:53  Finished training\n",
      "11:53  Training estimator 6 / 10 in ensemble\n",
      "11:53  Starting training\n",
      "11:53    Method:                 sally\n",
      "11:53    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_5.npy\n",
      "11:53                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_5.npy\n",
      "11:53    Features:               [26]\n",
      "11:53    Method:                 sally\n",
      "11:53    Hidden layers:          (100, 100, 100, 100)\n",
      "11:53    Activation function:    tanh\n",
      "11:53    Batch size:             128\n",
      "11:53    Trainer:                amsgrad\n",
      "11:53    Epochs:                 50\n",
      "11:53    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "11:53    Validation split:       None\n",
      "11:53    Early stopping:         True\n",
      "11:53    Scale inputs:           True\n",
      "11:53    Regularization:         None\n",
      "11:53  Loading training data\n",
      "11:53  Found 1000000 samples with 2 parameters and 27 observables\n",
      "11:53  Rescaling inputs\n",
      "11:53  Only using 1 of 27 observables\n",
      "11:53  Creating model for method sally\n",
      "11:53  Training model\n",
      "11:57    Epoch 5: train loss 482.8944 (mse_score: 482.8944)\n",
      "12:01    Epoch 10: train loss 482.6136 (mse_score: 482.6136)\n",
      "12:05    Epoch 15: train loss 482.5178 (mse_score: 482.5178)\n",
      "12:09    Epoch 20: train loss 482.4185 (mse_score: 482.4185)\n",
      "12:13    Epoch 25: train loss 482.3456 (mse_score: 482.3456)\n",
      "12:17    Epoch 30: train loss 482.3186 (mse_score: 482.3186)\n",
      "12:21    Epoch 35: train loss 482.2971 (mse_score: 482.2971)\n",
      "12:25    Epoch 40: train loss 482.3448 (mse_score: 482.3448)\n",
      "12:29    Epoch 45: train loss 482.3046 (mse_score: 482.3046)\n",
      "12:33    Epoch 50: train loss 482.3127 (mse_score: 482.3127)\n",
      "12:33  Finished training\n",
      "12:33  Training estimator 7 / 10 in ensemble\n",
      "12:33  Starting training\n",
      "12:33    Method:                 sally\n",
      "12:33    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_6.npy\n",
      "12:33                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_6.npy\n",
      "12:33    Features:               [26]\n",
      "12:33    Method:                 sally\n",
      "12:33    Hidden layers:          (100, 100, 100, 100)\n",
      "12:33    Activation function:    tanh\n",
      "12:33    Batch size:             128\n",
      "12:33    Trainer:                amsgrad\n",
      "12:33    Epochs:                 50\n",
      "12:33    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "12:33    Validation split:       None\n",
      "12:33    Early stopping:         True\n",
      "12:33    Scale inputs:           True\n",
      "12:33    Regularization:         None\n",
      "12:33  Loading training data\n",
      "12:33  Found 1000000 samples with 2 parameters and 27 observables\n",
      "12:33  Rescaling inputs\n",
      "12:33  Only using 1 of 27 observables\n",
      "12:33  Creating model for method sally\n",
      "12:33  Training model\n",
      "12:36    Epoch 5: train loss 481.9564 (mse_score: 481.9564)\n",
      "12:40    Epoch 10: train loss 481.7037 (mse_score: 481.7037)\n",
      "12:43    Epoch 15: train loss 481.6414 (mse_score: 481.6414)\n",
      "12:46    Epoch 20: train loss 481.5357 (mse_score: 481.5357)\n",
      "12:50    Epoch 25: train loss 481.5429 (mse_score: 481.5429)\n",
      "12:53    Epoch 30: train loss 481.4411 (mse_score: 481.4411)\n",
      "14:32    Epoch 35: train loss 481.4297 (mse_score: 481.4297)\n",
      "14:35    Epoch 40: train loss 481.4304 (mse_score: 481.4304)\n",
      "14:38    Epoch 45: train loss 481.4065 (mse_score: 481.4065)\n",
      "14:41    Epoch 50: train loss 481.4056 (mse_score: 481.4056)\n",
      "14:41  Finished training\n",
      "14:41  Training estimator 8 / 10 in ensemble\n",
      "14:41  Starting training\n",
      "14:41    Method:                 sally\n",
      "14:41    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_7.npy\n",
      "14:41                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_7.npy\n",
      "14:41    Features:               [26]\n",
      "14:41    Method:                 sally\n",
      "14:41    Hidden layers:          (100, 100, 100, 100)\n",
      "14:41    Activation function:    tanh\n",
      "14:41    Batch size:             128\n",
      "14:41    Trainer:                amsgrad\n",
      "14:41    Epochs:                 50\n",
      "14:41    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "14:41    Validation split:       None\n",
      "14:41    Early stopping:         True\n",
      "14:41    Scale inputs:           True\n",
      "14:41    Regularization:         None\n",
      "14:41  Loading training data\n",
      "14:41  Found 1000000 samples with 2 parameters and 27 observables\n",
      "14:41  Rescaling inputs\n",
      "14:41  Only using 1 of 27 observables\n",
      "14:41  Creating model for method sally\n",
      "14:41  Training model\n",
      "14:44    Epoch 5: train loss 470.8090 (mse_score: 470.8090)\n",
      "14:48    Epoch 10: train loss 470.5801 (mse_score: 470.5801)\n",
      "14:52    Epoch 15: train loss 470.4860 (mse_score: 470.4860)\n",
      "14:55    Epoch 20: train loss 470.4546 (mse_score: 470.4546)\n",
      "14:59    Epoch 25: train loss 467.0653 (mse_score: 467.0653)\n",
      "15:03    Epoch 30: train loss 466.2586 (mse_score: 466.2586)\n",
      "15:07    Epoch 35: train loss 466.1209 (mse_score: 466.1209)\n",
      "15:11    Epoch 40: train loss 466.0477 (mse_score: 466.0477)\n",
      "15:14    Epoch 45: train loss 466.0053 (mse_score: 466.0053)\n",
      "15:17    Epoch 50: train loss 465.9860 (mse_score: 465.9860)\n",
      "15:17  Finished training\n",
      "15:17  Training estimator 9 / 10 in ensemble\n",
      "15:17  Starting training\n",
      "15:17    Method:                 sally\n",
      "15:17    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_8.npy\n",
      "15:17                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_8.npy\n",
      "15:17    Features:               [26]\n",
      "15:17    Method:                 sally\n",
      "15:17    Hidden layers:          (100, 100, 100, 100)\n",
      "15:17    Activation function:    tanh\n",
      "15:17    Batch size:             128\n",
      "15:17    Trainer:                amsgrad\n",
      "15:17    Epochs:                 50\n",
      "15:17    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "15:17    Validation split:       None\n",
      "15:17    Early stopping:         True\n",
      "15:17    Scale inputs:           True\n",
      "15:17    Regularization:         None\n",
      "15:17  Loading training data\n",
      "15:17  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:17  Rescaling inputs\n",
      "15:17  Only using 1 of 27 observables\n",
      "15:17  Creating model for method sally\n",
      "15:17  Training model\n",
      "15:21    Epoch 5: train loss 452.2129 (mse_score: 452.2129)\n",
      "15:24    Epoch 10: train loss 452.2929 (mse_score: 452.2929)\n",
      "15:27    Epoch 15: train loss 452.1668 (mse_score: 452.1668)\n",
      "15:31    Epoch 20: train loss 452.0954 (mse_score: 452.0954)\n",
      "15:36    Epoch 25: train loss 452.0559 (mse_score: 452.0559)\n",
      "15:41    Epoch 30: train loss 451.9980 (mse_score: 451.9980)\n",
      "15:45    Epoch 35: train loss 451.9841 (mse_score: 451.9841)\n",
      "15:49    Epoch 40: train loss 451.9717 (mse_score: 451.9717)\n",
      "15:53    Epoch 45: train loss 451.9781 (mse_score: 451.9781)\n",
      "15:57    Epoch 50: train loss 451.9521 (mse_score: 451.9521)\n",
      "15:57  Finished training\n",
      "15:57  Training estimator 10 / 10 in ensemble\n",
      "15:57  Starting training\n",
      "15:57    Method:                 sally\n",
      "15:57    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_9.npy\n",
      "15:57                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_9.npy\n",
      "15:57    Features:               [26]\n",
      "15:57    Method:                 sally\n",
      "15:57    Hidden layers:          (100, 100, 100, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:57    Activation function:    tanh\n",
      "15:57    Batch size:             128\n",
      "15:57    Trainer:                amsgrad\n",
      "15:57    Epochs:                 50\n",
      "15:57    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "15:57    Validation split:       None\n",
      "15:57    Early stopping:         True\n",
      "15:57    Scale inputs:           True\n",
      "15:57    Regularization:         None\n",
      "15:57  Loading training data\n",
      "15:57  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:57  Rescaling inputs\n",
      "15:57  Only using 1 of 27 observables\n",
      "15:57  Creating model for method sally\n",
      "15:57  Training model\n",
      "16:00    Epoch 5: train loss 507.3529 (mse_score: 507.3529)\n",
      "16:03    Epoch 10: train loss 507.0352 (mse_score: 507.0352)\n",
      "16:07    Epoch 15: train loss 506.7964 (mse_score: 506.7964)\n",
      "16:11    Epoch 20: train loss 506.6972 (mse_score: 506.6972)\n",
      "16:15    Epoch 25: train loss 506.6570 (mse_score: 506.6570)\n",
      "16:18    Epoch 30: train loss 506.6066 (mse_score: 506.6066)\n",
      "16:22    Epoch 35: train loss 506.5862 (mse_score: 506.5862)\n",
      "16:26    Epoch 40: train loss 506.5806 (mse_score: 506.5806)\n",
      "16:30    Epoch 45: train loss 506.5769 (mse_score: 506.5769)\n",
      "16:33    Epoch 50: train loss 506.5642 (mse_score: 506.5642)\n",
      "16:33  Finished training\n",
      "16:33  Calculating expectation for 10 estimators in ensemble\n",
      "16:33  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "16:33  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "16:33  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "16:34  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "16:34  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "16:34  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "16:34  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "16:34  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "16:35  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "16:35  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'resurrection',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[26] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
