{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Marco Farina, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=n_estimators, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=False)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.calculate_expectation(\n",
    "        x_filename=sample_dir + 'validation{}/x_validation.npy'.format(cut_label)\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:26  \n",
      "19:26  ------------------------------------------------------------\n",
      "19:26  |                                                          |\n",
      "19:26  |  MadMiner v0.1.0                                         |\n",
      "19:26  |                                                          |\n",
      "19:26  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "19:26  |                                                          |\n",
      "19:26  ------------------------------------------------------------\n",
      "19:26  \n",
      "19:26  Training 10 estimators in ensemble\n",
      "19:26  Training estimator 1 / 10 in ensemble\n",
      "19:26  Starting training\n",
      "19:26    Method:                 sally\n",
      "19:26    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "19:26                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "19:26    Features:               all\n",
      "19:26    Method:                 sally\n",
      "19:26    Hidden layers:          (100, 100)\n",
      "19:26    Activation function:    tanh\n",
      "19:26    Batch size:             128\n",
      "19:26    Trainer:                amsgrad\n",
      "19:26    Epochs:                 50\n",
      "19:26    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "19:26    Validation split:       None\n",
      "19:26    Early stopping:         True\n",
      "19:26    Scale inputs:           True\n",
      "19:26    Shuffle labels          False\n",
      "19:26    Regularization:         None\n",
      "19:26  Loading training data\n",
      "19:26  Found 1000000 samples with 2 parameters and 27 observables\n",
      "19:26  Rescaling inputs\n",
      "19:26  Creating model for method sally\n",
      "19:26  Training model\n",
      "19:28    Epoch 5: train loss 3.9584 (mse_score: 3.9584)\n",
      "19:29    Epoch 10: train loss 3.6702 (mse_score: 3.6702)\n",
      "19:31    Epoch 15: train loss 3.4686 (mse_score: 3.4686)\n",
      "19:32    Epoch 20: train loss 3.3259 (mse_score: 3.3259)\n",
      "19:34    Epoch 25: train loss 3.2105 (mse_score: 3.2105)\n",
      "19:35    Epoch 30: train loss 3.1274 (mse_score: 3.1274)\n",
      "19:36    Epoch 35: train loss 3.0530 (mse_score: 3.0530)\n",
      "19:38    Epoch 40: train loss 3.0022 (mse_score: 3.0022)\n",
      "19:39    Epoch 45: train loss 2.9573 (mse_score: 2.9573)\n",
      "19:41    Epoch 50: train loss 2.9238 (mse_score: 2.9238)\n",
      "19:41  Finished training\n",
      "19:41  Training estimator 2 / 10 in ensemble\n",
      "19:41  Starting training\n",
      "19:41    Method:                 sally\n",
      "19:41    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "19:41                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "19:41    Features:               all\n",
      "19:41    Method:                 sally\n",
      "19:41    Hidden layers:          (100, 100)\n",
      "19:41    Activation function:    tanh\n",
      "19:41    Batch size:             128\n",
      "19:41    Trainer:                amsgrad\n",
      "19:41    Epochs:                 50\n",
      "19:41    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "19:41    Validation split:       None\n",
      "19:41    Early stopping:         True\n",
      "19:41    Scale inputs:           True\n",
      "19:41    Shuffle labels          False\n",
      "19:41    Regularization:         None\n",
      "19:41  Loading training data\n",
      "19:41  Found 1000000 samples with 2 parameters and 27 observables\n",
      "19:41  Rescaling inputs\n",
      "19:41  Creating model for method sally\n",
      "19:41  Training model\n",
      "19:43    Epoch 5: train loss 4.3540 (mse_score: 4.3540)\n",
      "19:44    Epoch 10: train loss 4.1070 (mse_score: 4.1070)\n",
      "19:46    Epoch 15: train loss 3.9115 (mse_score: 3.9115)\n",
      "19:47    Epoch 20: train loss 3.7106 (mse_score: 3.7106)\n",
      "19:49    Epoch 25: train loss 3.5577 (mse_score: 3.5577)\n",
      "19:50    Epoch 30: train loss 3.4462 (mse_score: 3.4462)\n",
      "19:52    Epoch 35: train loss 3.3496 (mse_score: 3.3496)\n",
      "19:53    Epoch 40: train loss 3.2799 (mse_score: 3.2799)\n",
      "19:55    Epoch 45: train loss 3.2202 (mse_score: 3.2202)\n",
      "19:56    Epoch 50: train loss 3.1773 (mse_score: 3.1773)\n",
      "19:56  Finished training\n",
      "19:56  Training estimator 3 / 10 in ensemble\n",
      "19:56  Starting training\n",
      "19:56    Method:                 sally\n",
      "19:56    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "19:56                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "19:56    Features:               all\n",
      "19:56    Method:                 sally\n",
      "19:56    Hidden layers:          (100, 100)\n",
      "19:56    Activation function:    tanh\n",
      "19:56    Batch size:             128\n",
      "19:56    Trainer:                amsgrad\n",
      "19:56    Epochs:                 50\n",
      "19:56    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "19:56    Validation split:       None\n",
      "19:56    Early stopping:         True\n",
      "19:56    Scale inputs:           True\n",
      "19:56    Shuffle labels          False\n",
      "19:56    Regularization:         None\n",
      "19:56  Loading training data\n",
      "19:56  Found 1000000 samples with 2 parameters and 27 observables\n",
      "19:56  Rescaling inputs\n",
      "19:56  Creating model for method sally\n",
      "19:56  Training model\n",
      "19:58    Epoch 5: train loss 3.6678 (mse_score: 3.6678)\n",
      "20:00    Epoch 10: train loss 3.4799 (mse_score: 3.4799)\n",
      "20:01    Epoch 15: train loss 3.3038 (mse_score: 3.3038)\n",
      "20:03    Epoch 20: train loss 3.1796 (mse_score: 3.1796)\n",
      "20:04    Epoch 25: train loss 3.0785 (mse_score: 3.0785)\n",
      "20:06    Epoch 30: train loss 3.0024 (mse_score: 3.0024)\n",
      "20:07    Epoch 35: train loss 2.9398 (mse_score: 2.9398)\n",
      "20:09    Epoch 40: train loss 2.8966 (mse_score: 2.8966)\n",
      "20:10    Epoch 45: train loss 2.8570 (mse_score: 2.8570)\n",
      "20:12    Epoch 50: train loss 2.8277 (mse_score: 2.8277)\n",
      "20:12  Finished training\n",
      "20:12  Training estimator 4 / 10 in ensemble\n",
      "20:12  Starting training\n",
      "20:12    Method:                 sally\n",
      "20:12    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "20:12                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "20:12    Features:               all\n",
      "20:12    Method:                 sally\n",
      "20:12    Hidden layers:          (100, 100)\n",
      "20:12    Activation function:    tanh\n",
      "20:12    Batch size:             128\n",
      "20:12    Trainer:                amsgrad\n",
      "20:12    Epochs:                 50\n",
      "20:12    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:12    Validation split:       None\n",
      "20:12    Early stopping:         True\n",
      "20:12    Scale inputs:           True\n",
      "20:12    Shuffle labels          False\n",
      "20:12    Regularization:         None\n",
      "20:12  Loading training data\n",
      "20:12  Found 1000000 samples with 2 parameters and 27 observables\n",
      "20:12  Rescaling inputs\n",
      "20:12  Creating model for method sally\n",
      "20:12  Training model\n",
      "20:14    Epoch 5: train loss 3.8036 (mse_score: 3.8036)\n",
      "20:15    Epoch 10: train loss 3.6406 (mse_score: 3.6406)\n",
      "20:17    Epoch 15: train loss 3.5136 (mse_score: 3.5136)\n",
      "20:18    Epoch 20: train loss 3.4079 (mse_score: 3.4079)\n",
      "20:20    Epoch 25: train loss 3.3301 (mse_score: 3.3301)\n",
      "20:21    Epoch 30: train loss 3.2609 (mse_score: 3.2609)\n",
      "20:23    Epoch 35: train loss 3.2035 (mse_score: 3.2035)\n",
      "20:24    Epoch 40: train loss 3.1575 (mse_score: 3.1575)\n",
      "20:26    Epoch 45: train loss 3.1221 (mse_score: 3.1221)\n",
      "20:27    Epoch 50: train loss 3.0935 (mse_score: 3.0935)\n",
      "20:27  Finished training\n",
      "20:27  Training estimator 5 / 10 in ensemble\n",
      "20:27  Starting training\n",
      "20:27    Method:                 sally\n",
      "20:27    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "20:27                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "20:27    Features:               all\n",
      "20:27    Method:                 sally\n",
      "20:27    Hidden layers:          (100, 100)\n",
      "20:27    Activation function:    tanh\n",
      "20:27    Batch size:             128\n",
      "20:27    Trainer:                amsgrad\n",
      "20:27    Epochs:                 50\n",
      "20:27    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:27    Validation split:       None\n",
      "20:27    Early stopping:         True\n",
      "20:27    Scale inputs:           True\n",
      "20:27    Shuffle labels          False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:27    Regularization:         None\n",
      "20:27  Loading training data\n",
      "20:27  Found 1000000 samples with 2 parameters and 27 observables\n",
      "20:27  Rescaling inputs\n",
      "20:27  Creating model for method sally\n",
      "20:27  Training model\n",
      "20:29    Epoch 5: train loss 3.3958 (mse_score: 3.3958)\n",
      "20:31    Epoch 10: train loss 3.2031 (mse_score: 3.2031)\n",
      "20:32    Epoch 15: train loss 3.0584 (mse_score: 3.0584)\n",
      "20:34    Epoch 20: train loss 2.9610 (mse_score: 2.9610)\n",
      "20:35    Epoch 25: train loss 2.8745 (mse_score: 2.8745)\n",
      "20:37    Epoch 30: train loss 2.8114 (mse_score: 2.8114)\n",
      "20:38    Epoch 35: train loss 2.7557 (mse_score: 2.7557)\n",
      "20:40    Epoch 40: train loss 2.7131 (mse_score: 2.7131)\n",
      "20:41    Epoch 45: train loss 2.6793 (mse_score: 2.6793)\n",
      "20:43    Epoch 50: train loss 2.6543 (mse_score: 2.6543)\n",
      "20:43  Finished training\n",
      "20:43  Training estimator 6 / 10 in ensemble\n",
      "20:43  Starting training\n",
      "20:43    Method:                 sally\n",
      "20:43    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "20:43                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "20:43    Features:               all\n",
      "20:43    Method:                 sally\n",
      "20:43    Hidden layers:          (100, 100)\n",
      "20:43    Activation function:    tanh\n",
      "20:43    Batch size:             128\n",
      "20:43    Trainer:                amsgrad\n",
      "20:43    Epochs:                 50\n",
      "20:43    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:43    Validation split:       None\n",
      "20:43    Early stopping:         True\n",
      "20:43    Scale inputs:           True\n",
      "20:43    Shuffle labels          False\n",
      "20:43    Regularization:         None\n",
      "20:43  Loading training data\n",
      "20:43  Found 1000000 samples with 2 parameters and 27 observables\n",
      "20:43  Rescaling inputs\n",
      "20:43  Creating model for method sally\n",
      "20:43  Training model\n",
      "20:45    Epoch 5: train loss 4.0935 (mse_score: 4.0935)\n",
      "20:46    Epoch 10: train loss 3.8631 (mse_score: 3.8631)\n",
      "20:48    Epoch 15: train loss 3.6909 (mse_score: 3.6909)\n",
      "20:49    Epoch 20: train loss 3.5533 (mse_score: 3.5533)\n",
      "20:51    Epoch 25: train loss 3.4469 (mse_score: 3.4469)\n",
      "20:52    Epoch 30: train loss 3.3547 (mse_score: 3.3547)\n",
      "20:54    Epoch 35: train loss 3.2732 (mse_score: 3.2732)\n",
      "20:56    Epoch 40: train loss 3.2030 (mse_score: 3.2030)\n",
      "20:57    Epoch 45: train loss 3.1455 (mse_score: 3.1455)\n",
      "20:59    Epoch 50: train loss 3.1082 (mse_score: 3.1082)\n",
      "20:59  Finished training\n",
      "20:59  Training estimator 7 / 10 in ensemble\n",
      "20:59  Starting training\n",
      "20:59    Method:                 sally\n",
      "20:59    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "20:59                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "20:59    Features:               all\n",
      "20:59    Method:                 sally\n",
      "20:59    Hidden layers:          (100, 100)\n",
      "20:59    Activation function:    tanh\n",
      "20:59    Batch size:             128\n",
      "20:59    Trainer:                amsgrad\n",
      "20:59    Epochs:                 50\n",
      "20:59    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:59    Validation split:       None\n",
      "20:59    Early stopping:         True\n",
      "20:59    Scale inputs:           True\n",
      "20:59    Shuffle labels          False\n",
      "20:59    Regularization:         None\n",
      "20:59  Loading training data\n",
      "20:59  Found 1000000 samples with 2 parameters and 27 observables\n",
      "20:59  Rescaling inputs\n",
      "20:59  Creating model for method sally\n",
      "20:59  Training model\n",
      "21:01    Epoch 5: train loss 7.3986 (mse_score: 7.3986)\n",
      "21:03    Epoch 10: train loss 7.1485 (mse_score: 7.1485)\n",
      "21:04    Epoch 15: train loss 6.9978 (mse_score: 6.9978)\n",
      "21:06    Epoch 20: train loss 6.8514 (mse_score: 6.8514)\n",
      "21:08    Epoch 25: train loss 6.7410 (mse_score: 6.7410)\n",
      "21:10    Epoch 30: train loss 6.6432 (mse_score: 6.6432)\n",
      "21:11    Epoch 35: train loss 6.5684 (mse_score: 6.5684)\n",
      "21:13    Epoch 40: train loss 6.5115 (mse_score: 6.5115)\n",
      "21:14    Epoch 45: train loss 6.4531 (mse_score: 6.4531)\n",
      "21:16    Epoch 50: train loss 6.4168 (mse_score: 6.4168)\n",
      "21:16  Finished training\n",
      "21:16  Training estimator 8 / 10 in ensemble\n",
      "21:16  Starting training\n",
      "21:16    Method:                 sally\n",
      "21:16    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "21:16                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "21:16    Features:               all\n",
      "21:16    Method:                 sally\n",
      "21:16    Hidden layers:          (100, 100)\n",
      "21:16    Activation function:    tanh\n",
      "21:16    Batch size:             128\n",
      "21:16    Trainer:                amsgrad\n",
      "21:16    Epochs:                 50\n",
      "21:16    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:16    Validation split:       None\n",
      "21:16    Early stopping:         True\n",
      "21:16    Scale inputs:           True\n",
      "21:16    Shuffle labels          False\n",
      "21:16    Regularization:         None\n",
      "21:16  Loading training data\n",
      "21:16  Found 1000000 samples with 2 parameters and 27 observables\n",
      "21:16  Rescaling inputs\n",
      "21:16  Creating model for method sally\n",
      "21:16  Training model\n",
      "21:18    Epoch 5: train loss 3.5117 (mse_score: 3.5117)\n",
      "21:20    Epoch 10: train loss 3.2905 (mse_score: 3.2905)\n",
      "21:22    Epoch 15: train loss 3.1204 (mse_score: 3.1204)\n",
      "21:23    Epoch 20: train loss 2.9778 (mse_score: 2.9778)\n",
      "21:25    Epoch 25: train loss 2.8507 (mse_score: 2.8507)\n",
      "21:26    Epoch 30: train loss 2.7594 (mse_score: 2.7594)\n",
      "21:28    Epoch 35: train loss 2.6905 (mse_score: 2.6905)\n",
      "21:30    Epoch 40: train loss 2.6305 (mse_score: 2.6305)\n",
      "21:31    Epoch 45: train loss 2.5811 (mse_score: 2.5811)\n",
      "21:33    Epoch 50: train loss 2.5452 (mse_score: 2.5452)\n",
      "21:33  Finished training\n",
      "21:33  Training estimator 9 / 10 in ensemble\n",
      "21:33  Starting training\n",
      "21:33    Method:                 sally\n",
      "21:33    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "21:33                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "21:33    Features:               all\n",
      "21:33    Method:                 sally\n",
      "21:33    Hidden layers:          (100, 100)\n",
      "21:33    Activation function:    tanh\n",
      "21:33    Batch size:             128\n",
      "21:33    Trainer:                amsgrad\n",
      "21:33    Epochs:                 50\n",
      "21:33    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:33    Validation split:       None\n",
      "21:33    Early stopping:         True\n",
      "21:33    Scale inputs:           True\n",
      "21:33    Shuffle labels          False\n",
      "21:33    Regularization:         None\n",
      "21:33  Loading training data\n",
      "21:33  Found 1000000 samples with 2 parameters and 27 observables\n",
      "21:33  Rescaling inputs\n",
      "21:33  Creating model for method sally\n",
      "21:33  Training model\n",
      "21:35    Epoch 5: train loss 3.7768 (mse_score: 3.7768)\n",
      "21:37    Epoch 10: train loss 3.5314 (mse_score: 3.5314)\n",
      "21:38    Epoch 15: train loss 3.3950 (mse_score: 3.3950)\n",
      "21:40    Epoch 20: train loss 3.2420 (mse_score: 3.2420)\n",
      "21:42    Epoch 25: train loss 3.1616 (mse_score: 3.1616)\n",
      "21:43    Epoch 30: train loss 3.0856 (mse_score: 3.0856)\n",
      "21:45    Epoch 35: train loss 3.0298 (mse_score: 3.0298)\n",
      "21:46    Epoch 40: train loss 2.9791 (mse_score: 2.9791)\n",
      "21:48    Epoch 45: train loss 2.9378 (mse_score: 2.9378)\n",
      "21:49    Epoch 50: train loss 2.9080 (mse_score: 2.9080)\n",
      "21:49  Finished training\n",
      "21:49  Training estimator 10 / 10 in ensemble\n",
      "21:49  Starting training\n",
      "21:49    Method:                 sally\n",
      "21:49    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "21:49                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "21:49    Features:               all\n",
      "21:49    Method:                 sally\n",
      "21:49    Hidden layers:          (100, 100)\n",
      "21:49    Activation function:    tanh\n",
      "21:49    Batch size:             128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:49    Trainer:                amsgrad\n",
      "21:49    Epochs:                 50\n",
      "21:49    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:49    Validation split:       None\n",
      "21:49    Early stopping:         True\n",
      "21:49    Scale inputs:           True\n",
      "21:49    Shuffle labels          False\n",
      "21:49    Regularization:         None\n",
      "21:49  Loading training data\n",
      "21:49  Found 1000000 samples with 2 parameters and 27 observables\n",
      "21:49  Rescaling inputs\n",
      "21:49  Creating model for method sally\n",
      "21:49  Training model\n",
      "21:51    Epoch 5: train loss 6.7792 (mse_score: 6.7792)\n",
      "21:53    Epoch 10: train loss 6.5311 (mse_score: 6.5311)\n",
      "21:54    Epoch 15: train loss 6.3307 (mse_score: 6.3307)\n",
      "21:56    Epoch 20: train loss 6.1841 (mse_score: 6.1841)\n",
      "21:58    Epoch 25: train loss 6.0529 (mse_score: 6.0529)\n",
      "21:59    Epoch 30: train loss 5.9517 (mse_score: 5.9517)\n",
      "22:01    Epoch 35: train loss 5.8785 (mse_score: 5.8785)\n",
      "22:02    Epoch 40: train loss 5.8195 (mse_score: 5.8195)\n",
      "22:03    Epoch 45: train loss 5.7727 (mse_score: 5.7727)\n",
      "22:05    Epoch 50: train loss 5.7360 (mse_score: 5.7360)\n",
      "22:05  Finished training\n",
      "22:05  Calculating expectation for 10 estimators in ensemble\n",
      "22:05  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "22:05  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "22:05  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "22:05  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "22:06  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "22:06  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "22:06  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "22:06  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "22:06  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "22:06  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'all',\n",
    "    use_tight_cuts=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:06  Training 10 estimators in ensemble\n",
      "22:06  Training estimator 1 / 10 in ensemble\n",
      "22:06  Starting training\n",
      "22:06    Method:                 sally\n",
      "22:06    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "22:06                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "22:06    Features:               all\n",
      "22:06    Method:                 sally\n",
      "22:06    Hidden layers:          (100, 100)\n",
      "22:06    Activation function:    tanh\n",
      "22:06    Batch size:             128\n",
      "22:06    Trainer:                amsgrad\n",
      "22:06    Epochs:                 50\n",
      "22:06    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:06    Validation split:       None\n",
      "22:06    Early stopping:         True\n",
      "22:06    Scale inputs:           True\n",
      "22:06    Shuffle labels          False\n",
      "22:06    Regularization:         None\n",
      "22:06  Loading training data\n",
      "22:06  Found 1000000 samples with 2 parameters and 27 observables\n",
      "22:06  Rescaling inputs\n",
      "22:06  Creating model for method sally\n",
      "22:06  Training model\n",
      "22:08    Epoch 5: train loss 408.5618 (mse_score: 408.5618)\n",
      "22:10    Epoch 10: train loss 399.6768 (mse_score: 399.6768)\n",
      "22:11    Epoch 15: train loss 392.9502 (mse_score: 392.9502)\n",
      "22:13    Epoch 20: train loss 388.9428 (mse_score: 388.9428)\n",
      "22:15    Epoch 25: train loss 385.3615 (mse_score: 385.3615)\n",
      "22:16    Epoch 30: train loss 383.0977 (mse_score: 383.0977)\n",
      "22:18    Epoch 35: train loss 381.0166 (mse_score: 381.0166)\n",
      "22:19    Epoch 40: train loss 379.4239 (mse_score: 379.4239)\n",
      "22:21    Epoch 45: train loss 378.1141 (mse_score: 378.1141)\n",
      "22:22    Epoch 50: train loss 377.1587 (mse_score: 377.1587)\n",
      "22:22  Finished training\n",
      "22:22  Training estimator 2 / 10 in ensemble\n",
      "22:22  Starting training\n",
      "22:22    Method:                 sally\n",
      "22:22    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "22:22                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "22:22    Features:               all\n",
      "22:22    Method:                 sally\n",
      "22:22    Hidden layers:          (100, 100)\n",
      "22:22    Activation function:    tanh\n",
      "22:22    Batch size:             128\n",
      "22:22    Trainer:                amsgrad\n",
      "22:22    Epochs:                 50\n",
      "22:22    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:22    Validation split:       None\n",
      "22:22    Early stopping:         True\n",
      "22:22    Scale inputs:           True\n",
      "22:22    Shuffle labels          False\n",
      "22:22    Regularization:         None\n",
      "22:22  Loading training data\n",
      "22:22  Found 1000000 samples with 2 parameters and 27 observables\n",
      "22:22  Rescaling inputs\n",
      "22:22  Creating model for method sally\n",
      "22:22  Training model\n",
      "22:24    Epoch 5: train loss 366.1763 (mse_score: 366.1763)\n",
      "22:26    Epoch 10: train loss 357.6209 (mse_score: 357.6209)\n",
      "22:27    Epoch 15: train loss 351.8700 (mse_score: 351.8700)\n",
      "22:29    Epoch 20: train loss 347.5124 (mse_score: 347.5124)\n",
      "22:30    Epoch 25: train loss 344.2709 (mse_score: 344.2709)\n",
      "22:32    Epoch 30: train loss 341.6076 (mse_score: 341.6076)\n",
      "22:33    Epoch 35: train loss 339.7226 (mse_score: 339.7226)\n",
      "22:35    Epoch 40: train loss 338.1185 (mse_score: 338.1185)\n",
      "22:36    Epoch 45: train loss 336.9138 (mse_score: 336.9138)\n",
      "22:38    Epoch 50: train loss 335.9407 (mse_score: 335.9407)\n",
      "22:38  Finished training\n",
      "22:38  Training estimator 3 / 10 in ensemble\n",
      "22:38  Starting training\n",
      "22:38    Method:                 sally\n",
      "22:38    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "22:38                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "22:38    Features:               all\n",
      "22:38    Method:                 sally\n",
      "22:38    Hidden layers:          (100, 100)\n",
      "22:38    Activation function:    tanh\n",
      "22:38    Batch size:             128\n",
      "22:38    Trainer:                amsgrad\n",
      "22:38    Epochs:                 50\n",
      "22:38    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:38    Validation split:       None\n",
      "22:38    Early stopping:         True\n",
      "22:38    Scale inputs:           True\n",
      "22:38    Shuffle labels          False\n",
      "22:38    Regularization:         None\n",
      "22:38  Loading training data\n",
      "22:38  Found 1000000 samples with 2 parameters and 27 observables\n",
      "22:38  Rescaling inputs\n",
      "22:38  Creating model for method sally\n",
      "22:38  Training model\n",
      "22:40    Epoch 5: train loss 429.6791 (mse_score: 429.6791)\n",
      "22:41    Epoch 10: train loss 420.0330 (mse_score: 420.0330)\n",
      "22:43    Epoch 15: train loss 413.3995 (mse_score: 413.3995)\n",
      "22:44    Epoch 20: train loss 408.9153 (mse_score: 408.9153)\n",
      "22:45    Epoch 25: train loss 405.2288 (mse_score: 405.2288)\n",
      "22:47    Epoch 30: train loss 402.3735 (mse_score: 402.3735)\n",
      "22:48    Epoch 35: train loss 400.0555 (mse_score: 400.0555)\n",
      "22:50    Epoch 40: train loss 398.1781 (mse_score: 398.1781)\n",
      "22:51    Epoch 45: train loss 396.5359 (mse_score: 396.5359)\n",
      "22:53    Epoch 50: train loss 395.1932 (mse_score: 395.1932)\n",
      "22:53  Finished training\n",
      "22:53  Training estimator 4 / 10 in ensemble\n",
      "22:53  Starting training\n",
      "22:53    Method:                 sally\n",
      "22:53    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_3.npy\n",
      "22:53                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_3.npy\n",
      "22:53    Features:               all\n",
      "22:53    Method:                 sally\n",
      "22:53    Hidden layers:          (100, 100)\n",
      "22:53    Activation function:    tanh\n",
      "22:53    Batch size:             128\n",
      "22:53    Trainer:                amsgrad\n",
      "22:53    Epochs:                 50\n",
      "22:53    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:53    Validation split:       None\n",
      "22:53    Early stopping:         True\n",
      "22:53    Scale inputs:           True\n",
      "22:53    Shuffle labels          False\n",
      "22:53    Regularization:         None\n",
      "22:53  Loading training data\n",
      "22:53  Found 1000000 samples with 2 parameters and 27 observables\n",
      "22:53  Rescaling inputs\n",
      "22:53  Creating model for method sally\n",
      "22:53  Training model\n",
      "22:54    Epoch 5: train loss 381.2793 (mse_score: 381.2793)\n",
      "22:56    Epoch 10: train loss 373.4362 (mse_score: 373.4362)\n",
      "22:57    Epoch 15: train loss 368.3910 (mse_score: 368.3910)\n",
      "22:59    Epoch 20: train loss 364.0346 (mse_score: 364.0346)\n",
      "23:00    Epoch 25: train loss 360.6732 (mse_score: 360.6732)\n",
      "23:02    Epoch 30: train loss 358.2626 (mse_score: 358.2626)\n",
      "23:03    Epoch 35: train loss 356.1952 (mse_score: 356.1952)\n",
      "23:05    Epoch 40: train loss 354.5580 (mse_score: 354.5580)\n",
      "23:06    Epoch 45: train loss 353.3148 (mse_score: 353.3148)\n",
      "23:08    Epoch 50: train loss 352.3210 (mse_score: 352.3210)\n",
      "23:08  Finished training\n",
      "23:08  Training estimator 5 / 10 in ensemble\n",
      "23:08  Starting training\n",
      "23:08    Method:                 sally\n",
      "23:08    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_4.npy\n",
      "23:08                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_4.npy\n",
      "23:08    Features:               all\n",
      "23:08    Method:                 sally\n",
      "23:08    Hidden layers:          (100, 100)\n",
      "23:08    Activation function:    tanh\n",
      "23:08    Batch size:             128\n",
      "23:08    Trainer:                amsgrad\n",
      "23:08    Epochs:                 50\n",
      "23:08    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "23:08    Validation split:       None\n",
      "23:08    Early stopping:         True\n",
      "23:08    Scale inputs:           True\n",
      "23:08    Shuffle labels          False\n",
      "23:08    Regularization:         None\n",
      "23:08  Loading training data\n",
      "23:08  Found 1000000 samples with 2 parameters and 27 observables\n",
      "23:08  Rescaling inputs\n",
      "23:08  Creating model for method sally\n",
      "23:08  Training model\n",
      "23:09    Epoch 5: train loss 348.1611 (mse_score: 348.1611)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:11    Epoch 10: train loss 338.6960 (mse_score: 338.6960)\n",
      "23:12    Epoch 15: train loss 332.8965 (mse_score: 332.8965)\n",
      "23:14    Epoch 20: train loss 328.5067 (mse_score: 328.5067)\n",
      "23:15    Epoch 25: train loss 325.4370 (mse_score: 325.4370)\n",
      "23:17    Epoch 30: train loss 323.1936 (mse_score: 323.1936)\n",
      "23:18    Epoch 35: train loss 322.2772 (mse_score: 322.2772)\n",
      "23:20    Epoch 40: train loss 320.0165 (mse_score: 320.0165)\n",
      "23:21    Epoch 45: train loss 318.8839 (mse_score: 318.8839)\n",
      "23:23    Epoch 50: train loss 318.1191 (mse_score: 318.1191)\n",
      "23:23  Finished training\n",
      "23:23  Training estimator 6 / 10 in ensemble\n",
      "23:23  Starting training\n",
      "23:23    Method:                 sally\n",
      "23:23    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_5.npy\n",
      "23:23                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_5.npy\n",
      "23:23    Features:               all\n",
      "23:23    Method:                 sally\n",
      "23:23    Hidden layers:          (100, 100)\n",
      "23:23    Activation function:    tanh\n",
      "23:23    Batch size:             128\n",
      "23:23    Trainer:                amsgrad\n",
      "23:23    Epochs:                 50\n",
      "23:23    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "23:23    Validation split:       None\n",
      "23:23    Early stopping:         True\n",
      "23:23    Scale inputs:           True\n",
      "23:23    Shuffle labels          False\n",
      "23:23    Regularization:         None\n",
      "23:23  Loading training data\n",
      "23:23  Found 1000000 samples with 2 parameters and 27 observables\n",
      "23:23  Rescaling inputs\n",
      "23:23  Creating model for method sally\n",
      "23:23  Training model\n",
      "23:25    Epoch 5: train loss 399.4137 (mse_score: 399.4137)\n",
      "23:26    Epoch 10: train loss 390.4104 (mse_score: 390.4104)\n",
      "23:28    Epoch 15: train loss 383.5602 (mse_score: 383.5602)\n",
      "23:29    Epoch 20: train loss 379.1804 (mse_score: 379.1804)\n",
      "23:31    Epoch 25: train loss 375.4756 (mse_score: 375.4756)\n",
      "23:32    Epoch 30: train loss 372.7237 (mse_score: 372.7237)\n",
      "23:34    Epoch 35: train loss 370.5763 (mse_score: 370.5763)\n",
      "23:35    Epoch 40: train loss 368.9639 (mse_score: 368.9639)\n",
      "23:37    Epoch 45: train loss 367.6210 (mse_score: 367.6210)\n",
      "23:38    Epoch 50: train loss 366.5514 (mse_score: 366.5514)\n",
      "23:38  Finished training\n",
      "23:38  Training estimator 7 / 10 in ensemble\n",
      "23:38  Starting training\n",
      "23:38    Method:                 sally\n",
      "23:38    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_6.npy\n",
      "23:38                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_6.npy\n",
      "23:38    Features:               all\n",
      "23:38    Method:                 sally\n",
      "23:38    Hidden layers:          (100, 100)\n",
      "23:38    Activation function:    tanh\n",
      "23:38    Batch size:             128\n",
      "23:38    Trainer:                amsgrad\n",
      "23:38    Epochs:                 50\n",
      "23:38    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "23:38    Validation split:       None\n",
      "23:38    Early stopping:         True\n",
      "23:38    Scale inputs:           True\n",
      "23:38    Shuffle labels          False\n",
      "23:38    Regularization:         None\n",
      "23:38  Loading training data\n",
      "23:38  Found 1000000 samples with 2 parameters and 27 observables\n",
      "23:38  Rescaling inputs\n",
      "23:38  Creating model for method sally\n",
      "23:38  Training model\n",
      "23:40    Epoch 5: train loss 395.4665 (mse_score: 395.4665)\n",
      "23:41    Epoch 10: train loss 386.0193 (mse_score: 386.0193)\n",
      "23:43    Epoch 15: train loss 378.9276 (mse_score: 378.9276)\n",
      "23:44    Epoch 20: train loss 373.4308 (mse_score: 373.4308)\n",
      "23:46    Epoch 25: train loss 369.6106 (mse_score: 369.6106)\n",
      "23:47    Epoch 30: train loss 366.7202 (mse_score: 366.7202)\n",
      "23:49    Epoch 35: train loss 364.4054 (mse_score: 364.4054)\n",
      "23:50    Epoch 40: train loss 362.4906 (mse_score: 362.4906)\n",
      "23:52    Epoch 45: train loss 361.0030 (mse_score: 361.0030)\n",
      "23:53    Epoch 50: train loss 359.9288 (mse_score: 359.9288)\n",
      "23:53  Finished training\n",
      "23:53  Training estimator 8 / 10 in ensemble\n",
      "23:53  Starting training\n",
      "23:53    Method:                 sally\n",
      "23:53    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_7.npy\n",
      "23:53                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_7.npy\n",
      "23:53    Features:               all\n",
      "23:53    Method:                 sally\n",
      "23:53    Hidden layers:          (100, 100)\n",
      "23:53    Activation function:    tanh\n",
      "23:53    Batch size:             128\n",
      "23:53    Trainer:                amsgrad\n",
      "23:53    Epochs:                 50\n",
      "23:53    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "23:53    Validation split:       None\n",
      "23:53    Early stopping:         True\n",
      "23:53    Scale inputs:           True\n",
      "23:53    Shuffle labels          False\n",
      "23:53    Regularization:         None\n",
      "23:53  Loading training data\n",
      "23:53  Found 1000000 samples with 2 parameters and 27 observables\n",
      "23:53  Rescaling inputs\n",
      "23:53  Creating model for method sally\n",
      "23:53  Training model\n",
      "23:55    Epoch 5: train loss 387.0188 (mse_score: 387.0188)\n",
      "23:57    Epoch 10: train loss 379.3103 (mse_score: 379.3103)\n",
      "23:58    Epoch 15: train loss 373.0011 (mse_score: 373.0011)\n",
      "00:00    Epoch 20: train loss 368.0233 (mse_score: 368.0233)\n",
      "00:01    Epoch 25: train loss 364.8460 (mse_score: 364.8460)\n",
      "00:03    Epoch 30: train loss 362.1490 (mse_score: 362.1490)\n",
      "00:04    Epoch 35: train loss 360.3641 (mse_score: 360.3641)\n",
      "00:06    Epoch 40: train loss 358.7666 (mse_score: 358.7666)\n",
      "00:07    Epoch 45: train loss 357.6434 (mse_score: 357.6434)\n",
      "00:09    Epoch 50: train loss 356.6731 (mse_score: 356.6731)\n",
      "00:09  Finished training\n",
      "00:09  Training estimator 9 / 10 in ensemble\n",
      "00:09  Starting training\n",
      "00:09    Method:                 sally\n",
      "00:09    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_8.npy\n",
      "00:09                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_8.npy\n",
      "00:09    Features:               all\n",
      "00:09    Method:                 sally\n",
      "00:09    Hidden layers:          (100, 100)\n",
      "00:09    Activation function:    tanh\n",
      "00:09    Batch size:             128\n",
      "00:09    Trainer:                amsgrad\n",
      "00:09    Epochs:                 50\n",
      "00:09    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "00:09    Validation split:       None\n",
      "00:09    Early stopping:         True\n",
      "00:09    Scale inputs:           True\n",
      "00:09    Shuffle labels          False\n",
      "00:09    Regularization:         None\n",
      "00:09  Loading training data\n",
      "00:09  Found 1000000 samples with 2 parameters and 27 observables\n",
      "00:09  Rescaling inputs\n",
      "00:09  Creating model for method sally\n",
      "00:09  Training model\n",
      "00:10    Epoch 5: train loss 372.1096 (mse_score: 372.1096)\n",
      "00:12    Epoch 10: train loss 363.9937 (mse_score: 363.9937)\n",
      "00:13    Epoch 15: train loss 358.5149 (mse_score: 358.5149)\n",
      "00:15    Epoch 20: train loss 354.4060 (mse_score: 354.4060)\n",
      "00:16    Epoch 25: train loss 351.6028 (mse_score: 351.6028)\n",
      "00:18    Epoch 30: train loss 349.3731 (mse_score: 349.3731)\n",
      "00:19    Epoch 35: train loss 347.1468 (mse_score: 347.1468)\n",
      "00:21    Epoch 40: train loss 345.5623 (mse_score: 345.5623)\n",
      "00:22    Epoch 45: train loss 344.3655 (mse_score: 344.3655)\n",
      "00:24    Epoch 50: train loss 343.4190 (mse_score: 343.4190)\n",
      "00:24  Finished training\n",
      "00:24  Training estimator 10 / 10 in ensemble\n",
      "00:24  Starting training\n",
      "00:24    Method:                 sally\n",
      "00:24    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_9.npy\n",
      "00:24                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_9.npy\n",
      "00:24    Features:               all\n",
      "00:24    Method:                 sally\n",
      "00:24    Hidden layers:          (100, 100)\n",
      "00:24    Activation function:    tanh\n",
      "00:24    Batch size:             128\n",
      "00:24    Trainer:                amsgrad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:24    Epochs:                 50\n",
      "00:24    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "00:24    Validation split:       None\n",
      "00:24    Early stopping:         True\n",
      "00:24    Scale inputs:           True\n",
      "00:24    Shuffle labels          False\n",
      "00:24    Regularization:         None\n",
      "00:24  Loading training data\n",
      "00:24  Found 1000000 samples with 2 parameters and 27 observables\n",
      "00:24  Rescaling inputs\n",
      "00:24  Creating model for method sally\n",
      "00:24  Training model\n",
      "00:26    Epoch 5: train loss 421.4048 (mse_score: 421.4048)\n",
      "00:27    Epoch 10: train loss 412.4377 (mse_score: 412.4377)\n",
      "00:29    Epoch 15: train loss 406.4234 (mse_score: 406.4234)\n",
      "00:30    Epoch 20: train loss 402.2608 (mse_score: 402.2608)\n",
      "00:32    Epoch 25: train loss 398.9653 (mse_score: 398.9653)\n",
      "00:33    Epoch 30: train loss 396.4053 (mse_score: 396.4053)\n",
      "00:35    Epoch 35: train loss 393.9905 (mse_score: 393.9905)\n",
      "00:36    Epoch 40: train loss 392.2333 (mse_score: 392.2333)\n",
      "00:38    Epoch 45: train loss 390.9694 (mse_score: 390.9694)\n",
      "00:39    Epoch 50: train loss 389.9678 (mse_score: 389.9678)\n",
      "00:39  Finished training\n",
      "00:39  Calculating expectation for 10 estimators in ensemble\n",
      "00:39  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "00:39  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "00:39  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "00:40  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "00:40  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "00:40  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "00:40  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "00:40  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "00:40  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "00:40  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'all_tight',\n",
    "    use_tight_cuts=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal observable basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_obs = [0,1] + list(range(4,12)) + list(range(16,27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:41  Training 10 estimators in ensemble\n",
      "00:41  Training estimator 1 / 10 in ensemble\n",
      "00:41  Starting training\n",
      "00:41    Method:                 sally\n",
      "00:41    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "00:41                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "00:41    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "00:41    Method:                 sally\n",
      "00:41    Hidden layers:          (100, 100)\n",
      "00:41    Activation function:    tanh\n",
      "00:41    Batch size:             128\n",
      "00:41    Trainer:                amsgrad\n",
      "00:41    Epochs:                 50\n",
      "00:41    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "00:41    Validation split:       None\n",
      "00:41    Early stopping:         True\n",
      "00:41    Scale inputs:           True\n",
      "00:41    Shuffle labels          False\n",
      "00:41    Regularization:         None\n",
      "00:41  Loading training data\n",
      "00:41  Found 1000000 samples with 2 parameters and 27 observables\n",
      "00:41  Rescaling inputs\n",
      "00:41  Only using 21 of 27 observables\n",
      "00:41  Creating model for method sally\n",
      "00:41  Training model\n",
      "00:42    Epoch 5: train loss 3.8704 (mse_score: 3.8704)\n",
      "00:44    Epoch 10: train loss 3.6807 (mse_score: 3.6807)\n",
      "00:45    Epoch 15: train loss 3.5423 (mse_score: 3.5423)\n",
      "00:47    Epoch 20: train loss 3.4088 (mse_score: 3.4088)\n",
      "00:48    Epoch 25: train loss 3.3143 (mse_score: 3.3143)\n",
      "00:50    Epoch 30: train loss 3.2356 (mse_score: 3.2356)\n",
      "00:51    Epoch 35: train loss 3.1758 (mse_score: 3.1758)\n",
      "00:53    Epoch 40: train loss 3.1310 (mse_score: 3.1310)\n",
      "00:54    Epoch 45: train loss 3.0864 (mse_score: 3.0864)\n",
      "00:56    Epoch 50: train loss 3.0593 (mse_score: 3.0593)\n",
      "00:56  Finished training\n",
      "00:56  Training estimator 2 / 10 in ensemble\n",
      "00:56  Starting training\n",
      "00:56    Method:                 sally\n",
      "00:56    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "00:56                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "00:56    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "00:56    Method:                 sally\n",
      "00:56    Hidden layers:          (100, 100)\n",
      "00:56    Activation function:    tanh\n",
      "00:56    Batch size:             128\n",
      "00:56    Trainer:                amsgrad\n",
      "00:56    Epochs:                 50\n",
      "00:56    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "00:56    Validation split:       None\n",
      "00:56    Early stopping:         True\n",
      "00:56    Scale inputs:           True\n",
      "00:56    Shuffle labels          False\n",
      "00:56    Regularization:         None\n",
      "00:56  Loading training data\n",
      "00:56  Found 1000000 samples with 2 parameters and 27 observables\n",
      "00:56  Rescaling inputs\n",
      "00:56  Only using 21 of 27 observables\n",
      "00:56  Creating model for method sally\n",
      "00:56  Training model\n",
      "00:57    Epoch 5: train loss 4.3665 (mse_score: 4.3665)\n",
      "00:59    Epoch 10: train loss 4.1122 (mse_score: 4.1122)\n",
      "01:00    Epoch 15: train loss 3.9124 (mse_score: 3.9124)\n",
      "01:02    Epoch 20: train loss 3.7900 (mse_score: 3.7900)\n",
      "01:03    Epoch 25: train loss 3.6621 (mse_score: 3.6621)\n",
      "01:05    Epoch 30: train loss 3.5832 (mse_score: 3.5832)\n",
      "01:06    Epoch 35: train loss 3.5190 (mse_score: 3.5190)\n",
      "01:08    Epoch 40: train loss 3.4636 (mse_score: 3.4636)\n",
      "01:09    Epoch 45: train loss 3.4213 (mse_score: 3.4213)\n",
      "01:11    Epoch 50: train loss 3.3888 (mse_score: 3.3888)\n",
      "01:11  Finished training\n",
      "01:11  Training estimator 3 / 10 in ensemble\n",
      "01:11  Starting training\n",
      "01:11    Method:                 sally\n",
      "01:11    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "01:11                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "01:11    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "01:11    Method:                 sally\n",
      "01:11    Hidden layers:          (100, 100)\n",
      "01:11    Activation function:    tanh\n",
      "01:11    Batch size:             128\n",
      "01:11    Trainer:                amsgrad\n",
      "01:11    Epochs:                 50\n",
      "01:11    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "01:11    Validation split:       None\n",
      "01:11    Early stopping:         True\n",
      "01:11    Scale inputs:           True\n",
      "01:11    Shuffle labels          False\n",
      "01:11    Regularization:         None\n",
      "01:11  Loading training data\n",
      "01:11  Found 1000000 samples with 2 parameters and 27 observables\n",
      "01:11  Rescaling inputs\n",
      "01:11  Only using 21 of 27 observables\n",
      "01:11  Creating model for method sally\n",
      "01:11  Training model\n",
      "01:13    Epoch 5: train loss 3.6358 (mse_score: 3.6358)\n",
      "01:14    Epoch 10: train loss 3.4967 (mse_score: 3.4967)\n",
      "01:16    Epoch 15: train loss 3.3469 (mse_score: 3.3469)\n",
      "01:17    Epoch 20: train loss 3.2376 (mse_score: 3.2376)\n",
      "01:19    Epoch 25: train loss 3.1448 (mse_score: 3.1448)\n",
      "01:20    Epoch 30: train loss 3.0799 (mse_score: 3.0799)\n",
      "01:21    Epoch 35: train loss 3.0258 (mse_score: 3.0258)\n",
      "01:23    Epoch 40: train loss 2.9845 (mse_score: 2.9845)\n",
      "01:24    Epoch 45: train loss 2.9521 (mse_score: 2.9521)\n",
      "01:26    Epoch 50: train loss 2.9244 (mse_score: 2.9244)\n",
      "01:26  Finished training\n",
      "01:26  Training estimator 4 / 10 in ensemble\n",
      "01:26  Starting training\n",
      "01:26    Method:                 sally\n",
      "01:26    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "01:26                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "01:26    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "01:26    Method:                 sally\n",
      "01:26    Hidden layers:          (100, 100)\n",
      "01:26    Activation function:    tanh\n",
      "01:26    Batch size:             128\n",
      "01:26    Trainer:                amsgrad\n",
      "01:26    Epochs:                 50\n",
      "01:26    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "01:26    Validation split:       None\n",
      "01:26    Early stopping:         True\n",
      "01:26    Scale inputs:           True\n",
      "01:26    Shuffle labels          False\n",
      "01:26    Regularization:         None\n",
      "01:26  Loading training data\n",
      "01:26  Found 1000000 samples with 2 parameters and 27 observables\n",
      "01:26  Rescaling inputs\n",
      "01:26  Only using 21 of 27 observables\n",
      "01:26  Creating model for method sally\n",
      "01:26  Training model\n",
      "01:28    Epoch 5: train loss 3.7684 (mse_score: 3.7684)\n",
      "01:29    Epoch 10: train loss 3.6101 (mse_score: 3.6101)\n",
      "01:31    Epoch 15: train loss 3.4831 (mse_score: 3.4831)\n",
      "01:32    Epoch 20: train loss 3.3758 (mse_score: 3.3758)\n",
      "01:34    Epoch 25: train loss 3.2992 (mse_score: 3.2992)\n",
      "01:35    Epoch 30: train loss 3.2438 (mse_score: 3.2438)\n",
      "01:37    Epoch 35: train loss 3.1941 (mse_score: 3.1941)\n",
      "01:38    Epoch 40: train loss 3.1550 (mse_score: 3.1550)\n",
      "01:40    Epoch 45: train loss 3.1204 (mse_score: 3.1204)\n",
      "01:41    Epoch 50: train loss 3.0987 (mse_score: 3.0987)\n",
      "01:41  Finished training\n",
      "01:41  Training estimator 5 / 10 in ensemble\n",
      "01:41  Starting training\n",
      "01:41    Method:                 sally\n",
      "01:41    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "01:41                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "01:41    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "01:41    Method:                 sally\n",
      "01:41    Hidden layers:          (100, 100)\n",
      "01:41    Activation function:    tanh\n",
      "01:41    Batch size:             128\n",
      "01:41    Trainer:                amsgrad\n",
      "01:41    Epochs:                 50\n",
      "01:41    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "01:41    Validation split:       None\n",
      "01:41    Early stopping:         True\n",
      "01:41    Scale inputs:           True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:41    Shuffle labels          False\n",
      "01:41    Regularization:         None\n",
      "01:41  Loading training data\n",
      "01:41  Found 1000000 samples with 2 parameters and 27 observables\n",
      "01:41  Rescaling inputs\n",
      "01:41  Only using 21 of 27 observables\n",
      "01:41  Creating model for method sally\n",
      "01:41  Training model\n",
      "01:43    Epoch 5: train loss 3.3806 (mse_score: 3.3806)\n",
      "01:44    Epoch 10: train loss 3.2256 (mse_score: 3.2256)\n",
      "01:46    Epoch 15: train loss 3.0871 (mse_score: 3.0871)\n",
      "01:47    Epoch 20: train loss 2.9972 (mse_score: 2.9972)\n",
      "01:49    Epoch 25: train loss 2.9390 (mse_score: 2.9390)\n",
      "01:50    Epoch 30: train loss 2.8811 (mse_score: 2.8811)\n",
      "01:52    Epoch 35: train loss 2.8422 (mse_score: 2.8422)\n",
      "01:53    Epoch 40: train loss 2.8104 (mse_score: 2.8104)\n",
      "01:55    Epoch 45: train loss 2.7860 (mse_score: 2.7860)\n",
      "01:56    Epoch 50: train loss 2.7628 (mse_score: 2.7628)\n",
      "01:56  Finished training\n",
      "01:56  Training estimator 6 / 10 in ensemble\n",
      "01:56  Starting training\n",
      "01:56    Method:                 sally\n",
      "01:56    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "01:56                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "01:56    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "01:56    Method:                 sally\n",
      "01:56    Hidden layers:          (100, 100)\n",
      "01:56    Activation function:    tanh\n",
      "01:56    Batch size:             128\n",
      "01:56    Trainer:                amsgrad\n",
      "01:56    Epochs:                 50\n",
      "01:56    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "01:56    Validation split:       None\n",
      "01:56    Early stopping:         True\n",
      "01:56    Scale inputs:           True\n",
      "01:56    Shuffle labels          False\n",
      "01:56    Regularization:         None\n",
      "01:56  Loading training data\n",
      "01:56  Found 1000000 samples with 2 parameters and 27 observables\n",
      "01:56  Rescaling inputs\n",
      "01:56  Only using 21 of 27 observables\n",
      "01:56  Creating model for method sally\n",
      "01:56  Training model\n",
      "01:58    Epoch 5: train loss 4.1128 (mse_score: 4.1128)\n",
      "01:59    Epoch 10: train loss 3.9136 (mse_score: 3.9136)\n",
      "02:01    Epoch 15: train loss 3.8049 (mse_score: 3.8049)\n",
      "02:02    Epoch 20: train loss 3.6850 (mse_score: 3.6850)\n",
      "02:04    Epoch 25: train loss 3.5919 (mse_score: 3.5919)\n",
      "02:05    Epoch 30: train loss 3.5294 (mse_score: 3.5294)\n",
      "02:07    Epoch 35: train loss 3.4723 (mse_score: 3.4723)\n",
      "02:08    Epoch 40: train loss 3.4296 (mse_score: 3.4296)\n",
      "02:10    Epoch 45: train loss 3.3954 (mse_score: 3.3954)\n",
      "02:11    Epoch 50: train loss 3.3696 (mse_score: 3.3696)\n",
      "02:11  Finished training\n",
      "02:11  Training estimator 7 / 10 in ensemble\n",
      "02:11  Starting training\n",
      "02:11    Method:                 sally\n",
      "02:11    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "02:11                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "02:11    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "02:11    Method:                 sally\n",
      "02:11    Hidden layers:          (100, 100)\n",
      "02:11    Activation function:    tanh\n",
      "02:11    Batch size:             128\n",
      "02:11    Trainer:                amsgrad\n",
      "02:11    Epochs:                 50\n",
      "02:11    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "02:11    Validation split:       None\n",
      "02:11    Early stopping:         True\n",
      "02:11    Scale inputs:           True\n",
      "02:11    Shuffle labels          False\n",
      "02:11    Regularization:         None\n",
      "02:11  Loading training data\n",
      "02:11  Found 1000000 samples with 2 parameters and 27 observables\n",
      "02:11  Rescaling inputs\n",
      "02:11  Only using 21 of 27 observables\n",
      "02:11  Creating model for method sally\n",
      "02:11  Training model\n",
      "02:13    Epoch 5: train loss 7.3824 (mse_score: 7.3824)\n",
      "02:15    Epoch 10: train loss 7.1593 (mse_score: 7.1593)\n",
      "02:16    Epoch 15: train loss 7.0062 (mse_score: 7.0062)\n",
      "02:17    Epoch 20: train loss 6.8780 (mse_score: 6.8780)\n",
      "02:19    Epoch 25: train loss 6.7786 (mse_score: 6.7786)\n",
      "02:20    Epoch 30: train loss 6.6929 (mse_score: 6.6929)\n",
      "02:22    Epoch 35: train loss 6.6256 (mse_score: 6.6256)\n",
      "02:23    Epoch 40: train loss 6.5739 (mse_score: 6.5739)\n",
      "02:25    Epoch 45: train loss 6.5306 (mse_score: 6.5306)\n",
      "02:26    Epoch 50: train loss 6.4963 (mse_score: 6.4963)\n",
      "02:26  Finished training\n",
      "02:26  Training estimator 8 / 10 in ensemble\n",
      "02:26  Starting training\n",
      "02:26    Method:                 sally\n",
      "02:26    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "02:26                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "02:26    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "02:26    Method:                 sally\n",
      "02:26    Hidden layers:          (100, 100)\n",
      "02:26    Activation function:    tanh\n",
      "02:26    Batch size:             128\n",
      "02:26    Trainer:                amsgrad\n",
      "02:26    Epochs:                 50\n",
      "02:26    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "02:26    Validation split:       None\n",
      "02:26    Early stopping:         True\n",
      "02:26    Scale inputs:           True\n",
      "02:26    Shuffle labels          False\n",
      "02:26    Regularization:         None\n",
      "02:26  Loading training data\n",
      "02:26  Found 1000000 samples with 2 parameters and 27 observables\n",
      "02:26  Rescaling inputs\n",
      "02:26  Only using 21 of 27 observables\n",
      "02:26  Creating model for method sally\n",
      "02:26  Training model\n",
      "02:28    Epoch 5: train loss 3.5328 (mse_score: 3.5328)\n",
      "02:30    Epoch 10: train loss 3.3306 (mse_score: 3.3306)\n",
      "02:31    Epoch 15: train loss 3.1596 (mse_score: 3.1596)\n",
      "02:33    Epoch 20: train loss 3.0539 (mse_score: 3.0539)\n",
      "02:34    Epoch 25: train loss 2.9628 (mse_score: 2.9628)\n",
      "02:36    Epoch 30: train loss 2.8939 (mse_score: 2.8939)\n",
      "02:37    Epoch 35: train loss 2.8400 (mse_score: 2.8400)\n",
      "02:39    Epoch 40: train loss 2.7891 (mse_score: 2.7891)\n",
      "02:40    Epoch 45: train loss 2.7580 (mse_score: 2.7580)\n",
      "02:42    Epoch 50: train loss 2.7297 (mse_score: 2.7297)\n",
      "02:42  Finished training\n",
      "02:42  Training estimator 9 / 10 in ensemble\n",
      "02:42  Starting training\n",
      "02:42    Method:                 sally\n",
      "02:42    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "02:42                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "02:42    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "02:42    Method:                 sally\n",
      "02:42    Hidden layers:          (100, 100)\n",
      "02:42    Activation function:    tanh\n",
      "02:42    Batch size:             128\n",
      "02:42    Trainer:                amsgrad\n",
      "02:42    Epochs:                 50\n",
      "02:42    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "02:42    Validation split:       None\n",
      "02:42    Early stopping:         True\n",
      "02:42    Scale inputs:           True\n",
      "02:42    Shuffle labels          False\n",
      "02:42    Regularization:         None\n",
      "02:42  Loading training data\n",
      "02:42  Found 1000000 samples with 2 parameters and 27 observables\n",
      "02:42  Rescaling inputs\n",
      "02:42  Only using 21 of 27 observables\n",
      "02:42  Creating model for method sally\n",
      "02:42  Training model\n",
      "02:43    Epoch 5: train loss 3.7742 (mse_score: 3.7742)\n",
      "02:45    Epoch 10: train loss 3.6315 (mse_score: 3.6315)\n",
      "02:46    Epoch 15: train loss 3.4855 (mse_score: 3.4855)\n",
      "02:48    Epoch 20: train loss 3.3743 (mse_score: 3.3743)\n",
      "02:49    Epoch 25: train loss 3.3092 (mse_score: 3.3092)\n",
      "02:51    Epoch 30: train loss 3.2538 (mse_score: 3.2538)\n",
      "02:52    Epoch 35: train loss 3.2075 (mse_score: 3.2075)\n",
      "02:54    Epoch 40: train loss 3.1704 (mse_score: 3.1704)\n",
      "02:55    Epoch 45: train loss 3.1450 (mse_score: 3.1450)\n",
      "02:57    Epoch 50: train loss 3.1212 (mse_score: 3.1212)\n",
      "02:57  Finished training\n",
      "02:57  Training estimator 10 / 10 in ensemble\n",
      "02:57  Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:57    Method:                 sally\n",
      "02:57    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "02:57                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "02:57    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "02:57    Method:                 sally\n",
      "02:57    Hidden layers:          (100, 100)\n",
      "02:57    Activation function:    tanh\n",
      "02:57    Batch size:             128\n",
      "02:57    Trainer:                amsgrad\n",
      "02:57    Epochs:                 50\n",
      "02:57    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "02:57    Validation split:       None\n",
      "02:57    Early stopping:         True\n",
      "02:57    Scale inputs:           True\n",
      "02:57    Shuffle labels          False\n",
      "02:57    Regularization:         None\n",
      "02:57  Loading training data\n",
      "02:57  Found 1000000 samples with 2 parameters and 27 observables\n",
      "02:57  Rescaling inputs\n",
      "02:57  Only using 21 of 27 observables\n",
      "02:57  Creating model for method sally\n",
      "02:57  Training model\n",
      "02:58    Epoch 5: train loss 6.8092 (mse_score: 6.8092)\n",
      "03:00    Epoch 10: train loss 6.5435 (mse_score: 6.5435)\n",
      "03:01    Epoch 15: train loss 6.4057 (mse_score: 6.4057)\n",
      "03:03    Epoch 20: train loss 6.2800 (mse_score: 6.2800)\n",
      "03:04    Epoch 25: train loss 6.1508 (mse_score: 6.1508)\n",
      "03:06    Epoch 30: train loss 6.0565 (mse_score: 6.0565)\n",
      "03:07    Epoch 35: train loss 5.9759 (mse_score: 5.9759)\n",
      "03:09    Epoch 40: train loss 5.9132 (mse_score: 5.9132)\n",
      "03:10    Epoch 45: train loss 5.8680 (mse_score: 5.8680)\n",
      "03:12    Epoch 50: train loss 5.8299 (mse_score: 5.8299)\n",
      "03:12  Finished training\n",
      "03:12  Calculating expectation for 10 estimators in ensemble\n",
      "03:12  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "03:12  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "03:12  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "03:12  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "03:12  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "03:13  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "03:13  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "03:13  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "03:13  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "03:13  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'minimal',\n",
    "    use_tight_cuts=False,\n",
    "    features=[min_obs for _ in range(n_estimators)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:13  Training 10 estimators in ensemble\n",
      "03:13  Training estimator 1 / 10 in ensemble\n",
      "03:13  Starting training\n",
      "03:13    Method:                 sally\n",
      "03:13    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "03:13                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "03:13    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "03:13    Method:                 sally\n",
      "03:13    Hidden layers:          (100, 100)\n",
      "03:13    Activation function:    tanh\n",
      "03:13    Batch size:             128\n",
      "03:13    Trainer:                amsgrad\n",
      "03:13    Epochs:                 50\n",
      "03:13    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "03:13    Validation split:       None\n",
      "03:13    Early stopping:         True\n",
      "03:13    Scale inputs:           True\n",
      "03:13    Shuffle labels          False\n",
      "03:13    Regularization:         None\n",
      "03:13  Loading training data\n",
      "03:13  Found 1000000 samples with 2 parameters and 27 observables\n",
      "03:13  Rescaling inputs\n",
      "03:13  Only using 21 of 27 observables\n",
      "03:13  Creating model for method sally\n",
      "03:13  Training model\n",
      "03:15    Epoch 5: train loss 411.1915 (mse_score: 411.1915)\n",
      "03:17    Epoch 10: train loss 403.8330 (mse_score: 403.8330)\n",
      "03:18    Epoch 15: train loss 398.1272 (mse_score: 398.1272)\n",
      "03:20    Epoch 20: train loss 394.0682 (mse_score: 394.0682)\n",
      "03:21    Epoch 25: train loss 391.1445 (mse_score: 391.1445)\n",
      "03:23    Epoch 30: train loss 388.9263 (mse_score: 388.9263)\n",
      "03:24    Epoch 35: train loss 387.5737 (mse_score: 387.5737)\n",
      "03:26    Epoch 40: train loss 386.3279 (mse_score: 386.3279)\n",
      "03:27    Epoch 45: train loss 385.3415 (mse_score: 385.3415)\n",
      "03:29    Epoch 50: train loss 384.3754 (mse_score: 384.3754)\n",
      "03:29  Finished training\n",
      "03:29  Training estimator 2 / 10 in ensemble\n",
      "03:29  Starting training\n",
      "03:29    Method:                 sally\n",
      "03:29    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "03:29                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "03:29    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "03:29    Method:                 sally\n",
      "03:29    Hidden layers:          (100, 100)\n",
      "03:29    Activation function:    tanh\n",
      "03:29    Batch size:             128\n",
      "03:29    Trainer:                amsgrad\n",
      "03:29    Epochs:                 50\n",
      "03:29    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "03:29    Validation split:       None\n",
      "03:29    Early stopping:         True\n",
      "03:29    Scale inputs:           True\n",
      "03:29    Shuffle labels          False\n",
      "03:29    Regularization:         None\n",
      "03:29  Loading training data\n",
      "03:29  Found 1000000 samples with 2 parameters and 27 observables\n",
      "03:29  Rescaling inputs\n",
      "03:29  Only using 21 of 27 observables\n",
      "03:29  Creating model for method sally\n",
      "03:29  Training model\n",
      "03:30    Epoch 5: train loss 366.8315 (mse_score: 366.8315)\n",
      "03:32    Epoch 10: train loss 358.5600 (mse_score: 358.5600)\n",
      "03:33    Epoch 15: train loss 353.5374 (mse_score: 353.5374)\n",
      "03:35    Epoch 20: train loss 349.9459 (mse_score: 349.9459)\n",
      "03:36    Epoch 25: train loss 347.3467 (mse_score: 347.3467)\n",
      "03:38    Epoch 30: train loss 345.2579 (mse_score: 345.2579)\n",
      "03:39    Epoch 35: train loss 343.5452 (mse_score: 343.5452)\n",
      "03:41    Epoch 40: train loss 342.4758 (mse_score: 342.4758)\n",
      "03:42    Epoch 45: train loss 341.3073 (mse_score: 341.3073)\n",
      "03:44    Epoch 50: train loss 340.5318 (mse_score: 340.5318)\n",
      "03:44  Finished training\n",
      "03:44  Training estimator 3 / 10 in ensemble\n",
      "03:44  Starting training\n",
      "03:44    Method:                 sally\n",
      "03:44    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "03:44                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "03:44    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "03:44    Method:                 sally\n",
      "03:44    Hidden layers:          (100, 100)\n",
      "03:44    Activation function:    tanh\n",
      "03:44    Batch size:             128\n",
      "03:44    Trainer:                amsgrad\n",
      "03:44    Epochs:                 50\n",
      "03:44    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "03:44    Validation split:       None\n",
      "03:44    Early stopping:         True\n",
      "03:44    Scale inputs:           True\n",
      "03:44    Shuffle labels          False\n",
      "03:44    Regularization:         None\n",
      "03:44  Loading training data\n",
      "03:44  Found 1000000 samples with 2 parameters and 27 observables\n",
      "03:44  Rescaling inputs\n",
      "03:44  Only using 21 of 27 observables\n",
      "03:44  Creating model for method sally\n",
      "03:44  Training model\n",
      "03:45    Epoch 5: train loss 430.9528 (mse_score: 430.9528)\n",
      "03:47    Epoch 10: train loss 422.1733 (mse_score: 422.1733)\n",
      "03:48    Epoch 15: train loss 416.0356 (mse_score: 416.0356)\n",
      "03:50    Epoch 20: train loss 411.2257 (mse_score: 411.2257)\n",
      "03:51    Epoch 25: train loss 408.1148 (mse_score: 408.1148)\n",
      "03:53    Epoch 30: train loss 405.8052 (mse_score: 405.8052)\n",
      "03:54    Epoch 35: train loss 404.0182 (mse_score: 404.0182)\n",
      "03:56    Epoch 40: train loss 402.4570 (mse_score: 402.4570)\n",
      "03:57    Epoch 45: train loss 401.2399 (mse_score: 401.2399)\n",
      "03:59    Epoch 50: train loss 400.3188 (mse_score: 400.3188)\n",
      "03:59  Finished training\n",
      "03:59  Training estimator 4 / 10 in ensemble\n",
      "03:59  Starting training\n",
      "03:59    Method:                 sally\n",
      "03:59    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_3.npy\n",
      "03:59                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_3.npy\n",
      "03:59    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "03:59    Method:                 sally\n",
      "03:59    Hidden layers:          (100, 100)\n",
      "03:59    Activation function:    tanh\n",
      "03:59    Batch size:             128\n",
      "03:59    Trainer:                amsgrad\n",
      "03:59    Epochs:                 50\n",
      "03:59    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "03:59    Validation split:       None\n",
      "03:59    Early stopping:         True\n",
      "03:59    Scale inputs:           True\n",
      "03:59    Shuffle labels          False\n",
      "03:59    Regularization:         None\n",
      "03:59  Loading training data\n",
      "03:59  Found 1000000 samples with 2 parameters and 27 observables\n",
      "03:59  Rescaling inputs\n",
      "03:59  Only using 21 of 27 observables\n",
      "03:59  Creating model for method sally\n",
      "03:59  Training model\n",
      "04:01    Epoch 5: train loss 382.2286 (mse_score: 382.2286)\n",
      "04:02    Epoch 10: train loss 374.0533 (mse_score: 374.0533)\n",
      "04:04    Epoch 15: train loss 369.1396 (mse_score: 369.1396)\n",
      "04:05    Epoch 20: train loss 365.1977 (mse_score: 365.1977)\n",
      "04:07    Epoch 25: train loss 362.4231 (mse_score: 362.4231)\n",
      "04:08    Epoch 30: train loss 360.4302 (mse_score: 360.4302)\n",
      "04:10    Epoch 35: train loss 358.8022 (mse_score: 358.8022)\n",
      "04:11    Epoch 40: train loss 357.4908 (mse_score: 357.4908)\n",
      "04:13    Epoch 45: train loss 356.4292 (mse_score: 356.4292)\n",
      "04:14    Epoch 50: train loss 355.6483 (mse_score: 355.6483)\n",
      "04:14  Finished training\n",
      "04:14  Training estimator 5 / 10 in ensemble\n",
      "04:14  Starting training\n",
      "04:14    Method:                 sally\n",
      "04:14    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_4.npy\n",
      "04:14                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_4.npy\n",
      "04:14    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "04:14    Method:                 sally\n",
      "04:14    Hidden layers:          (100, 100)\n",
      "04:14    Activation function:    tanh\n",
      "04:14    Batch size:             128\n",
      "04:14    Trainer:                amsgrad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:14    Epochs:                 50\n",
      "04:14    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "04:14    Validation split:       None\n",
      "04:14    Early stopping:         True\n",
      "04:14    Scale inputs:           True\n",
      "04:14    Shuffle labels          False\n",
      "04:14    Regularization:         None\n",
      "04:14  Loading training data\n",
      "04:14  Found 1000000 samples with 2 parameters and 27 observables\n",
      "04:14  Rescaling inputs\n",
      "04:14  Only using 21 of 27 observables\n",
      "04:14  Creating model for method sally\n",
      "04:14  Training model\n",
      "04:16    Epoch 5: train loss 350.6380 (mse_score: 350.6380)\n",
      "04:17    Epoch 10: train loss 343.6671 (mse_score: 343.6671)\n",
      "04:19    Epoch 15: train loss 338.3468 (mse_score: 338.3468)\n",
      "04:20    Epoch 20: train loss 334.7338 (mse_score: 334.7338)\n",
      "04:22    Epoch 25: train loss 332.1687 (mse_score: 332.1687)\n",
      "04:23    Epoch 30: train loss 330.1940 (mse_score: 330.1940)\n",
      "04:25    Epoch 35: train loss 328.6740 (mse_score: 328.6740)\n",
      "04:26    Epoch 40: train loss 327.4954 (mse_score: 327.4954)\n",
      "04:28    Epoch 45: train loss 326.5582 (mse_score: 326.5582)\n",
      "04:29    Epoch 50: train loss 325.8457 (mse_score: 325.8457)\n",
      "04:29  Finished training\n",
      "04:29  Training estimator 6 / 10 in ensemble\n",
      "04:29  Starting training\n",
      "04:29    Method:                 sally\n",
      "04:29    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_5.npy\n",
      "04:29                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_5.npy\n",
      "04:29    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "04:29    Method:                 sally\n",
      "04:29    Hidden layers:          (100, 100)\n",
      "04:29    Activation function:    tanh\n",
      "04:29    Batch size:             128\n",
      "04:29    Trainer:                amsgrad\n",
      "04:29    Epochs:                 50\n",
      "04:29    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "04:29    Validation split:       None\n",
      "04:29    Early stopping:         True\n",
      "04:29    Scale inputs:           True\n",
      "04:29    Shuffle labels          False\n",
      "04:29    Regularization:         None\n",
      "04:29  Loading training data\n",
      "04:29  Found 1000000 samples with 2 parameters and 27 observables\n",
      "04:29  Rescaling inputs\n",
      "04:29  Only using 21 of 27 observables\n",
      "04:29  Creating model for method sally\n",
      "04:29  Training model\n",
      "04:31    Epoch 5: train loss 400.8777 (mse_score: 400.8777)\n",
      "04:33    Epoch 10: train loss 392.5809 (mse_score: 392.5809)\n",
      "04:34    Epoch 15: train loss 386.4620 (mse_score: 386.4620)\n",
      "04:36    Epoch 20: train loss 383.0007 (mse_score: 383.0007)\n",
      "04:37    Epoch 25: train loss 380.2174 (mse_score: 380.2174)\n",
      "04:38    Epoch 30: train loss 378.2867 (mse_score: 378.2867)\n",
      "04:40    Epoch 35: train loss 376.6570 (mse_score: 376.6570)\n",
      "04:41    Epoch 40: train loss 375.5379 (mse_score: 375.5379)\n",
      "04:43    Epoch 45: train loss 374.5569 (mse_score: 374.5569)\n",
      "04:44    Epoch 50: train loss 373.8672 (mse_score: 373.8672)\n",
      "04:44  Finished training\n",
      "04:44  Training estimator 7 / 10 in ensemble\n",
      "04:44  Starting training\n",
      "04:44    Method:                 sally\n",
      "04:44    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_6.npy\n",
      "04:44                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_6.npy\n",
      "04:44    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "04:44    Method:                 sally\n",
      "04:44    Hidden layers:          (100, 100)\n",
      "04:44    Activation function:    tanh\n",
      "04:44    Batch size:             128\n",
      "04:44    Trainer:                amsgrad\n",
      "04:44    Epochs:                 50\n",
      "04:44    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "04:44    Validation split:       None\n",
      "04:44    Early stopping:         True\n",
      "04:44    Scale inputs:           True\n",
      "04:44    Shuffle labels          False\n",
      "04:44    Regularization:         None\n",
      "04:44  Loading training data\n",
      "04:44  Found 1000000 samples with 2 parameters and 27 observables\n",
      "04:44  Rescaling inputs\n",
      "04:44  Only using 21 of 27 observables\n",
      "04:44  Creating model for method sally\n",
      "04:44  Training model\n",
      "04:46    Epoch 5: train loss 396.6713 (mse_score: 396.6713)\n",
      "04:48    Epoch 10: train loss 388.6237 (mse_score: 388.6237)\n",
      "04:49    Epoch 15: train loss 382.1836 (mse_score: 382.1836)\n",
      "04:51    Epoch 20: train loss 378.2383 (mse_score: 378.2383)\n",
      "04:52    Epoch 25: train loss 374.7246 (mse_score: 374.7246)\n",
      "04:54    Epoch 30: train loss 372.5394 (mse_score: 372.5394)\n",
      "04:55    Epoch 35: train loss 370.7223 (mse_score: 370.7223)\n",
      "04:57    Epoch 40: train loss 369.2121 (mse_score: 369.2121)\n",
      "04:58    Epoch 45: train loss 368.1118 (mse_score: 368.1118)\n",
      "05:00    Epoch 50: train loss 367.2094 (mse_score: 367.2094)\n",
      "05:00  Finished training\n",
      "05:00  Training estimator 8 / 10 in ensemble\n",
      "05:00  Starting training\n",
      "05:00    Method:                 sally\n",
      "05:00    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_7.npy\n",
      "05:00                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_7.npy\n",
      "05:00    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "05:00    Method:                 sally\n",
      "05:00    Hidden layers:          (100, 100)\n",
      "05:00    Activation function:    tanh\n",
      "05:00    Batch size:             128\n",
      "05:00    Trainer:                amsgrad\n",
      "05:00    Epochs:                 50\n",
      "05:00    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "05:00    Validation split:       None\n",
      "05:00    Early stopping:         True\n",
      "05:00    Scale inputs:           True\n",
      "05:00    Shuffle labels          False\n",
      "05:00    Regularization:         None\n",
      "05:00  Loading training data\n",
      "05:00  Found 1000000 samples with 2 parameters and 27 observables\n",
      "05:00  Rescaling inputs\n",
      "05:00  Only using 21 of 27 observables\n",
      "05:00  Creating model for method sally\n",
      "05:00  Training model\n",
      "05:01    Epoch 5: train loss 389.4164 (mse_score: 389.4164)\n",
      "05:03    Epoch 10: train loss 381.9266 (mse_score: 381.9266)\n",
      "05:04    Epoch 15: train loss 375.9711 (mse_score: 375.9711)\n",
      "05:06    Epoch 20: train loss 371.8125 (mse_score: 371.8125)\n",
      "05:07    Epoch 25: train loss 369.0748 (mse_score: 369.0748)\n",
      "05:09    Epoch 30: train loss 366.8419 (mse_score: 366.8419)\n",
      "05:10    Epoch 35: train loss 365.0260 (mse_score: 365.0260)\n",
      "05:12    Epoch 40: train loss 363.7926 (mse_score: 363.7926)\n",
      "05:13    Epoch 45: train loss 362.6525 (mse_score: 362.6525)\n",
      "05:15    Epoch 50: train loss 361.8454 (mse_score: 361.8454)\n",
      "05:15  Finished training\n",
      "05:15  Training estimator 9 / 10 in ensemble\n",
      "05:15  Starting training\n",
      "05:15    Method:                 sally\n",
      "05:15    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_8.npy\n",
      "05:15                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_8.npy\n",
      "05:15    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "05:15    Method:                 sally\n",
      "05:15    Hidden layers:          (100, 100)\n",
      "05:15    Activation function:    tanh\n",
      "05:15    Batch size:             128\n",
      "05:15    Trainer:                amsgrad\n",
      "05:15    Epochs:                 50\n",
      "05:15    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "05:15    Validation split:       None\n",
      "05:15    Early stopping:         True\n",
      "05:15    Scale inputs:           True\n",
      "05:15    Shuffle labels          False\n",
      "05:15    Regularization:         None\n",
      "05:15  Loading training data\n",
      "05:15  Found 1000000 samples with 2 parameters and 27 observables\n",
      "05:15  Rescaling inputs\n",
      "05:15  Only using 21 of 27 observables\n",
      "05:15  Creating model for method sally\n",
      "05:15  Training model\n",
      "05:17    Epoch 5: train loss 373.2886 (mse_score: 373.2886)\n",
      "05:18    Epoch 10: train loss 366.1916 (mse_score: 366.1916)\n",
      "05:20    Epoch 15: train loss 362.2668 (mse_score: 362.2668)\n",
      "05:21    Epoch 20: train loss 357.9995 (mse_score: 357.9995)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:23    Epoch 25: train loss 355.3250 (mse_score: 355.3250)\n",
      "05:24    Epoch 30: train loss 353.3730 (mse_score: 353.3730)\n",
      "05:26    Epoch 35: train loss 351.8657 (mse_score: 351.8657)\n",
      "05:27    Epoch 40: train loss 350.7304 (mse_score: 350.7304)\n",
      "05:29    Epoch 45: train loss 349.8270 (mse_score: 349.8270)\n",
      "05:30    Epoch 50: train loss 349.0436 (mse_score: 349.0436)\n",
      "05:30  Finished training\n",
      "05:30  Training estimator 10 / 10 in ensemble\n",
      "05:30  Starting training\n",
      "05:30    Method:                 sally\n",
      "05:30    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_9.npy\n",
      "05:30                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_9.npy\n",
      "05:30    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "05:30    Method:                 sally\n",
      "05:30    Hidden layers:          (100, 100)\n",
      "05:30    Activation function:    tanh\n",
      "05:30    Batch size:             128\n",
      "05:30    Trainer:                amsgrad\n",
      "05:30    Epochs:                 50\n",
      "05:30    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "05:30    Validation split:       None\n",
      "05:30    Early stopping:         True\n",
      "05:30    Scale inputs:           True\n",
      "05:30    Shuffle labels          False\n",
      "05:30    Regularization:         None\n",
      "05:30  Loading training data\n",
      "05:30  Found 1000000 samples with 2 parameters and 27 observables\n",
      "05:30  Rescaling inputs\n",
      "05:30  Only using 21 of 27 observables\n",
      "05:30  Creating model for method sally\n",
      "05:30  Training model\n",
      "05:32    Epoch 5: train loss 424.6037 (mse_score: 424.6037)\n",
      "05:33    Epoch 10: train loss 414.6385 (mse_score: 414.6385)\n",
      "05:35    Epoch 15: train loss 409.3819 (mse_score: 409.3819)\n",
      "05:36    Epoch 20: train loss 403.8199 (mse_score: 403.8199)\n",
      "05:38    Epoch 25: train loss 400.8121 (mse_score: 400.8121)\n",
      "05:39    Epoch 30: train loss 398.4310 (mse_score: 398.4310)\n",
      "05:41    Epoch 35: train loss 396.5671 (mse_score: 396.5671)\n",
      "05:42    Epoch 40: train loss 395.1834 (mse_score: 395.1834)\n",
      "05:44    Epoch 45: train loss 394.1358 (mse_score: 394.1358)\n",
      "05:45    Epoch 50: train loss 393.3444 (mse_score: 393.3444)\n",
      "05:45  Finished training\n",
      "05:45  Calculating expectation for 10 estimators in ensemble\n",
      "05:45  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "05:45  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "05:46  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "05:46  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "05:46  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "05:46  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "05:46  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "05:46  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "05:46  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "05:47  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[min_obs for _ in range(10)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just resurrection phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:47  Training 10 estimators in ensemble\n",
      "05:47  Training estimator 1 / 10 in ensemble\n",
      "05:47  Starting training\n",
      "05:47    Method:                 sally\n",
      "05:47    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "05:47                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "05:47    Features:               [26]\n",
      "05:47    Method:                 sally\n",
      "05:47    Hidden layers:          (100, 100)\n",
      "05:47    Activation function:    tanh\n",
      "05:47    Batch size:             128\n",
      "05:47    Trainer:                amsgrad\n",
      "05:47    Epochs:                 50\n",
      "05:47    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "05:47    Validation split:       None\n",
      "05:47    Early stopping:         True\n",
      "05:47    Scale inputs:           True\n",
      "05:47    Shuffle labels          False\n",
      "05:47    Regularization:         None\n",
      "05:47  Loading training data\n",
      "05:47  Found 1000000 samples with 2 parameters and 27 observables\n",
      "05:47  Rescaling inputs\n",
      "05:47  Only using 1 of 27 observables\n",
      "05:47  Creating model for method sally\n",
      "05:47  Training model\n",
      "05:48    Epoch 5: train loss 482.5078 (mse_score: 482.5078)\n",
      "05:50    Epoch 10: train loss 482.4466 (mse_score: 482.4466)\n",
      "05:51    Epoch 15: train loss 482.4174 (mse_score: 482.4174)\n",
      "05:53    Epoch 20: train loss 482.3774 (mse_score: 482.3774)\n",
      "05:54    Epoch 25: train loss 482.3680 (mse_score: 482.3680)\n",
      "05:56    Epoch 30: train loss 482.3581 (mse_score: 482.3581)\n",
      "05:57    Epoch 35: train loss 482.3447 (mse_score: 482.3447)\n",
      "05:59    Epoch 40: train loss 482.3747 (mse_score: 482.3747)\n",
      "06:00    Epoch 45: train loss 482.3306 (mse_score: 482.3306)\n",
      "06:01    Epoch 50: train loss 482.3427 (mse_score: 482.3427)\n",
      "06:01  Finished training\n",
      "06:01  Training estimator 2 / 10 in ensemble\n",
      "06:01  Starting training\n",
      "06:01    Method:                 sally\n",
      "06:01    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "06:01                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "06:01    Features:               [26]\n",
      "06:01    Method:                 sally\n",
      "06:01    Hidden layers:          (100, 100)\n",
      "06:01    Activation function:    tanh\n",
      "06:01    Batch size:             128\n",
      "06:01    Trainer:                amsgrad\n",
      "06:01    Epochs:                 50\n",
      "06:01    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "06:01    Validation split:       None\n",
      "06:01    Early stopping:         True\n",
      "06:01    Scale inputs:           True\n",
      "06:01    Shuffle labels          False\n",
      "06:01    Regularization:         None\n",
      "06:01  Loading training data\n",
      "06:01  Found 1000000 samples with 2 parameters and 27 observables\n",
      "06:01  Rescaling inputs\n",
      "06:01  Only using 1 of 27 observables\n",
      "06:01  Creating model for method sally\n",
      "06:01  Training model\n",
      "06:03    Epoch 5: train loss 438.1708 (mse_score: 438.1708)\n",
      "06:05    Epoch 10: train loss 437.9806 (mse_score: 437.9806)\n",
      "06:06    Epoch 15: train loss 437.9390 (mse_score: 437.9390)\n",
      "06:07    Epoch 20: train loss 437.8935 (mse_score: 437.8935)\n",
      "06:09    Epoch 25: train loss 437.9134 (mse_score: 437.9134)\n",
      "06:10    Epoch 30: train loss 437.8841 (mse_score: 437.8841)\n",
      "06:12    Epoch 35: train loss 437.8799 (mse_score: 437.8799)\n",
      "06:13    Epoch 40: train loss 437.8423 (mse_score: 437.8423)\n",
      "06:15    Epoch 45: train loss 437.8419 (mse_score: 437.8419)\n",
      "06:16    Epoch 50: train loss 437.8277 (mse_score: 437.8277)\n",
      "06:16  Finished training\n",
      "06:16  Training estimator 3 / 10 in ensemble\n",
      "06:16  Starting training\n",
      "06:16    Method:                 sally\n",
      "06:16    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "06:16                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "06:16    Features:               [26]\n",
      "06:16    Method:                 sally\n",
      "06:16    Hidden layers:          (100, 100)\n",
      "06:16    Activation function:    tanh\n",
      "06:16    Batch size:             128\n",
      "06:16    Trainer:                amsgrad\n",
      "06:16    Epochs:                 50\n",
      "06:16    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "06:16    Validation split:       None\n",
      "06:16    Early stopping:         True\n",
      "06:16    Scale inputs:           True\n",
      "06:16    Shuffle labels          False\n",
      "06:16    Regularization:         None\n",
      "06:16  Loading training data\n",
      "06:16  Found 1000000 samples with 2 parameters and 27 observables\n",
      "06:16  Rescaling inputs\n",
      "06:16  Only using 1 of 27 observables\n",
      "06:16  Creating model for method sally\n",
      "06:16  Training model\n",
      "06:18    Epoch 5: train loss 504.2661 (mse_score: 504.2661)\n",
      "06:19    Epoch 10: train loss 504.2092 (mse_score: 504.2092)\n",
      "06:21    Epoch 15: train loss 504.2219 (mse_score: 504.2219)\n",
      "06:22    Epoch 20: train loss 504.1595 (mse_score: 504.1595)\n",
      "06:24    Epoch 25: train loss 504.1564 (mse_score: 504.1564)\n",
      "06:25    Epoch 30: train loss 504.1112 (mse_score: 504.1112)\n",
      "06:26    Epoch 35: train loss 504.1016 (mse_score: 504.1016)\n",
      "06:28    Epoch 40: train loss 504.1061 (mse_score: 504.1061)\n",
      "06:29    Epoch 45: train loss 504.1102 (mse_score: 504.1102)\n",
      "06:31    Epoch 50: train loss 504.0789 (mse_score: 504.0789)\n",
      "06:31  Finished training\n",
      "06:31  Training estimator 4 / 10 in ensemble\n",
      "06:31  Starting training\n",
      "06:31    Method:                 sally\n",
      "06:31    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_3.npy\n",
      "06:31                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_3.npy\n",
      "06:31    Features:               [26]\n",
      "06:31    Method:                 sally\n",
      "06:31    Hidden layers:          (100, 100)\n",
      "06:31    Activation function:    tanh\n",
      "06:31    Batch size:             128\n",
      "06:31    Trainer:                amsgrad\n",
      "06:31    Epochs:                 50\n",
      "06:31    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "06:31    Validation split:       None\n",
      "06:31    Early stopping:         True\n",
      "06:31    Scale inputs:           True\n",
      "06:31    Shuffle labels          False\n",
      "06:31    Regularization:         None\n",
      "06:31  Loading training data\n",
      "06:31  Found 1000000 samples with 2 parameters and 27 observables\n",
      "06:31  Rescaling inputs\n",
      "06:31  Only using 1 of 27 observables\n",
      "06:31  Creating model for method sally\n",
      "06:31  Training model\n",
      "06:32    Epoch 5: train loss 453.6440 (mse_score: 453.6440)\n",
      "06:34    Epoch 10: train loss 453.5764 (mse_score: 453.5764)\n",
      "06:35    Epoch 15: train loss 453.5466 (mse_score: 453.5466)\n",
      "06:37    Epoch 20: train loss 453.5290 (mse_score: 453.5290)\n",
      "06:38    Epoch 25: train loss 453.5051 (mse_score: 453.5051)\n",
      "06:40    Epoch 30: train loss 453.5101 (mse_score: 453.5101)\n",
      "06:41    Epoch 35: train loss 453.4867 (mse_score: 453.4867)\n",
      "06:43    Epoch 40: train loss 453.4611 (mse_score: 453.4611)\n",
      "06:44    Epoch 45: train loss 453.4492 (mse_score: 453.4492)\n",
      "06:45    Epoch 50: train loss 453.5060 (mse_score: 453.5060)\n",
      "06:45  Finished training\n",
      "06:45  Training estimator 5 / 10 in ensemble\n",
      "06:45  Starting training\n",
      "06:45    Method:                 sally\n",
      "06:45    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_4.npy\n",
      "06:45                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_4.npy\n",
      "06:45    Features:               [26]\n",
      "06:45    Method:                 sally\n",
      "06:45    Hidden layers:          (100, 100)\n",
      "06:45    Activation function:    tanh\n",
      "06:45    Batch size:             128\n",
      "06:45    Trainer:                amsgrad\n",
      "06:45    Epochs:                 50\n",
      "06:45    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "06:45    Validation split:       None\n",
      "06:45    Early stopping:         True\n",
      "06:45    Scale inputs:           True\n",
      "06:45    Shuffle labels          False\n",
      "06:45    Regularization:         None\n",
      "06:45  Loading training data\n",
      "06:45  Found 1000000 samples with 2 parameters and 27 observables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:45  Rescaling inputs\n",
      "06:45  Only using 1 of 27 observables\n",
      "06:45  Creating model for method sally\n",
      "06:45  Training model\n",
      "06:47    Epoch 5: train loss 419.4220 (mse_score: 419.4220)\n",
      "06:49    Epoch 10: train loss 419.3840 (mse_score: 419.3840)\n",
      "06:50    Epoch 15: train loss 419.3414 (mse_score: 419.3414)\n",
      "06:51    Epoch 20: train loss 419.3266 (mse_score: 419.3266)\n",
      "06:53    Epoch 25: train loss 419.2919 (mse_score: 419.2919)\n",
      "06:54    Epoch 30: train loss 419.3090 (mse_score: 419.3090)\n",
      "06:56    Epoch 35: train loss 419.2714 (mse_score: 419.2714)\n",
      "06:57    Epoch 40: train loss 419.2486 (mse_score: 419.2486)\n",
      "06:59    Epoch 45: train loss 419.2454 (mse_score: 419.2454)\n",
      "07:00    Epoch 50: train loss 419.2493 (mse_score: 419.2493)\n",
      "07:00  Finished training\n",
      "07:00  Training estimator 6 / 10 in ensemble\n",
      "07:00  Starting training\n",
      "07:00    Method:                 sally\n",
      "07:00    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_5.npy\n",
      "07:00                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_5.npy\n",
      "07:00    Features:               [26]\n",
      "07:00    Method:                 sally\n",
      "07:00    Hidden layers:          (100, 100)\n",
      "07:00    Activation function:    tanh\n",
      "07:00    Batch size:             128\n",
      "07:00    Trainer:                amsgrad\n",
      "07:00    Epochs:                 50\n",
      "07:00    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "07:00    Validation split:       None\n",
      "07:00    Early stopping:         True\n",
      "07:00    Scale inputs:           True\n",
      "07:00    Shuffle labels          False\n",
      "07:00    Regularization:         None\n",
      "07:00  Loading training data\n",
      "07:00  Found 1000000 samples with 2 parameters and 27 observables\n",
      "07:00  Rescaling inputs\n",
      "07:00  Only using 1 of 27 observables\n",
      "07:00  Creating model for method sally\n",
      "07:00  Training model\n",
      "07:02    Epoch 5: train loss 474.1628 (mse_score: 474.1628)\n",
      "07:03    Epoch 10: train loss 474.0893 (mse_score: 474.0893)\n",
      "07:05    Epoch 15: train loss 474.0383 (mse_score: 474.0383)\n",
      "07:06    Epoch 20: train loss 474.0374 (mse_score: 474.0374)\n",
      "07:07    Epoch 25: train loss 474.0541 (mse_score: 474.0541)\n",
      "07:09    Epoch 30: train loss 473.9746 (mse_score: 473.9746)\n",
      "07:10    Epoch 35: train loss 473.9638 (mse_score: 473.9638)\n",
      "07:12    Epoch 40: train loss 473.9573 (mse_score: 473.9573)\n",
      "07:13    Epoch 45: train loss 473.9747 (mse_score: 473.9747)\n",
      "07:15    Epoch 50: train loss 473.9608 (mse_score: 473.9608)\n",
      "07:15  Finished training\n",
      "07:15  Training estimator 7 / 10 in ensemble\n",
      "07:15  Starting training\n",
      "07:15    Method:                 sally\n",
      "07:15    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_6.npy\n",
      "07:15                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_6.npy\n",
      "07:15    Features:               [26]\n",
      "07:15    Method:                 sally\n",
      "07:15    Hidden layers:          (100, 100)\n",
      "07:15    Activation function:    tanh\n",
      "07:15    Batch size:             128\n",
      "07:15    Trainer:                amsgrad\n",
      "07:15    Epochs:                 50\n",
      "07:15    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "07:15    Validation split:       None\n",
      "07:15    Early stopping:         True\n",
      "07:15    Scale inputs:           True\n",
      "07:15    Shuffle labels          False\n",
      "07:15    Regularization:         None\n",
      "07:15  Loading training data\n",
      "07:15  Found 1000000 samples with 2 parameters and 27 observables\n",
      "07:15  Rescaling inputs\n",
      "07:15  Only using 1 of 27 observables\n",
      "07:15  Creating model for method sally\n",
      "07:15  Training model\n",
      "07:16    Epoch 5: train loss 472.8336 (mse_score: 472.8336)\n",
      "07:18    Epoch 10: train loss 472.7769 (mse_score: 472.7769)\n",
      "07:19    Epoch 15: train loss 472.7488 (mse_score: 472.7488)\n",
      "07:21    Epoch 20: train loss 472.7078 (mse_score: 472.7078)\n",
      "07:22    Epoch 25: train loss 472.6768 (mse_score: 472.6768)\n",
      "07:24    Epoch 30: train loss 472.6641 (mse_score: 472.6641)\n",
      "07:25    Epoch 35: train loss 472.6370 (mse_score: 472.6370)\n",
      "07:27    Epoch 40: train loss 472.6538 (mse_score: 472.6538)\n",
      "07:28    Epoch 45: train loss 472.6311 (mse_score: 472.6311)\n",
      "07:29    Epoch 50: train loss 472.6780 (mse_score: 472.6780)\n",
      "07:29  Finished training\n",
      "07:29  Training estimator 8 / 10 in ensemble\n",
      "07:29  Starting training\n",
      "07:29    Method:                 sally\n",
      "07:29    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_7.npy\n",
      "07:29                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_7.npy\n",
      "07:29    Features:               [26]\n",
      "07:29    Method:                 sally\n",
      "07:29    Hidden layers:          (100, 100)\n",
      "07:29    Activation function:    tanh\n",
      "07:29    Batch size:             128\n",
      "07:29    Trainer:                amsgrad\n",
      "07:29    Epochs:                 50\n",
      "07:29    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "07:29    Validation split:       None\n",
      "07:29    Early stopping:         True\n",
      "07:29    Scale inputs:           True\n",
      "07:29    Shuffle labels          False\n",
      "07:29    Regularization:         None\n",
      "07:29  Loading training data\n",
      "07:29  Found 1000000 samples with 2 parameters and 27 observables\n",
      "07:29  Rescaling inputs\n",
      "07:29  Only using 1 of 27 observables\n",
      "07:29  Creating model for method sally\n",
      "07:29  Training model\n",
      "07:31    Epoch 5: train loss 461.7796 (mse_score: 461.7796)\n",
      "07:33    Epoch 10: train loss 461.6866 (mse_score: 461.6866)\n",
      "07:34    Epoch 15: train loss 461.6699 (mse_score: 461.6699)\n",
      "07:35    Epoch 20: train loss 461.6241 (mse_score: 461.6241)\n",
      "07:37    Epoch 25: train loss 461.6124 (mse_score: 461.6124)\n",
      "07:38    Epoch 30: train loss 461.5928 (mse_score: 461.5928)\n",
      "07:40    Epoch 35: train loss 461.5918 (mse_score: 461.5918)\n",
      "07:41    Epoch 40: train loss 461.5678 (mse_score: 461.5678)\n",
      "07:43    Epoch 45: train loss 461.5760 (mse_score: 461.5760)\n",
      "07:44    Epoch 50: train loss 461.5633 (mse_score: 461.5633)\n",
      "07:44  Finished training\n",
      "07:44  Training estimator 9 / 10 in ensemble\n",
      "07:44  Starting training\n",
      "07:44    Method:                 sally\n",
      "07:44    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_8.npy\n",
      "07:44                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_8.npy\n",
      "07:44    Features:               [26]\n",
      "07:44    Method:                 sally\n",
      "07:44    Hidden layers:          (100, 100)\n",
      "07:44    Activation function:    tanh\n",
      "07:44    Batch size:             128\n",
      "07:44    Trainer:                amsgrad\n",
      "07:44    Epochs:                 50\n",
      "07:44    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "07:44    Validation split:       None\n",
      "07:44    Early stopping:         True\n",
      "07:44    Scale inputs:           True\n",
      "07:44    Shuffle labels          False\n",
      "07:44    Regularization:         None\n",
      "07:44  Loading training data\n",
      "07:44  Found 1000000 samples with 2 parameters and 27 observables\n",
      "07:44  Rescaling inputs\n",
      "07:44  Only using 1 of 27 observables\n",
      "07:44  Creating model for method sally\n",
      "07:44  Training model\n",
      "07:46    Epoch 5: train loss 443.4474 (mse_score: 443.4474)\n",
      "07:47    Epoch 10: train loss 443.3747 (mse_score: 443.3747)\n",
      "07:49    Epoch 15: train loss 443.3684 (mse_score: 443.3684)\n",
      "07:50    Epoch 20: train loss 443.3285 (mse_score: 443.3285)\n",
      "07:52    Epoch 25: train loss 443.3658 (mse_score: 443.3658)\n",
      "07:53    Epoch 30: train loss 443.2911 (mse_score: 443.2911)\n",
      "07:54    Epoch 35: train loss 443.2793 (mse_score: 443.2793)\n",
      "07:56    Epoch 40: train loss 443.2673 (mse_score: 443.2673)\n",
      "07:57    Epoch 45: train loss 443.2824 (mse_score: 443.2824)\n",
      "07:59    Epoch 50: train loss 443.2482 (mse_score: 443.2482)\n",
      "07:59  Finished training\n",
      "07:59  Training estimator 10 / 10 in ensemble\n",
      "07:59  Starting training\n",
      "07:59    Method:                 sally\n",
      "07:59    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_9.npy\n",
      "07:59                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_9.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:59    Features:               [26]\n",
      "07:59    Method:                 sally\n",
      "07:59    Hidden layers:          (100, 100)\n",
      "07:59    Activation function:    tanh\n",
      "07:59    Batch size:             128\n",
      "07:59    Trainer:                amsgrad\n",
      "07:59    Epochs:                 50\n",
      "07:59    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "07:59    Validation split:       None\n",
      "07:59    Early stopping:         True\n",
      "07:59    Scale inputs:           True\n",
      "07:59    Shuffle labels          False\n",
      "07:59    Regularization:         None\n",
      "07:59  Loading training data\n",
      "07:59  Found 1000000 samples with 2 parameters and 27 observables\n",
      "07:59  Rescaling inputs\n",
      "07:59  Only using 1 of 27 observables\n",
      "07:59  Creating model for method sally\n",
      "07:59  Training model\n",
      "08:01    Epoch 5: train loss 498.4867 (mse_score: 498.4867)\n",
      "08:03    Epoch 10: train loss 498.4280 (mse_score: 498.4280)\n",
      "08:06    Epoch 15: train loss 498.3980 (mse_score: 498.3980)\n",
      "08:09    Epoch 20: train loss 498.3540 (mse_score: 498.3540)\n",
      "08:12    Epoch 25: train loss 498.3732 (mse_score: 498.3732)\n",
      "08:15    Epoch 30: train loss 498.3186 (mse_score: 498.3186)\n",
      "08:18    Epoch 35: train loss 498.3093 (mse_score: 498.3093)\n",
      "08:21    Epoch 40: train loss 498.7162 (mse_score: 498.7162)\n",
      "08:23    Epoch 45: train loss 498.2965 (mse_score: 498.2965)\n",
      "08:26    Epoch 50: train loss 498.2817 (mse_score: 498.2817)\n",
      "08:26  Finished training\n",
      "08:26  Calculating expectation for 10 estimators in ensemble\n",
      "08:26  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "08:26  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "08:26  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "08:26  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "08:27  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "08:27  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "08:27  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "08:27  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "08:27  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "08:28  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'resurrection',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[26] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
