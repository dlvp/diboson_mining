{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Marco Farina, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=n_estimators, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=False)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.calculate_expectation(\n",
    "        x_filename=sample_dir + 'validation{}/x_validation.npy'.format(cut_label)\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:48  \n",
      "07:48  ------------------------------------------------------------\n",
      "07:48  |                                                          |\n",
      "07:48  |  MadMiner v0.1.1                                         |\n",
      "07:48  |                                                          |\n",
      "07:48  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "07:48  |                                                          |\n",
      "07:48  ------------------------------------------------------------\n",
      "07:48  \n",
      "07:48  Training 3 estimators in ensemble\n",
      "07:48  Training estimator 1 / 3 in ensemble\n",
      "07:48  Starting training\n",
      "07:48    Method:                 sally\n",
      "07:48    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "07:48                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "07:48    Features:               all\n",
      "07:48    Method:                 sally\n",
      "07:48    Hidden layers:          (100, 100)\n",
      "07:48    Activation function:    tanh\n",
      "07:48    Batch size:             128\n",
      "07:48    Trainer:                amsgrad\n",
      "07:48    Epochs:                 50\n",
      "07:48    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "07:48    Validation split:       None\n",
      "07:48    Early stopping:         True\n",
      "07:48    Scale inputs:           True\n",
      "07:48    Shuffle labels          False\n",
      "07:48    Regularization:         None\n",
      "07:48  Loading training data\n",
      "07:48  Found 1000000 samples with 2 parameters and 33 observables\n",
      "07:48  Rescaling inputs\n",
      "07:48  Creating model for method sally\n",
      "07:48  Training model\n",
      "07:52    Epoch 5: train loss 326.9453 (mse_score: 326.9453)\n",
      "07:53    Epoch 10: train loss 317.5373 (mse_score: 317.5373)\n",
      "07:55    Epoch 15: train loss 311.1900 (mse_score: 311.1900)\n",
      "08:18    Epoch 20: train loss 306.6332 (mse_score: 306.6332)\n",
      "08:21    Epoch 25: train loss 303.2922 (mse_score: 303.2922)\n",
      "08:25    Epoch 30: train loss 300.5474 (mse_score: 300.5474)\n",
      "08:27    Epoch 35: train loss 298.3335 (mse_score: 298.3335)\n",
      "08:30    Epoch 40: train loss 296.6590 (mse_score: 296.6590)\n",
      "08:32    Epoch 45: train loss 295.4468 (mse_score: 295.4468)\n",
      "08:34    Epoch 50: train loss 294.3335 (mse_score: 294.3335)\n",
      "08:34  Finished training\n",
      "08:34  Training estimator 2 / 3 in ensemble\n",
      "08:34  Starting training\n",
      "08:34    Method:                 sally\n",
      "08:34    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "08:34                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "08:34    Features:               all\n",
      "08:34    Method:                 sally\n",
      "08:34    Hidden layers:          (100, 100)\n",
      "08:34    Activation function:    tanh\n",
      "08:34    Batch size:             128\n",
      "08:34    Trainer:                amsgrad\n",
      "08:34    Epochs:                 50\n",
      "08:34    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "08:34    Validation split:       None\n",
      "08:34    Early stopping:         True\n",
      "08:34    Scale inputs:           True\n",
      "08:34    Shuffle labels          False\n",
      "08:34    Regularization:         None\n",
      "08:34  Loading training data\n",
      "08:34  Found 1000000 samples with 2 parameters and 33 observables\n",
      "08:34  Rescaling inputs\n",
      "08:34  Creating model for method sally\n",
      "08:34  Training model\n",
      "08:36    Epoch 5: train loss 305.2639 (mse_score: 305.2639)\n",
      "08:37    Epoch 10: train loss 297.8176 (mse_score: 297.8176)\n",
      "08:39    Epoch 15: train loss 291.7903 (mse_score: 291.7903)\n",
      "08:41    Epoch 20: train loss 287.2310 (mse_score: 287.2310)\n",
      "08:43    Epoch 25: train loss 283.7305 (mse_score: 283.7305)\n",
      "08:47    Epoch 30: train loss 280.9259 (mse_score: 280.9259)\n",
      "08:54    Epoch 35: train loss 278.8527 (mse_score: 278.8527)\n",
      "09:05    Epoch 40: train loss 277.1774 (mse_score: 277.1774)\n",
      "09:17    Epoch 45: train loss 275.8380 (mse_score: 275.8380)\n",
      "09:27    Epoch 50: train loss 274.7849 (mse_score: 274.7849)\n",
      "09:27  Finished training\n",
      "09:27  Training estimator 3 / 3 in ensemble\n",
      "09:27  Starting training\n",
      "09:27    Method:                 sally\n",
      "09:27    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "09:27                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "09:27    Features:               all\n",
      "09:27    Method:                 sally\n",
      "09:27    Hidden layers:          (100, 100)\n",
      "09:27    Activation function:    tanh\n",
      "09:27    Batch size:             128\n",
      "09:27    Trainer:                amsgrad\n",
      "09:27    Epochs:                 50\n",
      "09:27    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "09:27    Validation split:       None\n",
      "09:27    Early stopping:         True\n",
      "09:27    Scale inputs:           True\n",
      "09:27    Shuffle labels          False\n",
      "09:27    Regularization:         None\n",
      "09:27  Loading training data\n",
      "09:27  Found 1000000 samples with 2 parameters and 33 observables\n",
      "09:27  Rescaling inputs\n",
      "09:27  Creating model for method sally\n",
      "09:27  Training model\n",
      "09:37    Epoch 5: train loss 305.0755 (mse_score: 305.0755)\n",
      "09:46    Epoch 10: train loss 296.6304 (mse_score: 296.6304)\n",
      "09:56    Epoch 15: train loss 290.4539 (mse_score: 290.4539)\n",
      "10:01    Epoch 20: train loss 285.8260 (mse_score: 285.8260)\n",
      "10:03    Epoch 25: train loss 282.3735 (mse_score: 282.3735)\n",
      "10:05    Epoch 30: train loss 279.8066 (mse_score: 279.8066)\n",
      "10:12    Epoch 35: train loss 277.6072 (mse_score: 277.6072)\n",
      "10:21    Epoch 40: train loss 275.9522 (mse_score: 275.9522)\n",
      "10:31    Epoch 45: train loss 274.5964 (mse_score: 274.5964)\n",
      "10:39    Epoch 50: train loss 273.4973 (mse_score: 273.4973)\n",
      "10:39  Finished training\n",
      "10:39  Calculating expectation for 3 estimators in ensemble\n",
      "10:39  Starting evaluation for estimator 1 / 3 in ensemble\n",
      "10:39  Starting evaluation for estimator 2 / 3 in ensemble\n",
      "10:40  Starting evaluation for estimator 3 / 3 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'all',\n",
    "    use_tight_cuts=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:24  \n",
      "13:24  ------------------------------------------------------------\n",
      "13:24  |                                                          |\n",
      "13:24  |  MadMiner v0.1.1                                         |\n",
      "13:24  |                                                          |\n",
      "13:24  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "13:24  |                                                          |\n",
      "13:24  ------------------------------------------------------------\n",
      "13:24  \n",
      "13:24  Training 3 estimators in ensemble\n",
      "13:24  Training estimator 1 / 3 in ensemble\n",
      "13:24  Starting training\n",
      "13:24    Method:                 sally\n",
      "13:24    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "13:24                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "13:24    Features:               all\n",
      "13:24    Method:                 sally\n",
      "13:24    Hidden layers:          (100, 100)\n",
      "13:24    Activation function:    tanh\n",
      "13:24    Batch size:             128\n",
      "13:24    Trainer:                amsgrad\n",
      "13:24    Epochs:                 50\n",
      "13:24    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:24    Validation split:       None\n",
      "13:24    Early stopping:         True\n",
      "13:24    Scale inputs:           True\n",
      "13:24    Shuffle labels          False\n",
      "13:24    Regularization:         None\n",
      "13:24  Loading training data\n",
      "13:24  Found 1000000 samples with 2 parameters and 33 observables\n",
      "13:24  Rescaling inputs\n",
      "13:24  Creating model for method sally\n",
      "13:24  Training model\n",
      "13:27    Epoch 5: train loss 7001.6288 (mse_score: 7001.6288)\n",
      "13:34    Epoch 10: train loss 6815.7301 (mse_score: 6815.7301)\n",
      "13:44    Epoch 15: train loss 6703.0106 (mse_score: 6703.0106)\n",
      "13:54    Epoch 20: train loss 6622.2164 (mse_score: 6622.2164)\n",
      "14:07    Epoch 25: train loss 6561.7956 (mse_score: 6561.7956)\n",
      "14:21    Epoch 30: train loss 6519.1045 (mse_score: 6519.1045)\n",
      "14:35    Epoch 35: train loss 6485.7300 (mse_score: 6485.7300)\n",
      "14:50    Epoch 40: train loss 6460.2412 (mse_score: 6460.2412)\n",
      "15:02    Epoch 45: train loss 6440.4152 (mse_score: 6440.4152)\n",
      "15:12    Epoch 50: train loss 6424.6838 (mse_score: 6424.6838)\n",
      "15:12  Finished training\n",
      "15:12  Training estimator 2 / 3 in ensemble\n",
      "15:12  Starting training\n",
      "15:12    Method:                 sally\n",
      "15:12    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "15:12                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "15:12    Features:               all\n",
      "15:12    Method:                 sally\n",
      "15:12    Hidden layers:          (100, 100)\n",
      "15:12    Activation function:    tanh\n",
      "15:12    Batch size:             128\n",
      "15:12    Trainer:                amsgrad\n",
      "15:12    Epochs:                 50\n",
      "15:12    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:12    Validation split:       None\n",
      "15:12    Early stopping:         True\n",
      "15:12    Scale inputs:           True\n",
      "15:12    Shuffle labels          False\n",
      "15:12    Regularization:         None\n",
      "15:12  Loading training data\n",
      "15:12  Found 1000000 samples with 2 parameters and 33 observables\n",
      "15:12  Rescaling inputs\n",
      "15:12  Creating model for method sally\n",
      "15:12  Training model\n",
      "15:18    Epoch 5: train loss 6689.7539 (mse_score: 6689.7539)\n",
      "15:21    Epoch 10: train loss 6524.7009 (mse_score: 6524.7009)\n",
      "15:23    Epoch 15: train loss 6416.5035 (mse_score: 6416.5035)\n",
      "15:26    Epoch 20: train loss 6337.4710 (mse_score: 6337.4710)\n",
      "15:28    Epoch 25: train loss 6281.3924 (mse_score: 6281.3924)\n",
      "15:30    Epoch 30: train loss 6239.4995 (mse_score: 6239.4995)\n",
      "15:33    Epoch 35: train loss 6206.3860 (mse_score: 6206.3860)\n",
      "15:35    Epoch 40: train loss 6180.8013 (mse_score: 6180.8013)\n",
      "15:37    Epoch 45: train loss 6158.8928 (mse_score: 6158.8928)\n",
      "15:39    Epoch 50: train loss 6144.0650 (mse_score: 6144.0650)\n",
      "15:39  Finished training\n",
      "15:39  Training estimator 3 / 3 in ensemble\n",
      "15:39  Starting training\n",
      "15:39    Method:                 sally\n",
      "15:39    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "15:39                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "15:39    Features:               all\n",
      "15:39    Method:                 sally\n",
      "15:39    Hidden layers:          (100, 100)\n",
      "15:39    Activation function:    tanh\n",
      "15:39    Batch size:             128\n",
      "15:39    Trainer:                amsgrad\n",
      "15:39    Epochs:                 50\n",
      "15:39    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:39    Validation split:       None\n",
      "15:39    Early stopping:         True\n",
      "15:39    Scale inputs:           True\n",
      "15:39    Shuffle labels          False\n",
      "15:39    Regularization:         None\n",
      "15:39  Loading training data\n",
      "15:39  Found 1000000 samples with 2 parameters and 33 observables\n",
      "15:39  Rescaling inputs\n",
      "15:39  Creating model for method sally\n",
      "15:39  Training model\n",
      "15:42    Epoch 5: train loss 6930.4437 (mse_score: 6930.4437)\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'all_tight',\n",
    "    use_tight_cuts=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal observable basis (no jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_obs = [0,1] + list(range(4,12)) + list(range(16,33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal',\n",
    "    use_tight_cuts=False,\n",
    "    features=[min_obs for _ in range(n_estimators)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[min_obs for _ in range(n_estimators)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just resurrection phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'phi_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[32] for _ in range(n_estimators)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
