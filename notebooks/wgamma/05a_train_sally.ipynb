{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Marco Farina, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=n_estimators, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=False)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.calculate_expectation(\n",
    "        x_filename=sample_dir + 'validation{}/x_validation.npy'.format(cut_label)\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:48  \n",
      "07:48  ------------------------------------------------------------\n",
      "07:48  |                                                          |\n",
      "07:48  |  MadMiner v0.1.1                                         |\n",
      "07:48  |                                                          |\n",
      "07:48  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "07:48  |                                                          |\n",
      "07:48  ------------------------------------------------------------\n",
      "07:48  \n",
      "07:48  Training 3 estimators in ensemble\n",
      "07:48  Training estimator 1 / 3 in ensemble\n",
      "07:48  Starting training\n",
      "07:48    Method:                 sally\n",
      "07:48    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "07:48                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "07:48    Features:               all\n",
      "07:48    Method:                 sally\n",
      "07:48    Hidden layers:          (100, 100)\n",
      "07:48    Activation function:    tanh\n",
      "07:48    Batch size:             128\n",
      "07:48    Trainer:                amsgrad\n",
      "07:48    Epochs:                 50\n",
      "07:48    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "07:48    Validation split:       None\n",
      "07:48    Early stopping:         True\n",
      "07:48    Scale inputs:           True\n",
      "07:48    Shuffle labels          False\n",
      "07:48    Regularization:         None\n",
      "07:48  Loading training data\n",
      "07:48  Found 1000000 samples with 2 parameters and 33 observables\n",
      "07:48  Rescaling inputs\n",
      "07:48  Creating model for method sally\n",
      "07:48  Training model\n",
      "07:52    Epoch 5: train loss 326.9453 (mse_score: 326.9453)\n",
      "07:53    Epoch 10: train loss 317.5373 (mse_score: 317.5373)\n",
      "07:55    Epoch 15: train loss 311.1900 (mse_score: 311.1900)\n",
      "08:18    Epoch 20: train loss 306.6332 (mse_score: 306.6332)\n",
      "08:21    Epoch 25: train loss 303.2922 (mse_score: 303.2922)\n",
      "08:25    Epoch 30: train loss 300.5474 (mse_score: 300.5474)\n",
      "08:27    Epoch 35: train loss 298.3335 (mse_score: 298.3335)\n",
      "08:30    Epoch 40: train loss 296.6590 (mse_score: 296.6590)\n",
      "08:32    Epoch 45: train loss 295.4468 (mse_score: 295.4468)\n",
      "08:34    Epoch 50: train loss 294.3335 (mse_score: 294.3335)\n",
      "08:34  Finished training\n",
      "08:34  Training estimator 2 / 3 in ensemble\n",
      "08:34  Starting training\n",
      "08:34    Method:                 sally\n",
      "08:34    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "08:34                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "08:34    Features:               all\n",
      "08:34    Method:                 sally\n",
      "08:34    Hidden layers:          (100, 100)\n",
      "08:34    Activation function:    tanh\n",
      "08:34    Batch size:             128\n",
      "08:34    Trainer:                amsgrad\n",
      "08:34    Epochs:                 50\n",
      "08:34    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "08:34    Validation split:       None\n",
      "08:34    Early stopping:         True\n",
      "08:34    Scale inputs:           True\n",
      "08:34    Shuffle labels          False\n",
      "08:34    Regularization:         None\n",
      "08:34  Loading training data\n",
      "08:34  Found 1000000 samples with 2 parameters and 33 observables\n",
      "08:34  Rescaling inputs\n",
      "08:34  Creating model for method sally\n",
      "08:34  Training model\n",
      "08:36    Epoch 5: train loss 305.2639 (mse_score: 305.2639)\n",
      "08:37    Epoch 10: train loss 297.8176 (mse_score: 297.8176)\n",
      "08:39    Epoch 15: train loss 291.7903 (mse_score: 291.7903)\n",
      "08:41    Epoch 20: train loss 287.2310 (mse_score: 287.2310)\n",
      "08:43    Epoch 25: train loss 283.7305 (mse_score: 283.7305)\n",
      "08:47    Epoch 30: train loss 280.9259 (mse_score: 280.9259)\n",
      "08:54    Epoch 35: train loss 278.8527 (mse_score: 278.8527)\n",
      "09:05    Epoch 40: train loss 277.1774 (mse_score: 277.1774)\n",
      "09:17    Epoch 45: train loss 275.8380 (mse_score: 275.8380)\n",
      "09:27    Epoch 50: train loss 274.7849 (mse_score: 274.7849)\n",
      "09:27  Finished training\n",
      "09:27  Training estimator 3 / 3 in ensemble\n",
      "09:27  Starting training\n",
      "09:27    Method:                 sally\n",
      "09:27    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "09:27                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "09:27    Features:               all\n",
      "09:27    Method:                 sally\n",
      "09:27    Hidden layers:          (100, 100)\n",
      "09:27    Activation function:    tanh\n",
      "09:27    Batch size:             128\n",
      "09:27    Trainer:                amsgrad\n",
      "09:27    Epochs:                 50\n",
      "09:27    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "09:27    Validation split:       None\n",
      "09:27    Early stopping:         True\n",
      "09:27    Scale inputs:           True\n",
      "09:27    Shuffle labels          False\n",
      "09:27    Regularization:         None\n",
      "09:27  Loading training data\n",
      "09:27  Found 1000000 samples with 2 parameters and 33 observables\n",
      "09:27  Rescaling inputs\n",
      "09:27  Creating model for method sally\n",
      "09:27  Training model\n",
      "09:37    Epoch 5: train loss 305.0755 (mse_score: 305.0755)\n",
      "09:46    Epoch 10: train loss 296.6304 (mse_score: 296.6304)\n",
      "09:56    Epoch 15: train loss 290.4539 (mse_score: 290.4539)\n",
      "10:01    Epoch 20: train loss 285.8260 (mse_score: 285.8260)\n",
      "10:03    Epoch 25: train loss 282.3735 (mse_score: 282.3735)\n",
      "10:05    Epoch 30: train loss 279.8066 (mse_score: 279.8066)\n",
      "10:12    Epoch 35: train loss 277.6072 (mse_score: 277.6072)\n",
      "10:21    Epoch 40: train loss 275.9522 (mse_score: 275.9522)\n",
      "10:31    Epoch 45: train loss 274.5964 (mse_score: 274.5964)\n",
      "10:39    Epoch 50: train loss 273.4973 (mse_score: 273.4973)\n",
      "10:39  Finished training\n",
      "10:39  Calculating expectation for 3 estimators in ensemble\n",
      "10:39  Starting evaluation for estimator 1 / 3 in ensemble\n",
      "10:39  Starting evaluation for estimator 2 / 3 in ensemble\n",
      "10:40  Starting evaluation for estimator 3 / 3 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'all',\n",
    "    use_tight_cuts=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:24  \n",
      "13:24  ------------------------------------------------------------\n",
      "13:24  |                                                          |\n",
      "13:24  |  MadMiner v0.1.1                                         |\n",
      "13:24  |                                                          |\n",
      "13:24  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "13:24  |                                                          |\n",
      "13:24  ------------------------------------------------------------\n",
      "13:24  \n",
      "13:24  Training 3 estimators in ensemble\n",
      "13:24  Training estimator 1 / 3 in ensemble\n",
      "13:24  Starting training\n",
      "13:24    Method:                 sally\n",
      "13:24    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "13:24                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "13:24    Features:               all\n",
      "13:24    Method:                 sally\n",
      "13:24    Hidden layers:          (100, 100)\n",
      "13:24    Activation function:    tanh\n",
      "13:24    Batch size:             128\n",
      "13:24    Trainer:                amsgrad\n",
      "13:24    Epochs:                 50\n",
      "13:24    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:24    Validation split:       None\n",
      "13:24    Early stopping:         True\n",
      "13:24    Scale inputs:           True\n",
      "13:24    Shuffle labels          False\n",
      "13:24    Regularization:         None\n",
      "13:24  Loading training data\n",
      "13:24  Found 1000000 samples with 2 parameters and 33 observables\n",
      "13:24  Rescaling inputs\n",
      "13:24  Creating model for method sally\n",
      "13:24  Training model\n",
      "13:27    Epoch 5: train loss 7001.6288 (mse_score: 7001.6288)\n",
      "13:34    Epoch 10: train loss 6815.7301 (mse_score: 6815.7301)\n",
      "13:44    Epoch 15: train loss 6703.0106 (mse_score: 6703.0106)\n",
      "13:54    Epoch 20: train loss 6622.2164 (mse_score: 6622.2164)\n",
      "14:07    Epoch 25: train loss 6561.7956 (mse_score: 6561.7956)\n",
      "14:21    Epoch 30: train loss 6519.1045 (mse_score: 6519.1045)\n",
      "14:35    Epoch 35: train loss 6485.7300 (mse_score: 6485.7300)\n",
      "14:50    Epoch 40: train loss 6460.2412 (mse_score: 6460.2412)\n",
      "15:02    Epoch 45: train loss 6440.4152 (mse_score: 6440.4152)\n",
      "15:12    Epoch 50: train loss 6424.6838 (mse_score: 6424.6838)\n",
      "15:12  Finished training\n",
      "15:12  Training estimator 2 / 3 in ensemble\n",
      "15:12  Starting training\n",
      "15:12    Method:                 sally\n",
      "15:12    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "15:12                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "15:12    Features:               all\n",
      "15:12    Method:                 sally\n",
      "15:12    Hidden layers:          (100, 100)\n",
      "15:12    Activation function:    tanh\n",
      "15:12    Batch size:             128\n",
      "15:12    Trainer:                amsgrad\n",
      "15:12    Epochs:                 50\n",
      "15:12    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:12    Validation split:       None\n",
      "15:12    Early stopping:         True\n",
      "15:12    Scale inputs:           True\n",
      "15:12    Shuffle labels          False\n",
      "15:12    Regularization:         None\n",
      "15:12  Loading training data\n",
      "15:12  Found 1000000 samples with 2 parameters and 33 observables\n",
      "15:12  Rescaling inputs\n",
      "15:12  Creating model for method sally\n",
      "15:12  Training model\n",
      "15:18    Epoch 5: train loss 6689.7539 (mse_score: 6689.7539)\n",
      "15:21    Epoch 10: train loss 6524.7009 (mse_score: 6524.7009)\n",
      "15:23    Epoch 15: train loss 6416.5035 (mse_score: 6416.5035)\n",
      "15:26    Epoch 20: train loss 6337.4710 (mse_score: 6337.4710)\n",
      "15:28    Epoch 25: train loss 6281.3924 (mse_score: 6281.3924)\n",
      "15:30    Epoch 30: train loss 6239.4995 (mse_score: 6239.4995)\n",
      "15:33    Epoch 35: train loss 6206.3860 (mse_score: 6206.3860)\n",
      "15:35    Epoch 40: train loss 6180.8013 (mse_score: 6180.8013)\n",
      "15:37    Epoch 45: train loss 6158.8928 (mse_score: 6158.8928)\n",
      "15:39    Epoch 50: train loss 6144.0650 (mse_score: 6144.0650)\n",
      "15:39  Finished training\n",
      "15:39  Training estimator 3 / 3 in ensemble\n",
      "15:39  Starting training\n",
      "15:39    Method:                 sally\n",
      "15:39    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "15:39                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "15:39    Features:               all\n",
      "15:39    Method:                 sally\n",
      "15:39    Hidden layers:          (100, 100)\n",
      "15:39    Activation function:    tanh\n",
      "15:39    Batch size:             128\n",
      "15:39    Trainer:                amsgrad\n",
      "15:39    Epochs:                 50\n",
      "15:39    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:39    Validation split:       None\n",
      "15:39    Early stopping:         True\n",
      "15:39    Scale inputs:           True\n",
      "15:39    Shuffle labels          False\n",
      "15:39    Regularization:         None\n",
      "15:39  Loading training data\n",
      "15:39  Found 1000000 samples with 2 parameters and 33 observables\n",
      "15:39  Rescaling inputs\n",
      "15:39  Creating model for method sally\n",
      "15:39  Training model\n",
      "15:42    Epoch 5: train loss 6930.4437 (mse_score: 6930.4437)\n",
      "15:44    Epoch 10: train loss 6762.8407 (mse_score: 6762.8407)\n",
      "15:46    Epoch 15: train loss 6646.1350 (mse_score: 6646.1350)\n",
      "15:48    Epoch 20: train loss 6566.6477 (mse_score: 6566.6477)\n",
      "15:50    Epoch 25: train loss 6509.7067 (mse_score: 6509.7067)\n",
      "15:57    Epoch 30: train loss 6467.0531 (mse_score: 6467.0531)\n",
      "16:00    Epoch 35: train loss 6438.1777 (mse_score: 6438.1777)\n",
      "16:03    Epoch 40: train loss 6413.5677 (mse_score: 6413.5677)\n",
      "16:05    Epoch 45: train loss 6395.2560 (mse_score: 6395.2560)\n",
      "16:08    Epoch 50: train loss 6381.1991 (mse_score: 6381.1991)\n",
      "16:08  Finished training\n",
      "16:08  Calculating expectation for 3 estimators in ensemble\n",
      "16:08  Starting evaluation for estimator 1 / 3 in ensemble\n",
      "16:08  Starting evaluation for estimator 2 / 3 in ensemble\n",
      "16:08  Starting evaluation for estimator 3 / 3 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'all_tight',\n",
    "    use_tight_cuts=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal observable basis (no jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_obs = [0,1] + list(range(4,12)) + list(range(16,33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:09  Training 3 estimators in ensemble\n",
      "16:09  Training estimator 1 / 3 in ensemble\n",
      "16:09  Starting training\n",
      "16:09    Method:                 sally\n",
      "16:09    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "16:09                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "16:09    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "16:09    Method:                 sally\n",
      "16:09    Hidden layers:          (100, 100)\n",
      "16:09    Activation function:    tanh\n",
      "16:09    Batch size:             128\n",
      "16:09    Trainer:                amsgrad\n",
      "16:09    Epochs:                 50\n",
      "16:09    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:09    Validation split:       None\n",
      "16:09    Early stopping:         True\n",
      "16:09    Scale inputs:           True\n",
      "16:09    Shuffle labels          False\n",
      "16:09    Regularization:         None\n",
      "16:09  Loading training data\n",
      "16:09  Found 1000000 samples with 2 parameters and 33 observables\n",
      "16:09  Rescaling inputs\n",
      "16:09  Only using 27 of 33 observables\n",
      "16:09  Creating model for method sally\n",
      "16:09  Training model\n",
      "16:11    Epoch 5: train loss 326.3370 (mse_score: 326.3370)\n",
      "16:14    Epoch 10: train loss 317.1937 (mse_score: 317.1937)\n",
      "16:16    Epoch 15: train loss 311.6129 (mse_score: 311.6129)\n",
      "16:18    Epoch 20: train loss 307.2774 (mse_score: 307.2774)\n",
      "16:20    Epoch 25: train loss 304.1835 (mse_score: 304.1835)\n",
      "16:23    Epoch 30: train loss 302.0129 (mse_score: 302.0129)\n",
      "16:27    Epoch 35: train loss 300.0890 (mse_score: 300.0890)\n",
      "22:06    Epoch 40: train loss 298.7832 (mse_score: 298.7832)\n",
      "22:12    Epoch 45: train loss 297.7061 (mse_score: 297.7061)\n",
      "22:19    Epoch 50: train loss 296.7612 (mse_score: 296.7612)\n",
      "22:19  Finished training\n",
      "22:19  Training estimator 2 / 3 in ensemble\n",
      "22:19  Starting training\n",
      "22:19    Method:                 sally\n",
      "22:19    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "22:19                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "22:19    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "22:19    Method:                 sally\n",
      "22:19    Hidden layers:          (100, 100)\n",
      "22:19    Activation function:    tanh\n",
      "22:19    Batch size:             128\n",
      "22:19    Trainer:                amsgrad\n",
      "22:19    Epochs:                 50\n",
      "22:19    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:19    Validation split:       None\n",
      "22:19    Early stopping:         True\n",
      "22:19    Scale inputs:           True\n",
      "22:19    Shuffle labels          False\n",
      "22:19    Regularization:         None\n",
      "22:19  Loading training data\n",
      "22:19  Found 1000000 samples with 2 parameters and 33 observables\n",
      "22:19  Rescaling inputs\n",
      "22:19  Only using 27 of 33 observables\n",
      "22:19  Creating model for method sally\n",
      "22:19  Training model\n",
      "22:26    Epoch 5: train loss 305.8665 (mse_score: 305.8665)\n",
      "22:32    Epoch 10: train loss 298.0256 (mse_score: 298.0256)\n",
      "22:39    Epoch 15: train loss 292.7071 (mse_score: 292.7071)\n",
      "22:47    Epoch 20: train loss 288.8397 (mse_score: 288.8397)\n",
      "22:54    Epoch 25: train loss 285.3439 (mse_score: 285.3439)\n",
      "23:02    Epoch 30: train loss 283.1017 (mse_score: 283.1017)\n",
      "23:07    Epoch 35: train loss 281.2970 (mse_score: 281.2970)\n",
      "23:09    Epoch 40: train loss 279.7000 (mse_score: 279.7000)\n",
      "23:11    Epoch 45: train loss 278.5762 (mse_score: 278.5762)\n",
      "23:13    Epoch 50: train loss 277.7344 (mse_score: 277.7344)\n",
      "23:13  Finished training\n",
      "23:13  Training estimator 3 / 3 in ensemble\n",
      "23:13  Starting training\n",
      "23:13    Method:                 sally\n",
      "23:13    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "23:13                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "23:13    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "23:13    Method:                 sally\n",
      "23:13    Hidden layers:          (100, 100)\n",
      "23:13    Activation function:    tanh\n",
      "23:13    Batch size:             128\n",
      "23:13    Trainer:                amsgrad\n",
      "23:13    Epochs:                 50\n",
      "23:13    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "23:13    Validation split:       None\n",
      "23:13    Early stopping:         True\n",
      "23:13    Scale inputs:           True\n",
      "23:13    Shuffle labels          False\n",
      "23:13    Regularization:         None\n",
      "23:13  Loading training data\n",
      "23:13  Found 1000000 samples with 2 parameters and 33 observables\n",
      "23:13  Rescaling inputs\n",
      "23:13  Only using 27 of 33 observables\n",
      "23:13  Creating model for method sally\n",
      "23:13  Training model\n",
      "23:16    Epoch 5: train loss 304.0089 (mse_score: 304.0089)\n",
      "23:24    Epoch 10: train loss 296.5982 (mse_score: 296.5982)\n",
      "23:32    Epoch 15: train loss 290.9229 (mse_score: 290.9229)\n",
      "23:40    Epoch 20: train loss 286.5607 (mse_score: 286.5607)\n",
      "23:48    Epoch 25: train loss 283.3326 (mse_score: 283.3326)\n",
      "23:56    Epoch 30: train loss 280.8244 (mse_score: 280.8244)\n",
      "00:03    Epoch 35: train loss 278.6342 (mse_score: 278.6342)\n",
      "00:10    Epoch 40: train loss 276.9973 (mse_score: 276.9973)\n",
      "00:13    Epoch 45: train loss 275.8802 (mse_score: 275.8802)\n",
      "00:15    Epoch 50: train loss 276.6169 (mse_score: 276.6169)\n",
      "00:15  Finished training\n",
      "00:15  Calculating expectation for 3 estimators in ensemble\n",
      "00:15  Starting evaluation for estimator 1 / 3 in ensemble\n",
      "00:15  Starting evaluation for estimator 2 / 3 in ensemble\n",
      "00:16  Starting evaluation for estimator 3 / 3 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'minimal',\n",
    "    use_tight_cuts=False,\n",
    "    features=[min_obs for _ in range(n_estimators)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:16  Training 3 estimators in ensemble\n",
      "00:16  Training estimator 1 / 3 in ensemble\n",
      "00:16  Starting training\n",
      "00:16    Method:                 sally\n",
      "00:16    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "00:16                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "00:16    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "00:16    Method:                 sally\n",
      "00:16    Hidden layers:          (100, 100)\n",
      "00:16    Activation function:    tanh\n",
      "00:16    Batch size:             128\n",
      "00:16    Trainer:                amsgrad\n",
      "00:16    Epochs:                 50\n",
      "00:16    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "00:16    Validation split:       None\n",
      "00:16    Early stopping:         True\n",
      "00:16    Scale inputs:           True\n",
      "00:16    Shuffle labels          False\n",
      "00:16    Regularization:         None\n",
      "00:16  Loading training data\n",
      "00:16  Found 1000000 samples with 2 parameters and 33 observables\n",
      "00:16  Rescaling inputs\n",
      "00:16  Only using 27 of 33 observables\n",
      "00:16  Creating model for method sally\n",
      "00:16  Training model\n",
      "00:19    Epoch 5: train loss 7012.5989 (mse_score: 7012.5989)\n",
      "00:21    Epoch 10: train loss 6873.3467 (mse_score: 6873.3467)\n",
      "00:23    Epoch 15: train loss 6765.7556 (mse_score: 6765.7556)\n",
      "00:25    Epoch 20: train loss 6697.1192 (mse_score: 6697.1192)\n",
      "00:27    Epoch 25: train loss 6648.0574 (mse_score: 6648.0574)\n",
      "00:29    Epoch 30: train loss 6613.9282 (mse_score: 6613.9282)\n",
      "00:31    Epoch 35: train loss 6585.6016 (mse_score: 6585.6016)\n",
      "00:33    Epoch 40: train loss 6565.7866 (mse_score: 6565.7866)\n",
      "00:35    Epoch 45: train loss 6549.1758 (mse_score: 6549.1758)\n",
      "00:37    Epoch 50: train loss 6535.6172 (mse_score: 6535.6172)\n",
      "00:37  Finished training\n",
      "00:37  Training estimator 2 / 3 in ensemble\n",
      "00:37  Starting training\n",
      "00:37    Method:                 sally\n",
      "00:37    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "00:37                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "00:37    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "00:37    Method:                 sally\n",
      "00:37    Hidden layers:          (100, 100)\n",
      "00:37    Activation function:    tanh\n",
      "00:37    Batch size:             128\n",
      "00:37    Trainer:                amsgrad\n",
      "00:37    Epochs:                 50\n",
      "00:37    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "00:37    Validation split:       None\n",
      "00:37    Early stopping:         True\n",
      "00:37    Scale inputs:           True\n",
      "00:37    Shuffle labels          False\n",
      "00:37    Regularization:         None\n",
      "00:37  Loading training data\n",
      "00:37  Found 1000000 samples with 2 parameters and 33 observables\n",
      "00:37  Rescaling inputs\n",
      "00:37  Only using 27 of 33 observables\n",
      "00:37  Creating model for method sally\n",
      "00:37  Training model\n",
      "00:39    Epoch 5: train loss 6706.3274 (mse_score: 6706.3274)\n",
      "00:41    Epoch 10: train loss 6556.1068 (mse_score: 6556.1068)\n",
      "00:46    Epoch 15: train loss 6467.8943 (mse_score: 6467.8943)\n",
      "00:48    Epoch 20: train loss 6405.6256 (mse_score: 6405.6256)\n",
      "00:50    Epoch 25: train loss 6364.7478 (mse_score: 6364.7478)\n",
      "00:52    Epoch 30: train loss 6331.5251 (mse_score: 6331.5251)\n",
      "00:54    Epoch 35: train loss 6306.3048 (mse_score: 6306.3048)\n",
      "00:56    Epoch 40: train loss 6284.4132 (mse_score: 6284.4132)\n",
      "00:58    Epoch 45: train loss 6266.7238 (mse_score: 6266.7238)\n",
      "01:00    Epoch 50: train loss 6255.2300 (mse_score: 6255.2300)\n",
      "01:00  Finished training\n",
      "01:00  Training estimator 3 / 3 in ensemble\n",
      "01:00  Starting training\n",
      "01:00    Method:                 sally\n",
      "01:00    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "01:00                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "01:00    Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "01:00    Method:                 sally\n",
      "01:00    Hidden layers:          (100, 100)\n",
      "01:00    Activation function:    tanh\n",
      "01:00    Batch size:             128\n",
      "01:00    Trainer:                amsgrad\n",
      "01:00    Epochs:                 50\n",
      "01:00    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "01:00    Validation split:       None\n",
      "01:00    Early stopping:         True\n",
      "01:00    Scale inputs:           True\n",
      "01:00    Shuffle labels          False\n",
      "01:00    Regularization:         None\n",
      "01:00  Loading training data\n",
      "01:00  Found 1000000 samples with 2 parameters and 33 observables\n",
      "01:00  Rescaling inputs\n",
      "01:00  Only using 27 of 33 observables\n",
      "01:00  Creating model for method sally\n",
      "01:00  Training model\n",
      "01:02    Epoch 5: train loss 6944.2556 (mse_score: 6944.2556)\n",
      "01:04    Epoch 10: train loss 6797.3958 (mse_score: 6797.3958)\n",
      "01:06    Epoch 15: train loss 6704.0337 (mse_score: 6704.0337)\n",
      "01:07    Epoch 20: train loss 6643.4128 (mse_score: 6643.4128)\n",
      "01:09    Epoch 25: train loss 6598.2476 (mse_score: 6598.2476)\n",
      "01:17    Epoch 30: train loss 6565.9399 (mse_score: 6565.9399)\n",
      "01:25    Epoch 35: train loss 6540.8432 (mse_score: 6540.8432)\n",
      "01:32    Epoch 40: train loss 6521.5602 (mse_score: 6521.5602)\n",
      "01:40    Epoch 45: train loss 6506.6512 (mse_score: 6506.6512)\n",
      "01:48    Epoch 50: train loss 6494.6473 (mse_score: 6494.6473)\n",
      "01:48  Finished training\n",
      "01:48  Calculating expectation for 3 estimators in ensemble\n",
      "01:48  Starting evaluation for estimator 1 / 3 in ensemble\n",
      "01:48  Starting evaluation for estimator 2 / 3 in ensemble\n",
      "01:49  Starting evaluation for estimator 3 / 3 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[min_obs for _ in range(n_estimators)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just resurrection phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:49  Training 3 estimators in ensemble\n",
      "01:49  Training estimator 1 / 3 in ensemble\n",
      "01:49  Starting training\n",
      "01:49    Method:                 sally\n",
      "01:49    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "01:49                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "01:49    Features:               [32]\n",
      "01:49    Method:                 sally\n",
      "01:49    Hidden layers:          (100, 100)\n",
      "01:49    Activation function:    tanh\n",
      "01:49    Batch size:             128\n",
      "01:49    Trainer:                amsgrad\n",
      "01:49    Epochs:                 50\n",
      "01:49    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "01:49    Validation split:       None\n",
      "01:49    Early stopping:         True\n",
      "01:49    Scale inputs:           True\n",
      "01:49    Shuffle labels          False\n",
      "01:49    Regularization:         None\n",
      "01:49  Loading training data\n",
      "01:49  Found 1000000 samples with 2 parameters and 33 observables\n",
      "01:49  Rescaling inputs\n",
      "01:49  Only using 1 of 33 observables\n",
      "01:49  Creating model for method sally\n",
      "01:49  Training model\n",
      "01:57    Epoch 5: train loss 8198.5783 (mse_score: 8198.5783)\n",
      "02:05    Epoch 10: train loss 8196.3475 (mse_score: 8196.3475)\n",
      "02:12    Epoch 15: train loss 8195.3418 (mse_score: 8195.3418)\n",
      "02:20    Epoch 20: train loss 8194.8945 (mse_score: 8194.8945)\n",
      "02:26    Epoch 25: train loss 8193.9023 (mse_score: 8193.9023)\n",
      "02:28    Epoch 30: train loss 8193.6923 (mse_score: 8193.6923)\n",
      "02:30    Epoch 35: train loss 8193.1777 (mse_score: 8193.1777)\n",
      "02:32    Epoch 40: train loss 8192.8231 (mse_score: 8192.8231)\n",
      "02:33    Epoch 45: train loss 8192.5231 (mse_score: 8192.5231)\n",
      "02:37    Epoch 50: train loss 8192.3090 (mse_score: 8192.3090)\n",
      "02:37  Finished training\n",
      "02:37  Training estimator 2 / 3 in ensemble\n",
      "02:37  Starting training\n",
      "02:37    Method:                 sally\n",
      "02:37    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "02:37                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "02:37    Features:               [32]\n",
      "02:37    Method:                 sally\n",
      "02:37    Hidden layers:          (100, 100)\n",
      "02:37    Activation function:    tanh\n",
      "02:37    Batch size:             128\n",
      "02:37    Trainer:                amsgrad\n",
      "02:37    Epochs:                 50\n",
      "02:37    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "02:37    Validation split:       None\n",
      "02:37    Early stopping:         True\n",
      "02:37    Scale inputs:           True\n",
      "02:37    Shuffle labels          False\n",
      "02:37    Regularization:         None\n",
      "02:37  Loading training data\n",
      "02:37  Found 1000000 samples with 2 parameters and 33 observables\n",
      "02:37  Rescaling inputs\n",
      "02:37  Only using 1 of 33 observables\n",
      "02:37  Creating model for method sally\n",
      "02:37  Training model\n",
      "02:45    Epoch 5: train loss 7906.8689 (mse_score: 7906.8689)\n",
      "02:53    Epoch 10: train loss 7905.0763 (mse_score: 7905.0763)\n",
      "03:02    Epoch 15: train loss 7904.1293 (mse_score: 7904.1293)\n",
      "03:10    Epoch 20: train loss 7903.3543 (mse_score: 7903.3543)\n",
      "03:18    Epoch 25: train loss 7902.3408 (mse_score: 7902.3408)\n",
      "03:20    Epoch 30: train loss 7902.2102 (mse_score: 7902.2102)\n",
      "03:22    Epoch 35: train loss 7901.6723 (mse_score: 7901.6723)\n",
      "03:27    Epoch 40: train loss 7901.6727 (mse_score: 7901.6727)\n",
      "03:29    Epoch 45: train loss 7901.5219 (mse_score: 7901.5219)\n",
      "03:31    Epoch 50: train loss 7901.0620 (mse_score: 7901.0620)\n",
      "03:31  Finished training\n",
      "03:31  Training estimator 3 / 3 in ensemble\n",
      "03:31  Starting training\n",
      "03:31    Method:                 sally\n",
      "03:31    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "03:31                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "03:31    Features:               [32]\n",
      "03:31    Method:                 sally\n",
      "03:31    Hidden layers:          (100, 100)\n",
      "03:31    Activation function:    tanh\n",
      "03:31    Batch size:             128\n",
      "03:31    Trainer:                amsgrad\n",
      "03:31    Epochs:                 50\n",
      "03:31    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "03:31    Validation split:       None\n",
      "03:31    Early stopping:         True\n",
      "03:31    Scale inputs:           True\n",
      "03:31    Shuffle labels          False\n",
      "03:31    Regularization:         None\n",
      "03:31  Loading training data\n",
      "03:31  Found 1000000 samples with 2 parameters and 33 observables\n",
      "03:31  Rescaling inputs\n",
      "03:31  Only using 1 of 33 observables\n",
      "03:31  Creating model for method sally\n",
      "03:31  Training model\n",
      "03:33    Epoch 5: train loss 8127.9627 (mse_score: 8127.9627)\n",
      "03:35    Epoch 10: train loss 8125.9407 (mse_score: 8125.9407)\n",
      "03:37    Epoch 15: train loss 8125.1457 (mse_score: 8125.1457)\n",
      "03:39    Epoch 20: train loss 8124.0939 (mse_score: 8124.0939)\n",
      "03:41    Epoch 25: train loss 8123.4414 (mse_score: 8123.4414)\n",
      "03:43    Epoch 30: train loss 8122.9717 (mse_score: 8122.9717)\n",
      "03:45    Epoch 35: train loss 8122.5384 (mse_score: 8122.5384)\n",
      "03:46    Epoch 40: train loss 8122.2669 (mse_score: 8122.2669)\n",
      "03:48    Epoch 45: train loss 8122.6472 (mse_score: 8122.6472)\n",
      "03:50    Epoch 50: train loss 8122.2783 (mse_score: 8122.2783)\n",
      "03:50  Finished training\n",
      "03:50  Calculating expectation for 3 estimators in ensemble\n",
      "03:50  Starting evaluation for estimator 1 / 3 in ensemble\n",
      "03:50  Starting evaluation for estimator 2 / 3 in ensemble\n",
      "03:50  Starting evaluation for estimator 3 / 3 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'phi_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[32] for _ in range(n_estimators)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
