{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY with shuffled labels\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "from madminer.fisherinformation import FisherInformation\n",
    "from madminer.plotting import plot_fisher_information_contours_2d\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=10, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=False)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:17  \n",
      "17:17  ------------------------------------------------------------\n",
      "17:17  |                                                          |\n",
      "17:17  |  MadMiner v2018.11.12                                    |\n",
      "17:17  |                                                          |\n",
      "17:17  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "17:17  |                                                          |\n",
      "17:17  ------------------------------------------------------------\n",
      "17:17  \n",
      "17:17  Training 10 estimators in ensemble\n",
      "17:17  Training estimator 1 / 10 in ensemble\n",
      "17:17  Starting training\n",
      "17:17    Method:                 sally\n",
      "17:17    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "17:17                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "17:17    Features:               all\n",
      "17:17    Method:                 sally\n",
      "17:17    Hidden layers:          (100, 100, 100, 100)\n",
      "17:17    Activation function:    tanh\n",
      "17:17    Batch size:             128\n",
      "17:17    Trainer:                amsgrad\n",
      "17:17    Epochs:                 50\n",
      "17:17    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "17:17    Validation split:       None\n",
      "17:17    Early stopping:         True\n",
      "17:17    Scale inputs:           True\n",
      "17:17    Shuffle labels          True\n",
      "17:17    Regularization:         None\n",
      "17:17  Loading training data\n",
      "17:17  Found 1000000 samples with 2 parameters and 27 observables\n",
      "17:17  Rescaling inputs\n",
      "17:17  Creating model for method sally\n",
      "17:17  Training model\n",
      "17:23    Epoch 5: train loss 4.7894 (mse_score: 4.7894)\n",
      "17:27    Epoch 10: train loss 4.7710 (mse_score: 4.7710)\n",
      "17:32    Epoch 15: train loss 4.7619 (mse_score: 4.7619)\n",
      "17:37    Epoch 20: train loss 4.7561 (mse_score: 4.7561)\n",
      "17:41    Epoch 25: train loss 4.7505 (mse_score: 4.7505)\n",
      "17:46    Epoch 30: train loss 4.7467 (mse_score: 4.7467)\n",
      "17:52    Epoch 35: train loss 4.7404 (mse_score: 4.7404)\n",
      "17:57    Epoch 40: train loss 4.7367 (mse_score: 4.7367)\n",
      "18:02    Epoch 45: train loss 4.7346 (mse_score: 4.7346)\n",
      "18:07    Epoch 50: train loss 4.7330 (mse_score: 4.7330)\n",
      "18:07  Finished training\n",
      "18:07  Training estimator 2 / 10 in ensemble\n",
      "18:07  Starting training\n",
      "18:07    Method:                 sally\n",
      "18:07    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "18:07                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "18:07    Features:               all\n",
      "18:07    Method:                 sally\n",
      "18:07    Hidden layers:          (100, 100, 100, 100)\n",
      "18:07    Activation function:    tanh\n",
      "18:07    Batch size:             128\n",
      "18:07    Trainer:                amsgrad\n",
      "18:07    Epochs:                 50\n",
      "18:07    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "18:07    Validation split:       None\n",
      "18:07    Early stopping:         True\n",
      "18:07    Scale inputs:           True\n",
      "18:07    Shuffle labels          True\n",
      "18:07    Regularization:         None\n",
      "18:07  Loading training data\n",
      "18:07  Found 1000000 samples with 2 parameters and 27 observables\n",
      "18:07  Rescaling inputs\n",
      "18:07  Creating model for method sally\n",
      "18:07  Training model\n",
      "18:12    Epoch 5: train loss 4.9342 (mse_score: 4.9342)\n",
      "18:17    Epoch 10: train loss 4.9167 (mse_score: 4.9167)\n",
      "18:23    Epoch 15: train loss 4.9067 (mse_score: 4.9067)\n",
      "18:29    Epoch 20: train loss 4.9007 (mse_score: 4.9007)\n",
      "18:40    Epoch 25: train loss 4.8960 (mse_score: 4.8960)\n",
      "18:45    Epoch 30: train loss 4.8907 (mse_score: 4.8907)\n",
      "18:49    Epoch 35: train loss 4.8869 (mse_score: 4.8869)\n",
      "18:54    Epoch 40: train loss 4.8840 (mse_score: 4.8840)\n",
      "19:00    Epoch 45: train loss 4.8822 (mse_score: 4.8822)\n",
      "19:04    Epoch 50: train loss 4.8809 (mse_score: 4.8809)\n",
      "19:04  Finished training\n",
      "19:04  Training estimator 3 / 10 in ensemble\n",
      "19:04  Starting training\n",
      "19:04    Method:                 sally\n",
      "19:04    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "19:04                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "19:04    Features:               all\n",
      "19:04    Method:                 sally\n",
      "19:04    Hidden layers:          (100, 100, 100, 100)\n",
      "19:04    Activation function:    tanh\n",
      "19:04    Batch size:             128\n",
      "19:04    Trainer:                amsgrad\n",
      "19:04    Epochs:                 50\n",
      "19:04    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "19:04    Validation split:       None\n",
      "19:04    Early stopping:         True\n",
      "19:04    Scale inputs:           True\n",
      "19:04    Shuffle labels          True\n",
      "19:04    Regularization:         None\n",
      "19:04  Loading training data\n",
      "19:04  Found 1000000 samples with 2 parameters and 27 observables\n",
      "19:04  Rescaling inputs\n",
      "19:05  Creating model for method sally\n",
      "19:05  Training model\n",
      "19:10    Epoch 5: train loss 4.1795 (mse_score: 4.1795)\n",
      "19:15    Epoch 10: train loss 4.1677 (mse_score: 4.1677)\n",
      "19:20    Epoch 15: train loss 4.1581 (mse_score: 4.1581)\n",
      "19:25    Epoch 20: train loss 4.1532 (mse_score: 4.1532)\n",
      "19:30    Epoch 25: train loss 4.1487 (mse_score: 4.1487)\n",
      "19:35    Epoch 30: train loss 4.1450 (mse_score: 4.1450)\n",
      "19:40    Epoch 35: train loss 4.1429 (mse_score: 4.1429)\n",
      "19:46    Epoch 40: train loss 4.1410 (mse_score: 4.1410)\n",
      "19:51    Epoch 45: train loss 4.1398 (mse_score: 4.1398)\n",
      "19:56    Epoch 50: train loss 4.1390 (mse_score: 4.1390)\n",
      "19:56  Finished training\n",
      "19:56  Training estimator 4 / 10 in ensemble\n",
      "19:56  Starting training\n",
      "19:56    Method:                 sally\n",
      "19:56    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "19:56                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "19:56    Features:               all\n",
      "19:56    Method:                 sally\n",
      "19:56    Hidden layers:          (100, 100, 100, 100)\n",
      "19:56    Activation function:    tanh\n",
      "19:56    Batch size:             128\n",
      "19:56    Trainer:                amsgrad\n",
      "19:56    Epochs:                 50\n",
      "19:56    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "19:56    Validation split:       None\n",
      "19:56    Early stopping:         True\n",
      "19:56    Scale inputs:           True\n",
      "19:56    Shuffle labels          True\n",
      "19:56    Regularization:         None\n",
      "19:56  Loading training data\n",
      "19:56  Found 1000000 samples with 2 parameters and 27 observables\n",
      "19:56  Rescaling inputs\n",
      "19:56  Creating model for method sally\n",
      "19:56  Training model\n",
      "20:02    Epoch 5: train loss 4.3335 (mse_score: 4.3335)\n",
      "20:07    Epoch 10: train loss 4.3188 (mse_score: 4.3188)\n",
      "20:12    Epoch 15: train loss 4.3110 (mse_score: 4.3110)\n",
      "20:17    Epoch 20: train loss 4.3061 (mse_score: 4.3061)\n",
      "20:22    Epoch 25: train loss 4.3023 (mse_score: 4.3023)\n",
      "20:28    Epoch 30: train loss 4.3014 (mse_score: 4.3014)\n",
      "20:38    Epoch 35: train loss 4.2961 (mse_score: 4.2961)\n",
      "20:47    Epoch 40: train loss 4.2946 (mse_score: 4.2946)\n",
      "20:56    Epoch 45: train loss 4.2932 (mse_score: 4.2932)\n",
      "21:04    Epoch 50: train loss 4.2926 (mse_score: 4.2926)\n",
      "21:04  Finished training\n",
      "21:04  Training estimator 5 / 10 in ensemble\n",
      "21:04  Starting training\n",
      "21:04    Method:                 sally\n",
      "21:04    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "21:04                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "21:04    Features:               all\n",
      "21:04    Method:                 sally\n",
      "21:04    Hidden layers:          (100, 100, 100, 100)\n",
      "21:04    Activation function:    tanh\n",
      "21:04    Batch size:             128\n",
      "21:04    Trainer:                amsgrad\n",
      "21:04    Epochs:                 50\n",
      "21:04    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "21:04    Validation split:       None\n",
      "21:04    Early stopping:         True\n",
      "21:04    Scale inputs:           True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:04    Shuffle labels          True\n",
      "21:04    Regularization:         None\n",
      "21:04  Loading training data\n",
      "21:04  Found 1000000 samples with 2 parameters and 27 observables\n",
      "21:04  Rescaling inputs\n",
      "21:04  Creating model for method sally\n",
      "21:04  Training model\n",
      "21:12    Epoch 5: train loss 3.9698 (mse_score: 3.9698)\n",
      "21:19    Epoch 10: train loss 3.9565 (mse_score: 3.9565)\n",
      "21:26    Epoch 15: train loss 3.9469 (mse_score: 3.9469)\n",
      "21:32    Epoch 20: train loss 3.9408 (mse_score: 3.9408)\n",
      "21:37    Epoch 25: train loss 3.9362 (mse_score: 3.9362)\n",
      "21:43    Epoch 30: train loss 3.9319 (mse_score: 3.9319)\n",
      "21:48    Epoch 35: train loss 3.9271 (mse_score: 3.9271)\n",
      "21:54    Epoch 40: train loss 3.9237 (mse_score: 3.9237)\n",
      "22:00    Epoch 45: train loss 3.9215 (mse_score: 3.9215)\n",
      "22:06    Epoch 50: train loss 3.9200 (mse_score: 3.9200)\n",
      "22:06  Finished training\n",
      "22:06  Training estimator 6 / 10 in ensemble\n",
      "22:06  Starting training\n",
      "22:06    Method:                 sally\n",
      "22:06    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "22:06                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "22:06    Features:               all\n",
      "22:06    Method:                 sally\n",
      "22:06    Hidden layers:          (100, 100, 100, 100)\n",
      "22:06    Activation function:    tanh\n",
      "22:06    Batch size:             128\n",
      "22:06    Trainer:                amsgrad\n",
      "22:06    Epochs:                 50\n",
      "22:06    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "22:06    Validation split:       None\n",
      "22:06    Early stopping:         True\n",
      "22:06    Scale inputs:           True\n",
      "22:06    Shuffle labels          True\n",
      "22:06    Regularization:         None\n",
      "22:06  Loading training data\n",
      "22:06  Found 1000000 samples with 2 parameters and 27 observables\n",
      "22:06  Rescaling inputs\n",
      "22:06  Creating model for method sally\n",
      "22:06  Training model\n",
      "22:12    Epoch 5: train loss 4.6162 (mse_score: 4.6162)\n",
      "22:18    Epoch 10: train loss 4.6013 (mse_score: 4.6013)\n",
      "22:23    Epoch 15: train loss 4.5905 (mse_score: 4.5905)\n",
      "22:28    Epoch 20: train loss 4.5843 (mse_score: 4.5843)\n",
      "22:34    Epoch 25: train loss 4.5798 (mse_score: 4.5798)\n",
      "22:40    Epoch 30: train loss 4.5763 (mse_score: 4.5763)\n",
      "22:45    Epoch 35: train loss 4.5734 (mse_score: 4.5734)\n",
      "22:51    Epoch 40: train loss 4.5716 (mse_score: 4.5716)\n",
      "22:57    Epoch 45: train loss 4.5703 (mse_score: 4.5703)\n",
      "23:03    Epoch 50: train loss 4.5696 (mse_score: 4.5696)\n",
      "23:03  Finished training\n",
      "23:03  Training estimator 7 / 10 in ensemble\n",
      "23:03  Starting training\n",
      "23:03    Method:                 sally\n",
      "23:03    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "23:03                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "23:03    Features:               all\n",
      "23:03    Method:                 sally\n",
      "23:03    Hidden layers:          (100, 100, 100, 100)\n",
      "23:03    Activation function:    tanh\n",
      "23:03    Batch size:             128\n",
      "23:03    Trainer:                amsgrad\n",
      "23:03    Epochs:                 50\n",
      "23:03    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "23:03    Validation split:       None\n",
      "23:03    Early stopping:         True\n",
      "23:03    Scale inputs:           True\n",
      "23:03    Shuffle labels          True\n",
      "23:03    Regularization:         None\n",
      "23:03  Loading training data\n",
      "23:03  Found 1000000 samples with 2 parameters and 27 observables\n",
      "23:03  Rescaling inputs\n",
      "23:03  Creating model for method sally\n",
      "23:03  Training model\n",
      "23:09    Epoch 5: train loss 7.9201 (mse_score: 7.9201)\n",
      "23:15    Epoch 10: train loss 7.9031 (mse_score: 7.9031)\n",
      "23:21    Epoch 15: train loss 7.8953 (mse_score: 7.8953)\n",
      "23:27    Epoch 20: train loss 7.8896 (mse_score: 7.8896)\n",
      "23:32    Epoch 25: train loss 7.8857 (mse_score: 7.8857)\n",
      "23:38    Epoch 30: train loss 7.8828 (mse_score: 7.8828)\n",
      "23:44    Epoch 35: train loss 7.8812 (mse_score: 7.8812)\n",
      "23:50    Epoch 40: train loss 7.8806 (mse_score: 7.8806)\n",
      "23:56    Epoch 45: train loss 7.8797 (mse_score: 7.8797)\n",
      "00:01    Epoch 50: train loss 7.8794 (mse_score: 7.8794)\n",
      "00:01  Finished training\n",
      "00:01  Training estimator 8 / 10 in ensemble\n",
      "00:01  Starting training\n",
      "00:01    Method:                 sally\n",
      "00:01    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "00:01                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "00:01    Features:               all\n",
      "00:01    Method:                 sally\n",
      "00:01    Hidden layers:          (100, 100, 100, 100)\n",
      "00:01    Activation function:    tanh\n",
      "00:01    Batch size:             128\n",
      "00:01    Trainer:                amsgrad\n",
      "00:01    Epochs:                 50\n",
      "00:01    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "00:01    Validation split:       None\n",
      "00:01    Early stopping:         True\n",
      "00:01    Scale inputs:           True\n",
      "00:01    Shuffle labels          True\n",
      "00:01    Regularization:         None\n",
      "00:01  Loading training data\n",
      "00:01  Found 1000000 samples with 2 parameters and 27 observables\n",
      "00:01  Rescaling inputs\n",
      "00:01  Creating model for method sally\n",
      "00:01  Training model\n",
      "00:06    Epoch 5: train loss 4.1367 (mse_score: 4.1367)\n",
      "00:10    Epoch 10: train loss 4.1236 (mse_score: 4.1236)\n",
      "00:14    Epoch 15: train loss 4.1139 (mse_score: 4.1139)\n",
      "00:18    Epoch 20: train loss 4.1080 (mse_score: 4.1080)\n",
      "00:23    Epoch 25: train loss 4.1044 (mse_score: 4.1044)\n",
      "00:27    Epoch 30: train loss 4.1015 (mse_score: 4.1015)\n",
      "00:32    Epoch 35: train loss 4.0982 (mse_score: 4.0982)\n",
      "00:36    Epoch 40: train loss 4.0961 (mse_score: 4.0961)\n",
      "00:41    Epoch 45: train loss 4.0943 (mse_score: 4.0943)\n",
      "00:45    Epoch 50: train loss 4.0933 (mse_score: 4.0933)\n",
      "00:45  Finished training\n",
      "00:45  Training estimator 9 / 10 in ensemble\n",
      "00:45  Starting training\n",
      "00:45    Method:                 sally\n",
      "00:45    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "00:45                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "00:45    Features:               all\n",
      "00:45    Method:                 sally\n",
      "00:45    Hidden layers:          (100, 100, 100, 100)\n",
      "00:45    Activation function:    tanh\n",
      "00:45    Batch size:             128\n",
      "00:45    Trainer:                amsgrad\n",
      "00:45    Epochs:                 50\n",
      "00:45    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "00:45    Validation split:       None\n",
      "00:45    Early stopping:         True\n",
      "00:45    Scale inputs:           True\n",
      "00:45    Shuffle labels          True\n",
      "00:45    Regularization:         None\n",
      "00:45  Loading training data\n",
      "00:45  Found 1000000 samples with 2 parameters and 27 observables\n",
      "00:45  Rescaling inputs\n",
      "00:45  Creating model for method sally\n",
      "00:45  Training model\n",
      "00:50    Epoch 5: train loss 4.3211 (mse_score: 4.3211)\n",
      "00:54    Epoch 10: train loss 4.3094 (mse_score: 4.3094)\n",
      "00:59    Epoch 15: train loss 4.2997 (mse_score: 4.2997)\n",
      "01:03    Epoch 20: train loss 4.2942 (mse_score: 4.2942)\n",
      "01:08    Epoch 25: train loss 4.2901 (mse_score: 4.2901)\n",
      "01:12    Epoch 30: train loss 4.2866 (mse_score: 4.2866)\n",
      "01:17    Epoch 35: train loss 4.2817 (mse_score: 4.2817)\n",
      "01:21    Epoch 40: train loss 4.2780 (mse_score: 4.2780)\n",
      "01:26    Epoch 45: train loss 4.2760 (mse_score: 4.2760)\n",
      "01:30    Epoch 50: train loss 4.2746 (mse_score: 4.2746)\n",
      "01:30  Finished training\n",
      "01:30  Training estimator 10 / 10 in ensemble\n",
      "01:30  Starting training\n",
      "01:30    Method:                 sally\n",
      "01:30    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "01:30                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "01:30    Features:               all\n",
      "01:30    Method:                 sally\n",
      "01:30    Hidden layers:          (100, 100, 100, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:30    Activation function:    tanh\n",
      "01:30    Batch size:             128\n",
      "01:30    Trainer:                amsgrad\n",
      "01:30    Epochs:                 50\n",
      "01:30    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "01:30    Validation split:       None\n",
      "01:30    Early stopping:         True\n",
      "01:30    Scale inputs:           True\n",
      "01:30    Shuffle labels          True\n",
      "01:30    Regularization:         None\n",
      "01:30  Loading training data\n",
      "01:30  Found 1000000 samples with 2 parameters and 27 observables\n",
      "01:30  Rescaling inputs\n",
      "01:30  Creating model for method sally\n",
      "01:30  Training model\n",
      "01:35    Epoch 5: train loss 7.3313 (mse_score: 7.3313)\n",
      "01:39    Epoch 10: train loss 7.3133 (mse_score: 7.3133)\n",
      "01:44    Epoch 15: train loss 7.3051 (mse_score: 7.3051)\n",
      "01:48    Epoch 20: train loss 7.2989 (mse_score: 7.2989)\n",
      "01:53    Epoch 25: train loss 7.2946 (mse_score: 7.2946)\n",
      "01:57    Epoch 30: train loss 7.2916 (mse_score: 7.2916)\n",
      "02:02    Epoch 35: train loss 7.2882 (mse_score: 7.2882)\n",
      "02:06    Epoch 40: train loss 7.2852 (mse_score: 7.2852)\n",
      "02:10    Epoch 45: train loss 7.2834 (mse_score: 7.2834)\n",
      "02:15    Epoch 50: train loss 7.2825 (mse_score: 7.2825)\n",
      "02:15  Finished training\n"
     ]
    }
   ],
   "source": [
    "train_ensemble('shuffled', use_tight_cuts=False, shuffle_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:33  Found ensemble with 10 estimators and expectations None\n",
      "15:33  Calculating expectation for 10 estimators in ensemble\n",
      "15:33  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "15:33  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "15:33  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "15:34  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "15:34  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "15:34  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "15:35  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "15:35  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "15:35  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "15:36  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "ensemble = EnsembleForge()\n",
    "\n",
    "ensemble.load(model_dir + 'sally_ensemble_shuffled')\n",
    "\n",
    "ensemble.calculate_expectation(x_filename=sample_dir + 'validation/x_validation.npy')\n",
    "\n",
    "ensemble.save(model_dir + 'sally_ensemble_shuffled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:08  \n",
      "16:08  ------------------------------------------------------------\n",
      "16:08  |                                                          |\n",
      "16:08  |  MadMiner v2018.11.13                                    |\n",
      "16:08  |                                                          |\n",
      "16:08  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "16:08  |                                                          |\n",
      "16:08  ------------------------------------------------------------\n",
      "16:08  \n",
      "16:08  Loading data from /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/samples.h5\n",
      "16:08  Found 2 parameters:\n",
      "16:08     cWWW (LHA: dim6 1, maximal power in squared ME: (2,), range: (-0.02, 0.02))\n",
      "16:08     cWWWtilde (LHA: dim6 2, maximal power in squared ME: (2,), range: (-0.02, 0.02))\n",
      "16:08  Found 6 benchmarks:\n",
      "16:08     sm: cWWW = 0.00e+00, cWWWtilde = 0.00e+00\n",
      "16:08     morphing_basis_vector_1: cWWW = -6.07e-03, cWWWtilde = -1.84e-02\n",
      "16:08     morphing_basis_vector_2: cWWW = 1.00e-02, cWWWtilde = 1.70e-02\n",
      "16:08     morphing_basis_vector_3: cWWW = -1.99e-02, cWWWtilde = 1.87e-02\n",
      "16:08     morphing_basis_vector_4: cWWW = 1.97e-02, cWWWtilde = -1.53e-02\n",
      "16:08     morphing_basis_vector_5: cWWW = -1.65e-02, cWWWtilde = -6.33e-03\n",
      "16:08  Found 27 observables: et_miss, phi_miss, e_visible, eta_visible, e_l1, pt_l1, eta_l1, phi_l1, e_a1, pt_a1, eta_a1, phi_a1, e_j1, pt_j1, eta_j1, phi_j1, deltaphi_l1_met, deltaphi_a1_met, m_l1_met, pt_l1_met, m_l1_a1, deltaeta_l1_a1, deltaphi_l1_a1, m_a1_l1_met, pt_a1_l1_met, mt, phi_resurrection\n",
      "16:08  Found 1812119 events\n",
      "16:08  Found morphing setup with 6 components\n"
     ]
    }
   ],
   "source": [
    "fisher = FisherInformation(sample_dir + 'samples.h5', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10  Loading ensemble setup from /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_shuffled/ensemble.json\n",
      "16:10  Found ensemble with 10 estimators and expectations [[ 0.00945755  0.00319982]\n",
      " [-0.0086288  -0.0020026 ]\n",
      " [ 0.01502106  0.00133386]\n",
      " [ 0.00042372 -0.00176944]\n",
      " [ 0.00095995 -0.00785334]\n",
      " [-0.00104095 -0.004577  ]\n",
      " [-0.00298309  0.00376815]\n",
      " [ 0.00109536  0.00062438]\n",
      " [-0.00323397 -0.00100783]\n",
      " [ 0.00495658  0.00293472]]\n",
      "16:11  Evaluating kinematic Fisher information on batch 1 / 10\n",
      "16:11  Evaluating kinematic Fisher information on batch 2 / 10\n",
      "16:12  Evaluating kinematic Fisher information on batch 3 / 10\n"
     ]
    }
   ],
   "source": [
    "mean, cov = fisher.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.],\n",
    "    luminosity=300000.,\n",
    "    model_file=model_dir + 'sally_ensemble_shuffled',\n",
    "    test_split=0.5,\n",
    "    include_xsec_info=False,\n",
    "    uncertainty=\"expectation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmax = 0.01\n",
    "\n",
    "fig = plot_fisher_information_contours_2d(\n",
    "    fisher_information_matrices=[mean],\n",
    "    fisher_information_covariances=[cov],\n",
    "    contour_distance=1.,\n",
    "    xrange=(-xmax,xmax),\n",
    "    yrange=(-xmax,xmax),\n",
    "    xlabel=r'$f_{WWW}$',\n",
    "    ylabel=r'$f_{\\tilde{W}WW}$',\n",
    "    resolution=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
