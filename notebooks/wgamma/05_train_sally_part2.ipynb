{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=10, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=False)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.calculate_expectation(\n",
    "        x_filename=sample_dir + 'validation{}/x_validation.npy'.format(cut_label)\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_ensemble('all', use_tight_cuts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble('all_tight', use_tight_cuts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal observable basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal',\n",
    "    use_tight_cuts=False,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_reg',\n",
    "    use_tight_cuts=False,\n",
    "    grad_x_regularization=0.1,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight_reg',\n",
    "    use_tight_cuts=True,\n",
    "    grad_x_regularization=0.1,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just resurrection phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:16  \n",
      "10:16  ------------------------------------------------------------\n",
      "10:16  |                                                          |\n",
      "10:16  |  MadMiner v2018.11.06                                    |\n",
      "10:16  |                                                          |\n",
      "10:16  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "10:16  |                                                          |\n",
      "10:16  ------------------------------------------------------------\n",
      "10:16  \n",
      "10:16  Training 10 estimators in ensemble\n",
      "10:16  Training estimator 1 / 10 in ensemble\n",
      "10:16  Starting training\n",
      "10:16    Method:                 sally\n",
      "10:16    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "10:16                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "10:16    Features:               [26]\n",
      "10:16    Method:                 sally\n",
      "10:16    Hidden layers:          (100, 100)\n",
      "10:16    Activation function:    tanh\n",
      "10:16    Batch size:             128\n",
      "10:16    Trainer:                amsgrad\n",
      "10:16    Epochs:                 50\n",
      "10:16    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:16    Validation split:       0.25\n",
      "10:16    Early stopping:         True\n",
      "10:16    Scale inputs:           True\n",
      "10:16    Regularization:         None\n",
      "10:16  Loading training data\n",
      "10:16  Found 1000000 samples with 2 parameters and 27 observables\n",
      "10:16  Rescaling inputs\n",
      "10:17  Only using 1 of 27 observables\n",
      "10:17  Creating model for method sally\n",
      "10:17  Training model\n",
      "10:21    Epoch 5: train loss 4726.4709 (mse_score: 4726.4709)\n",
      "10:21              val. loss  7982.1814 (mse_score: 7982.1814) (*)\n",
      "10:25    Epoch 10: train loss 4726.6375 (mse_score: 4726.6375)\n",
      "10:25              val. loss  7982.0201 (mse_score: 7982.0201) (*)\n",
      "10:30    Epoch 15: train loss 4725.5710 (mse_score: 4725.5710)\n",
      "10:30              val. loss  7981.5960 (mse_score: 7981.5960) (*)\n",
      "10:34    Epoch 20: train loss 4725.2371 (mse_score: 4725.2371)\n",
      "10:34              val. loss  7982.4585 (mse_score: 7982.4585)\n",
      "10:39    Epoch 25: train loss 4725.1167 (mse_score: 4725.1167)\n",
      "10:39              val. loss  7981.2111 (mse_score: 7981.2111) (*)\n",
      "10:44    Epoch 30: train loss 4724.9350 (mse_score: 4724.9350)\n",
      "10:44              val. loss  7981.5038 (mse_score: 7981.5038)\n",
      "10:48    Epoch 35: train loss 4724.8682 (mse_score: 4724.8682)\n",
      "10:48              val. loss  7981.7505 (mse_score: 7981.7505)\n",
      "10:51    Epoch 40: train loss 4724.7906 (mse_score: 4724.7906)\n",
      "10:51              val. loss  7981.1378 (mse_score: 7981.1378)\n",
      "10:55    Epoch 45: train loss 4724.6615 (mse_score: 4724.6615)\n",
      "10:55              val. loss  7980.7307 (mse_score: 7980.7307) (*)\n",
      "10:59    Epoch 50: train loss 4724.7166 (mse_score: 4724.7166)\n",
      "10:59              val. loss  7980.8296 (mse_score: 7980.8296)\n",
      "10:59  Early stopping after epoch 47, with loss 7980.64 compared to final loss 7980.83\n",
      "10:59  Finished training\n",
      "10:59  Training estimator 2 / 10 in ensemble\n",
      "10:59  Starting training\n",
      "10:59    Method:                 sally\n",
      "10:59    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "10:59                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "10:59    Features:               [26]\n",
      "10:59    Method:                 sally\n",
      "10:59    Hidden layers:          (100, 100)\n",
      "10:59    Activation function:    tanh\n",
      "10:59    Batch size:             128\n",
      "10:59    Trainer:                amsgrad\n",
      "10:59    Epochs:                 50\n",
      "10:59    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:59    Validation split:       0.25\n",
      "10:59    Early stopping:         True\n",
      "10:59    Scale inputs:           True\n",
      "10:59    Regularization:         None\n",
      "10:59  Loading training data\n",
      "10:59  Found 1000000 samples with 2 parameters and 27 observables\n",
      "10:59  Rescaling inputs\n",
      "10:59  Only using 1 of 27 observables\n",
      "10:59  Creating model for method sally\n",
      "10:59  Training model\n",
      "11:04    Epoch 5: train loss 6028.8092 (mse_score: 6028.8092)\n",
      "11:04              val. loss  4708.9728 (mse_score: 4708.9728) (*)\n",
      "11:08    Epoch 10: train loss 6027.6904 (mse_score: 6027.6904)\n",
      "11:08              val. loss  4707.8902 (mse_score: 4707.8902)\n",
      "11:12    Epoch 15: train loss 6026.7653 (mse_score: 6026.7653)\n",
      "11:12              val. loss  4707.3384 (mse_score: 4707.3384) (*)\n",
      "11:17    Epoch 20: train loss 6026.5438 (mse_score: 6026.5438)\n",
      "11:17              val. loss  4713.2231 (mse_score: 4713.2231)\n",
      "11:22    Epoch 25: train loss 6026.3070 (mse_score: 6026.3070)\n",
      "11:22              val. loss  4707.9779 (mse_score: 4707.9779)\n",
      "11:26    Epoch 30: train loss 6026.0917 (mse_score: 6026.0917)\n",
      "11:26              val. loss  4707.2689 (mse_score: 4707.2689) (*)\n",
      "11:30    Epoch 35: train loss 6025.9503 (mse_score: 6025.9503)\n",
      "11:30              val. loss  4707.2098 (mse_score: 4707.2098)\n",
      "11:34    Epoch 40: train loss 6025.8607 (mse_score: 6025.8607)\n",
      "11:34              val. loss  4707.1611 (mse_score: 4707.1611)\n",
      "11:38    Epoch 45: train loss 6025.8493 (mse_score: 6025.8493)\n",
      "11:38              val. loss  4711.6339 (mse_score: 4711.6339)\n",
      "11:41    Epoch 50: train loss 6025.8046 (mse_score: 6025.8046)\n",
      "11:41              val. loss  4707.5915 (mse_score: 4707.5915)\n",
      "11:41  Early stopping after epoch 48, with loss 4707.03 compared to final loss 4707.59\n",
      "11:41  Finished training\n",
      "11:41  Training estimator 3 / 10 in ensemble\n",
      "11:41  Starting training\n",
      "11:41    Method:                 sally\n",
      "11:41    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "11:41                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "11:41    Features:               [26]\n",
      "11:41    Method:                 sally\n",
      "11:41    Hidden layers:          (100, 100)\n",
      "11:41    Activation function:    tanh\n",
      "11:41    Batch size:             128\n",
      "11:41    Trainer:                amsgrad\n",
      "11:41    Epochs:                 50\n",
      "11:41    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "11:41    Validation split:       0.25\n",
      "11:41    Early stopping:         True\n",
      "11:41    Scale inputs:           True\n",
      "11:41    Regularization:         None\n",
      "11:41  Loading training data\n",
      "11:41  Found 1000000 samples with 2 parameters and 27 observables\n",
      "11:41  Rescaling inputs\n",
      "11:41  Only using 1 of 27 observables\n",
      "11:41  Creating model for method sally\n",
      "11:41  Training model\n",
      "11:45    Epoch 5: train loss 4842.0059 (mse_score: 4842.0059)\n",
      "11:45              val. loss  5441.7220 (mse_score: 5441.7220) (*)\n",
      "11:49    Epoch 10: train loss 4841.4358 (mse_score: 4841.4358)\n",
      "11:49              val. loss  5441.3325 (mse_score: 5441.3325)\n",
      "11:52    Epoch 15: train loss 4841.0226 (mse_score: 4841.0226)\n",
      "11:52              val. loss  5440.8217 (mse_score: 5440.8217) (*)\n",
      "11:55    Epoch 20: train loss 4840.7533 (mse_score: 4840.7533)\n",
      "11:55              val. loss  5440.6009 (mse_score: 5440.6009) (*)\n",
      "11:58    Epoch 25: train loss 4840.6389 (mse_score: 4840.6389)\n",
      "11:58              val. loss  5441.6421 (mse_score: 5441.6421)\n",
      "12:02    Epoch 30: train loss 4840.5492 (mse_score: 4840.5492)\n",
      "12:02              val. loss  5440.5476 (mse_score: 5440.5476)\n",
      "12:05    Epoch 35: train loss 4840.4003 (mse_score: 4840.4003)\n",
      "12:05              val. loss  5440.6695 (mse_score: 5440.6695)\n",
      "12:08    Epoch 40: train loss 4840.3065 (mse_score: 4840.3065)\n",
      "12:08              val. loss  5444.9772 (mse_score: 5444.9772)\n",
      "12:11    Epoch 45: train loss 4840.3218 (mse_score: 4840.3218)\n",
      "12:11              val. loss  5440.2665 (mse_score: 5440.2665) (*)\n",
      "12:14    Epoch 50: train loss 4840.1161 (mse_score: 4840.1161)\n",
      "12:14              val. loss  5440.3484 (mse_score: 5440.3484)\n",
      "12:14  Early stopping after epoch 49, with loss 5440.22 compared to final loss 5440.35\n",
      "12:14  Finished training\n",
      "12:14  Training estimator 4 / 10 in ensemble\n",
      "12:14  Starting training\n",
      "12:14    Method:                 sally\n",
      "12:14    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_3.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:14                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_3.npy\n",
      "12:14    Features:               [26]\n",
      "12:14    Method:                 sally\n",
      "12:14    Hidden layers:          (100, 100)\n",
      "12:14    Activation function:    tanh\n",
      "12:14    Batch size:             128\n",
      "12:14    Trainer:                amsgrad\n",
      "12:14    Epochs:                 50\n",
      "12:14    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "12:14    Validation split:       0.25\n",
      "12:14    Early stopping:         True\n",
      "12:14    Scale inputs:           True\n",
      "12:14    Regularization:         None\n",
      "12:14  Loading training data\n",
      "12:14  Found 1000000 samples with 2 parameters and 27 observables\n",
      "12:14  Rescaling inputs\n",
      "12:14  Only using 1 of 27 observables\n",
      "12:14  Creating model for method sally\n",
      "12:14  Training model\n",
      "12:18    Epoch 5: train loss 6405.0802 (mse_score: 6405.0802)\n",
      "12:18              val. loss  5341.6985 (mse_score: 5341.6985) (*)\n",
      "12:21    Epoch 10: train loss 6404.6992 (mse_score: 6404.6992)\n",
      "12:21              val. loss  5340.8375 (mse_score: 5340.8375) (*)\n",
      "12:24    Epoch 15: train loss 6403.8424 (mse_score: 6403.8424)\n",
      "12:24              val. loss  5340.5798 (mse_score: 5340.5798)\n",
      "12:27    Epoch 20: train loss 6403.2745 (mse_score: 6403.2745)\n",
      "12:27              val. loss  5339.9473 (mse_score: 5339.9473) (*)\n",
      "12:31    Epoch 25: train loss 6403.1703 (mse_score: 6403.1703)\n",
      "12:31              val. loss  5339.9067 (mse_score: 5339.9067) (*)\n",
      "12:34    Epoch 30: train loss 6402.9796 (mse_score: 6402.9796)\n",
      "12:34              val. loss  5339.9121 (mse_score: 5339.9121)\n",
      "12:37    Epoch 35: train loss 6403.1353 (mse_score: 6403.1353)\n",
      "12:37              val. loss  5341.4073 (mse_score: 5341.4073)\n",
      "12:41    Epoch 40: train loss 6402.8442 (mse_score: 6402.8442)\n",
      "12:41              val. loss  5339.7605 (mse_score: 5339.7605)\n",
      "12:44    Epoch 45: train loss 6402.7104 (mse_score: 6402.7104)\n",
      "12:44              val. loss  5340.1940 (mse_score: 5340.1940)\n",
      "12:47    Epoch 50: train loss 6402.6878 (mse_score: 6402.6878)\n",
      "12:47              val. loss  5340.7493 (mse_score: 5340.7493)\n",
      "12:47  Early stopping after epoch 49, with loss 5339.51 compared to final loss 5340.75\n",
      "12:47  Finished training\n",
      "12:47  Training estimator 5 / 10 in ensemble\n",
      "12:47  Starting training\n",
      "12:47    Method:                 sally\n",
      "12:47    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_4.npy\n",
      "12:47                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_4.npy\n",
      "12:47    Features:               [26]\n",
      "12:47    Method:                 sally\n",
      "12:47    Hidden layers:          (100, 100)\n",
      "12:47    Activation function:    tanh\n",
      "12:47    Batch size:             128\n",
      "12:47    Trainer:                amsgrad\n",
      "12:47    Epochs:                 50\n",
      "12:47    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "12:47    Validation split:       0.25\n",
      "12:47    Early stopping:         True\n",
      "12:47    Scale inputs:           True\n",
      "12:47    Regularization:         None\n",
      "12:47  Loading training data\n",
      "12:47  Found 1000000 samples with 2 parameters and 27 observables\n",
      "12:47  Rescaling inputs\n",
      "12:47  Only using 1 of 27 observables\n",
      "12:47  Creating model for method sally\n",
      "12:47  Training model\n",
      "12:51    Epoch 5: train loss 9113.6124 (mse_score: 9113.6124)\n",
      "12:51              val. loss  5276.5117 (mse_score: 5276.5117) (*)\n",
      "12:55    Epoch 10: train loss 9106.0531 (mse_score: 9106.0531)\n",
      "12:55              val. loss  5276.5576 (mse_score: 5276.5576)\n",
      "12:59    Epoch 15: train loss 9104.7977 (mse_score: 9104.7977)\n",
      "12:59              val. loss  5277.3976 (mse_score: 5277.3976)\n",
      "13:02    Epoch 20: train loss 9104.6837 (mse_score: 9104.6837)\n",
      "13:02              val. loss  5276.9122 (mse_score: 5276.9122)\n",
      "13:06    Epoch 25: train loss 9104.0996 (mse_score: 9104.0996)\n",
      "13:06              val. loss  5275.4209 (mse_score: 5275.4209) (*)\n",
      "13:10    Epoch 30: train loss 9103.6237 (mse_score: 9103.6237)\n",
      "13:10              val. loss  5275.2158 (mse_score: 5275.2158) (*)\n",
      "13:13    Epoch 35: train loss 9103.4163 (mse_score: 9103.4163)\n",
      "13:13              val. loss  5275.7080 (mse_score: 5275.7080)\n",
      "13:17    Epoch 40: train loss 9103.0528 (mse_score: 9103.0528)\n",
      "13:17              val. loss  5274.8818 (mse_score: 5274.8818) (*)\n",
      "13:20    Epoch 45: train loss 9102.8932 (mse_score: 9102.8932)\n",
      "13:20              val. loss  5275.0951 (mse_score: 5275.0951)\n",
      "13:23    Epoch 50: train loss 9102.8567 (mse_score: 9102.8567)\n",
      "13:23              val. loss  5275.0238 (mse_score: 5275.0238)\n",
      "13:23  Early stopping after epoch 40, with loss 5274.88 compared to final loss 5275.02\n",
      "13:23  Finished training\n",
      "13:23  Training estimator 6 / 10 in ensemble\n",
      "13:23  Starting training\n",
      "13:23    Method:                 sally\n",
      "13:23    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_5.npy\n",
      "13:23                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_5.npy\n",
      "13:23    Features:               [26]\n",
      "13:23    Method:                 sally\n",
      "13:23    Hidden layers:          (100, 100)\n",
      "13:23    Activation function:    tanh\n",
      "13:23    Batch size:             128\n",
      "13:23    Trainer:                amsgrad\n",
      "13:23    Epochs:                 50\n",
      "13:23    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:23    Validation split:       0.25\n",
      "13:23    Early stopping:         True\n",
      "13:23    Scale inputs:           True\n",
      "13:23    Regularization:         None\n",
      "13:23  Loading training data\n",
      "13:23  Found 1000000 samples with 2 parameters and 27 observables\n",
      "13:23  Rescaling inputs\n",
      "13:23  Only using 1 of 27 observables\n",
      "13:23  Creating model for method sally\n",
      "13:23  Training model\n",
      "13:27    Epoch 5: train loss 5854.2362 (mse_score: 5854.2362)\n",
      "13:27              val. loss  6138.5260 (mse_score: 6138.5260)\n",
      "13:30    Epoch 10: train loss 5853.1674 (mse_score: 5853.1674)\n",
      "13:30              val. loss  6134.2694 (mse_score: 6134.2694) (*)\n",
      "13:33    Epoch 15: train loss 5852.6074 (mse_score: 5852.6074)\n",
      "13:33              val. loss  6133.3494 (mse_score: 6133.3494) (*)\n",
      "13:37    Epoch 20: train loss 5851.9283 (mse_score: 5851.9283)\n",
      "13:37              val. loss  6133.1037 (mse_score: 6133.1037)\n",
      "13:40    Epoch 25: train loss 5851.6212 (mse_score: 5851.6212)\n",
      "13:40              val. loss  6133.5666 (mse_score: 6133.5666)\n",
      "13:44    Epoch 30: train loss 5851.5055 (mse_score: 5851.5055)\n",
      "13:44              val. loss  6132.5896 (mse_score: 6132.5896)\n",
      "13:47    Epoch 35: train loss 5851.2119 (mse_score: 5851.2119)\n",
      "13:47              val. loss  6132.3062 (mse_score: 6132.3062)\n",
      "13:50    Epoch 40: train loss 5851.0779 (mse_score: 5851.0779)\n",
      "13:50              val. loss  6132.2243 (mse_score: 6132.2243) (*)\n",
      "13:53    Epoch 45: train loss 5851.5621 (mse_score: 5851.5621)\n",
      "13:53              val. loss  6132.2005 (mse_score: 6132.2005)\n",
      "13:57    Epoch 50: train loss 5851.0380 (mse_score: 5851.0380)\n",
      "13:57              val. loss  6132.0538 (mse_score: 6132.0538)\n",
      "13:57  Early stopping after epoch 44, with loss 6131.96 compared to final loss 6132.05\n",
      "13:57  Finished training\n",
      "13:57  Training estimator 7 / 10 in ensemble\n",
      "13:57  Starting training\n",
      "13:57    Method:                 sally\n",
      "13:57    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_6.npy\n",
      "13:57                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_6.npy\n",
      "13:57    Features:               [26]\n",
      "13:57    Method:                 sally\n",
      "13:57    Hidden layers:          (100, 100)\n",
      "13:57    Activation function:    tanh\n",
      "13:57    Batch size:             128\n",
      "13:57    Trainer:                amsgrad\n",
      "13:57    Epochs:                 50\n",
      "13:57    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:57    Validation split:       0.25\n",
      "13:57    Early stopping:         True\n",
      "13:57    Scale inputs:           True\n",
      "13:57    Regularization:         None\n",
      "13:57  Loading training data\n",
      "13:57  Found 1000000 samples with 2 parameters and 27 observables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:57  Rescaling inputs\n",
      "13:57  Only using 1 of 27 observables\n",
      "13:57  Creating model for method sally\n",
      "13:57  Training model\n",
      "14:01    Epoch 5: train loss 5899.8593 (mse_score: 5899.8593)\n",
      "14:01              val. loss  5117.2436 (mse_score: 5117.2436)\n",
      "14:04    Epoch 10: train loss 5899.0472 (mse_score: 5899.0472)\n",
      "14:04              val. loss  5117.3679 (mse_score: 5117.3679)\n",
      "14:09    Epoch 15: train loss 5898.5102 (mse_score: 5898.5102)\n",
      "14:09              val. loss  5115.5048 (mse_score: 5115.5048) (*)\n",
      "14:13    Epoch 20: train loss 5898.2822 (mse_score: 5898.2822)\n",
      "14:13              val. loss  5116.2670 (mse_score: 5116.2670)\n",
      "14:16    Epoch 25: train loss 5898.0643 (mse_score: 5898.0643)\n",
      "14:16              val. loss  5115.4544 (mse_score: 5115.4544)\n",
      "14:20    Epoch 30: train loss 5900.0306 (mse_score: 5900.0306)\n",
      "14:20              val. loss  5115.3264 (mse_score: 5115.3264) (*)\n",
      "14:24    Epoch 35: train loss 5897.8785 (mse_score: 5897.8785)\n",
      "14:24              val. loss  5115.2483 (mse_score: 5115.2483)\n",
      "14:27    Epoch 40: train loss 5897.9593 (mse_score: 5897.9593)\n",
      "14:27              val. loss  5115.3694 (mse_score: 5115.3694)\n",
      "14:30    Epoch 45: train loss 5897.6708 (mse_score: 5897.6708)\n",
      "14:30              val. loss  5115.3174 (mse_score: 5115.3174)\n",
      "14:34    Epoch 50: train loss 5897.7799 (mse_score: 5897.7799)\n",
      "14:34              val. loss  5115.3958 (mse_score: 5115.3958)\n",
      "14:34  Early stopping after epoch 34, with loss 5115.20 compared to final loss 5115.40\n",
      "14:34  Finished training\n",
      "14:34  Training estimator 8 / 10 in ensemble\n",
      "14:34  Starting training\n",
      "14:34    Method:                 sally\n",
      "14:34    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_7.npy\n",
      "14:34                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_7.npy\n",
      "14:34    Features:               [26]\n",
      "14:34    Method:                 sally\n",
      "14:34    Hidden layers:          (100, 100)\n",
      "14:34    Activation function:    tanh\n",
      "14:34    Batch size:             128\n",
      "14:34    Trainer:                amsgrad\n",
      "14:34    Epochs:                 50\n",
      "14:34    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:34    Validation split:       0.25\n",
      "14:34    Early stopping:         True\n",
      "14:34    Scale inputs:           True\n",
      "14:34    Regularization:         None\n",
      "14:34  Loading training data\n",
      "14:34  Found 1000000 samples with 2 parameters and 27 observables\n",
      "14:34  Rescaling inputs\n",
      "14:34  Only using 1 of 27 observables\n",
      "14:34  Creating model for method sally\n",
      "14:34  Training model\n",
      "14:38    Epoch 5: train loss 5751.3475 (mse_score: 5751.3475)\n",
      "14:38              val. loss  5911.4543 (mse_score: 5911.4543) (*)\n",
      "14:42    Epoch 10: train loss 5749.5522 (mse_score: 5749.5522)\n",
      "14:42              val. loss  5911.1520 (mse_score: 5911.1520)\n",
      "14:46    Epoch 15: train loss 5748.2827 (mse_score: 5748.2827)\n",
      "14:46              val. loss  5911.1442 (mse_score: 5911.1442)\n",
      "14:50    Epoch 20: train loss 5747.3901 (mse_score: 5747.3901)\n",
      "14:50              val. loss  5910.8793 (mse_score: 5910.8793)\n",
      "14:54    Epoch 25: train loss 5746.6665 (mse_score: 5746.6665)\n",
      "14:54              val. loss  5910.3383 (mse_score: 5910.3383)\n",
      "14:57    Epoch 30: train loss 5746.1698 (mse_score: 5746.1698)\n",
      "14:57              val. loss  5909.5659 (mse_score: 5909.5659) (*)\n",
      "15:00    Epoch 35: train loss 5745.7552 (mse_score: 5745.7552)\n",
      "15:00              val. loss  5910.0050 (mse_score: 5910.0050)\n",
      "15:04    Epoch 40: train loss 5745.0902 (mse_score: 5745.0902)\n",
      "15:04              val. loss  5909.5267 (mse_score: 5909.5267)\n",
      "15:07    Epoch 45: train loss 5748.2670 (mse_score: 5748.2670)\n",
      "15:07              val. loss  5910.0500 (mse_score: 5910.0500)\n",
      "15:12    Epoch 50: train loss 5744.6870 (mse_score: 5744.6870)\n",
      "15:12              val. loss  5910.0827 (mse_score: 5910.0827)\n",
      "15:12  Early stopping after epoch 42, with loss 5909.14 compared to final loss 5910.08\n",
      "15:12  Finished training\n",
      "15:12  Training estimator 9 / 10 in ensemble\n",
      "15:12  Starting training\n",
      "15:12    Method:                 sally\n",
      "15:12    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_8.npy\n",
      "15:12                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_8.npy\n",
      "15:12    Features:               [26]\n",
      "15:12    Method:                 sally\n",
      "15:12    Hidden layers:          (100, 100)\n",
      "15:12    Activation function:    tanh\n",
      "15:12    Batch size:             128\n",
      "15:12    Trainer:                amsgrad\n",
      "15:12    Epochs:                 50\n",
      "15:12    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:12    Validation split:       0.25\n",
      "15:12    Early stopping:         True\n",
      "15:12    Scale inputs:           True\n",
      "15:12    Regularization:         None\n",
      "15:12  Loading training data\n",
      "15:12  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:12  Rescaling inputs\n",
      "15:12  Only using 1 of 27 observables\n",
      "15:12  Creating model for method sally\n",
      "15:12  Training model\n",
      "15:16    Epoch 5: train loss 5486.9478 (mse_score: 5486.9478)\n",
      "15:16              val. loss  5589.1901 (mse_score: 5589.1901) (*)\n",
      "15:20    Epoch 10: train loss 5485.7949 (mse_score: 5485.7949)\n",
      "15:20              val. loss  5589.4945 (mse_score: 5589.4945)\n",
      "15:23    Epoch 15: train loss 5485.1955 (mse_score: 5485.1955)\n",
      "15:23              val. loss  5588.9398 (mse_score: 5588.9398)\n",
      "15:27    Epoch 20: train loss 5484.9031 (mse_score: 5484.9031)\n",
      "15:27              val. loss  5587.9708 (mse_score: 5587.9708) (*)\n",
      "15:30    Epoch 25: train loss 5484.6187 (mse_score: 5484.6187)\n",
      "15:30              val. loss  5588.3386 (mse_score: 5588.3386)\n",
      "15:34    Epoch 30: train loss 5484.3860 (mse_score: 5484.3860)\n",
      "15:34              val. loss  5589.7906 (mse_score: 5589.7906)\n",
      "15:37    Epoch 35: train loss 5484.2163 (mse_score: 5484.2163)\n",
      "15:37              val. loss  5588.7949 (mse_score: 5588.7949)\n",
      "15:40    Epoch 40: train loss 5484.1563 (mse_score: 5484.1563)\n",
      "15:40              val. loss  5587.7613 (mse_score: 5587.7613)\n",
      "15:43    Epoch 45: train loss 5488.5929 (mse_score: 5488.5929)\n",
      "15:43              val. loss  5588.3130 (mse_score: 5588.3130)\n",
      "15:46    Epoch 50: train loss 5483.8734 (mse_score: 5483.8734)\n",
      "15:46              val. loss  5588.6296 (mse_score: 5588.6296)\n",
      "15:46  Early stopping after epoch 39, with loss 5587.54 compared to final loss 5588.63\n",
      "15:46  Finished training\n",
      "15:46  Training estimator 10 / 10 in ensemble\n",
      "15:46  Starting training\n",
      "15:46    Method:                 sally\n",
      "15:46    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_9.npy\n",
      "15:46                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_9.npy\n",
      "15:46    Features:               [26]\n",
      "15:46    Method:                 sally\n",
      "15:46    Hidden layers:          (100, 100)\n",
      "15:46    Activation function:    tanh\n",
      "15:46    Batch size:             128\n",
      "15:46    Trainer:                amsgrad\n",
      "15:46    Epochs:                 50\n",
      "15:46    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:46    Validation split:       0.25\n",
      "15:46    Early stopping:         True\n",
      "15:46    Scale inputs:           True\n",
      "15:46    Regularization:         None\n",
      "15:46  Loading training data\n",
      "15:46  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:46  Rescaling inputs\n",
      "15:46  Only using 1 of 27 observables\n",
      "15:46  Creating model for method sally\n",
      "15:46  Training model\n",
      "15:50    Epoch 5: train loss 6170.8271 (mse_score: 6170.8271)\n",
      "15:50              val. loss  5349.7586 (mse_score: 5349.7586) (*)\n",
      "15:53    Epoch 10: train loss 6169.7303 (mse_score: 6169.7303)\n",
      "15:53              val. loss  5349.1315 (mse_score: 5349.1315)\n",
      "15:56    Epoch 15: train loss 6169.2904 (mse_score: 6169.2904)\n",
      "15:56              val. loss  5348.4278 (mse_score: 5348.4278) (*)\n",
      "15:59    Epoch 20: train loss 6168.6787 (mse_score: 6168.6787)\n",
      "15:59              val. loss  5348.6566 (mse_score: 5348.6566)\n",
      "16:03    Epoch 25: train loss 6168.4433 (mse_score: 6168.4433)\n",
      "16:03              val. loss  5348.1460 (mse_score: 5348.1460)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:06    Epoch 30: train loss 6168.5940 (mse_score: 6168.5940)\n",
      "16:06              val. loss  5347.7905 (mse_score: 5347.7905) (*)\n",
      "16:08    Epoch 35: train loss 6168.2177 (mse_score: 6168.2177)\n",
      "16:08              val. loss  5348.2809 (mse_score: 5348.2809)\n",
      "16:11    Epoch 40: train loss 6167.9663 (mse_score: 6167.9663)\n",
      "16:11              val. loss  5348.1841 (mse_score: 5348.1841)\n",
      "16:13    Epoch 45: train loss 6167.9075 (mse_score: 6167.9075)\n",
      "16:13              val. loss  5348.5417 (mse_score: 5348.5417)\n",
      "16:15    Epoch 50: train loss 6167.8521 (mse_score: 6167.8521)\n",
      "16:15              val. loss  5347.5072 (mse_score: 5347.5072) (*)\n",
      "16:15  Early stopping did not improve performance\n",
      "16:15  Finished training\n",
      "16:15  Calculating expectation for 10 estimators in ensemble\n",
      "16:15  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "16:15  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "16:15  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "16:15  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "16:15  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "16:16  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "16:16  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "16:16  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "16:16  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "16:16  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'resurrection',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[26] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
