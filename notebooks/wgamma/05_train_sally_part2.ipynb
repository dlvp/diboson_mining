{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=10, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=False)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.calculate_expectation(\n",
    "        x_filename=sample_dir + 'validation{}/x_validation.npy'.format(cut_label)\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble('all', use_tight_cuts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:15  Training 10 estimators in ensemble\n",
      "19:15  Training estimator 1 / 10 in ensemble\n",
      "19:15  Starting training\n",
      "19:15    Method:                 sally\n",
      "19:15    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "19:15                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "19:15    Features:               all\n",
      "19:15    Method:                 sally\n",
      "19:15    Hidden layers:          (100, 100)\n",
      "19:15    Activation function:    tanh\n",
      "19:15    Batch size:             128\n",
      "19:15    Trainer:                amsgrad\n",
      "19:15    Epochs:                 50\n",
      "19:15    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "19:15    Validation split:       0.25\n",
      "19:15    Early stopping:         True\n",
      "19:15    Scale inputs:           True\n",
      "19:15  Loading training data\n",
      "19:15  Found 1000000 samples with 2 parameters and 26 observables\n",
      "19:15  Rescaling inputs\n",
      "19:15  Creating model for method sally\n",
      "19:15  Training model\n",
      "19:18    Epoch 5: train loss 4986.59 ([4986.58596779]), validation loss 6150.35 ([6150.35241007]) (*)\n",
      "19:21    Epoch 10: train loss 4876.11 ([4876.11314402]), validation loss 6054.08 ([6054.07704762]) (*)\n",
      "19:24    Epoch 15: train loss 4783.81 ([4783.80699701]), validation loss 5972.65 ([5972.64528441]) (*)\n",
      "19:26    Epoch 20: train loss 4712.01 ([4712.00735277]), validation loss 5920.08 ([5920.07619845]) (*)\n",
      "19:29    Epoch 25: train loss 4660.30 ([4660.29800051]), validation loss 5878.98 ([5878.97677165]) (*)\n",
      "19:32    Epoch 30: train loss 4619.91 ([4619.91276421]), validation loss 5845.17 ([5845.17167054]) (*)\n",
      "19:34    Epoch 35: train loss 4589.31 ([4589.30518544]), validation loss 5823.25 ([5823.24738202]) (*)\n",
      "19:37    Epoch 40: train loss 4565.92 ([4565.92019892]), validation loss 5808.03 ([5808.02911721]) (*)\n",
      "19:40    Epoch 45: train loss 4547.09 ([4547.09048357]), validation loss 5795.40 ([5795.39910748]) (*)\n",
      "19:43    Epoch 50: train loss 4532.41 ([4532.40971086]), validation loss 5783.92 ([5783.91686146]) (*)\n",
      "19:43  Early stopping did not improve performance\n",
      "19:43  Finished training\n",
      "19:43  Training estimator 2 / 10 in ensemble\n",
      "19:43  Starting training\n",
      "19:43    Method:                 sally\n",
      "19:43    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "19:43                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "19:43    Features:               all\n",
      "19:43    Method:                 sally\n",
      "19:43    Hidden layers:          (100, 100)\n",
      "19:43    Activation function:    tanh\n",
      "19:43    Batch size:             128\n",
      "19:43    Trainer:                amsgrad\n",
      "19:43    Epochs:                 50\n",
      "19:43    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "19:43    Validation split:       0.25\n",
      "19:43    Early stopping:         True\n",
      "19:43    Scale inputs:           True\n",
      "19:43  Loading training data\n",
      "19:43  Found 1000000 samples with 2 parameters and 26 observables\n",
      "19:43  Rescaling inputs\n",
      "19:43  Creating model for method sally\n",
      "19:43  Training model\n",
      "19:46    Epoch 5: train loss 8521.11 ([8521.11116055]), validation loss 8354.54 ([8354.54125599]) (*)\n",
      "19:49    Epoch 10: train loss 8422.12 ([8422.11770508]), validation loss 8296.65 ([8296.64705391]) (*)\n",
      "19:52    Epoch 15: train loss 8330.80 ([8330.80161738]), validation loss 8247.72 ([8247.71501593]) (*)\n",
      "19:55    Epoch 20: train loss 8257.43 ([8257.4262435]), validation loss 8219.12 ([8219.1182368]) (*)\n",
      "19:58    Epoch 25: train loss 8199.08 ([8199.0843826]), validation loss 8184.78 ([8184.77644685]) (*)\n",
      "20:00    Epoch 30: train loss 8156.28 ([8156.28107272]), validation loss 8162.17 ([8162.16866178]) (*)\n",
      "20:03    Epoch 35: train loss 8121.20 ([8121.19592316]), validation loss 8144.54 ([8144.53873843]) (*)\n",
      "20:06    Epoch 40: train loss 8093.37 ([8093.36696939]), validation loss 8131.45 ([8131.45190491]) (*)\n",
      "20:09    Epoch 45: train loss 8071.92 ([8071.92497427]), validation loss 8121.01 ([8121.00510802])\n",
      "20:12    Epoch 50: train loss 8054.58 ([8054.57515276]), validation loss 8115.64 ([8115.64044755])\n",
      "20:12  Early stopping after epoch 48, with loss 8113.30 compared to final loss 8115.64\n",
      "20:12  Finished training\n",
      "20:12  Training estimator 3 / 10 in ensemble\n",
      "20:12  Starting training\n",
      "20:12    Method:                 sally\n",
      "20:12    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "20:12                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "20:12    Features:               all\n",
      "20:12    Method:                 sally\n",
      "20:12    Hidden layers:          (100, 100)\n",
      "20:12    Activation function:    tanh\n",
      "20:12    Batch size:             128\n",
      "20:12    Trainer:                amsgrad\n",
      "20:12    Epochs:                 50\n",
      "20:12    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:12    Validation split:       0.25\n",
      "20:12    Early stopping:         True\n",
      "20:12    Scale inputs:           True\n",
      "20:12  Loading training data\n",
      "20:12  Found 1000000 samples with 2 parameters and 26 observables\n",
      "20:12  Rescaling inputs\n",
      "20:12  Creating model for method sally\n",
      "20:12  Training model\n",
      "20:15    Epoch 5: train loss 6594.62 ([6594.61686012]), validation loss 4944.65 ([4944.65118036]) (*)\n",
      "20:18    Epoch 10: train loss 6498.77 ([6498.76928371]), validation loss 4867.84 ([4867.84402787]) (*)\n",
      "20:21    Epoch 15: train loss 6414.41 ([6414.40691778]), validation loss 4807.65 ([4807.65311032]) (*)\n",
      "20:24    Epoch 20: train loss 6345.19 ([6345.18619512]), validation loss 4774.50 ([4774.49833331])\n",
      "20:27    Epoch 25: train loss 6292.70 ([6292.69804552]), validation loss 4735.55 ([4735.54779185]) (*)\n",
      "20:30    Epoch 30: train loss 6251.68 ([6251.68035688]), validation loss 4711.68 ([4711.67509407]) (*)\n",
      "20:32    Epoch 35: train loss 6221.42 ([6221.41726053]), validation loss 4700.56 ([4700.55604158])\n",
      "20:35    Epoch 40: train loss 6197.78 ([6197.78198464]), validation loss 4684.08 ([4684.08129561]) (*)\n",
      "20:38    Epoch 45: train loss 6178.88 ([6178.87794223]), validation loss 4675.32 ([4675.32064971]) (*)\n",
      "20:41    Epoch 50: train loss 6164.53 ([6164.53439788]), validation loss 4669.53 ([4669.52951953])\n",
      "20:41  Early stopping after epoch 48, with loss 4667.42 compared to final loss 4669.53\n",
      "20:41  Finished training\n",
      "20:41  Training estimator 4 / 10 in ensemble\n",
      "20:41  Starting training\n",
      "20:41    Method:                 sally\n",
      "20:41    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_3.npy\n",
      "20:41                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_3.npy\n",
      "20:41    Features:               all\n",
      "20:41    Method:                 sally\n",
      "20:41    Hidden layers:          (100, 100)\n",
      "20:41    Activation function:    tanh\n",
      "20:41    Batch size:             128\n",
      "20:41    Trainer:                amsgrad\n",
      "20:41    Epochs:                 50\n",
      "20:41    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:41    Validation split:       0.25\n",
      "20:41    Early stopping:         True\n",
      "20:41    Scale inputs:           True\n",
      "20:41  Loading training data\n",
      "20:41  Found 1000000 samples with 2 parameters and 26 observables\n",
      "20:41  Rescaling inputs\n",
      "20:41  Creating model for method sally\n",
      "20:41  Training model\n",
      "20:44    Epoch 5: train loss 6014.98 ([6014.98018861]), validation loss 4418.78 ([4418.78010346]) (*)\n",
      "20:47    Epoch 10: train loss 5921.19 ([5921.18986323]), validation loss 4363.36 ([4363.35501873]) (*)\n",
      "20:50    Epoch 15: train loss 5829.06 ([5829.05863624]), validation loss 4321.66 ([4321.65765945]) (*)\n",
      "20:53    Epoch 20: train loss 5762.73 ([5762.72992739]), validation loss 4286.91 ([4286.91217771]) (*)\n",
      "20:56    Epoch 25: train loss 5712.94 ([5712.94274442]), validation loss 4262.10 ([4262.10473621]) (*)\n",
      "20:58    Epoch 30: train loss 5675.13 ([5675.12527786]), validation loss 4247.99 ([4247.98884825]) (*)\n",
      "21:01    Epoch 35: train loss 5646.05 ([5646.05093206]), validation loss 4233.22 ([4233.22159758]) (*)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:04    Epoch 40: train loss 5622.86 ([5622.85920959]), validation loss 4224.38 ([4224.37583181]) (*)\n",
      "21:07    Epoch 45: train loss 5605.31 ([5605.3090844]), validation loss 4217.34 ([4217.33883237])\n",
      "21:10    Epoch 50: train loss 5591.35 ([5591.34837712]), validation loss 4210.73 ([4210.73216875]) (*)\n",
      "21:10  Early stopping did not improve performance\n",
      "21:10  Finished training\n",
      "21:10  Training estimator 5 / 10 in ensemble\n",
      "21:10  Starting training\n",
      "21:10    Method:                 sally\n",
      "21:10    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_4.npy\n",
      "21:10                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_4.npy\n",
      "21:10    Features:               all\n",
      "21:10    Method:                 sally\n",
      "21:10    Hidden layers:          (100, 100)\n",
      "21:10    Activation function:    tanh\n",
      "21:10    Batch size:             128\n",
      "21:10    Trainer:                amsgrad\n",
      "21:10    Epochs:                 50\n",
      "21:10    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:10    Validation split:       0.25\n",
      "21:10    Early stopping:         True\n",
      "21:10    Scale inputs:           True\n",
      "21:10  Loading training data\n",
      "21:10  Found 1000000 samples with 2 parameters and 26 observables\n",
      "21:10  Rescaling inputs\n",
      "21:10  Creating model for method sally\n",
      "21:10  Training model\n",
      "21:13    Epoch 5: train loss 6321.02 ([6321.01908385]), validation loss 5473.60 ([5473.59919085]) (*)\n",
      "21:16    Epoch 10: train loss 6221.32 ([6221.3215724]), validation loss 5396.48 ([5396.47960381]) (*)\n",
      "21:19    Epoch 15: train loss 6134.89 ([6134.8878293]), validation loss 5339.10 ([5339.10261044]) (*)\n",
      "21:21    Epoch 20: train loss 6065.76 ([6065.76316816]), validation loss 5292.97 ([5292.97435564]) (*)\n",
      "21:24    Epoch 25: train loss 6013.34 ([6013.33758041]), validation loss 5259.91 ([5259.90834289]) (*)\n",
      "21:27    Epoch 30: train loss 5972.27 ([5972.26699633]), validation loss 5235.70 ([5235.70060116]) (*)\n",
      "21:30    Epoch 35: train loss 5941.88 ([5941.87984814]), validation loss 5215.75 ([5215.75279713]) (*)\n",
      "21:33    Epoch 40: train loss 5916.67 ([5916.66739358]), validation loss 5202.18 ([5202.18321487]) (*)\n",
      "21:36    Epoch 45: train loss 5897.58 ([5897.57814532]), validation loss 5189.15 ([5189.1468598]) (*)\n",
      "21:39    Epoch 50: train loss 5882.54 ([5882.54364186]), validation loss 5181.35 ([5181.35160354]) (*)\n",
      "21:39  Early stopping did not improve performance\n",
      "21:39  Finished training\n",
      "21:39  Training estimator 6 / 10 in ensemble\n",
      "21:39  Starting training\n",
      "21:39    Method:                 sally\n",
      "21:39    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_5.npy\n",
      "21:39                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_5.npy\n",
      "21:39    Features:               all\n",
      "21:39    Method:                 sally\n",
      "21:39    Hidden layers:          (100, 100)\n",
      "21:39    Activation function:    tanh\n",
      "21:39    Batch size:             128\n",
      "21:39    Trainer:                amsgrad\n",
      "21:39    Epochs:                 50\n",
      "21:39    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:39    Validation split:       0.25\n",
      "21:39    Early stopping:         True\n",
      "21:39    Scale inputs:           True\n",
      "21:39  Loading training data\n",
      "21:39  Found 1000000 samples with 2 parameters and 26 observables\n",
      "21:39  Rescaling inputs\n",
      "21:39  Creating model for method sally\n",
      "21:39  Training model\n",
      "21:42    Epoch 5: train loss 5107.95 ([5107.94556927]), validation loss 4323.40 ([4323.4048771]) (*)\n",
      "21:45    Epoch 10: train loss 4979.02 ([4979.0170657]), validation loss 4232.43 ([4232.43253207]) (*)\n",
      "21:48    Epoch 15: train loss 4868.14 ([4868.14407435]), validation loss 4165.86 ([4165.86494481]) (*)\n",
      "21:50    Epoch 20: train loss 4784.79 ([4784.79479114]), validation loss 4121.85 ([4121.84587692]) (*)\n",
      "21:53    Epoch 25: train loss 4723.22 ([4723.21520804]), validation loss 4090.22 ([4090.22261792]) (*)\n",
      "21:56    Epoch 30: train loss 4674.82 ([4674.81669842]), validation loss 4067.15 ([4067.15240729]) (*)\n",
      "21:59    Epoch 35: train loss 4636.91 ([4636.91287301]), validation loss 4049.23 ([4049.2307293]) (*)\n",
      "22:02    Epoch 40: train loss 4609.50 ([4609.49590546]), validation loss 4036.47 ([4036.47006953]) (*)\n",
      "22:05    Epoch 45: train loss 4587.32 ([4587.32034249]), validation loss 4026.59 ([4026.58817878]) (*)\n",
      "22:07    Epoch 50: train loss 4569.33 ([4569.33247586]), validation loss 4019.18 ([4019.1827613]) (*)\n",
      "22:07  Early stopping did not improve performance\n",
      "22:07  Finished training\n",
      "22:07  Training estimator 7 / 10 in ensemble\n",
      "22:07  Starting training\n",
      "22:07    Method:                 sally\n",
      "22:07    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_6.npy\n",
      "22:07                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_6.npy\n",
      "22:07    Features:               all\n",
      "22:07    Method:                 sally\n",
      "22:07    Hidden layers:          (100, 100)\n",
      "22:07    Activation function:    tanh\n",
      "22:07    Batch size:             128\n",
      "22:07    Trainer:                amsgrad\n",
      "22:07    Epochs:                 50\n",
      "22:07    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:07    Validation split:       0.25\n",
      "22:07    Early stopping:         True\n",
      "22:07    Scale inputs:           True\n",
      "22:07  Loading training data\n",
      "22:07  Found 1000000 samples with 2 parameters and 26 observables\n",
      "22:07  Rescaling inputs\n",
      "22:07  Creating model for method sally\n",
      "22:07  Training model\n",
      "22:11    Epoch 5: train loss 4194.21 ([4194.20646379]), validation loss 7835.75 ([7835.74939454]) (*)\n",
      "22:14    Epoch 10: train loss 4074.99 ([4074.99243167]), validation loss 7730.58 ([7730.58314556]) (*)\n",
      "22:16    Epoch 15: train loss 3979.91 ([3979.90770833]), validation loss 7649.17 ([7649.17430186]) (*)\n",
      "22:19    Epoch 20: train loss 3910.10 ([3910.10491074]), validation loss 7594.28 ([7594.28138932]) (*)\n",
      "22:22    Epoch 25: train loss 3859.44 ([3859.43641266]), validation loss 7557.37 ([7557.37196918]) (*)\n",
      "22:25    Epoch 30: train loss 3820.90 ([3820.89529612]), validation loss 7526.34 ([7526.34282007]) (*)\n",
      "22:28    Epoch 35: train loss 3792.00 ([3791.99504283]), validation loss 7502.65 ([7502.64748329]) (*)\n",
      "22:30    Epoch 40: train loss 3770.01 ([3770.0098341]), validation loss 7485.99 ([7485.98787318]) (*)\n",
      "22:32    Epoch 45: train loss 3751.93 ([3751.93182655]), validation loss 7481.51 ([7481.50609831])\n",
      "22:34    Epoch 50: train loss 3738.91 ([3738.90754574]), validation loss 7466.53 ([7466.53091399]) (*)\n",
      "22:34  Early stopping did not improve performance\n",
      "22:34  Finished training\n",
      "22:34  Training estimator 8 / 10 in ensemble\n",
      "22:34  Starting training\n",
      "22:34    Method:                 sally\n",
      "22:34    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_7.npy\n",
      "22:34                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_7.npy\n",
      "22:34    Features:               all\n",
      "22:34    Method:                 sally\n",
      "22:34    Hidden layers:          (100, 100)\n",
      "22:34    Activation function:    tanh\n",
      "22:34    Batch size:             128\n",
      "22:34    Trainer:                amsgrad\n",
      "22:34    Epochs:                 50\n",
      "22:34    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:34    Validation split:       0.25\n",
      "22:34    Early stopping:         True\n",
      "22:34    Scale inputs:           True\n",
      "22:34  Loading training data\n",
      "22:35  Found 1000000 samples with 2 parameters and 26 observables\n",
      "22:35  Rescaling inputs\n",
      "22:35  Creating model for method sally\n",
      "22:35  Training model\n",
      "22:37    Epoch 5: train loss 4871.74 ([4871.74206148]), validation loss 4496.46 ([4496.458452]) (*)\n",
      "22:39    Epoch 10: train loss 4744.90 ([4744.90462645]), validation loss 4393.26 ([4393.26426038]) (*)\n",
      "22:41    Epoch 15: train loss 4643.79 ([4643.79447479]), validation loss 4316.24 ([4316.23980597]) (*)\n",
      "22:43    Epoch 20: train loss 4568.27 ([4568.27383256]), validation loss 4263.42 ([4263.41625095]) (*)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:45    Epoch 25: train loss 4508.96 ([4508.96085884]), validation loss 4228.42 ([4228.42056288]) (*)\n",
      "22:48    Epoch 30: train loss 4467.84 ([4467.83964272]), validation loss 4202.91 ([4202.9077541]) (*)\n",
      "22:50    Epoch 35: train loss 4434.13 ([4434.12668301]), validation loss 4185.31 ([4185.30728255]) (*)\n",
      "22:52    Epoch 40: train loss 4407.15 ([4407.14500307]), validation loss 4167.85 ([4167.84575266]) (*)\n",
      "22:54    Epoch 45: train loss 4387.30 ([4387.29530211]), validation loss 4157.52 ([4157.52218282]) (*)\n",
      "22:56    Epoch 50: train loss 4373.32 ([4373.31881845]), validation loss 4148.20 ([4148.19841218]) (*)\n",
      "22:56  Early stopping did not improve performance\n",
      "22:56  Finished training\n",
      "22:56  Training estimator 9 / 10 in ensemble\n",
      "22:56  Starting training\n",
      "22:56    Method:                 sally\n",
      "22:56    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_8.npy\n",
      "22:56                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_8.npy\n",
      "22:56    Features:               all\n",
      "22:56    Method:                 sally\n",
      "22:56    Hidden layers:          (100, 100)\n",
      "22:56    Activation function:    tanh\n",
      "22:56    Batch size:             128\n",
      "22:56    Trainer:                amsgrad\n",
      "22:56    Epochs:                 50\n",
      "22:56    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:56    Validation split:       0.25\n",
      "22:56    Early stopping:         True\n",
      "22:56    Scale inputs:           True\n",
      "22:56  Loading training data\n",
      "22:56  Found 1000000 samples with 2 parameters and 26 observables\n",
      "22:56  Rescaling inputs\n",
      "22:56  Creating model for method sally\n",
      "22:56  Training model\n",
      "22:58    Epoch 5: train loss 5385.70 ([5385.70251083]), validation loss 6189.57 ([6189.56674822]) (*)\n",
      "23:01    Epoch 10: train loss 5283.69 ([5283.69003255]), validation loss 6107.59 ([6107.58860789]) (*)\n",
      "23:03    Epoch 15: train loss 5193.80 ([5193.80384269]), validation loss 6038.23 ([6038.23160685]) (*)\n",
      "23:05    Epoch 20: train loss 5124.76 ([5124.7632796]), validation loss 5992.42 ([5992.42368106]) (*)\n",
      "23:07    Epoch 25: train loss 5072.34 ([5072.33539362]), validation loss 5949.48 ([5949.48282966]) (*)\n",
      "23:09    Epoch 30: train loss 5033.65 ([5033.65430259]), validation loss 5925.58 ([5925.58277838]) (*)\n",
      "23:11    Epoch 35: train loss 5003.24 ([5003.24042178]), validation loss 5906.63 ([5906.63230263]) (*)\n",
      "23:13    Epoch 40: train loss 4979.77 ([4979.76505539]), validation loss 5891.87 ([5891.87136986]) (*)\n",
      "23:15    Epoch 45: train loss 4961.21 ([4961.21492837]), validation loss 5880.51 ([5880.50640862]) (*)\n",
      "23:18    Epoch 50: train loss 4946.86 ([4946.85930701]), validation loss 5872.32 ([5872.31951896]) (*)\n",
      "23:18  Early stopping did not improve performance\n",
      "23:18  Finished training\n",
      "23:18  Training estimator 10 / 10 in ensemble\n",
      "23:18  Starting training\n",
      "23:18    Method:                 sally\n",
      "23:18    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_9.npy\n",
      "23:18                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_9.npy\n",
      "23:18    Features:               all\n",
      "23:18    Method:                 sally\n",
      "23:18    Hidden layers:          (100, 100)\n",
      "23:18    Activation function:    tanh\n",
      "23:18    Batch size:             128\n",
      "23:18    Trainer:                amsgrad\n",
      "23:18    Epochs:                 50\n",
      "23:18    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "23:18    Validation split:       0.25\n",
      "23:18    Early stopping:         True\n",
      "23:18    Scale inputs:           True\n",
      "23:18  Loading training data\n",
      "23:18  Found 1000000 samples with 2 parameters and 26 observables\n",
      "23:18  Rescaling inputs\n",
      "23:18  Creating model for method sally\n",
      "23:18  Training model\n",
      "23:20    Epoch 5: train loss 5279.95 ([5279.94580852]), validation loss 6711.10 ([6711.10349122]) (*)\n",
      "23:22    Epoch 10: train loss 5142.07 ([5142.07449544]), validation loss 6630.15 ([6630.15165434]) (*)\n",
      "23:24    Epoch 15: train loss 5030.92 ([5030.91637468]), validation loss 6569.72 ([6569.71503446]) (*)\n",
      "23:26    Epoch 20: train loss 4946.29 ([4946.28506844]), validation loss 6539.83 ([6539.82692905]) (*)\n",
      "23:29    Epoch 25: train loss 4883.55 ([4883.54631367]), validation loss 6511.00 ([6510.99877535])\n",
      "23:31    Epoch 30: train loss 4834.33 ([4834.33291398]), validation loss 6482.32 ([6482.31654238]) (*)\n",
      "23:33    Epoch 35: train loss 4796.62 ([4796.61644702]), validation loss 6468.61 ([6468.6068499]) (*)\n",
      "23:35    Epoch 40: train loss 4767.69 ([4767.69352772]), validation loss 6473.49 ([6473.49044788])\n",
      "23:37    Epoch 45: train loss 4749.27 ([4749.26790069]), validation loss 6447.69 ([6447.68681481]) (*)\n",
      "23:39    Epoch 50: train loss 4726.76 ([4726.76353729]), validation loss 6442.79 ([6442.79174289])\n",
      "23:39  Early stopping after epoch 49, with loss 6441.90 compared to final loss 6442.79\n",
      "23:39  Finished training\n",
      "23:39  Calculating expectation for 10 estimators in ensemble\n",
      "23:39  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "23:39  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "23:40  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "23:40  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "23:40  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "23:40  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "23:40  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "23:41  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "23:41  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "23:41  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble('all_tight', use_tight_cuts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'all_tight_sgd',\n",
    "    use_tight_cuts=True,\n",
    "    trainer='sgd',\n",
    "    nesterov_momentum=0.9,\n",
    "    initial_lr=0.1,\n",
    "    final_lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble('resurrection_tight', use_tight_cuts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7d171470-6c7e-4a74-8ca8-5589004e32d6"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
