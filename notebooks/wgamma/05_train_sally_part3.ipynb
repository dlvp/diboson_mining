{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=10, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=False)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.calculate_expectation(\n",
    "        x_filename=sample_dir + 'validation{}/x_validation.npy'.format(cut_label)\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_ensemble('all', use_tight_cuts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble('all_tight', use_tight_cuts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal observable basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:17  \n",
      "10:17  ------------------------------------------------------------\n",
      "10:17  |                                                          |\n",
      "10:17  |  MadMiner v2018.11.06                                    |\n",
      "10:17  |                                                          |\n",
      "10:17  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "10:17  |                                                          |\n",
      "10:17  ------------------------------------------------------------\n",
      "10:17  \n",
      "10:17  Training 10 estimators in ensemble\n",
      "10:17  Training estimator 1 / 10 in ensemble\n",
      "10:17  Starting training\n",
      "10:17    Method:                 sally\n",
      "10:17    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "10:17                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "10:17    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "10:17    Method:                 sally\n",
      "10:17    Hidden layers:          (100, 100)\n",
      "10:17    Activation function:    tanh\n",
      "10:17    Batch size:             128\n",
      "10:17    Trainer:                amsgrad\n",
      "10:17    Epochs:                 50\n",
      "10:17    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:17    Validation split:       0.25\n",
      "10:17    Early stopping:         True\n",
      "10:17    Scale inputs:           True\n",
      "10:17    Regularization:         None\n",
      "10:17  Loading training data\n",
      "10:17  Found 1000000 samples with 2 parameters and 27 observables\n",
      "10:17  Rescaling inputs\n",
      "10:17  Only using 7 of 27 observables\n",
      "10:17  Creating model for method sally\n",
      "10:17  Training model\n",
      "10:22    Epoch 5: train loss 42.6732 (mse_score: 42.6732)\n",
      "10:22              val. loss  35.8175 (mse_score: 35.8175) (*)\n",
      "10:26    Epoch 10: train loss 42.6135 (mse_score: 42.6135)\n",
      "10:26              val. loss  35.8124 (mse_score: 35.8124)\n",
      "10:30    Epoch 15: train loss 42.5128 (mse_score: 42.5128)\n",
      "10:30              val. loss  35.8295 (mse_score: 35.8295)\n",
      "10:34    Epoch 20: train loss 42.3958 (mse_score: 42.3958)\n",
      "10:34              val. loss  35.8434 (mse_score: 35.8434)\n",
      "10:38    Epoch 25: train loss 42.2665 (mse_score: 42.2665)\n",
      "10:38              val. loss  35.8651 (mse_score: 35.8651)\n",
      "10:43    Epoch 30: train loss 42.1480 (mse_score: 42.1480)\n",
      "10:43              val. loss  35.9187 (mse_score: 35.9187)\n",
      "10:47    Epoch 35: train loss 42.0465 (mse_score: 42.0465)\n",
      "10:47              val. loss  35.9341 (mse_score: 35.9341)\n",
      "10:50    Epoch 40: train loss 41.9756 (mse_score: 41.9756)\n",
      "10:50              val. loss  35.9494 (mse_score: 35.9494)\n",
      "10:53    Epoch 45: train loss 41.8905 (mse_score: 41.8905)\n",
      "10:53              val. loss  35.9605 (mse_score: 35.9605)\n",
      "10:57    Epoch 50: train loss 41.8482 (mse_score: 41.8482)\n",
      "10:57              val. loss  35.9768 (mse_score: 35.9768)\n",
      "10:57  Early stopping after epoch 8, with loss 35.81 compared to final loss 35.98\n",
      "10:57  Finished training\n",
      "10:57  Training estimator 2 / 10 in ensemble\n",
      "10:57  Starting training\n",
      "10:57    Method:                 sally\n",
      "10:57    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "10:57                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "10:57    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "10:57    Method:                 sally\n",
      "10:57    Hidden layers:          (100, 100)\n",
      "10:57    Activation function:    tanh\n",
      "10:57    Batch size:             128\n",
      "10:57    Trainer:                amsgrad\n",
      "10:57    Epochs:                 50\n",
      "10:57    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:57    Validation split:       0.25\n",
      "10:57    Early stopping:         True\n",
      "10:57    Scale inputs:           True\n",
      "10:57    Regularization:         None\n",
      "10:57  Loading training data\n",
      "10:57  Found 1000000 samples with 2 parameters and 27 observables\n",
      "10:57  Rescaling inputs\n",
      "10:57  Only using 7 of 27 observables\n",
      "10:57  Creating model for method sally\n",
      "10:57  Training model\n",
      "11:02    Epoch 5: train loss 22.8771 (mse_score: 22.8771)\n",
      "11:02              val. loss  15.9959 (mse_score: 15.9959)\n",
      "11:06    Epoch 10: train loss 22.7940 (mse_score: 22.7940)\n",
      "11:06              val. loss  15.9666 (mse_score: 15.9666)\n",
      "11:10    Epoch 15: train loss 22.6597 (mse_score: 22.6597)\n",
      "11:10              val. loss  15.9587 (mse_score: 15.9587)\n",
      "11:14    Epoch 20: train loss 22.4951 (mse_score: 22.4951)\n",
      "11:14              val. loss  15.9641 (mse_score: 15.9641)\n",
      "11:18    Epoch 25: train loss 22.3317 (mse_score: 22.3317)\n",
      "11:18              val. loss  16.0188 (mse_score: 16.0188)\n",
      "11:23    Epoch 30: train loss 22.1872 (mse_score: 22.1872)\n",
      "11:23              val. loss  16.0135 (mse_score: 16.0135)\n",
      "11:27    Epoch 35: train loss 22.0840 (mse_score: 22.0840)\n",
      "11:27              val. loss  16.0078 (mse_score: 16.0078)\n",
      "11:30    Epoch 40: train loss 21.9926 (mse_score: 21.9926)\n",
      "11:30              val. loss  16.0262 (mse_score: 16.0262)\n",
      "11:34    Epoch 45: train loss 21.9160 (mse_score: 21.9160)\n",
      "11:34              val. loss  16.0197 (mse_score: 16.0197)\n",
      "11:37    Epoch 50: train loss 21.8510 (mse_score: 21.8510)\n",
      "11:37              val. loss  16.0250 (mse_score: 16.0250)\n",
      "11:37  Early stopping after epoch 21, with loss 15.95 compared to final loss 16.03\n",
      "11:37  Finished training\n",
      "11:37  Training estimator 3 / 10 in ensemble\n",
      "11:37  Starting training\n",
      "11:37    Method:                 sally\n",
      "11:37    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "11:37                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "11:37    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "11:37    Method:                 sally\n",
      "11:37    Hidden layers:          (100, 100)\n",
      "11:37    Activation function:    tanh\n",
      "11:37    Batch size:             128\n",
      "11:37    Trainer:                amsgrad\n",
      "11:37    Epochs:                 50\n",
      "11:37    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "11:37    Validation split:       0.25\n",
      "11:37    Early stopping:         True\n",
      "11:37    Scale inputs:           True\n",
      "11:37    Regularization:         None\n",
      "11:37  Loading training data\n",
      "11:37  Found 1000000 samples with 2 parameters and 27 observables\n",
      "11:37  Rescaling inputs\n",
      "11:38  Only using 7 of 27 observables\n",
      "11:38  Creating model for method sally\n",
      "11:38  Training model\n",
      "11:41    Epoch 5: train loss 24.1832 (mse_score: 24.1832)\n",
      "11:41              val. loss  16.8674 (mse_score: 16.8674)\n",
      "11:45    Epoch 10: train loss 24.1313 (mse_score: 24.1313)\n",
      "11:45              val. loss  16.8564 (mse_score: 16.8564)\n",
      "11:48    Epoch 15: train loss 24.0646 (mse_score: 24.0646)\n",
      "11:48              val. loss  16.8665 (mse_score: 16.8665)\n",
      "11:51    Epoch 20: train loss 23.9796 (mse_score: 23.9796)\n",
      "11:51              val. loss  16.8635 (mse_score: 16.8635)\n",
      "11:54    Epoch 25: train loss 23.8848 (mse_score: 23.8848)\n",
      "11:54              val. loss  16.9078 (mse_score: 16.9078)\n",
      "11:57    Epoch 30: train loss 23.8006 (mse_score: 23.8006)\n",
      "11:57              val. loss  16.9151 (mse_score: 16.9151)\n",
      "12:01    Epoch 35: train loss 23.7208 (mse_score: 23.7208)\n",
      "12:01              val. loss  16.8943 (mse_score: 16.8943)\n",
      "12:04    Epoch 40: train loss 23.6406 (mse_score: 23.6406)\n",
      "12:04              val. loss  16.9465 (mse_score: 16.9465)\n",
      "12:07    Epoch 45: train loss 23.5714 (mse_score: 23.5714)\n",
      "12:07              val. loss  16.9153 (mse_score: 16.9153)\n",
      "12:10    Epoch 50: train loss 23.5288 (mse_score: 23.5288)\n",
      "12:10              val. loss  16.9524 (mse_score: 16.9524)\n",
      "12:10  Early stopping after epoch 9, with loss 16.84 compared to final loss 16.95\n",
      "12:10  Finished training\n",
      "12:10  Training estimator 4 / 10 in ensemble\n",
      "12:10  Starting training\n",
      "12:10    Method:                 sally\n",
      "12:10    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "12:10                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "12:10    Features:               [0, 5, 6, 7, 9, 10, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:10    Method:                 sally\n",
      "12:10    Hidden layers:          (100, 100)\n",
      "12:10    Activation function:    tanh\n",
      "12:10    Batch size:             128\n",
      "12:10    Trainer:                amsgrad\n",
      "12:10    Epochs:                 50\n",
      "12:10    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "12:10    Validation split:       0.25\n",
      "12:10    Early stopping:         True\n",
      "12:10    Scale inputs:           True\n",
      "12:10    Regularization:         None\n",
      "12:10  Loading training data\n",
      "12:10  Found 1000000 samples with 2 parameters and 27 observables\n",
      "12:10  Rescaling inputs\n",
      "12:10  Only using 7 of 27 observables\n",
      "12:10  Creating model for method sally\n",
      "12:10  Training model\n",
      "12:13    Epoch 5: train loss 27.9968 (mse_score: 27.9968)\n",
      "12:13              val. loss  56.4557 (mse_score: 56.4557)\n",
      "12:16    Epoch 10: train loss 27.9399 (mse_score: 27.9399)\n",
      "12:16              val. loss  56.4850 (mse_score: 56.4850)\n",
      "12:20    Epoch 15: train loss 27.8691 (mse_score: 27.8691)\n",
      "12:20              val. loss  56.5434 (mse_score: 56.5434)\n",
      "12:23    Epoch 20: train loss 27.7649 (mse_score: 27.7649)\n",
      "12:23              val. loss  56.6091 (mse_score: 56.6091)\n",
      "12:26    Epoch 25: train loss 27.6617 (mse_score: 27.6617)\n",
      "12:26              val. loss  56.6867 (mse_score: 56.6867)\n",
      "12:29    Epoch 30: train loss 27.5367 (mse_score: 27.5367)\n",
      "12:29              val. loss  56.8753 (mse_score: 56.8753)\n",
      "12:32    Epoch 35: train loss 27.4471 (mse_score: 27.4471)\n",
      "12:32              val. loss  56.7154 (mse_score: 56.7154)\n",
      "12:35    Epoch 40: train loss 27.4198 (mse_score: 27.4198)\n",
      "12:35              val. loss  56.7266 (mse_score: 56.7266)\n",
      "12:39    Epoch 45: train loss 27.2754 (mse_score: 27.2754)\n",
      "12:39              val. loss  56.7012 (mse_score: 56.7012)\n",
      "12:42    Epoch 50: train loss 27.2150 (mse_score: 27.2150)\n",
      "12:42              val. loss  56.7705 (mse_score: 56.7705)\n",
      "12:42  Early stopping after epoch 2, with loss 56.44 compared to final loss 56.77\n",
      "12:42  Finished training\n",
      "12:42  Training estimator 5 / 10 in ensemble\n",
      "12:42  Starting training\n",
      "12:42    Method:                 sally\n",
      "12:42    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "12:42                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "12:42    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "12:42    Method:                 sally\n",
      "12:42    Hidden layers:          (100, 100)\n",
      "12:42    Activation function:    tanh\n",
      "12:42    Batch size:             128\n",
      "12:42    Trainer:                amsgrad\n",
      "12:42    Epochs:                 50\n",
      "12:42    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "12:42    Validation split:       0.25\n",
      "12:42    Early stopping:         True\n",
      "12:42    Scale inputs:           True\n",
      "12:42    Regularization:         None\n",
      "12:42  Loading training data\n",
      "12:42  Found 1000000 samples with 2 parameters and 27 observables\n",
      "12:42  Rescaling inputs\n",
      "12:42  Only using 7 of 27 observables\n",
      "12:42  Creating model for method sally\n",
      "12:42  Training model\n",
      "12:45    Epoch 5: train loss 24.4139 (mse_score: 24.4139)\n",
      "12:45              val. loss  33.7097 (mse_score: 33.7097)\n",
      "12:48    Epoch 10: train loss 24.2100 (mse_score: 24.2100)\n",
      "12:48              val. loss  33.7383 (mse_score: 33.7383)\n",
      "12:52    Epoch 15: train loss 23.9984 (mse_score: 23.9984)\n",
      "12:52              val. loss  33.7704 (mse_score: 33.7704)\n",
      "12:55    Epoch 20: train loss 23.7647 (mse_score: 23.7647)\n",
      "12:55              val. loss  33.8149 (mse_score: 33.8149)\n",
      "12:59    Epoch 25: train loss 23.5693 (mse_score: 23.5693)\n",
      "12:59              val. loss  33.7976 (mse_score: 33.7976)\n",
      "13:03    Epoch 30: train loss 23.4120 (mse_score: 23.4120)\n",
      "13:03              val. loss  33.7916 (mse_score: 33.7916)\n",
      "13:06    Epoch 35: train loss 23.2709 (mse_score: 23.2709)\n",
      "13:06              val. loss  33.8039 (mse_score: 33.8039)\n",
      "13:10    Epoch 40: train loss 23.1579 (mse_score: 23.1579)\n",
      "13:10              val. loss  33.8067 (mse_score: 33.8067)\n",
      "13:13    Epoch 45: train loss 23.0640 (mse_score: 23.0640)\n",
      "13:13              val. loss  33.7952 (mse_score: 33.7952)\n",
      "13:16    Epoch 50: train loss 22.9891 (mse_score: 22.9891)\n",
      "13:16              val. loss  33.8058 (mse_score: 33.8058)\n",
      "13:16  Early stopping after epoch 1, with loss 33.69 compared to final loss 33.81\n",
      "13:16  Finished training\n",
      "13:16  Training estimator 6 / 10 in ensemble\n",
      "13:16  Starting training\n",
      "13:16    Method:                 sally\n",
      "13:16    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "13:16                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "13:16    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "13:16    Method:                 sally\n",
      "13:16    Hidden layers:          (100, 100)\n",
      "13:16    Activation function:    tanh\n",
      "13:16    Batch size:             128\n",
      "13:16    Trainer:                amsgrad\n",
      "13:16    Epochs:                 50\n",
      "13:16    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:16    Validation split:       0.25\n",
      "13:16    Early stopping:         True\n",
      "13:16    Scale inputs:           True\n",
      "13:16    Regularization:         None\n",
      "13:16  Loading training data\n",
      "13:16  Found 1000000 samples with 2 parameters and 27 observables\n",
      "13:16  Rescaling inputs\n",
      "13:16  Only using 7 of 27 observables\n",
      "13:16  Creating model for method sally\n",
      "13:16  Training model\n",
      "13:20    Epoch 5: train loss 25.3873 (mse_score: 25.3873)\n",
      "13:20              val. loss  14.1302 (mse_score: 14.1302) (*)\n",
      "13:23    Epoch 10: train loss 25.3570 (mse_score: 25.3570)\n",
      "13:23              val. loss  14.1290 (mse_score: 14.1290)\n",
      "13:26    Epoch 15: train loss 25.2997 (mse_score: 25.2997)\n",
      "13:26              val. loss  14.1398 (mse_score: 14.1398)\n",
      "13:29    Epoch 20: train loss 25.2290 (mse_score: 25.2290)\n",
      "13:29              val. loss  14.1570 (mse_score: 14.1570)\n",
      "13:33    Epoch 25: train loss 25.1562 (mse_score: 25.1562)\n",
      "13:33              val. loss  14.1567 (mse_score: 14.1567)\n",
      "13:36    Epoch 30: train loss 25.0654 (mse_score: 25.0654)\n",
      "13:36              val. loss  14.1867 (mse_score: 14.1867)\n",
      "13:39    Epoch 35: train loss 24.9784 (mse_score: 24.9784)\n",
      "13:39              val. loss  14.2112 (mse_score: 14.2112)\n",
      "13:42    Epoch 40: train loss 24.8956 (mse_score: 24.8956)\n",
      "13:42              val. loss  14.2045 (mse_score: 14.2045)\n",
      "13:46    Epoch 45: train loss 24.8354 (mse_score: 24.8354)\n",
      "13:46              val. loss  14.2410 (mse_score: 14.2410)\n",
      "13:49    Epoch 50: train loss 24.7826 (mse_score: 24.7826)\n",
      "13:49              val. loss  14.2458 (mse_score: 14.2458)\n",
      "13:49  Early stopping after epoch 8, with loss 14.13 compared to final loss 14.25\n",
      "13:49  Finished training\n",
      "13:49  Training estimator 7 / 10 in ensemble\n",
      "13:49  Starting training\n",
      "13:49    Method:                 sally\n",
      "13:49    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "13:49                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "13:49    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "13:49    Method:                 sally\n",
      "13:49    Hidden layers:          (100, 100)\n",
      "13:49    Activation function:    tanh\n",
      "13:49    Batch size:             128\n",
      "13:49    Trainer:                amsgrad\n",
      "13:49    Epochs:                 50\n",
      "13:49    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:49    Validation split:       0.25\n",
      "13:49    Early stopping:         True\n",
      "13:49    Scale inputs:           True\n",
      "13:49    Regularization:         None\n",
      "13:49  Loading training data\n",
      "13:49  Found 1000000 samples with 2 parameters and 27 observables\n",
      "13:49  Rescaling inputs\n",
      "13:49  Only using 7 of 27 observables\n",
      "13:49  Creating model for method sally\n",
      "13:49  Training model\n",
      "13:52    Epoch 5: train loss 27.6271 (mse_score: 27.6271)\n",
      "13:52              val. loss  15.2674 (mse_score: 15.2674)\n",
      "13:56    Epoch 10: train loss 27.4750 (mse_score: 27.4750)\n",
      "13:56              val. loss  15.2378 (mse_score: 15.2378) (*)\n",
      "13:59    Epoch 15: train loss 27.2367 (mse_score: 27.2367)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:59              val. loss  15.2806 (mse_score: 15.2806)\n",
      "14:02    Epoch 20: train loss 27.0021 (mse_score: 27.0021)\n",
      "14:02              val. loss  15.3122 (mse_score: 15.3122)\n",
      "14:06    Epoch 25: train loss 26.7801 (mse_score: 26.7801)\n",
      "14:06              val. loss  15.2731 (mse_score: 15.2731)\n",
      "14:10    Epoch 30: train loss 26.6030 (mse_score: 26.6030)\n",
      "14:10              val. loss  15.3450 (mse_score: 15.3450)\n",
      "14:14    Epoch 35: train loss 26.4305 (mse_score: 26.4305)\n",
      "14:14              val. loss  15.3525 (mse_score: 15.3525)\n",
      "14:17    Epoch 40: train loss 26.2876 (mse_score: 26.2876)\n",
      "14:17              val. loss  15.3528 (mse_score: 15.3528)\n",
      "14:21    Epoch 45: train loss 26.1920 (mse_score: 26.1920)\n",
      "14:21              val. loss  15.3670 (mse_score: 15.3670)\n",
      "14:24    Epoch 50: train loss 26.0938 (mse_score: 26.0938)\n",
      "14:24              val. loss  15.3836 (mse_score: 15.3836)\n",
      "14:24  Early stopping after epoch 11, with loss 15.23 compared to final loss 15.38\n",
      "14:24  Finished training\n",
      "14:24  Training estimator 8 / 10 in ensemble\n",
      "14:24  Starting training\n",
      "14:24    Method:                 sally\n",
      "14:24    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "14:24                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "14:24    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "14:24    Method:                 sally\n",
      "14:24    Hidden layers:          (100, 100)\n",
      "14:24    Activation function:    tanh\n",
      "14:24    Batch size:             128\n",
      "14:24    Trainer:                amsgrad\n",
      "14:24    Epochs:                 50\n",
      "14:24    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:24    Validation split:       0.25\n",
      "14:24    Early stopping:         True\n",
      "14:24    Scale inputs:           True\n",
      "14:24    Regularization:         None\n",
      "14:24  Loading training data\n",
      "14:24  Found 1000000 samples with 2 parameters and 27 observables\n",
      "14:24  Rescaling inputs\n",
      "14:24  Only using 7 of 27 observables\n",
      "14:24  Creating model for method sally\n",
      "14:24  Training model\n",
      "14:28    Epoch 5: train loss 33.3442 (mse_score: 33.3442)\n",
      "14:28              val. loss  35.1271 (mse_score: 35.1271) (*)\n",
      "14:31    Epoch 10: train loss 33.2679 (mse_score: 33.2679)\n",
      "14:31              val. loss  35.1383 (mse_score: 35.1383)\n",
      "14:35    Epoch 15: train loss 33.1361 (mse_score: 33.1361)\n",
      "14:35              val. loss  35.1387 (mse_score: 35.1387)\n",
      "14:38    Epoch 20: train loss 33.0070 (mse_score: 33.0070)\n",
      "14:38              val. loss  35.1265 (mse_score: 35.1265)\n",
      "14:42    Epoch 25: train loss 32.8714 (mse_score: 32.8714)\n",
      "14:42              val. loss  35.0822 (mse_score: 35.0822)\n",
      "14:46    Epoch 30: train loss 32.7378 (mse_score: 32.7378)\n",
      "14:46              val. loss  35.1466 (mse_score: 35.1466)\n",
      "14:49    Epoch 35: train loss 32.6155 (mse_score: 32.6155)\n",
      "14:49              val. loss  35.1185 (mse_score: 35.1185)\n",
      "14:53    Epoch 40: train loss 32.5195 (mse_score: 32.5195)\n",
      "14:53              val. loss  35.0918 (mse_score: 35.0918)\n",
      "14:56    Epoch 45: train loss 32.4342 (mse_score: 32.4342)\n",
      "14:56              val. loss  35.1373 (mse_score: 35.1373)\n",
      "14:59    Epoch 50: train loss 32.3604 (mse_score: 32.3604)\n",
      "14:59              val. loss  35.0881 (mse_score: 35.0881)\n",
      "14:59  Early stopping after epoch 39, with loss 35.08 compared to final loss 35.09\n",
      "14:59  Finished training\n",
      "14:59  Training estimator 9 / 10 in ensemble\n",
      "14:59  Starting training\n",
      "14:59    Method:                 sally\n",
      "14:59    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "14:59                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "14:59    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "14:59    Method:                 sally\n",
      "14:59    Hidden layers:          (100, 100)\n",
      "14:59    Activation function:    tanh\n",
      "14:59    Batch size:             128\n",
      "14:59    Trainer:                amsgrad\n",
      "14:59    Epochs:                 50\n",
      "14:59    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:59    Validation split:       0.25\n",
      "14:59    Early stopping:         True\n",
      "14:59    Scale inputs:           True\n",
      "14:59    Regularization:         None\n",
      "14:59  Loading training data\n",
      "14:59  Found 1000000 samples with 2 parameters and 27 observables\n",
      "14:59  Rescaling inputs\n",
      "14:59  Only using 7 of 27 observables\n",
      "14:59  Creating model for method sally\n",
      "14:59  Training model\n",
      "15:03    Epoch 5: train loss 32.9724 (mse_score: 32.9724)\n",
      "15:03              val. loss  26.1641 (mse_score: 26.1641) (*)\n",
      "15:06    Epoch 10: train loss 32.9091 (mse_score: 32.9091)\n",
      "15:06              val. loss  26.1351 (mse_score: 26.1351) (*)\n",
      "15:10    Epoch 15: train loss 32.7529 (mse_score: 32.7529)\n",
      "15:10              val. loss  26.1327 (mse_score: 26.1327)\n",
      "15:14    Epoch 20: train loss 32.5813 (mse_score: 32.5813)\n",
      "15:14              val. loss  26.1138 (mse_score: 26.1138) (*)\n",
      "15:18    Epoch 25: train loss 32.4134 (mse_score: 32.4134)\n",
      "15:18              val. loss  26.1920 (mse_score: 26.1920)\n",
      "15:21    Epoch 30: train loss 32.2663 (mse_score: 32.2663)\n",
      "15:21              val. loss  26.0771 (mse_score: 26.0771)\n",
      "15:25    Epoch 35: train loss 32.1270 (mse_score: 32.1270)\n",
      "15:25              val. loss  26.0706 (mse_score: 26.0706)\n",
      "15:28    Epoch 40: train loss 31.9978 (mse_score: 31.9978)\n",
      "15:28              val. loss  26.1322 (mse_score: 26.1322)\n",
      "15:31    Epoch 45: train loss 31.9176 (mse_score: 31.9176)\n",
      "15:31              val. loss  26.0279 (mse_score: 26.0279)\n",
      "15:34    Epoch 50: train loss 31.8394 (mse_score: 31.8394)\n",
      "15:34              val. loss  26.0443 (mse_score: 26.0443)\n",
      "15:34  Early stopping after epoch 33, with loss 26.01 compared to final loss 26.04\n",
      "15:34  Finished training\n",
      "15:34  Training estimator 10 / 10 in ensemble\n",
      "15:34  Starting training\n",
      "15:34    Method:                 sally\n",
      "15:34    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "15:34                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "15:34    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "15:34    Method:                 sally\n",
      "15:34    Hidden layers:          (100, 100)\n",
      "15:34    Activation function:    tanh\n",
      "15:34    Batch size:             128\n",
      "15:34    Trainer:                amsgrad\n",
      "15:34    Epochs:                 50\n",
      "15:34    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:34    Validation split:       0.25\n",
      "15:34    Early stopping:         True\n",
      "15:34    Scale inputs:           True\n",
      "15:34    Regularization:         None\n",
      "15:34  Loading training data\n",
      "15:34  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:34  Rescaling inputs\n",
      "15:34  Only using 7 of 27 observables\n",
      "15:34  Creating model for method sally\n",
      "15:34  Training model\n",
      "15:38    Epoch 5: train loss 35.0197 (mse_score: 35.0197)\n",
      "15:38              val. loss  27.5716 (mse_score: 27.5716) (*)\n",
      "15:41    Epoch 10: train loss 34.9713 (mse_score: 34.9713)\n",
      "15:41              val. loss  27.6254 (mse_score: 27.6254)\n",
      "15:44    Epoch 15: train loss 34.9078 (mse_score: 34.9078)\n",
      "15:44              val. loss  27.6266 (mse_score: 27.6266)\n",
      "15:47    Epoch 20: train loss 34.8141 (mse_score: 34.8141)\n",
      "15:47              val. loss  27.9264 (mse_score: 27.9264)\n",
      "15:50    Epoch 25: train loss 34.7087 (mse_score: 34.7087)\n",
      "15:50              val. loss  27.6784 (mse_score: 27.6784)\n",
      "15:53    Epoch 30: train loss 34.6462 (mse_score: 34.6462)\n",
      "15:53              val. loss  27.7791 (mse_score: 27.7791)\n",
      "15:56    Epoch 35: train loss 34.5578 (mse_score: 34.5578)\n",
      "15:56              val. loss  27.7948 (mse_score: 27.7948)\n",
      "16:00    Epoch 40: train loss 34.4927 (mse_score: 34.4927)\n",
      "16:00              val. loss  27.8544 (mse_score: 27.8544)\n",
      "16:03    Epoch 45: train loss 34.4324 (mse_score: 34.4324)\n",
      "16:03              val. loss  27.8277 (mse_score: 27.8277)\n",
      "16:06    Epoch 50: train loss 34.3807 (mse_score: 34.3807)\n",
      "16:06              val. loss  27.8714 (mse_score: 27.8714)\n",
      "16:06  Early stopping after epoch 8, with loss 27.57 compared to final loss 27.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:06  Finished training\n",
      "16:06  Calculating expectation for 10 estimators in ensemble\n",
      "16:06  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "16:06  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "16:06  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "16:06  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "16:07  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "16:07  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "16:07  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "16:07  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "16:08  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "16:08  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'minimal',\n",
    "    use_tight_cuts=False,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:45  Training 10 estimators in ensemble\n",
      "17:45  Training estimator 1 / 10 in ensemble\n",
      "17:45  Starting training\n",
      "17:45    Method:                 sally\n",
      "17:45    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "17:45                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "17:45    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "17:45    Method:                 sally\n",
      "17:45    Hidden layers:          (100, 100)\n",
      "17:45    Activation function:    tanh\n",
      "17:45    Batch size:             128\n",
      "17:45    Trainer:                amsgrad\n",
      "17:45    Epochs:                 50\n",
      "17:45    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "17:45    Validation split:       0.25\n",
      "17:45    Early stopping:         True\n",
      "17:45    Scale inputs:           True\n",
      "17:45    Regularization:         None\n",
      "17:45  Loading training data\n",
      "17:45  Found 1000000 samples with 2 parameters and 27 observables\n",
      "17:45  Rescaling inputs\n",
      "17:45  Only using 7 of 27 observables\n",
      "17:45  Creating model for method sally\n",
      "17:45  Training model\n",
      "17:47    Epoch 5: train loss 5627.6866 (mse_score: 5627.6866)\n",
      "17:47              val. loss  5226.5781 (mse_score: 5226.5781) (*)\n",
      "17:49    Epoch 10: train loss 5566.9658 (mse_score: 5566.9658)\n",
      "17:49              val. loss  5187.9377 (mse_score: 5187.9377) (*)\n",
      "17:51    Epoch 15: train loss 5512.4371 (mse_score: 5512.4371)\n",
      "17:51              val. loss  5143.4715 (mse_score: 5143.4715) (*)\n",
      "17:55    Epoch 20: train loss 5467.5434 (mse_score: 5467.5434)\n",
      "17:55              val. loss  5114.9066 (mse_score: 5114.9066) (*)\n",
      "18:02    Epoch 25: train loss 5432.3961 (mse_score: 5432.3961)\n",
      "18:02              val. loss  5090.2156 (mse_score: 5090.2156) (*)\n",
      "18:08    Epoch 30: train loss 5403.9318 (mse_score: 5403.9318)\n",
      "18:08              val. loss  5071.8860 (mse_score: 5071.8860) (*)\n",
      "18:12    Epoch 35: train loss 5381.5770 (mse_score: 5381.5770)\n",
      "18:12              val. loss  5057.1919 (mse_score: 5057.1919) (*)\n",
      "18:16    Epoch 40: train loss 5364.0291 (mse_score: 5364.0291)\n",
      "18:16              val. loss  5046.8533 (mse_score: 5046.8533) (*)\n",
      "18:19    Epoch 45: train loss 5350.1267 (mse_score: 5350.1267)\n",
      "18:19              val. loss  5038.1016 (mse_score: 5038.1016) (*)\n",
      "18:22    Epoch 50: train loss 5339.2054 (mse_score: 5339.2054)\n",
      "18:22              val. loss  5030.6587 (mse_score: 5030.6587) (*)\n",
      "18:22  Early stopping did not improve performance\n",
      "18:22  Finished training\n",
      "18:22  Training estimator 2 / 10 in ensemble\n",
      "18:22  Starting training\n",
      "18:22    Method:                 sally\n",
      "18:22    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "18:22                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "18:22    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "18:22    Method:                 sally\n",
      "18:22    Hidden layers:          (100, 100)\n",
      "18:22    Activation function:    tanh\n",
      "18:22    Batch size:             128\n",
      "18:22    Trainer:                amsgrad\n",
      "18:22    Epochs:                 50\n",
      "18:22    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "18:22    Validation split:       0.25\n",
      "18:22    Early stopping:         True\n",
      "18:22    Scale inputs:           True\n",
      "18:22    Regularization:         None\n",
      "18:22  Loading training data\n",
      "18:22  Found 1000000 samples with 2 parameters and 27 observables\n",
      "18:22  Rescaling inputs\n",
      "18:22  Only using 7 of 27 observables\n",
      "18:22  Creating model for method sally\n",
      "18:22  Training model\n",
      "18:27    Epoch 5: train loss 5684.8590 (mse_score: 5684.8590)\n",
      "18:27              val. loss  5679.8037 (mse_score: 5679.8037) (*)\n",
      "18:30    Epoch 10: train loss 5632.8959 (mse_score: 5632.8959)\n",
      "18:30              val. loss  5641.7739 (mse_score: 5641.7739) (*)\n",
      "18:35    Epoch 15: train loss 5586.6399 (mse_score: 5586.6399)\n",
      "18:35              val. loss  5607.6657 (mse_score: 5607.6657) (*)\n",
      "18:37    Epoch 20: train loss 5548.4003 (mse_score: 5548.4003)\n",
      "18:37              val. loss  5583.1286 (mse_score: 5583.1286) (*)\n",
      "18:38    Epoch 25: train loss 5518.4872 (mse_score: 5518.4872)\n",
      "18:38              val. loss  5562.5550 (mse_score: 5562.5550) (*)\n",
      "18:40    Epoch 30: train loss 5494.5678 (mse_score: 5494.5678)\n",
      "18:40              val. loss  5548.6057 (mse_score: 5548.6057) (*)\n",
      "18:42    Epoch 35: train loss 5475.7704 (mse_score: 5475.7704)\n",
      "18:42              val. loss  5535.1346 (mse_score: 5535.1346) (*)\n",
      "18:44    Epoch 40: train loss 5461.0114 (mse_score: 5461.0114)\n",
      "18:44              val. loss  5525.0846 (mse_score: 5525.0846) (*)\n",
      "18:46    Epoch 45: train loss 5449.5253 (mse_score: 5449.5253)\n",
      "18:46              val. loss  5517.3000 (mse_score: 5517.3000) (*)\n",
      "18:49    Epoch 50: train loss 5439.4060 (mse_score: 5439.4060)\n",
      "18:49              val. loss  5511.3959 (mse_score: 5511.3959) (*)\n",
      "18:49  Early stopping did not improve performance\n",
      "18:49  Finished training\n",
      "18:49  Training estimator 3 / 10 in ensemble\n",
      "18:49  Starting training\n",
      "18:49    Method:                 sally\n",
      "18:49    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "18:49                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "18:49    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "18:49    Method:                 sally\n",
      "18:49    Hidden layers:          (100, 100)\n",
      "18:49    Activation function:    tanh\n",
      "18:49    Batch size:             128\n",
      "18:49    Trainer:                amsgrad\n",
      "18:49    Epochs:                 50\n",
      "18:49    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "18:49    Validation split:       0.25\n",
      "18:49    Early stopping:         True\n",
      "18:49    Scale inputs:           True\n",
      "18:49    Regularization:         None\n",
      "18:49  Loading training data\n",
      "18:49  Found 1000000 samples with 2 parameters and 27 observables\n",
      "18:49  Rescaling inputs\n",
      "18:49  Only using 7 of 27 observables\n",
      "18:49  Creating model for method sally\n",
      "18:49  Training model\n",
      "18:51    Epoch 5: train loss 4988.5113 (mse_score: 4988.5113)\n",
      "18:51              val. loss  4952.1502 (mse_score: 4952.1502) (*)\n",
      "18:54    Epoch 10: train loss 4934.4649 (mse_score: 4934.4649)\n",
      "18:54              val. loss  4914.0991 (mse_score: 4914.0991) (*)\n",
      "18:56    Epoch 15: train loss 4883.0835 (mse_score: 4883.0835)\n",
      "18:56              val. loss  4881.6190 (mse_score: 4881.6190) (*)\n",
      "18:59    Epoch 20: train loss 4840.5492 (mse_score: 4840.5492)\n",
      "18:59              val. loss  4852.9612 (mse_score: 4852.9612) (*)\n",
      "19:01    Epoch 25: train loss 4805.0863 (mse_score: 4805.0863)\n",
      "19:01              val. loss  4831.6348 (mse_score: 4831.6348) (*)\n",
      "19:04    Epoch 30: train loss 4777.8079 (mse_score: 4777.8079)\n",
      "19:04              val. loss  4814.4689 (mse_score: 4814.4689) (*)\n",
      "19:07    Epoch 35: train loss 4756.4155 (mse_score: 4756.4155)\n",
      "19:07              val. loss  4801.5388 (mse_score: 4801.5388) (*)\n",
      "19:10    Epoch 40: train loss 4739.2184 (mse_score: 4739.2184)\n",
      "19:10              val. loss  4791.9127 (mse_score: 4791.9127) (*)\n",
      "19:13    Epoch 45: train loss 4725.4822 (mse_score: 4725.4822)\n",
      "19:13              val. loss  4784.0047 (mse_score: 4784.0047) (*)\n",
      "19:16    Epoch 50: train loss 4714.6262 (mse_score: 4714.6262)\n",
      "19:16              val. loss  4776.6471 (mse_score: 4776.6471) (*)\n",
      "19:16  Early stopping did not improve performance\n",
      "19:16  Finished training\n",
      "19:16  Training estimator 4 / 10 in ensemble\n",
      "19:16  Starting training\n",
      "19:16    Method:                 sally\n",
      "19:16    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_3.npy\n",
      "19:16                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_3.npy\n",
      "19:16    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "19:16    Method:                 sally\n",
      "19:16    Hidden layers:          (100, 100)\n",
      "19:16    Activation function:    tanh\n",
      "19:16    Batch size:             128\n",
      "19:16    Trainer:                amsgrad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:16    Epochs:                 50\n",
      "19:16    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "19:16    Validation split:       0.25\n",
      "19:16    Early stopping:         True\n",
      "19:16    Scale inputs:           True\n",
      "19:16    Regularization:         None\n",
      "19:16  Loading training data\n",
      "19:16  Found 1000000 samples with 2 parameters and 27 observables\n",
      "19:16  Rescaling inputs\n",
      "19:16  Only using 7 of 27 observables\n",
      "19:16  Creating model for method sally\n",
      "19:16  Training model\n",
      "19:18    Epoch 5: train loss 6256.6170 (mse_score: 6256.6170)\n",
      "19:18              val. loss  5746.6299 (mse_score: 5746.6299) (*)\n",
      "19:20    Epoch 10: train loss 6213.3620 (mse_score: 6213.3620)\n",
      "19:20              val. loss  5713.3855 (mse_score: 5713.3855) (*)\n",
      "19:22    Epoch 15: train loss 6171.7914 (mse_score: 6171.7914)\n",
      "19:22              val. loss  5684.1580 (mse_score: 5684.1580) (*)\n",
      "19:24    Epoch 20: train loss 6137.1892 (mse_score: 6137.1892)\n",
      "19:24              val. loss  5680.8732 (mse_score: 5680.8732)\n",
      "19:26    Epoch 25: train loss 6109.6836 (mse_score: 6109.6836)\n",
      "19:26              val. loss  5643.9798 (mse_score: 5643.9798) (*)\n",
      "19:28    Epoch 30: train loss 6086.9014 (mse_score: 6086.9014)\n",
      "19:28              val. loss  5627.4638 (mse_score: 5627.4638) (*)\n",
      "19:31    Epoch 35: train loss 6068.8866 (mse_score: 6068.8866)\n",
      "19:31              val. loss  5615.2940 (mse_score: 5615.2940) (*)\n",
      "19:33    Epoch 40: train loss 6054.8385 (mse_score: 6054.8385)\n",
      "19:33              val. loss  5605.7790 (mse_score: 5605.7790) (*)\n",
      "19:35    Epoch 45: train loss 6043.5838 (mse_score: 6043.5838)\n",
      "19:35              val. loss  5598.1572 (mse_score: 5598.1572) (*)\n",
      "19:37    Epoch 50: train loss 6034.9137 (mse_score: 6034.9137)\n",
      "19:37              val. loss  5593.4361 (mse_score: 5593.4361) (*)\n",
      "19:37  Early stopping did not improve performance\n",
      "19:37  Finished training\n",
      "19:37  Training estimator 5 / 10 in ensemble\n",
      "19:37  Starting training\n",
      "19:37    Method:                 sally\n",
      "19:37    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_4.npy\n",
      "19:37                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_4.npy\n",
      "19:37    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "19:37    Method:                 sally\n",
      "19:37    Hidden layers:          (100, 100)\n",
      "19:37    Activation function:    tanh\n",
      "19:37    Batch size:             128\n",
      "19:37    Trainer:                amsgrad\n",
      "19:37    Epochs:                 50\n",
      "19:37    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "19:37    Validation split:       0.25\n",
      "19:37    Early stopping:         True\n",
      "19:37    Scale inputs:           True\n",
      "19:37    Regularization:         None\n",
      "19:37  Loading training data\n",
      "19:37  Found 1000000 samples with 2 parameters and 27 observables\n",
      "19:37  Rescaling inputs\n",
      "19:37  Only using 7 of 27 observables\n",
      "19:37  Creating model for method sally\n",
      "19:37  Training model\n",
      "19:39    Epoch 5: train loss 9190.9273 (mse_score: 9190.9273)\n",
      "19:39              val. loss  4950.1827 (mse_score: 4950.1827) (*)\n",
      "19:41    Epoch 10: train loss 9143.5526 (mse_score: 9143.5526)\n",
      "19:41              val. loss  4918.7786 (mse_score: 4918.7786) (*)\n",
      "19:44    Epoch 15: train loss 9100.1141 (mse_score: 9100.1141)\n",
      "19:44              val. loss  4886.7204 (mse_score: 4886.7204) (*)\n",
      "19:46    Epoch 20: train loss 9065.4180 (mse_score: 9065.4180)\n",
      "19:46              val. loss  4857.4265 (mse_score: 4857.4265) (*)\n",
      "19:48    Epoch 25: train loss 9037.9333 (mse_score: 9037.9333)\n",
      "19:48              val. loss  4841.1765 (mse_score: 4841.1765) (*)\n",
      "19:50    Epoch 30: train loss 9015.4783 (mse_score: 9015.4783)\n",
      "19:50              val. loss  4826.0245 (mse_score: 4826.0245) (*)\n",
      "19:53    Epoch 35: train loss 8997.2867 (mse_score: 8997.2867)\n",
      "19:53              val. loss  4813.2396 (mse_score: 4813.2396) (*)\n",
      "19:56    Epoch 40: train loss 8983.1327 (mse_score: 8983.1327)\n",
      "19:56              val. loss  4803.5048 (mse_score: 4803.5048) (*)\n",
      "19:59    Epoch 45: train loss 8971.5072 (mse_score: 8971.5072)\n",
      "19:59              val. loss  4795.8602 (mse_score: 4795.8602) (*)\n",
      "20:01    Epoch 50: train loss 8962.3766 (mse_score: 8962.3766)\n",
      "20:01              val. loss  4789.7016 (mse_score: 4789.7016) (*)\n",
      "20:01  Early stopping did not improve performance\n",
      "20:01  Finished training\n",
      "20:01  Training estimator 6 / 10 in ensemble\n",
      "20:01  Starting training\n",
      "20:01    Method:                 sally\n",
      "20:01    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_5.npy\n",
      "20:01                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_5.npy\n",
      "20:01    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "20:01    Method:                 sally\n",
      "20:01    Hidden layers:          (100, 100)\n",
      "20:01    Activation function:    tanh\n",
      "20:01    Batch size:             128\n",
      "20:01    Trainer:                amsgrad\n",
      "20:01    Epochs:                 50\n",
      "20:01    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:01    Validation split:       0.25\n",
      "20:01    Early stopping:         True\n",
      "20:01    Scale inputs:           True\n",
      "20:01    Regularization:         None\n",
      "20:01  Loading training data\n",
      "20:01  Found 1000000 samples with 2 parameters and 27 observables\n",
      "20:01  Rescaling inputs\n",
      "20:01  Only using 7 of 27 observables\n",
      "20:01  Creating model for method sally\n",
      "20:01  Training model\n",
      "20:05    Epoch 5: train loss 6231.2777 (mse_score: 6231.2777)\n",
      "20:05              val. loss  4916.4486 (mse_score: 4916.4486) (*)\n",
      "20:08    Epoch 10: train loss 6172.7013 (mse_score: 6172.7013)\n",
      "20:08              val. loss  4887.8187 (mse_score: 4887.8187) (*)\n",
      "20:11    Epoch 15: train loss 6119.6866 (mse_score: 6119.6866)\n",
      "20:11              val. loss  4862.3537 (mse_score: 4862.3537) (*)\n",
      "20:14    Epoch 20: train loss 6075.6774 (mse_score: 6075.6774)\n",
      "20:14              val. loss  4847.3820 (mse_score: 4847.3820)\n",
      "20:16    Epoch 25: train loss 6039.8439 (mse_score: 6039.8439)\n",
      "20:16              val. loss  4820.5019 (mse_score: 4820.5019) (*)\n",
      "20:18    Epoch 30: train loss 6011.1814 (mse_score: 6011.1814)\n",
      "20:18              val. loss  4807.8150 (mse_score: 4807.8150) (*)\n",
      "20:20    Epoch 35: train loss 5988.0464 (mse_score: 5988.0464)\n",
      "20:20              val. loss  4794.1033 (mse_score: 4794.1033) (*)\n",
      "20:23    Epoch 40: train loss 5969.5896 (mse_score: 5969.5896)\n",
      "20:23              val. loss  4785.4966 (mse_score: 4785.4966) (*)\n",
      "20:25    Epoch 45: train loss 5954.9267 (mse_score: 5954.9267)\n",
      "20:25              val. loss  4780.6273 (mse_score: 4780.6273)\n",
      "20:27    Epoch 50: train loss 5943.1743 (mse_score: 5943.1743)\n",
      "20:27              val. loss  4772.6620 (mse_score: 4772.6620) (*)\n",
      "20:27  Early stopping did not improve performance\n",
      "20:27  Finished training\n",
      "20:27  Training estimator 7 / 10 in ensemble\n",
      "20:27  Starting training\n",
      "20:27    Method:                 sally\n",
      "20:27    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_6.npy\n",
      "20:27                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_6.npy\n",
      "20:27    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "20:27    Method:                 sally\n",
      "20:27    Hidden layers:          (100, 100)\n",
      "20:27    Activation function:    tanh\n",
      "20:27    Batch size:             128\n",
      "20:27    Trainer:                amsgrad\n",
      "20:27    Epochs:                 50\n",
      "20:27    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:27    Validation split:       0.25\n",
      "20:27    Early stopping:         True\n",
      "20:27    Scale inputs:           True\n",
      "20:27    Regularization:         None\n",
      "20:27  Loading training data\n",
      "20:27  Found 1000000 samples with 2 parameters and 27 observables\n",
      "20:27  Rescaling inputs\n",
      "20:27  Only using 7 of 27 observables\n",
      "20:27  Creating model for method sally\n",
      "20:27  Training model\n",
      "20:29    Epoch 5: train loss 6052.7328 (mse_score: 6052.7328)\n",
      "20:29              val. loss  4643.9271 (mse_score: 4643.9271)\n",
      "20:32    Epoch 10: train loss 6018.6943 (mse_score: 6018.6943)\n",
      "20:32              val. loss  4605.8512 (mse_score: 4605.8512) (*)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:34    Epoch 15: train loss 5985.0754 (mse_score: 5985.0754)\n",
      "20:34              val. loss  4580.2710 (mse_score: 4580.2710) (*)\n",
      "20:36    Epoch 20: train loss 5955.6626 (mse_score: 5955.6626)\n",
      "20:36              val. loss  4558.9926 (mse_score: 4558.9926) (*)\n",
      "20:39    Epoch 25: train loss 5931.3096 (mse_score: 5931.3096)\n",
      "20:39              val. loss  4542.4598 (mse_score: 4542.4598) (*)\n",
      "20:42    Epoch 30: train loss 5912.1207 (mse_score: 5912.1207)\n",
      "20:42              val. loss  4529.3317 (mse_score: 4529.3317) (*)\n",
      "20:45    Epoch 35: train loss 5896.3959 (mse_score: 5896.3959)\n",
      "20:45              val. loss  4519.0245 (mse_score: 4519.0245) (*)\n",
      "20:48    Epoch 40: train loss 5884.4619 (mse_score: 5884.4619)\n",
      "20:48              val. loss  4510.9803 (mse_score: 4510.9803) (*)\n",
      "20:50    Epoch 45: train loss 5874.3930 (mse_score: 5874.3930)\n",
      "20:50              val. loss  4503.8867 (mse_score: 4503.8867) (*)\n",
      "20:52    Epoch 50: train loss 5866.3236 (mse_score: 5866.3236)\n",
      "20:52              val. loss  4498.4590 (mse_score: 4498.4590) (*)\n",
      "20:52  Early stopping did not improve performance\n",
      "20:52  Finished training\n",
      "20:52  Training estimator 8 / 10 in ensemble\n",
      "20:52  Starting training\n",
      "20:52    Method:                 sally\n",
      "20:52    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_7.npy\n",
      "20:52                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_7.npy\n",
      "20:52    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "20:52    Method:                 sally\n",
      "20:52    Hidden layers:          (100, 100)\n",
      "20:52    Activation function:    tanh\n",
      "20:52    Batch size:             128\n",
      "20:52    Trainer:                amsgrad\n",
      "20:52    Epochs:                 50\n",
      "20:52    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:52    Validation split:       0.25\n",
      "20:52    Early stopping:         True\n",
      "20:52    Scale inputs:           True\n",
      "20:52    Regularization:         None\n",
      "20:52  Loading training data\n",
      "20:52  Found 1000000 samples with 2 parameters and 27 observables\n",
      "20:52  Rescaling inputs\n",
      "20:52  Only using 7 of 27 observables\n",
      "20:52  Creating model for method sally\n",
      "20:52  Training model\n",
      "20:55    Epoch 5: train loss 5743.1114 (mse_score: 5743.1114)\n",
      "20:55              val. loss  5822.5429 (mse_score: 5822.5429) (*)\n",
      "20:57    Epoch 10: train loss 5672.6134 (mse_score: 5672.6134)\n",
      "20:57              val. loss  5765.5486 (mse_score: 5765.5486) (*)\n",
      "21:00    Epoch 15: train loss 5612.7025 (mse_score: 5612.7025)\n",
      "21:00              val. loss  5715.7418 (mse_score: 5715.7418) (*)\n",
      "21:02    Epoch 20: train loss 5564.9047 (mse_score: 5564.9047)\n",
      "21:02              val. loss  5678.8551 (mse_score: 5678.8551) (*)\n",
      "21:04    Epoch 25: train loss 5526.4263 (mse_score: 5526.4263)\n",
      "21:04              val. loss  5647.1110 (mse_score: 5647.1110) (*)\n",
      "21:06    Epoch 30: train loss 5495.3858 (mse_score: 5495.3858)\n",
      "21:06              val. loss  5623.1796 (mse_score: 5623.1796) (*)\n",
      "21:08    Epoch 35: train loss 5470.9517 (mse_score: 5470.9517)\n",
      "21:08              val. loss  5605.7429 (mse_score: 5605.7429) (*)\n",
      "21:10    Epoch 40: train loss 5451.5094 (mse_score: 5451.5094)\n",
      "21:10              val. loss  5590.3723 (mse_score: 5590.3723) (*)\n",
      "21:13    Epoch 45: train loss 5435.9561 (mse_score: 5435.9561)\n",
      "21:13              val. loss  5578.9961 (mse_score: 5578.9961) (*)\n",
      "21:16    Epoch 50: train loss 5423.4696 (mse_score: 5423.4696)\n",
      "21:16              val. loss  5577.0729 (mse_score: 5577.0729)\n",
      "21:16  Early stopping after epoch 48, with loss 5573.42 compared to final loss 5577.07\n",
      "21:16  Finished training\n",
      "21:16  Training estimator 9 / 10 in ensemble\n",
      "21:16  Starting training\n",
      "21:16    Method:                 sally\n",
      "21:16    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_8.npy\n",
      "21:16                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_8.npy\n",
      "21:16    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "21:16    Method:                 sally\n",
      "21:16    Hidden layers:          (100, 100)\n",
      "21:16    Activation function:    tanh\n",
      "21:16    Batch size:             128\n",
      "21:16    Trainer:                amsgrad\n",
      "21:16    Epochs:                 50\n",
      "21:16    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:16    Validation split:       0.25\n",
      "21:16    Early stopping:         True\n",
      "21:16    Scale inputs:           True\n",
      "21:16    Regularization:         None\n",
      "21:16  Loading training data\n",
      "21:16  Found 1000000 samples with 2 parameters and 27 observables\n",
      "21:16  Rescaling inputs\n",
      "21:16  Only using 7 of 27 observables\n",
      "21:16  Creating model for method sally\n",
      "21:16  Training model\n",
      "21:20    Epoch 5: train loss 5169.5984 (mse_score: 5169.5984)\n",
      "21:20              val. loss  6498.6267 (mse_score: 6498.6267) (*)\n",
      "21:24    Epoch 10: train loss 5120.6571 (mse_score: 5120.6571)\n",
      "21:24              val. loss  6457.7956 (mse_score: 6457.7956) (*)\n",
      "21:28    Epoch 15: train loss 5076.2310 (mse_score: 5076.2310)\n",
      "21:28              val. loss  6422.5953 (mse_score: 6422.5953) (*)\n",
      "21:32    Epoch 20: train loss 5038.6874 (mse_score: 5038.6874)\n",
      "21:32              val. loss  6390.0389 (mse_score: 6390.0389) (*)\n",
      "21:36    Epoch 25: train loss 5008.4516 (mse_score: 5008.4516)\n",
      "21:36              val. loss  6367.2418 (mse_score: 6367.2418) (*)\n",
      "21:39    Epoch 30: train loss 4983.2124 (mse_score: 4983.2124)\n",
      "21:39              val. loss  6345.9019 (mse_score: 6345.9019) (*)\n",
      "21:42    Epoch 35: train loss 4963.5655 (mse_score: 4963.5655)\n",
      "21:42              val. loss  6332.8819 (mse_score: 6332.8819)\n",
      "21:45    Epoch 40: train loss 4948.0174 (mse_score: 4948.0174)\n",
      "21:45              val. loss  6317.8976 (mse_score: 6317.8976) (*)\n",
      "21:47    Epoch 45: train loss 4935.7697 (mse_score: 4935.7697)\n",
      "21:47              val. loss  6309.6447 (mse_score: 6309.6447) (*)\n",
      "21:50    Epoch 50: train loss 4925.7315 (mse_score: 4925.7315)\n",
      "21:50              val. loss  6300.3502 (mse_score: 6300.3502) (*)\n",
      "21:50  Early stopping did not improve performance\n",
      "21:50  Finished training\n",
      "21:50  Training estimator 10 / 10 in ensemble\n",
      "21:50  Starting training\n",
      "21:50    Method:                 sally\n",
      "21:50    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_9.npy\n",
      "21:50                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_9.npy\n",
      "21:50    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "21:50    Method:                 sally\n",
      "21:50    Hidden layers:          (100, 100)\n",
      "21:50    Activation function:    tanh\n",
      "21:50    Batch size:             128\n",
      "21:50    Trainer:                amsgrad\n",
      "21:50    Epochs:                 50\n",
      "21:50    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:50    Validation split:       0.25\n",
      "21:50    Early stopping:         True\n",
      "21:50    Scale inputs:           True\n",
      "21:50    Regularization:         None\n",
      "21:50  Loading training data\n",
      "21:50  Found 1000000 samples with 2 parameters and 27 observables\n",
      "21:50  Rescaling inputs\n",
      "21:50  Only using 7 of 27 observables\n",
      "21:50  Creating model for method sally\n",
      "21:50  Training model\n",
      "21:54    Epoch 5: train loss 6016.3814 (mse_score: 6016.3814)\n",
      "21:54              val. loss  5748.8221 (mse_score: 5748.8221) (*)\n",
      "21:58    Epoch 10: train loss 5958.6671 (mse_score: 5958.6671)\n",
      "21:58              val. loss  5698.2626 (mse_score: 5698.2626) (*)\n",
      "22:02    Epoch 15: train loss 5906.0801 (mse_score: 5906.0801)\n",
      "22:02              val. loss  5661.4717 (mse_score: 5661.4717) (*)\n",
      "22:08    Epoch 20: train loss 5861.9402 (mse_score: 5861.9402)\n",
      "22:08              val. loss  5622.2286 (mse_score: 5622.2286) (*)\n",
      "22:14    Epoch 25: train loss 5825.3004 (mse_score: 5825.3004)\n",
      "22:14              val. loss  5595.5924 (mse_score: 5595.5924) (*)\n",
      "22:19    Epoch 30: train loss 5796.6766 (mse_score: 5796.6766)\n",
      "22:19              val. loss  5572.7852 (mse_score: 5572.7852) (*)\n",
      "22:24    Epoch 35: train loss 5774.2955 (mse_score: 5774.2955)\n",
      "22:24              val. loss  5556.2746 (mse_score: 5556.2746) (*)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:28    Epoch 40: train loss 5756.0633 (mse_score: 5756.0633)\n",
      "22:28              val. loss  5544.0166 (mse_score: 5544.0166) (*)\n",
      "22:31    Epoch 45: train loss 5742.0018 (mse_score: 5742.0018)\n",
      "22:31              val. loss  5533.3237 (mse_score: 5533.3237) (*)\n",
      "22:35    Epoch 50: train loss 5730.9445 (mse_score: 5730.9445)\n",
      "22:35              val. loss  5525.4781 (mse_score: 5525.4781) (*)\n",
      "22:35  Early stopping did not improve performance\n",
      "22:35  Finished training\n",
      "22:35  Calculating expectation for 10 estimators in ensemble\n",
      "22:35  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "22:35  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "22:36  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "22:36  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "22:36  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "22:37  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "22:37  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "22:38  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "22:38  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "22:38  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_reg',\n",
    "    use_tight_cuts=False,\n",
    "    grad_x_regularization=0.1,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight_reg',\n",
    "    use_tight_cuts=True,\n",
    "    grad_x_regularization=0.1,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just resurrection phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'resurrection',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[26] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
