{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=10, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=False)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.calculate_expectation(\n",
    "        x_filename=sample_dir + 'validation{}/x_validation.npy'.format(cut_label)\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_ensemble('all', use_tight_cuts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble('all_tight', use_tight_cuts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal observable basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:17  \n",
      "10:17  ------------------------------------------------------------\n",
      "10:17  |                                                          |\n",
      "10:17  |  MadMiner v2018.11.06                                    |\n",
      "10:17  |                                                          |\n",
      "10:17  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "10:17  |                                                          |\n",
      "10:17  ------------------------------------------------------------\n",
      "10:17  \n",
      "10:17  Training 10 estimators in ensemble\n",
      "10:17  Training estimator 1 / 10 in ensemble\n",
      "10:17  Starting training\n",
      "10:17    Method:                 sally\n",
      "10:17    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "10:17                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "10:17    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "10:17    Method:                 sally\n",
      "10:17    Hidden layers:          (100, 100)\n",
      "10:17    Activation function:    tanh\n",
      "10:17    Batch size:             128\n",
      "10:17    Trainer:                amsgrad\n",
      "10:17    Epochs:                 50\n",
      "10:17    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:17    Validation split:       0.25\n",
      "10:17    Early stopping:         True\n",
      "10:17    Scale inputs:           True\n",
      "10:17    Regularization:         None\n",
      "10:17  Loading training data\n",
      "10:17  Found 1000000 samples with 2 parameters and 27 observables\n",
      "10:17  Rescaling inputs\n",
      "10:17  Only using 7 of 27 observables\n",
      "10:17  Creating model for method sally\n",
      "10:17  Training model\n",
      "10:22    Epoch 5: train loss 42.6732 (mse_score: 42.6732)\n",
      "10:22              val. loss  35.8175 (mse_score: 35.8175) (*)\n",
      "10:26    Epoch 10: train loss 42.6135 (mse_score: 42.6135)\n",
      "10:26              val. loss  35.8124 (mse_score: 35.8124)\n",
      "10:30    Epoch 15: train loss 42.5128 (mse_score: 42.5128)\n",
      "10:30              val. loss  35.8295 (mse_score: 35.8295)\n",
      "10:34    Epoch 20: train loss 42.3958 (mse_score: 42.3958)\n",
      "10:34              val. loss  35.8434 (mse_score: 35.8434)\n",
      "10:38    Epoch 25: train loss 42.2665 (mse_score: 42.2665)\n",
      "10:38              val. loss  35.8651 (mse_score: 35.8651)\n",
      "10:43    Epoch 30: train loss 42.1480 (mse_score: 42.1480)\n",
      "10:43              val. loss  35.9187 (mse_score: 35.9187)\n",
      "10:47    Epoch 35: train loss 42.0465 (mse_score: 42.0465)\n",
      "10:47              val. loss  35.9341 (mse_score: 35.9341)\n",
      "10:50    Epoch 40: train loss 41.9756 (mse_score: 41.9756)\n",
      "10:50              val. loss  35.9494 (mse_score: 35.9494)\n",
      "10:53    Epoch 45: train loss 41.8905 (mse_score: 41.8905)\n",
      "10:53              val. loss  35.9605 (mse_score: 35.9605)\n",
      "10:57    Epoch 50: train loss 41.8482 (mse_score: 41.8482)\n",
      "10:57              val. loss  35.9768 (mse_score: 35.9768)\n",
      "10:57  Early stopping after epoch 8, with loss 35.81 compared to final loss 35.98\n",
      "10:57  Finished training\n",
      "10:57  Training estimator 2 / 10 in ensemble\n",
      "10:57  Starting training\n",
      "10:57    Method:                 sally\n",
      "10:57    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "10:57                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "10:57    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "10:57    Method:                 sally\n",
      "10:57    Hidden layers:          (100, 100)\n",
      "10:57    Activation function:    tanh\n",
      "10:57    Batch size:             128\n",
      "10:57    Trainer:                amsgrad\n",
      "10:57    Epochs:                 50\n",
      "10:57    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:57    Validation split:       0.25\n",
      "10:57    Early stopping:         True\n",
      "10:57    Scale inputs:           True\n",
      "10:57    Regularization:         None\n",
      "10:57  Loading training data\n",
      "10:57  Found 1000000 samples with 2 parameters and 27 observables\n",
      "10:57  Rescaling inputs\n",
      "10:57  Only using 7 of 27 observables\n",
      "10:57  Creating model for method sally\n",
      "10:57  Training model\n",
      "11:02    Epoch 5: train loss 22.8771 (mse_score: 22.8771)\n",
      "11:02              val. loss  15.9959 (mse_score: 15.9959)\n",
      "11:06    Epoch 10: train loss 22.7940 (mse_score: 22.7940)\n",
      "11:06              val. loss  15.9666 (mse_score: 15.9666)\n",
      "11:10    Epoch 15: train loss 22.6597 (mse_score: 22.6597)\n",
      "11:10              val. loss  15.9587 (mse_score: 15.9587)\n",
      "11:14    Epoch 20: train loss 22.4951 (mse_score: 22.4951)\n",
      "11:14              val. loss  15.9641 (mse_score: 15.9641)\n",
      "11:18    Epoch 25: train loss 22.3317 (mse_score: 22.3317)\n",
      "11:18              val. loss  16.0188 (mse_score: 16.0188)\n",
      "11:23    Epoch 30: train loss 22.1872 (mse_score: 22.1872)\n",
      "11:23              val. loss  16.0135 (mse_score: 16.0135)\n",
      "11:27    Epoch 35: train loss 22.0840 (mse_score: 22.0840)\n",
      "11:27              val. loss  16.0078 (mse_score: 16.0078)\n",
      "11:30    Epoch 40: train loss 21.9926 (mse_score: 21.9926)\n",
      "11:30              val. loss  16.0262 (mse_score: 16.0262)\n",
      "11:34    Epoch 45: train loss 21.9160 (mse_score: 21.9160)\n",
      "11:34              val. loss  16.0197 (mse_score: 16.0197)\n",
      "11:37    Epoch 50: train loss 21.8510 (mse_score: 21.8510)\n",
      "11:37              val. loss  16.0250 (mse_score: 16.0250)\n",
      "11:37  Early stopping after epoch 21, with loss 15.95 compared to final loss 16.03\n",
      "11:37  Finished training\n",
      "11:37  Training estimator 3 / 10 in ensemble\n",
      "11:37  Starting training\n",
      "11:37    Method:                 sally\n",
      "11:37    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "11:37                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "11:37    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "11:37    Method:                 sally\n",
      "11:37    Hidden layers:          (100, 100)\n",
      "11:37    Activation function:    tanh\n",
      "11:37    Batch size:             128\n",
      "11:37    Trainer:                amsgrad\n",
      "11:37    Epochs:                 50\n",
      "11:37    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "11:37    Validation split:       0.25\n",
      "11:37    Early stopping:         True\n",
      "11:37    Scale inputs:           True\n",
      "11:37    Regularization:         None\n",
      "11:37  Loading training data\n",
      "11:37  Found 1000000 samples with 2 parameters and 27 observables\n",
      "11:37  Rescaling inputs\n",
      "11:38  Only using 7 of 27 observables\n",
      "11:38  Creating model for method sally\n",
      "11:38  Training model\n",
      "11:41    Epoch 5: train loss 24.1832 (mse_score: 24.1832)\n",
      "11:41              val. loss  16.8674 (mse_score: 16.8674)\n",
      "11:45    Epoch 10: train loss 24.1313 (mse_score: 24.1313)\n",
      "11:45              val. loss  16.8564 (mse_score: 16.8564)\n",
      "11:48    Epoch 15: train loss 24.0646 (mse_score: 24.0646)\n",
      "11:48              val. loss  16.8665 (mse_score: 16.8665)\n",
      "11:51    Epoch 20: train loss 23.9796 (mse_score: 23.9796)\n",
      "11:51              val. loss  16.8635 (mse_score: 16.8635)\n",
      "11:54    Epoch 25: train loss 23.8848 (mse_score: 23.8848)\n",
      "11:54              val. loss  16.9078 (mse_score: 16.9078)\n",
      "11:57    Epoch 30: train loss 23.8006 (mse_score: 23.8006)\n",
      "11:57              val. loss  16.9151 (mse_score: 16.9151)\n",
      "12:01    Epoch 35: train loss 23.7208 (mse_score: 23.7208)\n",
      "12:01              val. loss  16.8943 (mse_score: 16.8943)\n",
      "12:04    Epoch 40: train loss 23.6406 (mse_score: 23.6406)\n",
      "12:04              val. loss  16.9465 (mse_score: 16.9465)\n",
      "12:07    Epoch 45: train loss 23.5714 (mse_score: 23.5714)\n",
      "12:07              val. loss  16.9153 (mse_score: 16.9153)\n",
      "12:10    Epoch 50: train loss 23.5288 (mse_score: 23.5288)\n",
      "12:10              val. loss  16.9524 (mse_score: 16.9524)\n",
      "12:10  Early stopping after epoch 9, with loss 16.84 compared to final loss 16.95\n",
      "12:10  Finished training\n",
      "12:10  Training estimator 4 / 10 in ensemble\n",
      "12:10  Starting training\n",
      "12:10    Method:                 sally\n",
      "12:10    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "12:10                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "12:10    Features:               [0, 5, 6, 7, 9, 10, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:10    Method:                 sally\n",
      "12:10    Hidden layers:          (100, 100)\n",
      "12:10    Activation function:    tanh\n",
      "12:10    Batch size:             128\n",
      "12:10    Trainer:                amsgrad\n",
      "12:10    Epochs:                 50\n",
      "12:10    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "12:10    Validation split:       0.25\n",
      "12:10    Early stopping:         True\n",
      "12:10    Scale inputs:           True\n",
      "12:10    Regularization:         None\n",
      "12:10  Loading training data\n",
      "12:10  Found 1000000 samples with 2 parameters and 27 observables\n",
      "12:10  Rescaling inputs\n",
      "12:10  Only using 7 of 27 observables\n",
      "12:10  Creating model for method sally\n",
      "12:10  Training model\n",
      "12:13    Epoch 5: train loss 27.9968 (mse_score: 27.9968)\n",
      "12:13              val. loss  56.4557 (mse_score: 56.4557)\n",
      "12:16    Epoch 10: train loss 27.9399 (mse_score: 27.9399)\n",
      "12:16              val. loss  56.4850 (mse_score: 56.4850)\n",
      "12:20    Epoch 15: train loss 27.8691 (mse_score: 27.8691)\n",
      "12:20              val. loss  56.5434 (mse_score: 56.5434)\n",
      "12:23    Epoch 20: train loss 27.7649 (mse_score: 27.7649)\n",
      "12:23              val. loss  56.6091 (mse_score: 56.6091)\n",
      "12:26    Epoch 25: train loss 27.6617 (mse_score: 27.6617)\n",
      "12:26              val. loss  56.6867 (mse_score: 56.6867)\n",
      "12:29    Epoch 30: train loss 27.5367 (mse_score: 27.5367)\n",
      "12:29              val. loss  56.8753 (mse_score: 56.8753)\n",
      "12:32    Epoch 35: train loss 27.4471 (mse_score: 27.4471)\n",
      "12:32              val. loss  56.7154 (mse_score: 56.7154)\n",
      "12:35    Epoch 40: train loss 27.4198 (mse_score: 27.4198)\n",
      "12:35              val. loss  56.7266 (mse_score: 56.7266)\n",
      "12:39    Epoch 45: train loss 27.2754 (mse_score: 27.2754)\n",
      "12:39              val. loss  56.7012 (mse_score: 56.7012)\n",
      "12:42    Epoch 50: train loss 27.2150 (mse_score: 27.2150)\n",
      "12:42              val. loss  56.7705 (mse_score: 56.7705)\n",
      "12:42  Early stopping after epoch 2, with loss 56.44 compared to final loss 56.77\n",
      "12:42  Finished training\n",
      "12:42  Training estimator 5 / 10 in ensemble\n",
      "12:42  Starting training\n",
      "12:42    Method:                 sally\n",
      "12:42    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "12:42                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "12:42    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "12:42    Method:                 sally\n",
      "12:42    Hidden layers:          (100, 100)\n",
      "12:42    Activation function:    tanh\n",
      "12:42    Batch size:             128\n",
      "12:42    Trainer:                amsgrad\n",
      "12:42    Epochs:                 50\n",
      "12:42    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "12:42    Validation split:       0.25\n",
      "12:42    Early stopping:         True\n",
      "12:42    Scale inputs:           True\n",
      "12:42    Regularization:         None\n",
      "12:42  Loading training data\n",
      "12:42  Found 1000000 samples with 2 parameters and 27 observables\n",
      "12:42  Rescaling inputs\n",
      "12:42  Only using 7 of 27 observables\n",
      "12:42  Creating model for method sally\n",
      "12:42  Training model\n",
      "12:45    Epoch 5: train loss 24.4139 (mse_score: 24.4139)\n",
      "12:45              val. loss  33.7097 (mse_score: 33.7097)\n",
      "12:48    Epoch 10: train loss 24.2100 (mse_score: 24.2100)\n",
      "12:48              val. loss  33.7383 (mse_score: 33.7383)\n",
      "12:52    Epoch 15: train loss 23.9984 (mse_score: 23.9984)\n",
      "12:52              val. loss  33.7704 (mse_score: 33.7704)\n",
      "12:55    Epoch 20: train loss 23.7647 (mse_score: 23.7647)\n",
      "12:55              val. loss  33.8149 (mse_score: 33.8149)\n",
      "12:59    Epoch 25: train loss 23.5693 (mse_score: 23.5693)\n",
      "12:59              val. loss  33.7976 (mse_score: 33.7976)\n",
      "13:03    Epoch 30: train loss 23.4120 (mse_score: 23.4120)\n",
      "13:03              val. loss  33.7916 (mse_score: 33.7916)\n",
      "13:06    Epoch 35: train loss 23.2709 (mse_score: 23.2709)\n",
      "13:06              val. loss  33.8039 (mse_score: 33.8039)\n",
      "13:10    Epoch 40: train loss 23.1579 (mse_score: 23.1579)\n",
      "13:10              val. loss  33.8067 (mse_score: 33.8067)\n",
      "13:13    Epoch 45: train loss 23.0640 (mse_score: 23.0640)\n",
      "13:13              val. loss  33.7952 (mse_score: 33.7952)\n",
      "13:16    Epoch 50: train loss 22.9891 (mse_score: 22.9891)\n",
      "13:16              val. loss  33.8058 (mse_score: 33.8058)\n",
      "13:16  Early stopping after epoch 1, with loss 33.69 compared to final loss 33.81\n",
      "13:16  Finished training\n",
      "13:16  Training estimator 6 / 10 in ensemble\n",
      "13:16  Starting training\n",
      "13:16    Method:                 sally\n",
      "13:16    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "13:16                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "13:16    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "13:16    Method:                 sally\n",
      "13:16    Hidden layers:          (100, 100)\n",
      "13:16    Activation function:    tanh\n",
      "13:16    Batch size:             128\n",
      "13:16    Trainer:                amsgrad\n",
      "13:16    Epochs:                 50\n",
      "13:16    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:16    Validation split:       0.25\n",
      "13:16    Early stopping:         True\n",
      "13:16    Scale inputs:           True\n",
      "13:16    Regularization:         None\n",
      "13:16  Loading training data\n",
      "13:16  Found 1000000 samples with 2 parameters and 27 observables\n",
      "13:16  Rescaling inputs\n",
      "13:16  Only using 7 of 27 observables\n",
      "13:16  Creating model for method sally\n",
      "13:16  Training model\n",
      "13:20    Epoch 5: train loss 25.3873 (mse_score: 25.3873)\n",
      "13:20              val. loss  14.1302 (mse_score: 14.1302) (*)\n",
      "13:23    Epoch 10: train loss 25.3570 (mse_score: 25.3570)\n",
      "13:23              val. loss  14.1290 (mse_score: 14.1290)\n",
      "13:26    Epoch 15: train loss 25.2997 (mse_score: 25.2997)\n",
      "13:26              val. loss  14.1398 (mse_score: 14.1398)\n",
      "13:29    Epoch 20: train loss 25.2290 (mse_score: 25.2290)\n",
      "13:29              val. loss  14.1570 (mse_score: 14.1570)\n",
      "13:33    Epoch 25: train loss 25.1562 (mse_score: 25.1562)\n",
      "13:33              val. loss  14.1567 (mse_score: 14.1567)\n",
      "13:36    Epoch 30: train loss 25.0654 (mse_score: 25.0654)\n",
      "13:36              val. loss  14.1867 (mse_score: 14.1867)\n",
      "13:39    Epoch 35: train loss 24.9784 (mse_score: 24.9784)\n",
      "13:39              val. loss  14.2112 (mse_score: 14.2112)\n",
      "13:42    Epoch 40: train loss 24.8956 (mse_score: 24.8956)\n",
      "13:42              val. loss  14.2045 (mse_score: 14.2045)\n",
      "13:46    Epoch 45: train loss 24.8354 (mse_score: 24.8354)\n",
      "13:46              val. loss  14.2410 (mse_score: 14.2410)\n",
      "13:49    Epoch 50: train loss 24.7826 (mse_score: 24.7826)\n",
      "13:49              val. loss  14.2458 (mse_score: 14.2458)\n",
      "13:49  Early stopping after epoch 8, with loss 14.13 compared to final loss 14.25\n",
      "13:49  Finished training\n",
      "13:49  Training estimator 7 / 10 in ensemble\n",
      "13:49  Starting training\n",
      "13:49    Method:                 sally\n",
      "13:49    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "13:49                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "13:49    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "13:49    Method:                 sally\n",
      "13:49    Hidden layers:          (100, 100)\n",
      "13:49    Activation function:    tanh\n",
      "13:49    Batch size:             128\n",
      "13:49    Trainer:                amsgrad\n",
      "13:49    Epochs:                 50\n",
      "13:49    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:49    Validation split:       0.25\n",
      "13:49    Early stopping:         True\n",
      "13:49    Scale inputs:           True\n",
      "13:49    Regularization:         None\n",
      "13:49  Loading training data\n",
      "13:49  Found 1000000 samples with 2 parameters and 27 observables\n",
      "13:49  Rescaling inputs\n",
      "13:49  Only using 7 of 27 observables\n",
      "13:49  Creating model for method sally\n",
      "13:49  Training model\n",
      "13:52    Epoch 5: train loss 27.6271 (mse_score: 27.6271)\n",
      "13:52              val. loss  15.2674 (mse_score: 15.2674)\n",
      "13:56    Epoch 10: train loss 27.4750 (mse_score: 27.4750)\n",
      "13:56              val. loss  15.2378 (mse_score: 15.2378) (*)\n",
      "13:59    Epoch 15: train loss 27.2367 (mse_score: 27.2367)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:59              val. loss  15.2806 (mse_score: 15.2806)\n",
      "14:02    Epoch 20: train loss 27.0021 (mse_score: 27.0021)\n",
      "14:02              val. loss  15.3122 (mse_score: 15.3122)\n",
      "14:06    Epoch 25: train loss 26.7801 (mse_score: 26.7801)\n",
      "14:06              val. loss  15.2731 (mse_score: 15.2731)\n",
      "14:10    Epoch 30: train loss 26.6030 (mse_score: 26.6030)\n",
      "14:10              val. loss  15.3450 (mse_score: 15.3450)\n",
      "14:14    Epoch 35: train loss 26.4305 (mse_score: 26.4305)\n",
      "14:14              val. loss  15.3525 (mse_score: 15.3525)\n",
      "14:17    Epoch 40: train loss 26.2876 (mse_score: 26.2876)\n",
      "14:17              val. loss  15.3528 (mse_score: 15.3528)\n",
      "14:21    Epoch 45: train loss 26.1920 (mse_score: 26.1920)\n",
      "14:21              val. loss  15.3670 (mse_score: 15.3670)\n",
      "14:24    Epoch 50: train loss 26.0938 (mse_score: 26.0938)\n",
      "14:24              val. loss  15.3836 (mse_score: 15.3836)\n",
      "14:24  Early stopping after epoch 11, with loss 15.23 compared to final loss 15.38\n",
      "14:24  Finished training\n",
      "14:24  Training estimator 8 / 10 in ensemble\n",
      "14:24  Starting training\n",
      "14:24    Method:                 sally\n",
      "14:24    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "14:24                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "14:24    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "14:24    Method:                 sally\n",
      "14:24    Hidden layers:          (100, 100)\n",
      "14:24    Activation function:    tanh\n",
      "14:24    Batch size:             128\n",
      "14:24    Trainer:                amsgrad\n",
      "14:24    Epochs:                 50\n",
      "14:24    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:24    Validation split:       0.25\n",
      "14:24    Early stopping:         True\n",
      "14:24    Scale inputs:           True\n",
      "14:24    Regularization:         None\n",
      "14:24  Loading training data\n",
      "14:24  Found 1000000 samples with 2 parameters and 27 observables\n",
      "14:24  Rescaling inputs\n",
      "14:24  Only using 7 of 27 observables\n",
      "14:24  Creating model for method sally\n",
      "14:24  Training model\n",
      "14:28    Epoch 5: train loss 33.3442 (mse_score: 33.3442)\n",
      "14:28              val. loss  35.1271 (mse_score: 35.1271) (*)\n",
      "14:31    Epoch 10: train loss 33.2679 (mse_score: 33.2679)\n",
      "14:31              val. loss  35.1383 (mse_score: 35.1383)\n",
      "14:35    Epoch 15: train loss 33.1361 (mse_score: 33.1361)\n",
      "14:35              val. loss  35.1387 (mse_score: 35.1387)\n",
      "14:38    Epoch 20: train loss 33.0070 (mse_score: 33.0070)\n",
      "14:38              val. loss  35.1265 (mse_score: 35.1265)\n",
      "14:42    Epoch 25: train loss 32.8714 (mse_score: 32.8714)\n",
      "14:42              val. loss  35.0822 (mse_score: 35.0822)\n",
      "14:46    Epoch 30: train loss 32.7378 (mse_score: 32.7378)\n",
      "14:46              val. loss  35.1466 (mse_score: 35.1466)\n",
      "14:49    Epoch 35: train loss 32.6155 (mse_score: 32.6155)\n",
      "14:49              val. loss  35.1185 (mse_score: 35.1185)\n",
      "14:53    Epoch 40: train loss 32.5195 (mse_score: 32.5195)\n",
      "14:53              val. loss  35.0918 (mse_score: 35.0918)\n",
      "14:56    Epoch 45: train loss 32.4342 (mse_score: 32.4342)\n",
      "14:56              val. loss  35.1373 (mse_score: 35.1373)\n",
      "14:59    Epoch 50: train loss 32.3604 (mse_score: 32.3604)\n",
      "14:59              val. loss  35.0881 (mse_score: 35.0881)\n",
      "14:59  Early stopping after epoch 39, with loss 35.08 compared to final loss 35.09\n",
      "14:59  Finished training\n",
      "14:59  Training estimator 9 / 10 in ensemble\n",
      "14:59  Starting training\n",
      "14:59    Method:                 sally\n",
      "14:59    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "14:59                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "14:59    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "14:59    Method:                 sally\n",
      "14:59    Hidden layers:          (100, 100)\n",
      "14:59    Activation function:    tanh\n",
      "14:59    Batch size:             128\n",
      "14:59    Trainer:                amsgrad\n",
      "14:59    Epochs:                 50\n",
      "14:59    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:59    Validation split:       0.25\n",
      "14:59    Early stopping:         True\n",
      "14:59    Scale inputs:           True\n",
      "14:59    Regularization:         None\n",
      "14:59  Loading training data\n",
      "14:59  Found 1000000 samples with 2 parameters and 27 observables\n",
      "14:59  Rescaling inputs\n",
      "14:59  Only using 7 of 27 observables\n",
      "14:59  Creating model for method sally\n",
      "14:59  Training model\n",
      "15:03    Epoch 5: train loss 32.9724 (mse_score: 32.9724)\n",
      "15:03              val. loss  26.1641 (mse_score: 26.1641) (*)\n",
      "15:06    Epoch 10: train loss 32.9091 (mse_score: 32.9091)\n",
      "15:06              val. loss  26.1351 (mse_score: 26.1351) (*)\n",
      "15:10    Epoch 15: train loss 32.7529 (mse_score: 32.7529)\n",
      "15:10              val. loss  26.1327 (mse_score: 26.1327)\n",
      "15:14    Epoch 20: train loss 32.5813 (mse_score: 32.5813)\n",
      "15:14              val. loss  26.1138 (mse_score: 26.1138) (*)\n",
      "15:18    Epoch 25: train loss 32.4134 (mse_score: 32.4134)\n",
      "15:18              val. loss  26.1920 (mse_score: 26.1920)\n",
      "15:21    Epoch 30: train loss 32.2663 (mse_score: 32.2663)\n",
      "15:21              val. loss  26.0771 (mse_score: 26.0771)\n",
      "15:25    Epoch 35: train loss 32.1270 (mse_score: 32.1270)\n",
      "15:25              val. loss  26.0706 (mse_score: 26.0706)\n",
      "15:28    Epoch 40: train loss 31.9978 (mse_score: 31.9978)\n",
      "15:28              val. loss  26.1322 (mse_score: 26.1322)\n",
      "15:31    Epoch 45: train loss 31.9176 (mse_score: 31.9176)\n",
      "15:31              val. loss  26.0279 (mse_score: 26.0279)\n",
      "15:34    Epoch 50: train loss 31.8394 (mse_score: 31.8394)\n",
      "15:34              val. loss  26.0443 (mse_score: 26.0443)\n",
      "15:34  Early stopping after epoch 33, with loss 26.01 compared to final loss 26.04\n",
      "15:34  Finished training\n",
      "15:34  Training estimator 10 / 10 in ensemble\n",
      "15:34  Starting training\n",
      "15:34    Method:                 sally\n",
      "15:34    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "15:34                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "15:34    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "15:34    Method:                 sally\n",
      "15:34    Hidden layers:          (100, 100)\n",
      "15:34    Activation function:    tanh\n",
      "15:34    Batch size:             128\n",
      "15:34    Trainer:                amsgrad\n",
      "15:34    Epochs:                 50\n",
      "15:34    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:34    Validation split:       0.25\n",
      "15:34    Early stopping:         True\n",
      "15:34    Scale inputs:           True\n",
      "15:34    Regularization:         None\n",
      "15:34  Loading training data\n",
      "15:34  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:34  Rescaling inputs\n",
      "15:34  Only using 7 of 27 observables\n",
      "15:34  Creating model for method sally\n",
      "15:34  Training model\n",
      "15:38    Epoch 5: train loss 35.0197 (mse_score: 35.0197)\n",
      "15:38              val. loss  27.5716 (mse_score: 27.5716) (*)\n",
      "15:41    Epoch 10: train loss 34.9713 (mse_score: 34.9713)\n",
      "15:41              val. loss  27.6254 (mse_score: 27.6254)\n",
      "15:44    Epoch 15: train loss 34.9078 (mse_score: 34.9078)\n",
      "15:44              val. loss  27.6266 (mse_score: 27.6266)\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'minimal',\n",
    "    use_tight_cuts=False,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_reg',\n",
    "    use_tight_cuts=False,\n",
    "    grad_x_regularization=0.1,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight_reg',\n",
    "    use_tight_cuts=True,\n",
    "    grad_x_regularization=0.1,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just resurrection phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'resurrection',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[26] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
