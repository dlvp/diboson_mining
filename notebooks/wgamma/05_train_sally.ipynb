{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9c4780e4-775d-4cbd-8f3d-64fbd2b45344"
    }
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "98a6ae38-08cd-4e31-a7a4-5a370df5c84e"
    }
   },
   "outputs": [],
   "source": [
    "n_estimators = 20\n",
    "n_hidden = (100,100)\n",
    "n_epochs = 20\n",
    "batch_size = 128\n",
    "initial_lr = 0.001\n",
    "final_lr = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## Train SALLY on all observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "be3db4db-cc81-42e8-8796-dfc021acf234"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:56  \n",
      "22:56  ------------------------------------------------------------\n",
      "22:56  |                                                          |\n",
      "22:56  |  MadMiner v2018.10.30                                    |\n",
      "22:56  |                                                          |\n",
      "22:56  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "22:56  |                                                          |\n",
      "22:56  ------------------------------------------------------------\n",
      "22:56  \n",
      "22:56  Training 20 estimators in ensemble\n",
      "22:56  Training estimator 1 / 20 in ensemble\n",
      "22:56  Starting training\n",
      "22:56    Method:                 sally\n",
      "22:56    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "22:56                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "22:56    Features:               all\n",
      "22:56    Method:                 sally\n",
      "22:56    Hidden layers:          (100, 100)\n",
      "22:56    Activation function:    tanh\n",
      "22:56    Batch size:             128\n",
      "22:56    Epochs:                 20\n",
      "22:56    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "22:56    Validation split:       None\n",
      "22:56    Early stopping:         True\n",
      "22:56  Loading training data\n",
      "22:56  Found 1000000 samples with 2 parameters and 30 observables\n",
      "22:56  Creating model for method sally\n",
      "22:56  Training model\n",
      "22:57    Epoch 2: train loss 30.96 ([30.95997013])\n",
      "22:58    Epoch 4: train loss 30.95 ([30.94847595])\n",
      "22:58    Epoch 6: train loss 30.94 ([30.94082114])\n",
      "22:59    Epoch 8: train loss 30.93 ([30.93298157])\n",
      "23:00    Epoch 10: train loss 30.93 ([30.92747579])\n",
      "23:00    Epoch 12: train loss 30.92 ([30.92090304])\n",
      "23:01    Epoch 14: train loss 30.92 ([30.91839063])\n",
      "23:02    Epoch 16: train loss 30.92 ([30.91662361])\n",
      "23:02    Epoch 18: train loss 30.91 ([30.9135921])\n",
      "23:03    Epoch 20: train loss 30.91 ([30.91149171])\n",
      "23:03  Finished training\n",
      "23:03  Training estimator 2 / 20 in ensemble\n",
      "23:03  Starting training\n",
      "23:03    Method:                 sally\n",
      "23:03    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "23:03                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "23:03    Features:               all\n",
      "23:03    Method:                 sally\n",
      "23:03    Hidden layers:          (100, 100)\n",
      "23:03    Activation function:    tanh\n",
      "23:03    Batch size:             128\n",
      "23:03    Epochs:                 20\n",
      "23:03    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:03    Validation split:       None\n",
      "23:03    Early stopping:         True\n",
      "23:03  Loading training data\n",
      "23:03  Found 1000000 samples with 2 parameters and 30 observables\n",
      "23:03  Creating model for method sally\n",
      "23:03  Training model\n",
      "23:04    Epoch 2: train loss 34.11 ([34.10776721])\n",
      "23:04    Epoch 4: train loss 34.09 ([34.09490212])\n",
      "23:05    Epoch 6: train loss 34.08 ([34.08332948])\n",
      "23:06    Epoch 8: train loss 34.08 ([34.07718703])\n",
      "23:06    Epoch 10: train loss 34.07 ([34.07156896])\n",
      "23:07    Epoch 12: train loss 34.06 ([34.06466467])\n",
      "23:08    Epoch 14: train loss 34.06 ([34.06424049])\n",
      "23:08    Epoch 16: train loss 34.06 ([34.05696614])\n",
      "23:09    Epoch 18: train loss 34.05 ([34.05250622])\n",
      "23:10    Epoch 20: train loss 34.05 ([34.05201125])\n",
      "23:10  Finished training\n",
      "23:10  Training estimator 3 / 20 in ensemble\n",
      "23:10  Starting training\n",
      "23:10    Method:                 sally\n",
      "23:10    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "23:10                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "23:10    Features:               all\n",
      "23:10    Method:                 sally\n",
      "23:10    Hidden layers:          (100, 100)\n",
      "23:10    Activation function:    tanh\n",
      "23:10    Batch size:             128\n",
      "23:10    Epochs:                 20\n",
      "23:10    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:10    Validation split:       None\n",
      "23:10    Early stopping:         True\n",
      "23:10  Loading training data\n",
      "23:10  Found 1000000 samples with 2 parameters and 30 observables\n",
      "23:10  Creating model for method sally\n",
      "23:10  Training model\n",
      "23:10    Epoch 2: train loss 33.78 ([33.77819529])\n",
      "23:11    Epoch 4: train loss 33.77 ([33.76880507])\n",
      "23:12    Epoch 6: train loss 33.75 ([33.75311228])\n",
      "23:12    Epoch 8: train loss 33.75 ([33.74534672])\n",
      "23:13    Epoch 10: train loss 33.74 ([33.73618272])\n",
      "23:14    Epoch 12: train loss 33.73 ([33.73159575])\n",
      "23:14    Epoch 14: train loss 33.72 ([33.72413124])\n",
      "23:15    Epoch 16: train loss 33.73 ([33.73338966])\n",
      "23:16    Epoch 18: train loss 33.79 ([33.79037488])\n",
      "23:16    Epoch 20: train loss 33.72 ([33.72193739])\n",
      "23:16  Finished training\n",
      "23:16  Training estimator 4 / 20 in ensemble\n",
      "23:16  Starting training\n",
      "23:16    Method:                 sally\n",
      "23:16    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "23:16                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "23:16    Features:               all\n",
      "23:16    Method:                 sally\n",
      "23:16    Hidden layers:          (100, 100)\n",
      "23:16    Activation function:    tanh\n",
      "23:16    Batch size:             128\n",
      "23:16    Epochs:                 20\n",
      "23:16    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:16    Validation split:       None\n",
      "23:16    Early stopping:         True\n",
      "23:16  Loading training data\n",
      "23:16  Found 1000000 samples with 2 parameters and 30 observables\n",
      "23:16  Creating model for method sally\n",
      "23:16  Training model\n",
      "23:17    Epoch 2: train loss 30.62 ([30.62214049])\n",
      "23:18    Epoch 4: train loss 30.61 ([30.60832409])\n",
      "23:19    Epoch 6: train loss 30.61 ([30.60876513])\n",
      "23:19    Epoch 8: train loss 30.60 ([30.59876031])\n",
      "23:20    Epoch 10: train loss 30.59 ([30.58841553])\n",
      "23:21    Epoch 12: train loss 30.58 ([30.58065229])\n",
      "23:21    Epoch 14: train loss 30.57 ([30.57175951])\n",
      "23:22    Epoch 16: train loss 30.57 ([30.5668311])\n",
      "23:23    Epoch 18: train loss 30.56 ([30.563029])\n",
      "23:23    Epoch 20: train loss 30.56 ([30.55906452])\n",
      "23:23  Finished training\n",
      "23:23  Training estimator 5 / 20 in ensemble\n",
      "23:23  Starting training\n",
      "23:23    Method:                 sally\n",
      "23:23    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "23:23                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "23:23    Features:               all\n",
      "23:23    Method:                 sally\n",
      "23:23    Hidden layers:          (100, 100)\n",
      "23:23    Activation function:    tanh\n",
      "23:23    Batch size:             128\n",
      "23:23    Epochs:                 20\n",
      "23:23    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:23    Validation split:       None\n",
      "23:23    Early stopping:         True\n",
      "23:23  Loading training data\n",
      "23:23  Found 1000000 samples with 2 parameters and 30 observables\n",
      "23:23  Creating model for method sally\n",
      "23:23  Training model\n",
      "23:24    Epoch 2: train loss 38.13 ([38.13112421])\n",
      "23:25    Epoch 4: train loss 38.12 ([38.12152241])\n",
      "23:25    Epoch 6: train loss 38.11 ([38.10940352])\n",
      "23:26    Epoch 8: train loss 38.10 ([38.10054822])\n",
      "23:27    Epoch 10: train loss 38.10 ([38.09666461])\n",
      "23:27    Epoch 12: train loss 38.09 ([38.08843586])\n",
      "23:28    Epoch 14: train loss 38.08 ([38.08411501])\n",
      "23:29    Epoch 16: train loss 38.08 ([38.08256096])\n",
      "23:29    Epoch 18: train loss 38.08 ([38.07908612])\n",
      "23:30    Epoch 20: train loss 38.08 ([38.0769372])\n",
      "23:30  Finished training\n",
      "23:30  Training estimator 6 / 20 in ensemble\n",
      "23:30  Starting training\n",
      "23:30    Method:                 sally\n",
      "23:30    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "23:30                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:30    Features:               all\n",
      "23:30    Method:                 sally\n",
      "23:30    Hidden layers:          (100, 100)\n",
      "23:30    Activation function:    tanh\n",
      "23:30    Batch size:             128\n",
      "23:30    Epochs:                 20\n",
      "23:30    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:30    Validation split:       None\n",
      "23:30    Early stopping:         True\n",
      "23:30  Loading training data\n",
      "23:30  Found 1000000 samples with 2 parameters and 30 observables\n",
      "23:30  Creating model for method sally\n",
      "23:30  Training model\n",
      "23:31    Epoch 2: train loss 24.33 ([24.32849032])\n",
      "23:32    Epoch 4: train loss 24.32 ([24.32031129])\n",
      "23:32    Epoch 6: train loss 24.31 ([24.31024602])\n",
      "23:33    Epoch 8: train loss 24.30 ([24.30384182])\n",
      "23:34    Epoch 10: train loss 24.30 ([24.30124268])\n",
      "23:34    Epoch 12: train loss 24.29 ([24.29117569])\n",
      "23:35    Epoch 14: train loss 24.29 ([24.28974107])\n",
      "23:36    Epoch 16: train loss 24.28 ([24.28351236])\n",
      "23:36    Epoch 18: train loss 24.28 ([24.27662582])\n",
      "23:37    Epoch 20: train loss 24.27 ([24.27356965])\n",
      "23:37  Finished training\n",
      "23:37  Training estimator 7 / 20 in ensemble\n",
      "23:37  Starting training\n",
      "23:37    Method:                 sally\n",
      "23:37    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "23:37                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "23:37    Features:               all\n",
      "23:37    Method:                 sally\n",
      "23:37    Hidden layers:          (100, 100)\n",
      "23:37    Activation function:    tanh\n",
      "23:37    Batch size:             128\n",
      "23:37    Epochs:                 20\n",
      "23:37    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:37    Validation split:       None\n",
      "23:37    Early stopping:         True\n",
      "23:37  Loading training data\n",
      "23:37  Found 1000000 samples with 2 parameters and 30 observables\n",
      "23:37  Creating model for method sally\n",
      "23:37  Training model\n",
      "23:38    Epoch 2: train loss 19.90 ([19.89962636])\n",
      "23:38    Epoch 4: train loss 19.89 ([19.8909538])\n",
      "23:39    Epoch 6: train loss 19.88 ([19.88182458])\n",
      "23:40    Epoch 8: train loss 19.88 ([19.87532815])\n",
      "23:40    Epoch 10: train loss 19.87 ([19.86744463])\n",
      "23:41    Epoch 12: train loss 19.87 ([19.86734672])\n",
      "23:42    Epoch 14: train loss 19.86 ([19.86011863])\n",
      "23:42    Epoch 16: train loss 19.86 ([19.85781809])\n",
      "23:43    Epoch 18: train loss 19.86 ([19.85546746])\n",
      "23:44    Epoch 20: train loss 19.85 ([19.85424564])\n",
      "23:44  Finished training\n",
      "23:44  Training estimator 8 / 20 in ensemble\n",
      "23:44  Starting training\n",
      "23:44    Method:                 sally\n",
      "23:44    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "23:44                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "23:44    Features:               all\n",
      "23:44    Method:                 sally\n",
      "23:44    Hidden layers:          (100, 100)\n",
      "23:44    Activation function:    tanh\n",
      "23:44    Batch size:             128\n",
      "23:44    Epochs:                 20\n",
      "23:44    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:44    Validation split:       None\n",
      "23:44    Early stopping:         True\n",
      "23:44  Loading training data\n",
      "23:44  Found 1000000 samples with 2 parameters and 30 observables\n",
      "23:44  Creating model for method sally\n",
      "23:44  Training model\n",
      "23:45    Epoch 2: train loss 26.41 ([26.41167451])\n",
      "23:45    Epoch 4: train loss 26.40 ([26.3986637])\n",
      "23:46    Epoch 6: train loss 26.39 ([26.39098451])\n",
      "23:47    Epoch 8: train loss 26.38 ([26.38136645])\n",
      "23:47    Epoch 10: train loss 26.37 ([26.37426464])\n",
      "23:48    Epoch 12: train loss 26.37 ([26.36850186])\n",
      "23:49    Epoch 14: train loss 26.37 ([26.36533271])\n",
      "23:49    Epoch 16: train loss 26.36 ([26.36139056])\n",
      "23:50    Epoch 18: train loss 26.35 ([26.35455834])\n",
      "23:51    Epoch 20: train loss 26.35 ([26.35375806])\n",
      "23:51  Finished training\n",
      "23:51  Training estimator 9 / 20 in ensemble\n",
      "23:51  Starting training\n",
      "23:51    Method:                 sally\n",
      "23:51    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "23:51                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "23:51    Features:               all\n",
      "23:51    Method:                 sally\n",
      "23:51    Hidden layers:          (100, 100)\n",
      "23:51    Activation function:    tanh\n",
      "23:51    Batch size:             128\n",
      "23:51    Epochs:                 20\n",
      "23:51    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:51    Validation split:       None\n",
      "23:51    Early stopping:         True\n",
      "23:51  Loading training data\n",
      "23:51  Found 1000000 samples with 2 parameters and 30 observables\n",
      "23:51  Creating model for method sally\n",
      "23:51  Training model\n",
      "23:51    Epoch 2: train loss 26.49 ([26.48886748])\n",
      "23:52    Epoch 4: train loss 26.48 ([26.48405801])\n",
      "23:53    Epoch 6: train loss 26.47 ([26.46661702])\n",
      "23:53    Epoch 8: train loss 26.46 ([26.45863926])\n",
      "23:54    Epoch 10: train loss 26.45 ([26.45240994])\n",
      "23:55    Epoch 12: train loss 26.45 ([26.4490407])\n",
      "23:55    Epoch 14: train loss 26.44 ([26.44236669])\n",
      "23:56    Epoch 16: train loss 26.44 ([26.43905854])\n",
      "23:57    Epoch 18: train loss 26.44 ([26.43602498])\n",
      "23:57    Epoch 20: train loss 26.44 ([26.43650102])\n",
      "23:57  Finished training\n",
      "23:57  Training estimator 10 / 20 in ensemble\n",
      "23:57  Starting training\n",
      "23:57    Method:                 sally\n",
      "23:57    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "23:57                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "23:57    Features:               all\n",
      "23:57    Method:                 sally\n",
      "23:57    Hidden layers:          (100, 100)\n",
      "23:57    Activation function:    tanh\n",
      "23:57    Batch size:             128\n",
      "23:57    Epochs:                 20\n",
      "23:57    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:57    Validation split:       None\n",
      "23:57    Early stopping:         True\n",
      "23:57  Loading training data\n",
      "23:57  Found 1000000 samples with 2 parameters and 30 observables\n",
      "23:57  Creating model for method sally\n",
      "23:57  Training model\n",
      "23:58    Epoch 2: train loss 31.22 ([31.22027225])\n",
      "23:59    Epoch 4: train loss 31.21 ([31.20671746])\n",
      "00:00    Epoch 6: train loss 31.20 ([31.19849308])\n",
      "00:00    Epoch 8: train loss 31.19 ([31.19313548])\n",
      "00:01    Epoch 10: train loss 31.18 ([31.18330661])\n",
      "00:02    Epoch 12: train loss 31.18 ([31.18089886])\n",
      "00:02    Epoch 14: train loss 31.17 ([31.17358018])\n",
      "00:03    Epoch 16: train loss 31.17 ([31.17097077])\n",
      "00:04    Epoch 18: train loss 31.16 ([31.16467907])\n",
      "00:04    Epoch 20: train loss 31.16 ([31.16444392])\n",
      "00:04  Finished training\n",
      "00:04  Training estimator 11 / 20 in ensemble\n",
      "00:04  Starting training\n",
      "00:04    Method:                 sally\n",
      "00:04    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_10.npy\n",
      "00:04                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_10.npy\n",
      "00:04    Features:               all\n",
      "00:04    Method:                 sally\n",
      "00:04    Hidden layers:          (100, 100)\n",
      "00:04    Activation function:    tanh\n",
      "00:04    Batch size:             128\n",
      "00:04    Epochs:                 20\n",
      "00:04    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:04    Validation split:       None\n",
      "00:04    Early stopping:         True\n",
      "00:04  Loading training data\n",
      "00:04  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:04  Creating model for method sally\n",
      "00:04  Training model\n",
      "00:05    Epoch 2: train loss 28.49 ([28.49130589])\n",
      "00:06    Epoch 4: train loss 28.48 ([28.48430634])\n",
      "00:06    Epoch 6: train loss 28.47 ([28.47275322])\n",
      "00:07    Epoch 8: train loss 28.47 ([28.46763762])\n",
      "00:08    Epoch 10: train loss 28.46 ([28.45837276])\n",
      "00:08    Epoch 12: train loss 28.46 ([28.45559416])\n",
      "00:09    Epoch 14: train loss 28.45 ([28.45153139])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:10    Epoch 16: train loss 28.45 ([28.45021547])\n",
      "00:10    Epoch 18: train loss 28.45 ([28.44517417])\n",
      "00:11    Epoch 20: train loss 28.44 ([28.44359385])\n",
      "00:11  Finished training\n",
      "00:11  Training estimator 12 / 20 in ensemble\n",
      "00:11  Starting training\n",
      "00:11    Method:                 sally\n",
      "00:11    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_11.npy\n",
      "00:11                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_11.npy\n",
      "00:11    Features:               all\n",
      "00:11    Method:                 sally\n",
      "00:11    Hidden layers:          (100, 100)\n",
      "00:11    Activation function:    tanh\n",
      "00:11    Batch size:             128\n",
      "00:11    Epochs:                 20\n",
      "00:11    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:11    Validation split:       None\n",
      "00:11    Early stopping:         True\n",
      "00:11  Loading training data\n",
      "00:11  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:11  Creating model for method sally\n",
      "00:11  Training model\n",
      "00:12    Epoch 2: train loss 26.42 ([26.42155796])\n",
      "00:13    Epoch 4: train loss 26.41 ([26.41175868])\n",
      "00:13    Epoch 6: train loss 26.40 ([26.40228799])\n",
      "00:14    Epoch 8: train loss 26.39 ([26.39477151])\n",
      "00:15    Epoch 10: train loss 26.39 ([26.38895196])\n",
      "00:15    Epoch 12: train loss 26.39 ([26.38572248])\n",
      "00:16    Epoch 14: train loss 26.38 ([26.38405783])\n",
      "00:17    Epoch 16: train loss 26.38 ([26.37739835])\n",
      "00:17    Epoch 18: train loss 26.38 ([26.3779684])\n",
      "00:18    Epoch 20: train loss 26.37 ([26.37237752])\n",
      "00:18  Finished training\n",
      "00:18  Training estimator 13 / 20 in ensemble\n",
      "00:18  Starting training\n",
      "00:18    Method:                 sally\n",
      "00:18    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_12.npy\n",
      "00:18                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_12.npy\n",
      "00:18    Features:               all\n",
      "00:18    Method:                 sally\n",
      "00:18    Hidden layers:          (100, 100)\n",
      "00:18    Activation function:    tanh\n",
      "00:18    Batch size:             128\n",
      "00:18    Epochs:                 20\n",
      "00:18    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:18    Validation split:       None\n",
      "00:18    Early stopping:         True\n",
      "00:18  Loading training data\n",
      "00:18  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:18  Creating model for method sally\n",
      "00:18  Training model\n",
      "00:19    Epoch 2: train loss 36.75 ([36.75310885])\n",
      "00:20    Epoch 4: train loss 36.73 ([36.73237544])\n",
      "00:20    Epoch 6: train loss 36.72 ([36.72460374])\n",
      "00:21    Epoch 8: train loss 36.72 ([36.71626611])\n",
      "00:22    Epoch 10: train loss 36.72 ([36.71606562])\n",
      "00:22    Epoch 12: train loss 36.70 ([36.70471203])\n",
      "00:23    Epoch 14: train loss 36.70 ([36.69838593])\n",
      "00:24    Epoch 16: train loss 36.69 ([36.69280531])\n",
      "00:24    Epoch 18: train loss 36.69 ([36.68773747])\n",
      "00:25    Epoch 20: train loss 36.68 ([36.68463835])\n",
      "00:25  Finished training\n",
      "00:25  Training estimator 14 / 20 in ensemble\n",
      "00:25  Starting training\n",
      "00:25    Method:                 sally\n",
      "00:25    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_13.npy\n",
      "00:25                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_13.npy\n",
      "00:25    Features:               all\n",
      "00:25    Method:                 sally\n",
      "00:25    Hidden layers:          (100, 100)\n",
      "00:25    Activation function:    tanh\n",
      "00:25    Batch size:             128\n",
      "00:25    Epochs:                 20\n",
      "00:25    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:25    Validation split:       None\n",
      "00:25    Early stopping:         True\n",
      "00:25  Loading training data\n",
      "00:25  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:25  Creating model for method sally\n",
      "00:25  Training model\n",
      "00:26    Epoch 2: train loss 25.34 ([25.33934668])\n",
      "00:27    Epoch 4: train loss 25.33 ([25.328744])\n",
      "00:27    Epoch 6: train loss 25.32 ([25.3170463])\n",
      "00:28    Epoch 8: train loss 25.31 ([25.31302005])\n",
      "00:29    Epoch 10: train loss 25.31 ([25.3064307])\n",
      "00:29    Epoch 12: train loss 25.30 ([25.30027294])\n",
      "00:30    Epoch 14: train loss 25.29 ([25.29286102])\n",
      "00:31    Epoch 16: train loss 25.29 ([25.28955285])\n",
      "00:31    Epoch 18: train loss 25.29 ([25.28605899])\n",
      "00:32    Epoch 20: train loss 25.28 ([25.28020601])\n",
      "00:32  Finished training\n",
      "00:32  Training estimator 15 / 20 in ensemble\n",
      "00:32  Starting training\n",
      "00:32    Method:                 sally\n",
      "00:32    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_14.npy\n",
      "00:32                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_14.npy\n",
      "00:32    Features:               all\n",
      "00:32    Method:                 sally\n",
      "00:32    Hidden layers:          (100, 100)\n",
      "00:32    Activation function:    tanh\n",
      "00:32    Batch size:             128\n",
      "00:32    Epochs:                 20\n",
      "00:32    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:32    Validation split:       None\n",
      "00:32    Early stopping:         True\n",
      "00:32  Loading training data\n",
      "00:32  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:32  Creating model for method sally\n",
      "00:32  Training model\n",
      "00:33    Epoch 2: train loss 65.20 ([65.20186089])\n",
      "00:33    Epoch 4: train loss 65.18 ([65.18449787])\n",
      "00:34    Epoch 6: train loss 65.18 ([65.17731894])\n",
      "00:35    Epoch 8: train loss 65.17 ([65.16688386])\n",
      "00:35    Epoch 10: train loss 65.16 ([65.1616306])\n",
      "00:36    Epoch 12: train loss 65.16 ([65.15793399])\n",
      "00:37    Epoch 14: train loss 65.15 ([65.15435602])\n",
      "00:38    Epoch 16: train loss 65.15 ([65.14563414])\n",
      "00:38    Epoch 18: train loss 65.14 ([65.14437013])\n",
      "00:39    Epoch 20: train loss 65.14 ([65.13954953])\n",
      "00:39  Finished training\n",
      "00:39  Training estimator 16 / 20 in ensemble\n",
      "00:39  Starting training\n",
      "00:39    Method:                 sally\n",
      "00:39    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_15.npy\n",
      "00:39                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_15.npy\n",
      "00:39    Features:               all\n",
      "00:39    Method:                 sally\n",
      "00:39    Hidden layers:          (100, 100)\n",
      "00:39    Activation function:    tanh\n",
      "00:39    Batch size:             128\n",
      "00:39    Epochs:                 20\n",
      "00:39    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:39    Validation split:       None\n",
      "00:39    Early stopping:         True\n",
      "00:39  Loading training data\n",
      "00:39  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:39  Creating model for method sally\n",
      "00:39  Training model\n",
      "00:40    Epoch 2: train loss 21.33 ([21.32683135])\n",
      "00:40    Epoch 4: train loss 21.31 ([21.31355302])\n",
      "00:41    Epoch 6: train loss 21.31 ([21.30978964])\n",
      "00:42    Epoch 8: train loss 21.30 ([21.29917103])\n",
      "00:42    Epoch 10: train loss 21.29 ([21.29189024])\n",
      "00:43    Epoch 12: train loss 21.29 ([21.28724128])\n",
      "00:44    Epoch 14: train loss 21.28 ([21.2827734])\n",
      "00:44    Epoch 16: train loss 21.28 ([21.28064791])\n",
      "00:45    Epoch 18: train loss 21.28 ([21.27622415])\n",
      "00:46    Epoch 20: train loss 21.27 ([21.2725537])\n",
      "00:46  Finished training\n",
      "00:46  Training estimator 17 / 20 in ensemble\n",
      "00:46  Starting training\n",
      "00:46    Method:                 sally\n",
      "00:46    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_16.npy\n",
      "00:46                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_16.npy\n",
      "00:46    Features:               all\n",
      "00:46    Method:                 sally\n",
      "00:46    Hidden layers:          (100, 100)\n",
      "00:46    Activation function:    tanh\n",
      "00:46    Batch size:             128\n",
      "00:46    Epochs:                 20\n",
      "00:46    Learning rate:          0.002 initially, decaying to 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:46    Validation split:       None\n",
      "00:46    Early stopping:         True\n",
      "00:46  Loading training data\n",
      "00:46  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:46  Creating model for method sally\n",
      "00:46  Training model\n",
      "00:46    Epoch 2: train loss 20.86 ([20.86496105])\n",
      "00:47    Epoch 4: train loss 20.86 ([20.86055829])\n",
      "00:48    Epoch 6: train loss 20.84 ([20.84274821])\n",
      "00:48    Epoch 8: train loss 20.84 ([20.83727186])\n",
      "00:49    Epoch 10: train loss 20.84 ([20.83520169])\n",
      "00:50    Epoch 12: train loss 20.83 ([20.83014117])\n",
      "00:50    Epoch 14: train loss 20.83 ([20.8262549])\n",
      "00:51    Epoch 16: train loss 20.82 ([20.82119595])\n",
      "00:52    Epoch 18: train loss 20.82 ([20.81750904])\n",
      "00:52    Epoch 20: train loss 20.81 ([20.81498551])\n",
      "00:52  Finished training\n",
      "00:52  Training estimator 18 / 20 in ensemble\n",
      "00:52  Starting training\n",
      "00:52    Method:                 sally\n",
      "00:52    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_17.npy\n",
      "00:52                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_17.npy\n",
      "00:52    Features:               all\n",
      "00:52    Method:                 sally\n",
      "00:52    Hidden layers:          (100, 100)\n",
      "00:52    Activation function:    tanh\n",
      "00:52    Batch size:             128\n",
      "00:52    Epochs:                 20\n",
      "00:52    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:52    Validation split:       None\n",
      "00:52    Early stopping:         True\n",
      "00:52  Loading training data\n",
      "00:52  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:52  Creating model for method sally\n",
      "00:52  Training model\n",
      "00:53    Epoch 2: train loss 39.50 ([39.49920945])\n",
      "00:54    Epoch 4: train loss 39.48 ([39.48288756])\n",
      "00:54    Epoch 6: train loss 39.47 ([39.47345023])\n",
      "00:55    Epoch 8: train loss 39.47 ([39.46525799])\n",
      "00:56    Epoch 10: train loss 39.46 ([39.45697538])\n",
      "00:56    Epoch 12: train loss 39.45 ([39.45362976])\n",
      "00:57    Epoch 14: train loss 39.45 ([39.45060667])\n",
      "00:58    Epoch 16: train loss 39.45 ([39.44528112])\n",
      "00:59    Epoch 18: train loss 39.44 ([39.444558])\n",
      "00:59    Epoch 20: train loss 39.44 ([39.44198594])\n",
      "00:59  Finished training\n",
      "00:59  Training estimator 19 / 20 in ensemble\n",
      "00:59  Starting training\n",
      "00:59    Method:                 sally\n",
      "00:59    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_18.npy\n",
      "00:59                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_18.npy\n",
      "00:59    Features:               all\n",
      "00:59    Method:                 sally\n",
      "00:59    Hidden layers:          (100, 100)\n",
      "00:59    Activation function:    tanh\n",
      "00:59    Batch size:             128\n",
      "00:59    Epochs:                 20\n",
      "00:59    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:59    Validation split:       None\n",
      "00:59    Early stopping:         True\n",
      "00:59  Loading training data\n",
      "00:59  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:59  Creating model for method sally\n",
      "00:59  Training model\n",
      "01:00    Epoch 2: train loss 26.19 ([26.18631662])\n",
      "01:01    Epoch 4: train loss 26.17 ([26.17056663])\n",
      "01:01    Epoch 6: train loss 26.16 ([26.16314239])\n",
      "01:02    Epoch 8: train loss 26.16 ([26.1577998])\n",
      "01:03    Epoch 10: train loss 26.15 ([26.15167788])\n",
      "01:03    Epoch 12: train loss 26.15 ([26.14790782])\n",
      "01:04    Epoch 14: train loss 26.15 ([26.14530342])\n",
      "01:05    Epoch 16: train loss 26.14 ([26.13950666])\n",
      "01:05    Epoch 18: train loss 26.14 ([26.13740142])\n",
      "01:06    Epoch 20: train loss 26.13 ([26.13425248])\n",
      "01:06  Finished training\n",
      "01:06  Training estimator 20 / 20 in ensemble\n",
      "01:06  Starting training\n",
      "01:06    Method:                 sally\n",
      "01:06    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_19.npy\n",
      "01:06                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_19.npy\n",
      "01:06    Features:               all\n",
      "01:06    Method:                 sally\n",
      "01:06    Hidden layers:          (100, 100)\n",
      "01:06    Activation function:    tanh\n",
      "01:06    Batch size:             128\n",
      "01:06    Epochs:                 20\n",
      "01:06    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:06    Validation split:       None\n",
      "01:06    Early stopping:         True\n",
      "01:06  Loading training data\n",
      "01:06  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:06  Creating model for method sally\n",
      "01:06  Training model\n",
      "01:07    Epoch 2: train loss 38.77 ([38.76898063])\n",
      "01:07    Epoch 4: train loss 38.76 ([38.76004233])\n",
      "01:08    Epoch 6: train loss 38.75 ([38.74958584])\n",
      "01:09    Epoch 8: train loss 38.74 ([38.74067254])\n",
      "01:09    Epoch 10: train loss 38.74 ([38.73617975])\n",
      "01:10    Epoch 12: train loss 38.73 ([38.73122862])\n",
      "01:11    Epoch 14: train loss 38.73 ([38.72767871])\n",
      "01:11    Epoch 16: train loss 38.72 ([38.72294558])\n",
      "01:12    Epoch 18: train loss 38.72 ([38.7196175])\n",
      "01:13    Epoch 20: train loss 38.72 ([38.71639453])\n",
      "01:13  Finished training\n"
     ]
    }
   ],
   "source": [
    "ensemble_all = EnsembleForge(n_estimators)\n",
    "\n",
    "ensemble_all.train_all(\n",
    "    method='sally',\n",
    "    x_filename=[sample_dir + 'train_local/x_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    t_xz0_filename=[sample_dir + 'train_local/t_xz_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=None,\n",
    "    n_hidden=n_hidden\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "add9d876-2e73-4578-8d43-722df0c790d5"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:13  Calculating expectation for 20 estimators in ensemble\n",
      "01:13  Starting evaluation for estimator 1 / 20 in ensemble\n",
      "01:13  Starting evaluation for estimator 2 / 20 in ensemble\n",
      "01:13  Starting evaluation for estimator 3 / 20 in ensemble\n",
      "01:13  Starting evaluation for estimator 4 / 20 in ensemble\n",
      "01:13  Starting evaluation for estimator 5 / 20 in ensemble\n",
      "01:14  Starting evaluation for estimator 6 / 20 in ensemble\n",
      "01:14  Starting evaluation for estimator 7 / 20 in ensemble\n",
      "01:14  Starting evaluation for estimator 8 / 20 in ensemble\n",
      "01:14  Starting evaluation for estimator 9 / 20 in ensemble\n",
      "01:14  Starting evaluation for estimator 10 / 20 in ensemble\n",
      "01:14  Starting evaluation for estimator 11 / 20 in ensemble\n",
      "01:15  Starting evaluation for estimator 12 / 20 in ensemble\n",
      "01:15  Starting evaluation for estimator 13 / 20 in ensemble\n",
      "01:15  Starting evaluation for estimator 14 / 20 in ensemble\n",
      "01:15  Starting evaluation for estimator 15 / 20 in ensemble\n",
      "01:15  Starting evaluation for estimator 16 / 20 in ensemble\n",
      "01:15  Starting evaluation for estimator 17 / 20 in ensemble\n",
      "01:15  Starting evaluation for estimator 18 / 20 in ensemble\n",
      "01:16  Starting evaluation for estimator 19 / 20 in ensemble\n",
      "01:16  Starting evaluation for estimator 20 / 20 in ensemble\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03204757, -0.04126279],\n",
       "       [-0.02273991,  0.05664141],\n",
       "       [ 0.05441407,  0.01417246],\n",
       "       [-0.03075575, -0.04044286],\n",
       "       [-0.02910792, -0.07734152],\n",
       "       [ 0.00972841, -0.06626226],\n",
       "       [ 0.0228152 ,  0.1098204 ],\n",
       "       [-0.05857595,  0.01893184],\n",
       "       [ 0.02957168, -0.08383568],\n",
       "       [-0.3199839 ,  0.15417713],\n",
       "       [-0.04143749, -0.0601419 ],\n",
       "       [ 0.01575857,  0.0789884 ],\n",
       "       [ 0.05620619, -0.05044384],\n",
       "       [ 0.04178018,  0.01603008],\n",
       "       [-0.03680419,  0.0245736 ],\n",
       "       [-0.02385111,  0.02015075],\n",
       "       [ 0.02806177, -0.01884802],\n",
       "       [ 0.06489246, -0.02968051],\n",
       "       [-0.01572647, -0.00190887],\n",
       "       [ 0.05007704, -0.02756988]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_all.calculate_expectation(\n",
    "    x_filename=sample_dir + 'validation/x_validation.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "be8f7de9-1057-4b69-990f-b80f8752be24"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_all.save(model_dir + 'sally_ensemble_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ede24e6c-05b8-4c8f-9a32-fd890e75873a"
    }
   },
   "source": [
    "## 1d toy study (delta phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbpresent": {
     "id": "c43f5a5b-fc70-4c69-b4c6-4d64dc7eba29"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:16  Training 20 estimators in ensemble\n",
      "01:16  Training estimator 1 / 20 in ensemble\n",
      "01:16  Starting training\n",
      "01:16    Method:                 sally\n",
      "01:16    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "01:16                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "01:16    Features:               [20]\n",
      "01:16    Method:                 sally\n",
      "01:16    Hidden layers:          (100, 100)\n",
      "01:16    Activation function:    tanh\n",
      "01:16    Batch size:             128\n",
      "01:16    Epochs:                 20\n",
      "01:16    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:16    Validation split:       None\n",
      "01:16    Early stopping:         True\n",
      "01:16  Loading training data\n",
      "01:16  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:16  Only using 1 of 30 observables\n",
      "01:16  Creating model for method sally\n",
      "01:16  Training model\n",
      "01:17    Epoch 2: train loss 30.94 ([30.94138785])\n",
      "01:17    Epoch 4: train loss 30.94 ([30.9366034])\n",
      "01:18    Epoch 6: train loss 30.94 ([30.93621583])\n",
      "01:18    Epoch 8: train loss 30.93 ([30.93484023])\n",
      "01:19    Epoch 10: train loss 30.93 ([30.93197087])\n",
      "01:20    Epoch 12: train loss 30.93 ([30.931104])\n",
      "01:20    Epoch 14: train loss 30.93 ([30.92987857])\n",
      "01:21    Epoch 16: train loss 30.93 ([30.92884015])\n",
      "01:21    Epoch 18: train loss 30.93 ([30.92887324])\n",
      "01:22    Epoch 20: train loss 30.93 ([30.93074148])\n",
      "01:22  Finished training\n",
      "01:22  Training estimator 2 / 20 in ensemble\n",
      "01:22  Starting training\n",
      "01:22    Method:                 sally\n",
      "01:22    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "01:22                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "01:22    Features:               [20]\n",
      "01:22    Method:                 sally\n",
      "01:22    Hidden layers:          (100, 100)\n",
      "01:22    Activation function:    tanh\n",
      "01:22    Batch size:             128\n",
      "01:22    Epochs:                 20\n",
      "01:22    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:22    Validation split:       None\n",
      "01:22    Early stopping:         True\n",
      "01:22  Loading training data\n",
      "01:22  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:22  Only using 1 of 30 observables\n",
      "01:22  Creating model for method sally\n",
      "01:22  Training model\n",
      "01:23    Epoch 2: train loss 34.11 ([34.10655207])\n",
      "01:23    Epoch 4: train loss 34.09 ([34.09466415])\n",
      "01:24    Epoch 6: train loss 34.09 ([34.08778739])\n",
      "01:24    Epoch 8: train loss 34.08 ([34.08359562])\n",
      "01:25    Epoch 10: train loss 34.08 ([34.08023406])\n",
      "01:26    Epoch 12: train loss 34.08 ([34.07696048])\n",
      "01:26    Epoch 14: train loss 34.07 ([34.07494924])\n",
      "01:27    Epoch 16: train loss 34.07 ([34.07423139])\n",
      "01:27    Epoch 18: train loss 34.07 ([34.0729217])\n",
      "01:28    Epoch 20: train loss 34.07 ([34.0722265])\n",
      "01:28  Finished training\n",
      "01:28  Training estimator 3 / 20 in ensemble\n",
      "01:28  Starting training\n",
      "01:28    Method:                 sally\n",
      "01:28    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "01:28                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "01:28    Features:               [20]\n",
      "01:28    Method:                 sally\n",
      "01:28    Hidden layers:          (100, 100)\n",
      "01:28    Activation function:    tanh\n",
      "01:28    Batch size:             128\n",
      "01:28    Epochs:                 20\n",
      "01:28    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:28    Validation split:       None\n",
      "01:28    Early stopping:         True\n",
      "01:28  Loading training data\n",
      "01:28  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:28  Only using 1 of 30 observables\n",
      "01:28  Creating model for method sally\n",
      "01:28  Training model\n",
      "01:29    Epoch 2: train loss 33.77 ([33.77245691])\n",
      "01:29    Epoch 4: train loss 33.76 ([33.76477818])\n",
      "01:30    Epoch 6: train loss 33.76 ([33.7578701])\n",
      "01:30    Epoch 8: train loss 33.75 ([33.75188461])\n",
      "01:31    Epoch 10: train loss 33.75 ([33.75056915])\n",
      "01:31    Epoch 12: train loss 33.75 ([33.74820314])\n",
      "01:32    Epoch 14: train loss 33.75 ([33.7465779])\n",
      "01:33    Epoch 16: train loss 33.75 ([33.74542017])\n",
      "01:33    Epoch 18: train loss 33.74 ([33.74456336])\n",
      "01:34    Epoch 20: train loss 33.74 ([33.74385761])\n",
      "01:34  Finished training\n",
      "01:34  Training estimator 4 / 20 in ensemble\n",
      "01:34  Starting training\n",
      "01:34    Method:                 sally\n",
      "01:34    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "01:34                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "01:34    Features:               [20]\n",
      "01:34    Method:                 sally\n",
      "01:34    Hidden layers:          (100, 100)\n",
      "01:34    Activation function:    tanh\n",
      "01:34    Batch size:             128\n",
      "01:34    Epochs:                 20\n",
      "01:34    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:34    Validation split:       None\n",
      "01:34    Early stopping:         True\n",
      "01:34  Loading training data\n",
      "01:34  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:34  Only using 1 of 30 observables\n",
      "01:34  Creating model for method sally\n",
      "01:34  Training model\n",
      "01:35    Epoch 2: train loss 30.62 ([30.61885134])\n",
      "01:35    Epoch 4: train loss 30.61 ([30.61010376])\n",
      "01:36    Epoch 6: train loss 30.60 ([30.60448333])\n",
      "01:36    Epoch 8: train loss 30.60 ([30.5997818])\n",
      "01:37    Epoch 10: train loss 30.60 ([30.60096854])\n",
      "01:37    Epoch 12: train loss 30.59 ([30.59424535])\n",
      "01:38    Epoch 14: train loss 30.59 ([30.59264679])\n",
      "01:39    Epoch 16: train loss 30.59 ([30.59200718])\n",
      "01:39    Epoch 18: train loss 30.59 ([30.5914651])\n",
      "01:40    Epoch 20: train loss 30.59 ([30.59016827])\n",
      "01:40  Finished training\n",
      "01:40  Training estimator 5 / 20 in ensemble\n",
      "01:40  Starting training\n",
      "01:40    Method:                 sally\n",
      "01:40    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "01:40                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "01:40    Features:               [20]\n",
      "01:40    Method:                 sally\n",
      "01:40    Hidden layers:          (100, 100)\n",
      "01:40    Activation function:    tanh\n",
      "01:40    Batch size:             128\n",
      "01:40    Epochs:                 20\n",
      "01:40    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:40    Validation split:       None\n",
      "01:40    Early stopping:         True\n",
      "01:40  Loading training data\n",
      "01:40  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:40  Only using 1 of 30 observables\n",
      "01:40  Creating model for method sally\n",
      "01:40  Training model\n",
      "01:40    Epoch 2: train loss 38.11 ([38.11406007])\n",
      "01:41    Epoch 4: train loss 38.12 ([38.11547401])\n",
      "01:42    Epoch 6: train loss 38.11 ([38.11267017])\n",
      "01:42    Epoch 8: train loss 38.11 ([38.10642541])\n",
      "01:43    Epoch 10: train loss 38.10 ([38.10194474])\n",
      "01:43    Epoch 12: train loss 38.10 ([38.10019584])\n",
      "01:44    Epoch 14: train loss 38.10 ([38.09905711])\n",
      "01:44    Epoch 16: train loss 38.10 ([38.09749313])\n",
      "01:45    Epoch 18: train loss 38.10 ([38.09578688])\n",
      "01:46    Epoch 20: train loss 38.10 ([38.09541871])\n",
      "01:46  Finished training\n",
      "01:46  Training estimator 6 / 20 in ensemble\n",
      "01:46  Starting training\n",
      "01:46    Method:                 sally\n",
      "01:46    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "01:46                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "01:46    Features:               [20]\n",
      "01:46    Method:                 sally\n",
      "01:46    Hidden layers:          (100, 100)\n",
      "01:46    Activation function:    tanh\n",
      "01:46    Batch size:             128\n",
      "01:46    Epochs:                 20\n",
      "01:46    Learning rate:          0.002 initially, decaying to 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:46    Validation split:       None\n",
      "01:46    Early stopping:         True\n",
      "01:46  Loading training data\n",
      "01:46  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:46  Only using 1 of 30 observables\n",
      "01:46  Creating model for method sally\n",
      "01:46  Training model\n",
      "01:46    Epoch 2: train loss 24.31 ([24.30815676])\n",
      "01:47    Epoch 4: train loss 24.31 ([24.30557742])\n",
      "01:48    Epoch 6: train loss 24.30 ([24.30285415])\n",
      "01:48    Epoch 8: train loss 24.30 ([24.30458712])\n",
      "01:49    Epoch 10: train loss 24.30 ([24.30048575])\n",
      "01:49    Epoch 12: train loss 24.30 ([24.2997615])\n",
      "01:50    Epoch 14: train loss 24.30 ([24.29957524])\n",
      "01:50    Epoch 16: train loss 24.30 ([24.29908328])\n",
      "01:51    Epoch 18: train loss 24.30 ([24.29886703])\n",
      "01:52    Epoch 20: train loss 24.30 ([24.29891977])\n",
      "01:52  Finished training\n",
      "01:52  Training estimator 7 / 20 in ensemble\n",
      "01:52  Starting training\n",
      "01:52    Method:                 sally\n",
      "01:52    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "01:52                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "01:52    Features:               [20]\n",
      "01:52    Method:                 sally\n",
      "01:52    Hidden layers:          (100, 100)\n",
      "01:52    Activation function:    tanh\n",
      "01:52    Batch size:             128\n",
      "01:52    Epochs:                 20\n",
      "01:52    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:52    Validation split:       None\n",
      "01:52    Early stopping:         True\n",
      "01:52  Loading training data\n",
      "01:52  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:52  Only using 1 of 30 observables\n",
      "01:52  Creating model for method sally\n",
      "01:52  Training model\n",
      "01:52    Epoch 2: train loss 19.88 ([19.88407936])\n",
      "01:53    Epoch 4: train loss 19.88 ([19.87934113])\n",
      "01:54    Epoch 6: train loss 19.88 ([19.87590776])\n",
      "01:54    Epoch 8: train loss 19.87 ([19.87437418])\n",
      "01:55    Epoch 10: train loss 19.87 ([19.87322408])\n",
      "01:55    Epoch 12: train loss 19.87 ([19.87315314])\n",
      "01:56    Epoch 14: train loss 19.87 ([19.87238373])\n",
      "01:56    Epoch 16: train loss 19.87 ([19.87218923])\n",
      "01:57    Epoch 18: train loss 19.87 ([19.87203702])\n",
      "01:58    Epoch 20: train loss 19.87 ([19.8719019])\n",
      "01:58  Finished training\n",
      "01:58  Training estimator 8 / 20 in ensemble\n",
      "01:58  Starting training\n",
      "01:58    Method:                 sally\n",
      "01:58    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "01:58                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "01:58    Features:               [20]\n",
      "01:58    Method:                 sally\n",
      "01:58    Hidden layers:          (100, 100)\n",
      "01:58    Activation function:    tanh\n",
      "01:58    Batch size:             128\n",
      "01:58    Epochs:                 20\n",
      "01:58    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:58    Validation split:       None\n",
      "01:58    Early stopping:         True\n",
      "01:58  Loading training data\n",
      "01:58  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:58  Only using 1 of 30 observables\n",
      "01:58  Creating model for method sally\n",
      "01:58  Training model\n",
      "01:58    Epoch 2: train loss 26.40 ([26.40438813])\n",
      "01:59    Epoch 4: train loss 26.40 ([26.39927874])\n",
      "02:00    Epoch 6: train loss 26.39 ([26.39189511])\n",
      "02:00    Epoch 8: train loss 26.39 ([26.38864014])\n",
      "02:01    Epoch 10: train loss 26.39 ([26.39473554])\n",
      "02:01    Epoch 12: train loss 26.38 ([26.38320472])\n",
      "02:02    Epoch 14: train loss 26.38 ([26.38112084])\n",
      "02:02    Epoch 16: train loss 26.38 ([26.38123295])\n",
      "02:03    Epoch 18: train loss 26.38 ([26.37976054])\n",
      "02:04    Epoch 20: train loss 26.38 ([26.37938549])\n",
      "02:04  Finished training\n",
      "02:04  Training estimator 9 / 20 in ensemble\n",
      "02:04  Starting training\n",
      "02:04    Method:                 sally\n",
      "02:04    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "02:04                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "02:04    Features:               [20]\n",
      "02:04    Method:                 sally\n",
      "02:04    Hidden layers:          (100, 100)\n",
      "02:04    Activation function:    tanh\n",
      "02:04    Batch size:             128\n",
      "02:04    Epochs:                 20\n",
      "02:04    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:04    Validation split:       None\n",
      "02:04    Early stopping:         True\n",
      "02:04  Loading training data\n",
      "02:04  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:04  Only using 1 of 30 observables\n",
      "02:04  Creating model for method sally\n",
      "02:04  Training model\n",
      "02:04    Epoch 2: train loss 26.47 ([26.47037064])\n",
      "02:05    Epoch 4: train loss 26.47 ([26.46641999])\n",
      "02:05    Epoch 6: train loss 26.46 ([26.46346003])\n",
      "02:06    Epoch 8: train loss 26.46 ([26.4604141])\n",
      "02:07    Epoch 10: train loss 26.46 ([26.45921568])\n",
      "02:07    Epoch 12: train loss 26.46 ([26.45792225])\n",
      "02:08    Epoch 14: train loss 26.46 ([26.45712433])\n",
      "02:08    Epoch 16: train loss 26.46 ([26.45655])\n",
      "02:09    Epoch 18: train loss 26.46 ([26.4561849])\n",
      "02:09    Epoch 20: train loss 26.46 ([26.45616275])\n",
      "02:09  Finished training\n",
      "02:09  Training estimator 10 / 20 in ensemble\n",
      "02:09  Starting training\n",
      "02:09    Method:                 sally\n",
      "02:09    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "02:09                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "02:09    Features:               [20]\n",
      "02:09    Method:                 sally\n",
      "02:09    Hidden layers:          (100, 100)\n",
      "02:09    Activation function:    tanh\n",
      "02:09    Batch size:             128\n",
      "02:09    Epochs:                 20\n",
      "02:09    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:09    Validation split:       None\n",
      "02:09    Early stopping:         True\n",
      "02:09  Loading training data\n",
      "02:09  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:09  Only using 1 of 30 observables\n",
      "02:09  Creating model for method sally\n",
      "02:09  Training model\n",
      "02:10    Epoch 2: train loss 31.21 ([31.20796481])\n",
      "02:11    Epoch 4: train loss 31.19 ([31.19207236])\n",
      "02:11    Epoch 6: train loss 31.19 ([31.18935719])\n",
      "02:12    Epoch 8: train loss 31.19 ([31.18814779])\n",
      "02:13    Epoch 10: train loss 31.19 ([31.18734426])\n",
      "02:13    Epoch 12: train loss 31.19 ([31.18703655])\n",
      "02:14    Epoch 14: train loss 31.19 ([31.18681486])\n",
      "02:14    Epoch 16: train loss 31.19 ([31.18652693])\n",
      "02:15    Epoch 18: train loss 31.19 ([31.18634875])\n",
      "02:15    Epoch 20: train loss 31.19 ([31.18621694])\n",
      "02:15  Finished training\n",
      "02:15  Training estimator 11 / 20 in ensemble\n",
      "02:15  Starting training\n",
      "02:15    Method:                 sally\n",
      "02:15    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_10.npy\n",
      "02:15                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_10.npy\n",
      "02:15    Features:               [20]\n",
      "02:15    Method:                 sally\n",
      "02:15    Hidden layers:          (100, 100)\n",
      "02:15    Activation function:    tanh\n",
      "02:15    Batch size:             128\n",
      "02:15    Epochs:                 20\n",
      "02:15    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:15    Validation split:       None\n",
      "02:15    Early stopping:         True\n",
      "02:15  Loading training data\n",
      "02:15  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:15  Only using 1 of 30 observables\n",
      "02:15  Creating model for method sally\n",
      "02:15  Training model\n",
      "02:16    Epoch 2: train loss 28.48 ([28.47788864])\n",
      "02:17    Epoch 4: train loss 28.46 ([28.46298448])\n",
      "02:17    Epoch 6: train loss 28.46 ([28.46035052])\n",
      "02:18    Epoch 8: train loss 28.46 ([28.45876317])\n",
      "02:18    Epoch 10: train loss 28.46 ([28.45810063])\n",
      "02:19    Epoch 12: train loss 28.46 ([28.45718188])\n",
      "02:20    Epoch 14: train loss 28.46 ([28.45727323])\n",
      "02:20    Epoch 16: train loss 28.46 ([28.45652806])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:21    Epoch 18: train loss 28.46 ([28.45653325])\n",
      "02:21    Epoch 20: train loss 28.46 ([28.45730849])\n",
      "02:21  Finished training\n",
      "02:21  Training estimator 12 / 20 in ensemble\n",
      "02:21  Starting training\n",
      "02:21    Method:                 sally\n",
      "02:21    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_11.npy\n",
      "02:21                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_11.npy\n",
      "02:21    Features:               [20]\n",
      "02:21    Method:                 sally\n",
      "02:21    Hidden layers:          (100, 100)\n",
      "02:21    Activation function:    tanh\n",
      "02:21    Batch size:             128\n",
      "02:21    Epochs:                 20\n",
      "02:21    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:21    Validation split:       None\n",
      "02:21    Early stopping:         True\n",
      "02:21  Loading training data\n",
      "02:21  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:21  Only using 1 of 30 observables\n",
      "02:21  Creating model for method sally\n",
      "02:21  Training model\n",
      "02:22    Epoch 2: train loss 26.42 ([26.41763916])\n",
      "02:23    Epoch 4: train loss 26.41 ([26.41226041])\n",
      "02:23    Epoch 6: train loss 26.40 ([26.40386762])\n",
      "02:24    Epoch 8: train loss 26.40 ([26.3997991])\n",
      "02:24    Epoch 10: train loss 26.40 ([26.39677216])\n",
      "02:25    Epoch 12: train loss 26.40 ([26.39929194])\n",
      "02:26    Epoch 14: train loss 26.39 ([26.39292495])\n",
      "02:26    Epoch 16: train loss 26.39 ([26.39087414])\n",
      "02:27    Epoch 18: train loss 26.39 ([26.39051499])\n",
      "02:27    Epoch 20: train loss 26.39 ([26.38977051])\n",
      "02:27  Finished training\n",
      "02:27  Training estimator 13 / 20 in ensemble\n",
      "02:27  Starting training\n",
      "02:27    Method:                 sally\n",
      "02:27    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_12.npy\n",
      "02:27                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_12.npy\n",
      "02:27    Features:               [20]\n",
      "02:27    Method:                 sally\n",
      "02:27    Hidden layers:          (100, 100)\n",
      "02:27    Activation function:    tanh\n",
      "02:27    Batch size:             128\n",
      "02:27    Epochs:                 20\n",
      "02:27    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:27    Validation split:       None\n",
      "02:27    Early stopping:         True\n",
      "02:27  Loading training data\n",
      "02:27  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:27  Only using 1 of 30 observables\n",
      "02:27  Creating model for method sally\n",
      "02:27  Training model\n",
      "02:28    Epoch 2: train loss 36.74 ([36.74003012])\n",
      "02:29    Epoch 4: train loss 36.73 ([36.72581982])\n",
      "02:29    Epoch 6: train loss 36.72 ([36.71533065])\n",
      "02:30    Epoch 8: train loss 36.71 ([36.71175064])\n",
      "02:30    Epoch 10: train loss 36.71 ([36.71023165])\n",
      "02:31    Epoch 12: train loss 36.71 ([36.70946255])\n",
      "02:32    Epoch 14: train loss 36.71 ([36.70897286])\n",
      "02:32    Epoch 16: train loss 36.71 ([36.70849837])\n",
      "02:33    Epoch 18: train loss 36.71 ([36.71109653])\n",
      "02:33    Epoch 20: train loss 36.71 ([36.70816738])\n",
      "02:33  Finished training\n",
      "02:33  Training estimator 14 / 20 in ensemble\n",
      "02:33  Starting training\n",
      "02:33    Method:                 sally\n",
      "02:33    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_13.npy\n",
      "02:33                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_13.npy\n",
      "02:33    Features:               [20]\n",
      "02:33    Method:                 sally\n",
      "02:33    Hidden layers:          (100, 100)\n",
      "02:33    Activation function:    tanh\n",
      "02:33    Batch size:             128\n",
      "02:33    Epochs:                 20\n",
      "02:33    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:33    Validation split:       None\n",
      "02:33    Early stopping:         True\n",
      "02:33  Loading training data\n",
      "02:33  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:33  Only using 1 of 30 observables\n",
      "02:33  Creating model for method sally\n",
      "02:33  Training model\n",
      "02:34    Epoch 2: train loss 25.32 ([25.32047371])\n",
      "02:35    Epoch 4: train loss 25.31 ([25.31482048])\n",
      "02:35    Epoch 6: train loss 25.31 ([25.31119384])\n",
      "02:36    Epoch 8: train loss 25.31 ([25.30916924])\n",
      "02:36    Epoch 10: train loss 25.31 ([25.309434])\n",
      "02:37    Epoch 12: train loss 25.31 ([25.30726048])\n",
      "02:37    Epoch 14: train loss 25.31 ([25.3070978])\n",
      "02:38    Epoch 16: train loss 25.31 ([25.30652412])\n",
      "02:39    Epoch 18: train loss 25.31 ([25.30595381])\n",
      "02:39    Epoch 20: train loss 25.31 ([25.30606614])\n",
      "02:39  Finished training\n",
      "02:39  Training estimator 15 / 20 in ensemble\n",
      "02:39  Starting training\n",
      "02:39    Method:                 sally\n",
      "02:39    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_14.npy\n",
      "02:39                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_14.npy\n",
      "02:39    Features:               [20]\n",
      "02:39    Method:                 sally\n",
      "02:39    Hidden layers:          (100, 100)\n",
      "02:39    Activation function:    tanh\n",
      "02:39    Batch size:             128\n",
      "02:39    Epochs:                 20\n",
      "02:39    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:39    Validation split:       None\n",
      "02:39    Early stopping:         True\n",
      "02:39  Loading training data\n",
      "02:39  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:39  Only using 1 of 30 observables\n",
      "02:39  Creating model for method sally\n",
      "02:39  Training model\n",
      "02:40    Epoch 2: train loss 65.18 ([65.1763608])\n",
      "02:41    Epoch 4: train loss 65.18 ([65.1755523])\n",
      "02:41    Epoch 6: train loss 65.17 ([65.16818974])\n",
      "02:42    Epoch 8: train loss 65.17 ([65.16746095])\n",
      "02:42    Epoch 10: train loss 65.16 ([65.16244324])\n",
      "02:43    Epoch 12: train loss 65.16 ([65.15826188])\n",
      "02:43    Epoch 14: train loss 65.16 ([65.15726839])\n",
      "02:44    Epoch 16: train loss 65.16 ([65.15682039])\n",
      "02:45    Epoch 18: train loss 65.16 ([65.15755966])\n",
      "02:45    Epoch 20: train loss 65.16 ([65.15539738])\n",
      "02:45  Finished training\n",
      "02:45  Training estimator 16 / 20 in ensemble\n",
      "02:45  Starting training\n",
      "02:45    Method:                 sally\n",
      "02:45    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_15.npy\n",
      "02:45                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_15.npy\n",
      "02:45    Features:               [20]\n",
      "02:45    Method:                 sally\n",
      "02:45    Hidden layers:          (100, 100)\n",
      "02:45    Activation function:    tanh\n",
      "02:45    Batch size:             128\n",
      "02:45    Epochs:                 20\n",
      "02:45    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:45    Validation split:       None\n",
      "02:45    Early stopping:         True\n",
      "02:45  Loading training data\n",
      "02:45  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:45  Only using 1 of 30 observables\n",
      "02:45  Creating model for method sally\n",
      "02:45  Training model\n",
      "02:46    Epoch 2: train loss 21.32 ([21.3162439])\n",
      "02:46    Epoch 4: train loss 21.31 ([21.30677254])\n",
      "02:47    Epoch 6: train loss 21.30 ([21.30088371])\n",
      "02:48    Epoch 8: train loss 21.30 ([21.30207779])\n",
      "02:48    Epoch 10: train loss 21.30 ([21.29979526])\n",
      "02:49    Epoch 12: train loss 21.30 ([21.29878164])\n",
      "02:49    Epoch 14: train loss 21.30 ([21.29825072])\n",
      "02:50    Epoch 16: train loss 21.30 ([21.29809047])\n",
      "02:50    Epoch 18: train loss 21.30 ([21.29786948])\n",
      "02:51    Epoch 20: train loss 21.30 ([21.29767506])\n",
      "02:51  Finished training\n",
      "02:51  Training estimator 17 / 20 in ensemble\n",
      "02:51  Starting training\n",
      "02:51    Method:                 sally\n",
      "02:51    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_16.npy\n",
      "02:51                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_16.npy\n",
      "02:51    Features:               [20]\n",
      "02:51    Method:                 sally\n",
      "02:51    Hidden layers:          (100, 100)\n",
      "02:51    Activation function:    tanh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:51    Batch size:             128\n",
      "02:51    Epochs:                 20\n",
      "02:51    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:51    Validation split:       None\n",
      "02:51    Early stopping:         True\n",
      "02:51  Loading training data\n",
      "02:51  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:51  Only using 1 of 30 observables\n",
      "02:51  Creating model for method sally\n",
      "02:51  Training model\n",
      "02:52    Epoch 2: train loss 20.85 ([20.84531817])\n",
      "02:52    Epoch 4: train loss 20.84 ([20.83725951])\n",
      "02:53    Epoch 6: train loss 20.84 ([20.83585127])\n",
      "02:54    Epoch 8: train loss 20.84 ([20.8355927])\n",
      "02:54    Epoch 10: train loss 20.83 ([20.83496546])\n",
      "02:55    Epoch 12: train loss 20.84 ([20.83512471])\n",
      "02:55    Epoch 14: train loss 20.83 ([20.83469329])\n",
      "02:56    Epoch 16: train loss 20.83 ([20.83485474])\n",
      "02:57    Epoch 18: train loss 20.84 ([20.83506246])\n",
      "02:57    Epoch 20: train loss 20.83 ([20.83369829])\n",
      "02:57  Finished training\n",
      "02:57  Training estimator 18 / 20 in ensemble\n",
      "02:57  Starting training\n",
      "02:57    Method:                 sally\n",
      "02:57    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_17.npy\n",
      "02:57                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_17.npy\n",
      "02:57    Features:               [20]\n",
      "02:57    Method:                 sally\n",
      "02:57    Hidden layers:          (100, 100)\n",
      "02:57    Activation function:    tanh\n",
      "02:57    Batch size:             128\n",
      "02:57    Epochs:                 20\n",
      "02:57    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:57    Validation split:       None\n",
      "02:57    Early stopping:         True\n",
      "02:57  Loading training data\n",
      "02:57  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:57  Only using 1 of 30 observables\n",
      "02:57  Creating model for method sally\n",
      "02:57  Training model\n",
      "02:58    Epoch 2: train loss 39.49 ([39.48668955])\n",
      "02:59    Epoch 4: train loss 39.48 ([39.47865455])\n",
      "02:59    Epoch 6: train loss 39.47 ([39.47376936])\n",
      "03:00    Epoch 8: train loss 39.47 ([39.46754217])\n",
      "03:00    Epoch 10: train loss 39.46 ([39.46397738])\n",
      "03:01    Epoch 12: train loss 39.46 ([39.46055454])\n",
      "03:01    Epoch 14: train loss 39.46 ([39.45927387])\n",
      "03:02    Epoch 16: train loss 39.46 ([39.45820729])\n",
      "03:03    Epoch 18: train loss 39.46 ([39.45711807])\n",
      "03:03    Epoch 20: train loss 39.46 ([39.45653831])\n",
      "03:03  Finished training\n",
      "03:03  Training estimator 19 / 20 in ensemble\n",
      "03:03  Starting training\n",
      "03:03    Method:                 sally\n",
      "03:03    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_18.npy\n",
      "03:03                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_18.npy\n",
      "03:03    Features:               [20]\n",
      "03:03    Method:                 sally\n",
      "03:03    Hidden layers:          (100, 100)\n",
      "03:03    Activation function:    tanh\n",
      "03:03    Batch size:             128\n",
      "03:03    Epochs:                 20\n",
      "03:03    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:03    Validation split:       None\n",
      "03:03    Early stopping:         True\n",
      "03:03  Loading training data\n",
      "03:03  Found 1000000 samples with 2 parameters and 30 observables\n",
      "03:03  Only using 1 of 30 observables\n",
      "03:03  Creating model for method sally\n",
      "03:03  Training model\n",
      "03:04    Epoch 2: train loss 26.16 ([26.16135709])\n",
      "03:04    Epoch 4: train loss 26.16 ([26.15639656])\n",
      "03:05    Epoch 6: train loss 26.15 ([26.15379294])\n",
      "03:06    Epoch 8: train loss 26.15 ([26.15237774])\n",
      "03:06    Epoch 10: train loss 26.15 ([26.15171096])\n",
      "03:07    Epoch 12: train loss 26.15 ([26.15091252])\n",
      "03:07    Epoch 14: train loss 26.15 ([26.15060837])\n",
      "03:08    Epoch 16: train loss 26.15 ([26.15070983])\n",
      "03:08    Epoch 18: train loss 26.15 ([26.15044293])\n",
      "03:09    Epoch 20: train loss 26.15 ([26.15008506])\n",
      "03:09  Finished training\n",
      "03:09  Training estimator 20 / 20 in ensemble\n",
      "03:09  Starting training\n",
      "03:09    Method:                 sally\n",
      "03:09    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_19.npy\n",
      "03:09                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_19.npy\n",
      "03:09    Features:               [20]\n",
      "03:09    Method:                 sally\n",
      "03:09    Hidden layers:          (100, 100)\n",
      "03:09    Activation function:    tanh\n",
      "03:09    Batch size:             128\n",
      "03:09    Epochs:                 20\n",
      "03:09    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:09    Validation split:       None\n",
      "03:09    Early stopping:         True\n",
      "03:09  Loading training data\n",
      "03:09  Found 1000000 samples with 2 parameters and 30 observables\n",
      "03:09  Only using 1 of 30 observables\n",
      "03:09  Creating model for method sally\n",
      "03:09  Training model\n",
      "03:10    Epoch 2: train loss 38.77 ([38.76524824])\n",
      "03:10    Epoch 4: train loss 38.75 ([38.7539767])\n",
      "03:11    Epoch 6: train loss 38.75 ([38.74974957])\n",
      "03:12    Epoch 8: train loss 38.75 ([38.74751016])\n",
      "03:12    Epoch 10: train loss 38.74 ([38.74295126])\n",
      "03:13    Epoch 12: train loss 38.74 ([38.74111501])\n",
      "03:13    Epoch 14: train loss 38.74 ([38.73874453])\n",
      "03:14    Epoch 16: train loss 38.74 ([38.73802676])\n",
      "03:14    Epoch 18: train loss 38.74 ([38.73679183])\n",
      "03:15    Epoch 20: train loss 38.74 ([38.73588282])\n",
      "03:15  Finished training\n"
     ]
    }
   ],
   "source": [
    "ensemble_deltaphi = EnsembleForge(n_estimators)\n",
    "\n",
    "ensemble_deltaphi.train_all(\n",
    "    features=[ [20] for _ in range(n_estimators)],\n",
    "    method='sally',\n",
    "    x_filename=[sample_dir + 'train_local/x_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    t_xz0_filename=[sample_dir + 'train_local/t_xz_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=None,\n",
    "    n_hidden=n_hidden\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbpresent": {
     "id": "239d930c-cd8c-4586-862d-a68f24491d1e"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:15  Calculating expectation for 20 estimators in ensemble\n",
      "03:15  Starting evaluation for estimator 1 / 20 in ensemble\n",
      "03:15  Starting evaluation for estimator 2 / 20 in ensemble\n",
      "03:15  Starting evaluation for estimator 3 / 20 in ensemble\n",
      "03:15  Starting evaluation for estimator 4 / 20 in ensemble\n",
      "03:16  Starting evaluation for estimator 5 / 20 in ensemble\n",
      "03:16  Starting evaluation for estimator 6 / 20 in ensemble\n",
      "03:16  Starting evaluation for estimator 7 / 20 in ensemble\n",
      "03:16  Starting evaluation for estimator 8 / 20 in ensemble\n",
      "03:16  Starting evaluation for estimator 9 / 20 in ensemble\n",
      "03:16  Starting evaluation for estimator 10 / 20 in ensemble\n",
      "03:16  Starting evaluation for estimator 11 / 20 in ensemble\n",
      "03:16  Starting evaluation for estimator 12 / 20 in ensemble\n",
      "03:17  Starting evaluation for estimator 13 / 20 in ensemble\n",
      "03:17  Starting evaluation for estimator 14 / 20 in ensemble\n",
      "03:17  Starting evaluation for estimator 15 / 20 in ensemble\n",
      "03:17  Starting evaluation for estimator 16 / 20 in ensemble\n",
      "03:17  Starting evaluation for estimator 17 / 20 in ensemble\n",
      "03:17  Starting evaluation for estimator 18 / 20 in ensemble\n",
      "03:17  Starting evaluation for estimator 19 / 20 in ensemble\n",
      "03:18  Starting evaluation for estimator 20 / 20 in ensemble\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.01428281,  0.00531502],\n",
       "       [-0.00108248, -0.0074827 ],\n",
       "       [ 0.00490564,  0.00044522],\n",
       "       [ 0.00950521,  0.00056793],\n",
       "       [ 0.00695332,  0.00272985],\n",
       "       [-0.00010192, -0.00506927],\n",
       "       [-0.00208527, -0.00592995],\n",
       "       [ 0.02129456,  0.01054829],\n",
       "       [ 0.00924479, -0.00488937],\n",
       "       [-0.00105973,  0.00674628],\n",
       "       [-0.00418518, -0.00493223],\n",
       "       [-0.00204606, -0.00598308],\n",
       "       [ 0.00781755,  0.00542245],\n",
       "       [ 0.00062396,  0.00213693],\n",
       "       [ 0.00560929,  0.00486967],\n",
       "       [ 0.0071533 ,  0.0064686 ],\n",
       "       [ 0.00738978, -0.00364019],\n",
       "       [-0.00066078,  0.00456094],\n",
       "       [-0.0004693 ,  0.00517488],\n",
       "       [-0.00488432, -0.00201409]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_deltaphi.calculate_expectation(\n",
    "    x_filename=sample_dir + 'validation/x_validation.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbpresent": {
     "id": "cba2d9c4-c9db-4362-889f-06e8197075ca"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_deltaphi.save(model_dir + 'sally_ensemble_deltaphi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "45eafc2d-c3ba-493e-b2c3-7f36175d31e5"
    }
   },
   "source": [
    "## 1d toy study (MET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbpresent": {
     "id": "eacce39b-9ad3-46c4-915a-2d4d412918c0"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:18  Training 20 estimators in ensemble\n",
      "03:18  Training estimator 1 / 20 in ensemble\n",
      "03:18  Starting training\n",
      "03:18    Method:                 sally\n",
      "03:18    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "03:18                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "03:18    Features:               [0]\n",
      "03:18    Method:                 sally\n",
      "03:18    Hidden layers:          (100, 100)\n",
      "03:18    Activation function:    tanh\n",
      "03:18    Batch size:             128\n",
      "03:18    Epochs:                 20\n",
      "03:18    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:18    Validation split:       None\n",
      "03:18    Early stopping:         True\n",
      "03:18  Loading training data\n",
      "03:18  Found 1000000 samples with 2 parameters and 30 observables\n",
      "03:18  Only using 1 of 30 observables\n",
      "03:18  Creating model for method sally\n",
      "03:18  Training model\n",
      "03:18    Epoch 2: train loss 30.96 ([30.96483529])\n",
      "03:19    Epoch 4: train loss 30.96 ([30.95690368])\n",
      "03:20    Epoch 6: train loss 30.95 ([30.94987444])\n",
      "03:20    Epoch 8: train loss 30.94 ([30.94444135])\n",
      "03:21    Epoch 10: train loss 30.94 ([30.94121308])\n",
      "03:22    Epoch 12: train loss 30.94 ([30.93779955])\n",
      "03:22    Epoch 14: train loss 30.94 ([30.935254])\n",
      "03:23    Epoch 16: train loss 30.93 ([30.93455499])\n",
      "03:24    Epoch 18: train loss 30.93 ([30.93404659])\n",
      "03:24    Epoch 20: train loss 30.93 ([30.93282859])\n",
      "03:24  Finished training\n",
      "03:24  Training estimator 2 / 20 in ensemble\n",
      "03:24  Starting training\n",
      "03:24    Method:                 sally\n",
      "03:24    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "03:24                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "03:24    Features:               [0]\n",
      "03:24    Method:                 sally\n",
      "03:24    Hidden layers:          (100, 100)\n",
      "03:24    Activation function:    tanh\n",
      "03:24    Batch size:             128\n",
      "03:24    Epochs:                 20\n",
      "03:24    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:24    Validation split:       None\n",
      "03:24    Early stopping:         True\n",
      "03:24  Loading training data\n",
      "03:24  Found 1000000 samples with 2 parameters and 30 observables\n",
      "03:24  Only using 1 of 30 observables\n",
      "03:24  Creating model for method sally\n",
      "03:24  Training model\n",
      "03:25    Epoch 2: train loss 34.11 ([34.11400634])\n",
      "03:26    Epoch 4: train loss 34.10 ([34.09984984])\n",
      "03:27    Epoch 6: train loss 34.10 ([34.09534615])\n",
      "03:27    Epoch 8: train loss 34.09 ([34.08976806])\n",
      "03:28    Epoch 10: train loss 34.08 ([34.08493121])\n",
      "03:28    Epoch 12: train loss 34.08 ([34.08308277])\n",
      "03:29    Epoch 14: train loss 34.08 ([34.08022733])\n",
      "03:30    Epoch 16: train loss 34.08 ([34.07828695])\n",
      "03:31    Epoch 18: train loss 34.08 ([34.07717112])\n",
      "03:31    Epoch 20: train loss 34.08 ([34.07553463])\n",
      "03:31  Finished training\n",
      "03:31  Training estimator 3 / 20 in ensemble\n",
      "03:31  Starting training\n",
      "03:31    Method:                 sally\n",
      "03:31    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "03:31                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "03:31    Features:               [0]\n",
      "03:31    Method:                 sally\n",
      "03:31    Hidden layers:          (100, 100)\n",
      "03:31    Activation function:    tanh\n",
      "03:31    Batch size:             128\n",
      "03:31    Epochs:                 20\n",
      "03:31    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:31    Validation split:       None\n",
      "03:31    Early stopping:         True\n",
      "03:31  Loading training data\n",
      "03:31  Found 1000000 samples with 2 parameters and 30 observables\n",
      "03:31  Only using 1 of 30 observables\n",
      "03:31  Creating model for method sally\n",
      "03:31  Training model\n",
      "03:32    Epoch 2: train loss 33.78 ([33.78244145])\n",
      "03:33    Epoch 4: train loss 33.77 ([33.77027875])\n",
      "03:33    Epoch 6: train loss 33.76 ([33.76494477])\n",
      "03:34    Epoch 8: train loss 33.76 ([33.76009086])\n",
      "03:35    Epoch 10: train loss 33.76 ([33.75603599])\n",
      "03:35    Epoch 12: train loss 33.75 ([33.75314607])\n",
      "03:36    Epoch 14: train loss 33.75 ([33.75178324])\n",
      "03:37    Epoch 16: train loss 33.75 ([33.75021092])\n",
      "03:37    Epoch 18: train loss 33.75 ([33.7497529])\n",
      "03:38    Epoch 20: train loss 33.75 ([33.74793939])\n",
      "03:38  Finished training\n",
      "03:38  Training estimator 4 / 20 in ensemble\n",
      "03:38  Starting training\n",
      "03:38    Method:                 sally\n",
      "03:38    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "03:38                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "03:38    Features:               [0]\n",
      "03:38    Method:                 sally\n",
      "03:38    Hidden layers:          (100, 100)\n",
      "03:38    Activation function:    tanh\n",
      "03:38    Batch size:             128\n",
      "03:38    Epochs:                 20\n",
      "03:38    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:38    Validation split:       None\n",
      "03:38    Early stopping:         True\n",
      "03:38  Loading training data\n",
      "03:38  Found 1000000 samples with 2 parameters and 30 observables\n",
      "03:38  Only using 1 of 30 observables\n",
      "03:38  Creating model for method sally\n",
      "03:38  Training model\n",
      "03:39    Epoch 2: train loss 30.63 ([30.6298458])\n",
      "03:40    Epoch 4: train loss 30.62 ([30.62238012])\n",
      "03:40    Epoch 6: train loss 30.61 ([30.61247174])\n",
      "03:41    Epoch 8: train loss 30.61 ([30.60664667])\n",
      "03:41    Epoch 10: train loss 30.60 ([30.60258241])\n",
      "03:42    Epoch 12: train loss 30.60 ([30.60031817])\n",
      "03:43    Epoch 14: train loss 30.60 ([30.59805799])\n",
      "03:43    Epoch 16: train loss 30.60 ([30.59682896])\n",
      "03:44    Epoch 18: train loss 30.60 ([30.59569974])\n",
      "03:45    Epoch 20: train loss 30.59 ([30.59439672])\n",
      "03:45  Finished training\n",
      "03:45  Training estimator 5 / 20 in ensemble\n",
      "03:45  Starting training\n",
      "03:45    Method:                 sally\n",
      "03:45    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "03:45                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "03:45    Features:               [0]\n",
      "03:45    Method:                 sally\n",
      "03:45    Hidden layers:          (100, 100)\n",
      "03:45    Activation function:    tanh\n",
      "03:45    Batch size:             128\n",
      "03:45    Epochs:                 20\n",
      "03:45    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:45    Validation split:       None\n",
      "03:45    Early stopping:         True\n",
      "03:45  Loading training data\n",
      "03:45  Found 1000000 samples with 2 parameters and 30 observables\n",
      "03:45  Only using 1 of 30 observables\n",
      "03:45  Creating model for method sally\n",
      "03:45  Training model\n",
      "03:46    Epoch 2: train loss 38.14 ([38.13676252])\n",
      "03:46    Epoch 4: train loss 38.12 ([38.12497579])\n",
      "03:47    Epoch 6: train loss 38.12 ([38.12053907])\n",
      "03:48    Epoch 8: train loss 38.11 ([38.11156177])\n",
      "03:48    Epoch 10: train loss 38.11 ([38.10793008])\n",
      "03:49    Epoch 12: train loss 38.11 ([38.1054732])\n",
      "03:49    Epoch 14: train loss 38.10 ([38.10415771])\n",
      "03:50    Epoch 16: train loss 38.10 ([38.10072523])\n",
      "03:51    Epoch 18: train loss 38.10 ([38.09962929])\n",
      "03:51    Epoch 20: train loss 38.10 ([38.10040508])\n",
      "03:51  Finished training\n",
      "03:51  Training estimator 6 / 20 in ensemble\n",
      "03:51  Starting training\n",
      "03:51    Method:                 sally\n",
      "03:51    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "03:51                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "03:51    Features:               [0]\n",
      "03:51    Method:                 sally\n",
      "03:51    Hidden layers:          (100, 100)\n",
      "03:51    Activation function:    tanh\n",
      "03:51    Batch size:             128\n",
      "03:51    Epochs:                 20\n",
      "03:51    Learning rate:          0.002 initially, decaying to 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:51    Validation split:       None\n",
      "03:51    Early stopping:         True\n",
      "03:51  Loading training data\n",
      "03:51  Found 1000000 samples with 2 parameters and 30 observables\n",
      "03:51  Only using 1 of 30 observables\n",
      "03:51  Creating model for method sally\n",
      "03:51  Training model\n",
      "03:52    Epoch 2: train loss 24.34 ([24.33624374])\n",
      "03:53    Epoch 4: train loss 24.33 ([24.32618657])\n",
      "03:53    Epoch 6: train loss 24.32 ([24.31784503])\n",
      "03:54    Epoch 8: train loss 24.31 ([24.3141966])\n",
      "03:55    Epoch 10: train loss 24.31 ([24.30976874])\n",
      "03:55    Epoch 12: train loss 24.31 ([24.30660416])\n",
      "03:56    Epoch 14: train loss 24.31 ([24.30582231])\n",
      "03:57    Epoch 16: train loss 24.30 ([24.3038493])\n",
      "03:57    Epoch 18: train loss 24.30 ([24.30312586])\n",
      "03:58    Epoch 20: train loss 24.30 ([24.30403295])\n",
      "03:58  Finished training\n",
      "03:58  Training estimator 7 / 20 in ensemble\n",
      "03:58  Starting training\n",
      "03:58    Method:                 sally\n",
      "03:58    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "03:58                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "03:58    Features:               [0]\n",
      "03:58    Method:                 sally\n",
      "03:58    Hidden layers:          (100, 100)\n",
      "03:58    Activation function:    tanh\n",
      "03:58    Batch size:             128\n",
      "03:58    Epochs:                 20\n",
      "03:58    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:58    Validation split:       None\n",
      "03:58    Early stopping:         True\n",
      "03:58  Loading training data\n",
      "03:58  Found 1000000 samples with 2 parameters and 30 observables\n",
      "03:58  Only using 1 of 30 observables\n",
      "03:58  Creating model for method sally\n",
      "03:58  Training model\n",
      "03:59    Epoch 2: train loss 19.90 ([19.9039515])\n",
      "03:59    Epoch 4: train loss 19.90 ([19.89756019])\n",
      "04:00    Epoch 6: train loss 19.89 ([19.89256763])\n",
      "04:01    Epoch 8: train loss 19.89 ([19.88783574])\n",
      "04:01    Epoch 10: train loss 19.88 ([19.88409822])\n",
      "04:02    Epoch 12: train loss 19.88 ([19.88169249])\n",
      "04:02    Epoch 14: train loss 19.88 ([19.88042784])\n",
      "04:03    Epoch 16: train loss 19.88 ([19.87895962])\n",
      "04:04    Epoch 18: train loss 19.88 ([19.87790852])\n",
      "04:04    Epoch 20: train loss 19.88 ([19.87986136])\n",
      "04:04  Finished training\n",
      "04:04  Training estimator 8 / 20 in ensemble\n",
      "04:04  Starting training\n",
      "04:04    Method:                 sally\n",
      "04:04    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "04:04                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "04:04    Features:               [0]\n",
      "04:04    Method:                 sally\n",
      "04:04    Hidden layers:          (100, 100)\n",
      "04:04    Activation function:    tanh\n",
      "04:04    Batch size:             128\n",
      "04:04    Epochs:                 20\n",
      "04:04    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "04:04    Validation split:       None\n",
      "04:04    Early stopping:         True\n",
      "04:04  Loading training data\n",
      "04:04  Found 1000000 samples with 2 parameters and 30 observables\n",
      "04:04  Only using 1 of 30 observables\n",
      "04:04  Creating model for method sally\n",
      "04:04  Training model\n",
      "04:05    Epoch 2: train loss 26.41 ([26.41477038])\n",
      "04:06    Epoch 4: train loss 26.41 ([26.4092203])\n",
      "04:06    Epoch 6: train loss 26.40 ([26.39948879])\n",
      "04:07    Epoch 8: train loss 26.39 ([26.39495446])\n",
      "04:07    Epoch 10: train loss 26.39 ([26.39211966])\n",
      "04:08    Epoch 12: train loss 26.39 ([26.38930798])\n",
      "04:09    Epoch 14: train loss 26.39 ([26.38743486])\n",
      "04:10    Epoch 16: train loss 26.39 ([26.38564031])\n",
      "04:10    Epoch 18: train loss 26.38 ([26.38456324])\n",
      "04:11    Epoch 20: train loss 26.38 ([26.38463408])\n",
      "04:11  Finished training\n",
      "04:11  Training estimator 9 / 20 in ensemble\n",
      "04:11  Starting training\n",
      "04:11    Method:                 sally\n",
      "04:11    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "04:11                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "04:11    Features:               [0]\n",
      "04:11    Method:                 sally\n",
      "04:11    Hidden layers:          (100, 100)\n",
      "04:11    Activation function:    tanh\n",
      "04:11    Batch size:             128\n",
      "04:11    Epochs:                 20\n",
      "04:11    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "04:11    Validation split:       None\n",
      "04:11    Early stopping:         True\n",
      "04:11  Loading training data\n",
      "04:11  Found 1000000 samples with 2 parameters and 30 observables\n",
      "04:11  Only using 1 of 30 observables\n",
      "04:11  Creating model for method sally\n",
      "04:11  Training model\n",
      "04:12    Epoch 2: train loss 26.49 ([26.49181204])\n",
      "04:12    Epoch 4: train loss 26.48 ([26.48240401])\n",
      "04:13    Epoch 6: train loss 26.47 ([26.47479644])\n",
      "04:14    Epoch 8: train loss 26.47 ([26.47354911])\n",
      "04:14    Epoch 10: train loss 26.47 ([26.46778148])\n",
      "04:15    Epoch 12: train loss 26.47 ([26.46556413])\n",
      "04:16    Epoch 14: train loss 26.46 ([26.46272237])\n",
      "04:16    Epoch 16: train loss 26.48 ([26.47871245])\n",
      "04:17    Epoch 18: train loss 26.46 ([26.46023276])\n",
      "04:17    Epoch 20: train loss 26.46 ([26.45937191])\n",
      "04:17  Finished training\n",
      "04:17  Training estimator 10 / 20 in ensemble\n",
      "04:17  Starting training\n",
      "04:17    Method:                 sally\n",
      "04:17    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "04:17                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "04:17    Features:               [0]\n",
      "04:17    Method:                 sally\n",
      "04:17    Hidden layers:          (100, 100)\n",
      "04:17    Activation function:    tanh\n",
      "04:17    Batch size:             128\n",
      "04:17    Epochs:                 20\n",
      "04:17    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "04:17    Validation split:       None\n",
      "04:17    Early stopping:         True\n",
      "04:17  Loading training data\n",
      "04:18  Found 1000000 samples with 2 parameters and 30 observables\n",
      "04:18  Only using 1 of 30 observables\n",
      "04:18  Creating model for method sally\n",
      "04:18  Training model\n",
      "04:18    Epoch 2: train loss 31.22 ([31.22377955])\n",
      "04:19    Epoch 4: train loss 31.21 ([31.21339592])\n",
      "04:20    Epoch 6: train loss 31.21 ([31.20755592])\n",
      "04:20    Epoch 8: train loss 31.20 ([31.20216232])\n",
      "04:21    Epoch 10: train loss 31.20 ([31.1978332])\n",
      "04:21    Epoch 12: train loss 31.20 ([31.19512975])\n",
      "04:22    Epoch 14: train loss 31.19 ([31.19244381])\n",
      "04:23    Epoch 16: train loss 31.19 ([31.19194594])\n",
      "04:23    Epoch 18: train loss 31.19 ([31.190285])\n",
      "04:24    Epoch 20: train loss 31.19 ([31.18747554])\n",
      "04:24  Finished training\n",
      "04:24  Training estimator 11 / 20 in ensemble\n",
      "04:24  Starting training\n",
      "04:24    Method:                 sally\n",
      "04:24    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_10.npy\n",
      "04:24                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_10.npy\n",
      "04:24    Features:               [0]\n",
      "04:24    Method:                 sally\n",
      "04:24    Hidden layers:          (100, 100)\n",
      "04:24    Activation function:    tanh\n",
      "04:24    Batch size:             128\n",
      "04:24    Epochs:                 20\n",
      "04:24    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "04:24    Validation split:       None\n",
      "04:24    Early stopping:         True\n",
      "04:24  Loading training data\n",
      "04:24  Found 1000000 samples with 2 parameters and 30 observables\n",
      "04:24  Only using 1 of 30 observables\n",
      "04:24  Creating model for method sally\n",
      "04:24  Training model\n",
      "04:24    Epoch 2: train loss 28.50 ([28.49604899])\n",
      "04:25    Epoch 4: train loss 28.49 ([28.4863945])\n",
      "04:26    Epoch 6: train loss 28.48 ([28.47714883])\n",
      "04:26    Epoch 8: train loss 28.47 ([28.47163527])\n",
      "04:27    Epoch 10: train loss 28.47 ([28.46840007])\n",
      "04:28    Epoch 12: train loss 28.47 ([28.46667213])\n",
      "04:28    Epoch 14: train loss 28.46 ([28.46412108])\n",
      "04:29    Epoch 16: train loss 28.46 ([28.46337503])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:30    Epoch 18: train loss 28.46 ([28.46122071])\n",
      "04:30    Epoch 20: train loss 28.52 ([28.52363099])\n",
      "04:30  Finished training\n",
      "04:30  Training estimator 12 / 20 in ensemble\n",
      "04:30  Starting training\n",
      "04:30    Method:                 sally\n",
      "04:30    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_11.npy\n",
      "04:30                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_11.npy\n",
      "04:30    Features:               [0]\n",
      "04:30    Method:                 sally\n",
      "04:30    Hidden layers:          (100, 100)\n",
      "04:30    Activation function:    tanh\n",
      "04:30    Batch size:             128\n",
      "04:30    Epochs:                 20\n",
      "04:30    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "04:30    Validation split:       None\n",
      "04:30    Early stopping:         True\n",
      "04:30  Loading training data\n",
      "04:30  Found 1000000 samples with 2 parameters and 30 observables\n",
      "04:30  Only using 1 of 30 observables\n",
      "04:30  Creating model for method sally\n",
      "04:30  Training model\n",
      "04:31    Epoch 2: train loss 26.43 ([26.42749367])\n",
      "04:32    Epoch 4: train loss 26.42 ([26.41667193])\n",
      "04:32    Epoch 6: train loss 26.41 ([26.4135965])\n",
      "04:33    Epoch 8: train loss 26.41 ([26.40575314])\n",
      "04:34    Epoch 10: train loss 26.40 ([26.40260698])\n",
      "04:34    Epoch 12: train loss 26.40 ([26.40013487])\n",
      "04:35    Epoch 14: train loss 26.40 ([26.39777735])\n",
      "04:36    Epoch 16: train loss 26.40 ([26.39644438])\n",
      "04:37    Epoch 18: train loss 26.40 ([26.39506959])\n",
      "04:37    Epoch 20: train loss 26.39 ([26.39428286])\n",
      "04:37  Finished training\n",
      "04:37  Training estimator 13 / 20 in ensemble\n",
      "04:37  Starting training\n",
      "04:37    Method:                 sally\n",
      "04:37    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_12.npy\n",
      "04:37                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_12.npy\n",
      "04:37    Features:               [0]\n",
      "04:37    Method:                 sally\n",
      "04:37    Hidden layers:          (100, 100)\n",
      "04:37    Activation function:    tanh\n",
      "04:37    Batch size:             128\n",
      "04:37    Epochs:                 20\n",
      "04:37    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "04:37    Validation split:       None\n",
      "04:37    Early stopping:         True\n",
      "04:37  Loading training data\n",
      "04:37  Found 1000000 samples with 2 parameters and 30 observables\n",
      "04:37  Only using 1 of 30 observables\n",
      "04:37  Creating model for method sally\n",
      "04:37  Training model\n",
      "04:38    Epoch 2: train loss 36.75 ([36.7505547])\n",
      "04:39    Epoch 4: train loss 36.74 ([36.74146997])\n",
      "04:39    Epoch 6: train loss 36.73 ([36.73049566])\n",
      "04:40    Epoch 8: train loss 36.73 ([36.72669475])\n",
      "04:41    Epoch 10: train loss 36.72 ([36.72284449])\n",
      "04:41    Epoch 12: train loss 36.72 ([36.71944018])\n",
      "04:42    Epoch 14: train loss 36.72 ([36.71674424])\n",
      "04:43    Epoch 16: train loss 36.71 ([36.71494896])\n",
      "04:43    Epoch 18: train loss 36.71 ([36.71450373])\n",
      "04:44    Epoch 20: train loss 36.71 ([36.71276875])\n",
      "04:44  Finished training\n",
      "04:44  Training estimator 14 / 20 in ensemble\n",
      "04:44  Starting training\n",
      "04:44    Method:                 sally\n",
      "04:44    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_13.npy\n",
      "04:44                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_13.npy\n",
      "04:44    Features:               [0]\n",
      "04:44    Method:                 sally\n",
      "04:44    Hidden layers:          (100, 100)\n",
      "04:44    Activation function:    tanh\n",
      "04:44    Batch size:             128\n",
      "04:44    Epochs:                 20\n",
      "04:44    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "04:44    Validation split:       None\n",
      "04:44    Early stopping:         True\n",
      "04:44  Loading training data\n",
      "04:44  Found 1000000 samples with 2 parameters and 30 observables\n",
      "04:44  Only using 1 of 30 observables\n",
      "04:44  Creating model for method sally\n",
      "04:44  Training model\n",
      "04:45    Epoch 2: train loss 25.34 ([25.34404755])\n",
      "04:46    Epoch 4: train loss 25.33 ([25.33414298])\n",
      "04:46    Epoch 6: train loss 25.33 ([25.32567699])\n",
      "04:47    Epoch 8: train loss 25.32 ([25.32170473])\n",
      "04:48    Epoch 10: train loss 25.32 ([25.31831262])\n",
      "04:48    Epoch 12: train loss 25.32 ([25.31511799])\n",
      "04:49    Epoch 14: train loss 25.31 ([25.3131877])\n",
      "04:50    Epoch 16: train loss 25.31 ([25.31230483])\n",
      "04:50    Epoch 18: train loss 25.31 ([25.31116276])\n",
      "04:51    Epoch 20: train loss 25.31 ([25.3100806])\n",
      "04:51  Finished training\n",
      "04:51  Training estimator 15 / 20 in ensemble\n",
      "04:51  Starting training\n",
      "04:51    Method:                 sally\n",
      "04:51    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_14.npy\n",
      "04:51                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_14.npy\n",
      "04:51    Features:               [0]\n",
      "04:51    Method:                 sally\n",
      "04:51    Hidden layers:          (100, 100)\n",
      "04:51    Activation function:    tanh\n",
      "04:51    Batch size:             128\n",
      "04:51    Epochs:                 20\n",
      "04:51    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "04:51    Validation split:       None\n",
      "04:51    Early stopping:         True\n",
      "04:51  Loading training data\n",
      "04:51  Found 1000000 samples with 2 parameters and 30 observables\n",
      "04:51  Only using 1 of 30 observables\n",
      "04:51  Creating model for method sally\n",
      "04:51  Training model\n",
      "04:52    Epoch 2: train loss 65.20 ([65.20154662])\n",
      "04:52    Epoch 4: train loss 65.19 ([65.1909872])\n",
      "04:53    Epoch 6: train loss 65.18 ([65.1822514])\n",
      "04:54    Epoch 8: train loss 65.18 ([65.17638462])\n",
      "04:54    Epoch 10: train loss 65.17 ([65.17149972])\n",
      "04:55    Epoch 12: train loss 65.17 ([65.16929737])\n",
      "04:56    Epoch 14: train loss 65.17 ([65.16688051])\n",
      "04:56    Epoch 16: train loss 65.17 ([65.16532186])\n",
      "04:57    Epoch 18: train loss 65.16 ([65.16323427])\n",
      "04:57    Epoch 20: train loss 65.16 ([65.16219767])\n",
      "04:57  Finished training\n",
      "04:57  Training estimator 16 / 20 in ensemble\n",
      "04:57  Starting training\n",
      "04:57    Method:                 sally\n",
      "04:57    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_15.npy\n",
      "04:57                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_15.npy\n",
      "04:57    Features:               [0]\n",
      "04:57    Method:                 sally\n",
      "04:57    Hidden layers:          (100, 100)\n",
      "04:57    Activation function:    tanh\n",
      "04:57    Batch size:             128\n",
      "04:57    Epochs:                 20\n",
      "04:57    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "04:57    Validation split:       None\n",
      "04:57    Early stopping:         True\n",
      "04:57  Loading training data\n",
      "04:57  Found 1000000 samples with 2 parameters and 30 observables\n",
      "04:57  Only using 1 of 30 observables\n",
      "04:57  Creating model for method sally\n",
      "04:57  Training model\n",
      "04:58    Epoch 2: train loss 21.33 ([21.33190893])\n",
      "04:59    Epoch 4: train loss 21.32 ([21.32165525])\n",
      "04:59    Epoch 6: train loss 21.32 ([21.31632324])\n",
      "05:00    Epoch 8: train loss 21.31 ([21.31227204])\n",
      "05:01    Epoch 10: train loss 21.31 ([21.30971845])\n",
      "05:01    Epoch 12: train loss 21.31 ([21.30671416])\n",
      "05:02    Epoch 14: train loss 21.30 ([21.30494124])\n",
      "05:03    Epoch 16: train loss 21.30 ([21.30320021])\n",
      "05:03    Epoch 18: train loss 21.30 ([21.30267634])\n",
      "05:04    Epoch 20: train loss 21.30 ([21.29914308])\n",
      "05:04  Finished training\n",
      "05:04  Training estimator 17 / 20 in ensemble\n",
      "05:04  Starting training\n",
      "05:04    Method:                 sally\n",
      "05:04    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_16.npy\n",
      "05:04                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_16.npy\n",
      "05:04    Features:               [0]\n",
      "05:04    Method:                 sally\n",
      "05:04    Hidden layers:          (100, 100)\n",
      "05:04    Activation function:    tanh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:04    Batch size:             128\n",
      "05:04    Epochs:                 20\n",
      "05:04    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "05:04    Validation split:       None\n",
      "05:04    Early stopping:         True\n",
      "05:04  Loading training data\n",
      "05:04  Found 1000000 samples with 2 parameters and 30 observables\n",
      "05:04  Only using 1 of 30 observables\n",
      "05:04  Creating model for method sally\n",
      "05:04  Training model\n",
      "05:05    Epoch 2: train loss 20.86 ([20.86238446])\n",
      "05:05    Epoch 4: train loss 20.86 ([20.86030294])\n",
      "05:06    Epoch 6: train loss 20.85 ([20.85164231])\n",
      "05:07    Epoch 8: train loss 20.85 ([20.84855587])\n",
      "05:07    Epoch 10: train loss 20.84 ([20.84386759])\n",
      "05:08    Epoch 12: train loss 20.84 ([20.84115645])\n",
      "05:08    Epoch 14: train loss 20.84 ([20.84137902])\n",
      "05:09    Epoch 16: train loss 20.84 ([20.83879972])\n",
      "05:10    Epoch 18: train loss 20.84 ([20.8376898])\n",
      "05:11    Epoch 20: train loss 20.84 ([20.83776957])\n",
      "05:11  Finished training\n",
      "05:11  Training estimator 18 / 20 in ensemble\n",
      "05:11  Starting training\n",
      "05:11    Method:                 sally\n",
      "05:11    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_17.npy\n",
      "05:11                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_17.npy\n",
      "05:11    Features:               [0]\n",
      "05:11    Method:                 sally\n",
      "05:11    Hidden layers:          (100, 100)\n",
      "05:11    Activation function:    tanh\n",
      "05:11    Batch size:             128\n",
      "05:11    Epochs:                 20\n",
      "05:11    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "05:11    Validation split:       None\n",
      "05:11    Early stopping:         True\n",
      "05:11  Loading training data\n",
      "05:11  Found 1000000 samples with 2 parameters and 30 observables\n",
      "05:11  Only using 1 of 30 observables\n",
      "05:11  Creating model for method sally\n",
      "05:11  Training model\n",
      "05:11    Epoch 2: train loss 39.50 ([39.49775969])\n",
      "05:12    Epoch 4: train loss 39.49 ([39.48748924])\n",
      "05:13    Epoch 6: train loss 39.49 ([39.48805552])\n",
      "05:13    Epoch 8: train loss 39.47 ([39.47466694])\n",
      "05:14    Epoch 10: train loss 39.47 ([39.47214064])\n",
      "05:15    Epoch 12: train loss 39.47 ([39.47316945])\n",
      "05:15    Epoch 14: train loss 39.47 ([39.46925684])\n",
      "05:16    Epoch 16: train loss 39.46 ([39.46372806])\n",
      "05:17    Epoch 18: train loss 39.46 ([39.46243685])\n",
      "05:17    Epoch 20: train loss 39.46 ([39.46188798])\n",
      "05:17  Finished training\n",
      "05:17  Training estimator 19 / 20 in ensemble\n",
      "05:17  Starting training\n",
      "05:17    Method:                 sally\n",
      "05:17    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_18.npy\n",
      "05:17                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_18.npy\n",
      "05:17    Features:               [0]\n",
      "05:17    Method:                 sally\n",
      "05:17    Hidden layers:          (100, 100)\n",
      "05:17    Activation function:    tanh\n",
      "05:17    Batch size:             128\n",
      "05:17    Epochs:                 20\n",
      "05:17    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "05:17    Validation split:       None\n",
      "05:17    Early stopping:         True\n",
      "05:17  Loading training data\n",
      "05:17  Found 1000000 samples with 2 parameters and 30 observables\n",
      "05:17  Only using 1 of 30 observables\n",
      "05:17  Creating model for method sally\n",
      "05:17  Training model\n",
      "05:18    Epoch 2: train loss 26.19 ([26.18818995])\n",
      "05:19    Epoch 4: train loss 26.18 ([26.17882639])\n",
      "05:19    Epoch 6: train loss 26.17 ([26.17143672])\n",
      "05:20    Epoch 8: train loss 26.17 ([26.1691546])\n",
      "05:21    Epoch 10: train loss 26.16 ([26.16335466])\n",
      "05:21    Epoch 12: train loss 26.16 ([26.15984196])\n",
      "05:22    Epoch 14: train loss 26.16 ([26.15927696])\n",
      "05:23    Epoch 16: train loss 26.16 ([26.15669375])\n",
      "05:23    Epoch 18: train loss 26.16 ([26.15572049])\n",
      "05:24    Epoch 20: train loss 26.15 ([26.15488883])\n",
      "05:24  Finished training\n",
      "05:24  Training estimator 20 / 20 in ensemble\n",
      "05:24  Starting training\n",
      "05:24    Method:                 sally\n",
      "05:24    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_19.npy\n",
      "05:24                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_19.npy\n",
      "05:24    Features:               [0]\n",
      "05:24    Method:                 sally\n",
      "05:24    Hidden layers:          (100, 100)\n",
      "05:24    Activation function:    tanh\n",
      "05:24    Batch size:             128\n",
      "05:24    Epochs:                 20\n",
      "05:24    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "05:24    Validation split:       None\n",
      "05:24    Early stopping:         True\n",
      "05:24  Loading training data\n",
      "05:24  Found 1000000 samples with 2 parameters and 30 observables\n",
      "05:24  Only using 1 of 30 observables\n",
      "05:24  Creating model for method sally\n",
      "05:24  Training model\n",
      "05:25    Epoch 2: train loss 38.77 ([38.77284813])\n",
      "05:25    Epoch 4: train loss 38.76 ([38.76449007])\n",
      "05:26    Epoch 6: train loss 38.76 ([38.75818762])\n",
      "05:27    Epoch 8: train loss 38.75 ([38.75073723])\n",
      "05:27    Epoch 10: train loss 38.75 ([38.74643282])\n",
      "05:28    Epoch 12: train loss 38.74 ([38.74403232])\n",
      "05:29    Epoch 14: train loss 38.74 ([38.74159673])\n",
      "05:29    Epoch 16: train loss 38.75 ([38.7458195])\n",
      "05:30    Epoch 18: train loss 38.74 ([38.7393127])\n",
      "05:31    Epoch 20: train loss 38.74 ([38.7385914])\n",
      "05:31  Finished training\n"
     ]
    }
   ],
   "source": [
    "ensemble_met = EnsembleForge(n_estimators)\n",
    "\n",
    "ensemble_met.train_all(\n",
    "    features=[ [0] for _ in range(n_estimators)],\n",
    "    method='sally',\n",
    "    x_filename=[sample_dir + 'train_local/x_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    t_xz0_filename=[sample_dir + 'train_local/t_xz_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=None,\n",
    "    n_hidden=n_hidden\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "4dd7a14b-7f6f-40ad-a23f-5180dd46c98d"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:31  Calculating expectation for 20 estimators in ensemble\n",
      "05:31  Starting evaluation for estimator 1 / 20 in ensemble\n",
      "05:31  Starting evaluation for estimator 2 / 20 in ensemble\n",
      "05:31  Starting evaluation for estimator 3 / 20 in ensemble\n",
      "05:31  Starting evaluation for estimator 4 / 20 in ensemble\n",
      "05:31  Starting evaluation for estimator 5 / 20 in ensemble\n",
      "05:31  Starting evaluation for estimator 6 / 20 in ensemble\n",
      "05:31  Starting evaluation for estimator 7 / 20 in ensemble\n",
      "05:31  Starting evaluation for estimator 8 / 20 in ensemble\n",
      "05:32  Starting evaluation for estimator 9 / 20 in ensemble\n",
      "05:32  Starting evaluation for estimator 10 / 20 in ensemble\n",
      "05:32  Starting evaluation for estimator 11 / 20 in ensemble\n",
      "05:32  Starting evaluation for estimator 12 / 20 in ensemble\n",
      "05:32  Starting evaluation for estimator 13 / 20 in ensemble\n",
      "05:32  Starting evaluation for estimator 14 / 20 in ensemble\n",
      "05:32  Starting evaluation for estimator 15 / 20 in ensemble\n",
      "05:33  Starting evaluation for estimator 16 / 20 in ensemble\n",
      "05:33  Starting evaluation for estimator 17 / 20 in ensemble\n",
      "05:33  Starting evaluation for estimator 18 / 20 in ensemble\n",
      "05:33  Starting evaluation for estimator 19 / 20 in ensemble\n",
      "05:33  Starting evaluation for estimator 20 / 20 in ensemble\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.02272519, -0.01953053],\n",
       "       [ 0.02308884,  0.00164204],\n",
       "       [ 0.00784216,  0.00719034],\n",
       "       [-0.01566505,  0.00717336],\n",
       "       [-0.06711584, -0.04679087],\n",
       "       [ 0.06617928,  0.00036818],\n",
       "       [-0.03279284, -0.00281756],\n",
       "       [-0.03282881,  0.00791341],\n",
       "       [ 0.00587244, -0.00919737],\n",
       "       [ 0.03765859,  0.01425881],\n",
       "       [-0.0481303 , -0.00811759],\n",
       "       [-0.00631865, -0.02273407],\n",
       "       [-0.03515056, -0.00830877],\n",
       "       [ 0.00461069,  0.04432376],\n",
       "       [-0.02545383, -0.04821105],\n",
       "       [ 0.0126791 , -0.0854085 ],\n",
       "       [-0.01543825, -0.0395059 ],\n",
       "       [-0.05105131, -0.00641482],\n",
       "       [ 0.044085  ,  0.07499459],\n",
       "       [-0.09711312, -0.0586315 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_met.calculate_expectation(\n",
    "    x_filename=sample_dir + 'validation/x_validation.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbpresent": {
     "id": "5c37b210-f29b-45f3-9988-b82ebba909a9"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ensemble_met.save(model_dir + 'sally_ensemble_met')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "84cb788d-da26-4e03-8bc1-cddc335d12ed"
    }
   },
   "source": [
    "## 1d toy study (dummy observable: phi(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbpresent": {
     "id": "9056bfd0-0c5a-4810-b89e-fa6976c5efa5"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:33  Training 20 estimators in ensemble\n",
      "05:33  Training estimator 1 / 20 in ensemble\n",
      "05:33  Starting training\n",
      "05:33    Method:                 sally\n",
      "05:33    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "05:33                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "05:33    Features:               [1]\n",
      "05:33    Method:                 sally\n",
      "05:33    Hidden layers:          (100, 100)\n",
      "05:33    Activation function:    tanh\n",
      "05:33    Batch size:             128\n",
      "05:33    Epochs:                 20\n",
      "05:33    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "05:33    Validation split:       None\n",
      "05:33    Early stopping:         True\n",
      "05:33  Loading training data\n",
      "05:33  Found 1000000 samples with 2 parameters and 30 observables\n",
      "05:33  Only using 1 of 30 observables\n",
      "05:33  Creating model for method sally\n",
      "05:33  Training model\n",
      "05:34    Epoch 2: train loss 30.93 ([30.93325337])\n",
      "05:35    Epoch 4: train loss 30.93 ([30.93192797])\n",
      "05:35    Epoch 6: train loss 30.93 ([30.93148334])\n",
      "05:36    Epoch 8: train loss 30.93 ([30.93115198])\n",
      "05:36    Epoch 10: train loss 30.93 ([30.93113924])\n",
      "05:37    Epoch 12: train loss 30.93 ([30.93098031])\n",
      "05:37    Epoch 14: train loss 30.93 ([30.9309903])\n",
      "05:38    Epoch 16: train loss 30.93 ([30.93118247])\n",
      "05:39    Epoch 18: train loss 30.93 ([30.93083049])\n",
      "05:39    Epoch 20: train loss 30.93 ([30.93077618])\n",
      "05:39  Finished training\n",
      "05:39  Training estimator 2 / 20 in ensemble\n",
      "05:39  Starting training\n",
      "05:39    Method:                 sally\n",
      "05:39    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "05:39                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "05:39    Features:               [1]\n",
      "05:39    Method:                 sally\n",
      "05:39    Hidden layers:          (100, 100)\n",
      "05:39    Activation function:    tanh\n",
      "05:39    Batch size:             128\n",
      "05:39    Epochs:                 20\n",
      "05:39    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "05:39    Validation split:       None\n",
      "05:39    Early stopping:         True\n",
      "05:39  Loading training data\n",
      "05:39  Found 1000000 samples with 2 parameters and 30 observables\n",
      "05:39  Only using 1 of 30 observables\n",
      "05:39  Creating model for method sally\n",
      "05:39  Training model\n",
      "05:40    Epoch 2: train loss 34.08 ([34.07623117])\n",
      "05:41    Epoch 4: train loss 34.07 ([34.0748588])\n",
      "05:41    Epoch 6: train loss 34.08 ([34.07683346])\n",
      "05:42    Epoch 8: train loss 34.07 ([34.07444473])\n",
      "05:42    Epoch 10: train loss 34.07 ([34.07427658])\n",
      "05:43    Epoch 12: train loss 34.07 ([34.07397128])\n",
      "05:43    Epoch 14: train loss 34.07 ([34.07396397])\n",
      "05:44    Epoch 16: train loss 34.07 ([34.07430807])\n",
      "05:45    Epoch 18: train loss 34.07 ([34.07453934])\n",
      "05:45    Epoch 20: train loss 34.07 ([34.07382091])\n",
      "05:45  Finished training\n",
      "05:45  Training estimator 3 / 20 in ensemble\n",
      "05:45  Starting training\n",
      "05:45    Method:                 sally\n",
      "05:45    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "05:45                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "05:45    Features:               [1]\n",
      "05:45    Method:                 sally\n",
      "05:45    Hidden layers:          (100, 100)\n",
      "05:45    Activation function:    tanh\n",
      "05:45    Batch size:             128\n",
      "05:45    Epochs:                 20\n",
      "05:45    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "05:45    Validation split:       None\n",
      "05:45    Early stopping:         True\n",
      "05:45  Loading training data\n",
      "05:45  Found 1000000 samples with 2 parameters and 30 observables\n",
      "05:45  Only using 1 of 30 observables\n",
      "05:45  Creating model for method sally\n",
      "05:45  Training model\n",
      "05:46    Epoch 2: train loss 33.75 ([33.75062242])\n",
      "05:46    Epoch 4: train loss 33.75 ([33.7492146])\n",
      "05:47    Epoch 6: train loss 33.75 ([33.74630137])\n",
      "05:48    Epoch 8: train loss 33.75 ([33.74685123])\n",
      "05:48    Epoch 10: train loss 33.75 ([33.7456756])\n",
      "05:49    Epoch 12: train loss 33.75 ([33.7465466])\n",
      "05:49    Epoch 14: train loss 33.75 ([33.74579093])\n",
      "05:50    Epoch 16: train loss 33.75 ([33.74563497])\n",
      "05:50    Epoch 18: train loss 33.75 ([33.74567066])\n",
      "05:51    Epoch 20: train loss 33.75 ([33.74980266])\n",
      "05:51  Finished training\n",
      "05:51  Training estimator 4 / 20 in ensemble\n",
      "05:51  Starting training\n",
      "05:51    Method:                 sally\n",
      "05:51    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "05:51                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "05:51    Features:               [1]\n",
      "05:51    Method:                 sally\n",
      "05:51    Hidden layers:          (100, 100)\n",
      "05:51    Activation function:    tanh\n",
      "05:51    Batch size:             128\n",
      "05:51    Epochs:                 20\n",
      "05:51    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "05:51    Validation split:       None\n",
      "05:51    Early stopping:         True\n",
      "05:51  Loading training data\n",
      "05:51  Found 1000000 samples with 2 parameters and 30 observables\n",
      "05:51  Only using 1 of 30 observables\n",
      "05:51  Creating model for method sally\n",
      "05:51  Training model\n",
      "05:52    Epoch 2: train loss 30.60 ([30.59571606])\n",
      "05:52    Epoch 4: train loss 30.59 ([30.59329682])\n",
      "05:53    Epoch 6: train loss 30.59 ([30.59262768])\n",
      "05:54    Epoch 8: train loss 30.59 ([30.59258044])\n",
      "05:54    Epoch 10: train loss 30.59 ([30.59258225])\n",
      "05:55    Epoch 12: train loss 30.59 ([30.59268618])\n",
      "05:55    Epoch 14: train loss 30.59 ([30.5936499])\n",
      "05:56    Epoch 16: train loss 30.59 ([30.59228058])\n",
      "05:57    Epoch 18: train loss 30.59 ([30.59219708])\n",
      "05:57    Epoch 20: train loss 30.59 ([30.59223206])\n",
      "05:57  Finished training\n",
      "05:57  Training estimator 5 / 20 in ensemble\n",
      "05:57  Starting training\n",
      "05:57    Method:                 sally\n",
      "05:57    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "05:57                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "05:57    Features:               [1]\n",
      "05:57    Method:                 sally\n",
      "05:57    Hidden layers:          (100, 100)\n",
      "05:57    Activation function:    tanh\n",
      "05:57    Batch size:             128\n",
      "05:57    Epochs:                 20\n",
      "05:57    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "05:57    Validation split:       None\n",
      "05:57    Early stopping:         True\n",
      "05:57  Loading training data\n",
      "05:57  Found 1000000 samples with 2 parameters and 30 observables\n",
      "05:57  Only using 1 of 30 observables\n",
      "05:57  Creating model for method sally\n",
      "05:57  Training model\n",
      "05:58    Epoch 2: train loss 38.10 ([38.10129566])\n",
      "05:58    Epoch 4: train loss 38.10 ([38.09750179])\n",
      "05:59    Epoch 6: train loss 38.10 ([38.09693148])\n",
      "06:00    Epoch 8: train loss 38.10 ([38.09675385])\n",
      "06:00    Epoch 10: train loss 38.10 ([38.0969242])\n",
      "06:01    Epoch 12: train loss 38.10 ([38.09658119])\n",
      "06:01    Epoch 14: train loss 38.10 ([38.09654007])\n",
      "06:02    Epoch 16: train loss 38.10 ([38.10138838])\n",
      "06:02    Epoch 18: train loss 38.10 ([38.09663977])\n",
      "06:03    Epoch 20: train loss 38.11 ([38.11201823])\n",
      "06:03  Finished training\n",
      "06:03  Training estimator 6 / 20 in ensemble\n",
      "06:03  Starting training\n",
      "06:03    Method:                 sally\n",
      "06:03    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "06:03                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "06:03    Features:               [1]\n",
      "06:03    Method:                 sally\n",
      "06:03    Hidden layers:          (100, 100)\n",
      "06:03    Activation function:    tanh\n",
      "06:03    Batch size:             128\n",
      "06:03    Epochs:                 20\n",
      "06:03    Learning rate:          0.002 initially, decaying to 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:03    Validation split:       None\n",
      "06:03    Early stopping:         True\n",
      "06:03  Loading training data\n",
      "06:03  Found 1000000 samples with 2 parameters and 30 observables\n",
      "06:03  Only using 1 of 30 observables\n",
      "06:03  Creating model for method sally\n",
      "06:03  Training model\n",
      "06:04    Epoch 2: train loss 24.30 ([24.30225461])\n",
      "06:04    Epoch 4: train loss 24.30 ([24.30140223])\n",
      "06:05    Epoch 6: train loss 24.30 ([24.30086916])\n",
      "06:06    Epoch 8: train loss 24.30 ([24.30054559])\n",
      "06:06    Epoch 10: train loss 24.30 ([24.30057391])\n",
      "06:07    Epoch 12: train loss 24.30 ([24.30096213])\n",
      "06:07    Epoch 14: train loss 24.30 ([24.30062188])\n",
      "06:08    Epoch 16: train loss 24.30 ([24.30042757])\n",
      "06:08    Epoch 18: train loss 24.30 ([24.30051092])\n",
      "06:09    Epoch 20: train loss 24.30 ([24.30032086])\n",
      "06:09  Finished training\n",
      "06:09  Training estimator 7 / 20 in ensemble\n",
      "06:09  Starting training\n",
      "06:09    Method:                 sally\n",
      "06:09    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "06:09                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "06:09    Features:               [1]\n",
      "06:09    Method:                 sally\n",
      "06:09    Hidden layers:          (100, 100)\n",
      "06:09    Activation function:    tanh\n",
      "06:09    Batch size:             128\n",
      "06:09    Epochs:                 20\n",
      "06:09    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "06:09    Validation split:       None\n",
      "06:09    Early stopping:         True\n",
      "06:09  Loading training data\n",
      "06:09  Found 1000000 samples with 2 parameters and 30 observables\n",
      "06:09  Only using 1 of 30 observables\n",
      "06:09  Creating model for method sally\n",
      "06:09  Training model\n",
      "06:10    Epoch 2: train loss 19.88 ([19.87677962])\n",
      "06:10    Epoch 4: train loss 19.88 ([19.87603937])\n",
      "06:11    Epoch 6: train loss 19.88 ([19.87537021])\n",
      "06:12    Epoch 8: train loss 19.88 ([19.87589896])\n",
      "06:12    Epoch 10: train loss 19.88 ([19.87514781])\n",
      "06:13    Epoch 12: train loss 19.88 ([19.87514241])\n",
      "06:13    Epoch 14: train loss 19.88 ([19.8753703])\n",
      "06:14    Epoch 16: train loss 19.88 ([19.87517557])\n",
      "06:14    Epoch 18: train loss 19.87 ([19.87495308])\n",
      "06:15    Epoch 20: train loss 19.88 ([19.87527576])\n",
      "06:15  Finished training\n",
      "06:15  Training estimator 8 / 20 in ensemble\n",
      "06:15  Starting training\n",
      "06:15    Method:                 sally\n",
      "06:15    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "06:15                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "06:15    Features:               [1]\n",
      "06:15    Method:                 sally\n",
      "06:15    Hidden layers:          (100, 100)\n",
      "06:15    Activation function:    tanh\n",
      "06:15    Batch size:             128\n",
      "06:15    Epochs:                 20\n",
      "06:15    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "06:15    Validation split:       None\n",
      "06:15    Early stopping:         True\n",
      "06:15  Loading training data\n",
      "06:15  Found 1000000 samples with 2 parameters and 30 observables\n",
      "06:15  Only using 1 of 30 observables\n",
      "06:15  Creating model for method sally\n",
      "06:15  Training model\n",
      "06:16    Epoch 2: train loss 26.38 ([26.38343444])\n",
      "06:16    Epoch 4: train loss 26.38 ([26.38252463])\n",
      "06:17    Epoch 6: train loss 26.38 ([26.38273235])\n",
      "06:17    Epoch 8: train loss 26.38 ([26.38209671])\n",
      "06:18    Epoch 10: train loss 26.38 ([26.38187512])\n",
      "06:19    Epoch 12: train loss 26.38 ([26.3819691])\n",
      "06:19    Epoch 14: train loss 26.38 ([26.38178426])\n",
      "06:20    Epoch 16: train loss 26.38 ([26.38183717])\n",
      "06:20    Epoch 18: train loss 26.38 ([26.38250699])\n",
      "06:21    Epoch 20: train loss 26.38 ([26.38179001])\n",
      "06:21  Finished training\n",
      "06:21  Training estimator 9 / 20 in ensemble\n",
      "06:21  Starting training\n",
      "06:21    Method:                 sally\n",
      "06:21    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "06:21                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "06:21    Features:               [1]\n",
      "06:21    Method:                 sally\n",
      "06:21    Hidden layers:          (100, 100)\n",
      "06:21    Activation function:    tanh\n",
      "06:21    Batch size:             128\n",
      "06:21    Epochs:                 20\n",
      "06:21    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "06:21    Validation split:       None\n",
      "06:21    Early stopping:         True\n",
      "06:21  Loading training data\n",
      "06:21  Found 1000000 samples with 2 parameters and 30 observables\n",
      "06:21  Only using 1 of 30 observables\n",
      "06:21  Creating model for method sally\n",
      "06:21  Training model\n",
      "06:22    Epoch 2: train loss 26.46 ([26.45909726])\n",
      "06:22    Epoch 4: train loss 26.46 ([26.46079863])\n",
      "06:23    Epoch 6: train loss 26.46 ([26.45790901])\n",
      "06:23    Epoch 8: train loss 26.46 ([26.4578954])\n",
      "06:24    Epoch 10: train loss 26.46 ([26.45813424])\n",
      "06:25    Epoch 12: train loss 26.46 ([26.45743587])\n",
      "06:25    Epoch 14: train loss 26.46 ([26.45733356])\n",
      "06:26    Epoch 16: train loss 26.46 ([26.45729491])\n",
      "06:26    Epoch 18: train loss 26.46 ([26.45725485])\n",
      "06:27    Epoch 20: train loss 26.46 ([26.45743646])\n",
      "06:27  Finished training\n",
      "06:27  Training estimator 10 / 20 in ensemble\n",
      "06:27  Starting training\n",
      "06:27    Method:                 sally\n",
      "06:27    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "06:27                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "06:27    Features:               [1]\n",
      "06:27    Method:                 sally\n",
      "06:27    Hidden layers:          (100, 100)\n",
      "06:27    Activation function:    tanh\n",
      "06:27    Batch size:             128\n",
      "06:27    Epochs:                 20\n",
      "06:27    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "06:27    Validation split:       None\n",
      "06:27    Early stopping:         True\n",
      "06:27  Loading training data\n",
      "06:27  Found 1000000 samples with 2 parameters and 30 observables\n",
      "06:27  Only using 1 of 30 observables\n",
      "06:27  Creating model for method sally\n",
      "06:27  Training model\n",
      "06:28    Epoch 2: train loss 31.19 ([31.18966631])\n",
      "06:28    Epoch 4: train loss 31.19 ([31.18804193])\n",
      "06:29    Epoch 6: train loss 31.19 ([31.18801712])\n",
      "06:29    Epoch 8: train loss 31.19 ([31.18843218])\n",
      "06:30    Epoch 10: train loss 31.19 ([31.18760472])\n",
      "06:31    Epoch 12: train loss 31.19 ([31.18760896])\n",
      "06:31    Epoch 14: train loss 31.19 ([31.18756078])\n",
      "06:32    Epoch 16: train loss 31.19 ([31.18764417])\n",
      "06:32    Epoch 18: train loss 31.19 ([31.18737616])\n",
      "06:33    Epoch 20: train loss 31.19 ([31.18733385])\n",
      "06:33  Finished training\n",
      "06:33  Training estimator 11 / 20 in ensemble\n",
      "06:33  Starting training\n",
      "06:33    Method:                 sally\n",
      "06:33    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_10.npy\n",
      "06:33                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_10.npy\n",
      "06:33    Features:               [1]\n",
      "06:33    Method:                 sally\n",
      "06:33    Hidden layers:          (100, 100)\n",
      "06:33    Activation function:    tanh\n",
      "06:33    Batch size:             128\n",
      "06:33    Epochs:                 20\n",
      "06:33    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "06:33    Validation split:       None\n",
      "06:33    Early stopping:         True\n",
      "06:33  Loading training data\n",
      "06:33  Found 1000000 samples with 2 parameters and 30 observables\n",
      "06:33  Only using 1 of 30 observables\n",
      "06:33  Creating model for method sally\n",
      "06:33  Training model\n",
      "06:34    Epoch 2: train loss 28.46 ([28.46173699])\n",
      "06:34    Epoch 4: train loss 28.46 ([28.45984507])\n",
      "06:35    Epoch 6: train loss 28.46 ([28.45889593])\n",
      "06:35    Epoch 8: train loss 28.46 ([28.4590042])\n",
      "06:36    Epoch 10: train loss 28.46 ([28.46145255])\n",
      "06:36    Epoch 12: train loss 28.46 ([28.45828644])\n",
      "06:37    Epoch 14: train loss 28.46 ([28.4583039])\n",
      "06:38    Epoch 16: train loss 28.46 ([28.45811461])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:38    Epoch 18: train loss 28.46 ([28.458448])\n",
      "06:39    Epoch 20: train loss 28.46 ([28.45820355])\n",
      "06:39  Finished training\n",
      "06:39  Training estimator 12 / 20 in ensemble\n",
      "06:39  Starting training\n",
      "06:39    Method:                 sally\n",
      "06:39    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_11.npy\n",
      "06:39                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_11.npy\n",
      "06:39    Features:               [1]\n",
      "06:39    Method:                 sally\n",
      "06:39    Hidden layers:          (100, 100)\n",
      "06:39    Activation function:    tanh\n",
      "06:39    Batch size:             128\n",
      "06:39    Epochs:                 20\n",
      "06:39    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "06:39    Validation split:       None\n",
      "06:39    Early stopping:         True\n",
      "06:39  Loading training data\n",
      "06:39  Found 1000000 samples with 2 parameters and 30 observables\n",
      "06:39  Only using 1 of 30 observables\n",
      "06:39  Creating model for method sally\n",
      "06:39  Training model\n",
      "06:40    Epoch 2: train loss 26.39 ([26.39455393])\n",
      "06:40    Epoch 4: train loss 26.39 ([26.3933198])\n",
      "06:41    Epoch 6: train loss 26.39 ([26.39313153])\n",
      "06:41    Epoch 8: train loss 26.39 ([26.39277808])\n",
      "06:42    Epoch 10: train loss 26.39 ([26.39293958])\n",
      "06:42    Epoch 12: train loss 26.39 ([26.39253031])\n",
      "06:43    Epoch 14: train loss 26.39 ([26.39254325])\n",
      "06:44    Epoch 16: train loss 26.39 ([26.39241245])\n",
      "06:44    Epoch 18: train loss 26.39 ([26.39236646])\n",
      "06:45    Epoch 20: train loss 26.39 ([26.39234089])\n",
      "06:45  Finished training\n",
      "06:45  Training estimator 13 / 20 in ensemble\n",
      "06:45  Starting training\n",
      "06:45    Method:                 sally\n",
      "06:45    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_12.npy\n",
      "06:45                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_12.npy\n",
      "06:45    Features:               [1]\n",
      "06:45    Method:                 sally\n",
      "06:45    Hidden layers:          (100, 100)\n",
      "06:45    Activation function:    tanh\n",
      "06:45    Batch size:             128\n",
      "06:45    Epochs:                 20\n",
      "06:45    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "06:45    Validation split:       None\n",
      "06:45    Early stopping:         True\n",
      "06:45  Loading training data\n",
      "06:45  Found 1000000 samples with 2 parameters and 30 observables\n",
      "06:45  Only using 1 of 30 observables\n",
      "06:45  Creating model for method sally\n",
      "06:45  Training model\n",
      "06:46    Epoch 2: train loss 36.71 ([36.71377939])\n",
      "06:46    Epoch 4: train loss 36.72 ([36.71542232])\n",
      "06:47    Epoch 6: train loss 36.71 ([36.71106895])\n",
      "06:47    Epoch 8: train loss 36.71 ([36.71072765])\n",
      "06:48    Epoch 10: train loss 36.71 ([36.71077192])\n",
      "06:48    Epoch 12: train loss 36.71 ([36.71223847])\n",
      "06:49    Epoch 14: train loss 36.71 ([36.71073947])\n",
      "06:50    Epoch 16: train loss 36.71 ([36.71058604])\n",
      "06:50    Epoch 18: train loss 36.71 ([36.71057669])\n",
      "06:51    Epoch 20: train loss 36.71 ([36.71087518])\n",
      "06:51  Finished training\n",
      "06:51  Training estimator 14 / 20 in ensemble\n",
      "06:51  Starting training\n",
      "06:51    Method:                 sally\n",
      "06:51    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_13.npy\n",
      "06:51                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_13.npy\n",
      "06:51    Features:               [1]\n",
      "06:51    Method:                 sally\n",
      "06:51    Hidden layers:          (100, 100)\n",
      "06:51    Activation function:    tanh\n",
      "06:51    Batch size:             128\n",
      "06:51    Epochs:                 20\n",
      "06:51    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "06:51    Validation split:       None\n",
      "06:51    Early stopping:         True\n",
      "06:51  Loading training data\n",
      "06:51  Found 1000000 samples with 2 parameters and 30 observables\n",
      "06:51  Only using 1 of 30 observables\n",
      "06:51  Creating model for method sally\n",
      "06:51  Training model\n",
      "06:51    Epoch 2: train loss 25.31 ([25.31106847])\n",
      "06:52    Epoch 4: train loss 25.31 ([25.30886606])\n",
      "06:53    Epoch 6: train loss 25.31 ([25.30872635])\n",
      "06:53    Epoch 8: train loss 25.31 ([25.30857358])\n",
      "06:54    Epoch 10: train loss 25.31 ([25.30892701])\n",
      "06:54    Epoch 12: train loss 25.31 ([25.30825488])\n",
      "06:55    Epoch 14: train loss 25.31 ([25.3082583])\n",
      "06:55    Epoch 16: train loss 25.31 ([25.30819112])\n",
      "06:56    Epoch 18: train loss 25.31 ([25.30831447])\n",
      "06:57    Epoch 20: train loss 25.31 ([25.30813421])\n",
      "06:57  Finished training\n",
      "06:57  Training estimator 15 / 20 in ensemble\n",
      "06:57  Starting training\n",
      "06:57    Method:                 sally\n",
      "06:57    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_14.npy\n",
      "06:57                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_14.npy\n",
      "06:57    Features:               [1]\n",
      "06:57    Method:                 sally\n",
      "06:57    Hidden layers:          (100, 100)\n",
      "06:57    Activation function:    tanh\n",
      "06:57    Batch size:             128\n",
      "06:57    Epochs:                 20\n",
      "06:57    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "06:57    Validation split:       None\n",
      "06:57    Early stopping:         True\n",
      "06:57  Loading training data\n",
      "06:57  Found 1000000 samples with 2 parameters and 30 observables\n",
      "06:57  Only using 1 of 30 observables\n",
      "06:57  Creating model for method sally\n",
      "06:57  Training model\n",
      "06:57    Epoch 2: train loss 65.16 ([65.16380404])\n",
      "06:58    Epoch 4: train loss 65.16 ([65.16105027])\n",
      "06:59    Epoch 6: train loss 65.16 ([65.16086711])\n",
      "06:59    Epoch 8: train loss 65.16 ([65.16051806])\n",
      "07:00    Epoch 10: train loss 65.16 ([65.16034974])\n",
      "07:00    Epoch 12: train loss 65.16 ([65.16140147])\n",
      "07:01    Epoch 14: train loss 65.16 ([65.16018527])\n",
      "07:02    Epoch 16: train loss 65.16 ([65.16012675])\n",
      "07:02    Epoch 18: train loss 65.16 ([65.16036168])\n",
      "07:03    Epoch 20: train loss 65.16 ([65.16018761])\n",
      "07:03  Finished training\n",
      "07:03  Training estimator 16 / 20 in ensemble\n",
      "07:03  Starting training\n",
      "07:03    Method:                 sally\n",
      "07:03    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_15.npy\n",
      "07:03                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_15.npy\n",
      "07:03    Features:               [1]\n",
      "07:03    Method:                 sally\n",
      "07:03    Hidden layers:          (100, 100)\n",
      "07:03    Activation function:    tanh\n",
      "07:03    Batch size:             128\n",
      "07:03    Epochs:                 20\n",
      "07:03    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "07:03    Validation split:       None\n",
      "07:03    Early stopping:         True\n",
      "07:03  Loading training data\n",
      "07:03  Found 1000000 samples with 2 parameters and 30 observables\n",
      "07:03  Only using 1 of 30 observables\n",
      "07:03  Creating model for method sally\n",
      "07:03  Training model\n",
      "07:03    Epoch 2: train loss 21.30 ([21.30145465])\n",
      "07:04    Epoch 4: train loss 21.30 ([21.3019772])\n",
      "07:05    Epoch 6: train loss 21.30 ([21.30007976])\n",
      "07:05    Epoch 8: train loss 21.30 ([21.30048475])\n",
      "07:06    Epoch 10: train loss 21.30 ([21.30163564])\n",
      "07:06    Epoch 12: train loss 21.30 ([21.2998969])\n",
      "07:07    Epoch 14: train loss 21.30 ([21.30261284])\n",
      "07:07    Epoch 16: train loss 21.30 ([21.29979505])\n",
      "07:08    Epoch 18: train loss 21.30 ([21.29977177])\n",
      "07:09    Epoch 20: train loss 21.30 ([21.2998479])\n",
      "07:09  Finished training\n",
      "07:09  Training estimator 17 / 20 in ensemble\n",
      "07:09  Starting training\n",
      "07:09    Method:                 sally\n",
      "07:09    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_16.npy\n",
      "07:09                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_16.npy\n",
      "07:09    Features:               [1]\n",
      "07:09    Method:                 sally\n",
      "07:09    Hidden layers:          (100, 100)\n",
      "07:09    Activation function:    tanh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:09    Batch size:             128\n",
      "07:09    Epochs:                 20\n",
      "07:09    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "07:09    Validation split:       None\n",
      "07:09    Early stopping:         True\n",
      "07:09  Loading training data\n",
      "07:09  Found 1000000 samples with 2 parameters and 30 observables\n",
      "07:09  Only using 1 of 30 observables\n",
      "07:09  Creating model for method sally\n",
      "07:09  Training model\n",
      "07:09    Epoch 2: train loss 20.84 ([20.83650621])\n",
      "07:10    Epoch 4: train loss 20.84 ([20.83588832])\n",
      "07:11    Epoch 6: train loss 20.84 ([20.83547923])\n",
      "07:11    Epoch 8: train loss 20.84 ([20.8353616])\n",
      "07:12    Epoch 10: train loss 20.84 ([20.835339])\n",
      "07:12    Epoch 12: train loss 20.84 ([20.835413])\n",
      "07:13    Epoch 14: train loss 20.84 ([20.83536101])\n",
      "07:13    Epoch 16: train loss 20.84 ([20.83514525])\n",
      "07:14    Epoch 18: train loss 20.84 ([20.83520989])\n",
      "07:15    Epoch 20: train loss 20.84 ([20.83528076])\n",
      "07:15  Finished training\n",
      "07:15  Training estimator 18 / 20 in ensemble\n",
      "07:15  Starting training\n",
      "07:15    Method:                 sally\n",
      "07:15    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_17.npy\n",
      "07:15                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_17.npy\n",
      "07:15    Features:               [1]\n",
      "07:15    Method:                 sally\n",
      "07:15    Hidden layers:          (100, 100)\n",
      "07:15    Activation function:    tanh\n",
      "07:15    Batch size:             128\n",
      "07:15    Epochs:                 20\n",
      "07:15    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "07:15    Validation split:       None\n",
      "07:15    Early stopping:         True\n",
      "07:15  Loading training data\n",
      "07:15  Found 1000000 samples with 2 parameters and 30 observables\n",
      "07:15  Only using 1 of 30 observables\n",
      "07:15  Creating model for method sally\n",
      "07:15  Training model\n",
      "07:15    Epoch 2: train loss 39.46 ([39.46212031])\n",
      "07:16    Epoch 4: train loss 39.46 ([39.46102219])\n",
      "07:17    Epoch 6: train loss 39.46 ([39.46007965])\n",
      "07:17    Epoch 8: train loss 39.46 ([39.45994043])\n",
      "07:18    Epoch 10: train loss 39.46 ([39.45952909])\n",
      "07:18    Epoch 12: train loss 39.46 ([39.45942348])\n",
      "07:19    Epoch 14: train loss 39.46 ([39.45947079])\n",
      "07:19    Epoch 16: train loss 39.46 ([39.46066619])\n",
      "07:20    Epoch 18: train loss 39.46 ([39.45929233])\n",
      "07:21    Epoch 20: train loss 39.46 ([39.45927136])\n",
      "07:21  Finished training\n",
      "07:21  Training estimator 19 / 20 in ensemble\n",
      "07:21  Starting training\n",
      "07:21    Method:                 sally\n",
      "07:21    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_18.npy\n",
      "07:21                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_18.npy\n",
      "07:21    Features:               [1]\n",
      "07:21    Method:                 sally\n",
      "07:21    Hidden layers:          (100, 100)\n",
      "07:21    Activation function:    tanh\n",
      "07:21    Batch size:             128\n",
      "07:21    Epochs:                 20\n",
      "07:21    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "07:21    Validation split:       None\n",
      "07:21    Early stopping:         True\n",
      "07:21  Loading training data\n",
      "07:21  Found 1000000 samples with 2 parameters and 30 observables\n",
      "07:21  Only using 1 of 30 observables\n",
      "07:21  Creating model for method sally\n",
      "07:21  Training model\n",
      "07:21    Epoch 2: train loss 26.15 ([26.15457276])\n",
      "07:22    Epoch 4: train loss 26.15 ([26.15360682])\n",
      "07:22    Epoch 6: train loss 26.15 ([26.15437534])\n",
      "07:23    Epoch 8: train loss 26.15 ([26.15299976])\n",
      "07:24    Epoch 10: train loss 26.15 ([26.15498355])\n",
      "07:24    Epoch 12: train loss 26.15 ([26.15287575])\n",
      "07:25    Epoch 14: train loss 26.15 ([26.15281424])\n",
      "07:25    Epoch 16: train loss 26.15 ([26.1533325])\n",
      "07:26    Epoch 18: train loss 26.15 ([26.15326154])\n",
      "07:26    Epoch 20: train loss 26.15 ([26.15265086])\n",
      "07:26  Finished training\n",
      "07:26  Training estimator 20 / 20 in ensemble\n",
      "07:26  Starting training\n",
      "07:26    Method:                 sally\n",
      "07:26    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_19.npy\n",
      "07:26                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_19.npy\n",
      "07:26    Features:               [1]\n",
      "07:26    Method:                 sally\n",
      "07:26    Hidden layers:          (100, 100)\n",
      "07:26    Activation function:    tanh\n",
      "07:26    Batch size:             128\n",
      "07:26    Epochs:                 20\n",
      "07:26    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "07:26    Validation split:       None\n",
      "07:26    Early stopping:         True\n",
      "07:26  Loading training data\n",
      "07:26  Found 1000000 samples with 2 parameters and 30 observables\n",
      "07:26  Only using 1 of 30 observables\n",
      "07:26  Creating model for method sally\n",
      "07:26  Training model\n",
      "07:27    Epoch 2: train loss 38.74 ([38.73858601])\n",
      "07:28    Epoch 4: train loss 38.74 ([38.7374085])\n",
      "07:28    Epoch 6: train loss 38.74 ([38.73669032])\n",
      "07:29    Epoch 8: train loss 38.74 ([38.73656472])\n",
      "07:30    Epoch 10: train loss 38.74 ([38.73648854])\n",
      "07:30    Epoch 12: train loss 38.74 ([38.73634504])\n",
      "07:31    Epoch 14: train loss 38.74 ([38.7362722])\n",
      "07:31    Epoch 16: train loss 38.74 ([38.73621868])\n",
      "07:32    Epoch 18: train loss 38.74 ([38.73604412])\n",
      "07:32    Epoch 20: train loss 38.74 ([38.73649101])\n",
      "07:32  Finished training\n"
     ]
    }
   ],
   "source": [
    "ensemble_dummy = EnsembleForge(n_estimators)\n",
    "\n",
    "ensemble_dummy.train_all(\n",
    "    features=[ [1] for _ in range(n_estimators)],\n",
    "    method='sally',\n",
    "    x_filename=[sample_dir + 'train_local/x_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    t_xz0_filename=[sample_dir + 'train_local/t_xz_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=None,\n",
    "    n_hidden=n_hidden\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbpresent": {
     "id": "cbfb4bf2-14a0-4b4c-a0f2-d2bd6484ef0d"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:32  Calculating expectation for 20 estimators in ensemble\n",
      "07:32  Starting evaluation for estimator 1 / 20 in ensemble\n",
      "07:33  Starting evaluation for estimator 2 / 20 in ensemble\n",
      "07:33  Starting evaluation for estimator 3 / 20 in ensemble\n",
      "07:33  Starting evaluation for estimator 4 / 20 in ensemble\n",
      "07:33  Starting evaluation for estimator 5 / 20 in ensemble\n",
      "07:33  Starting evaluation for estimator 6 / 20 in ensemble\n",
      "07:33  Starting evaluation for estimator 7 / 20 in ensemble\n",
      "07:33  Starting evaluation for estimator 8 / 20 in ensemble\n",
      "07:34  Starting evaluation for estimator 9 / 20 in ensemble\n",
      "07:34  Starting evaluation for estimator 10 / 20 in ensemble\n",
      "07:34  Starting evaluation for estimator 11 / 20 in ensemble\n",
      "07:34  Starting evaluation for estimator 12 / 20 in ensemble\n",
      "07:34  Starting evaluation for estimator 13 / 20 in ensemble\n",
      "07:34  Starting evaluation for estimator 14 / 20 in ensemble\n",
      "07:34  Starting evaluation for estimator 15 / 20 in ensemble\n",
      "07:34  Starting evaluation for estimator 16 / 20 in ensemble\n",
      "07:35  Starting evaluation for estimator 17 / 20 in ensemble\n",
      "07:35  Starting evaluation for estimator 18 / 20 in ensemble\n",
      "07:35  Starting evaluation for estimator 19 / 20 in ensemble\n",
      "07:35  Starting evaluation for estimator 20 / 20 in ensemble\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00336673, -0.00288502],\n",
       "       [ 0.0068463 , -0.00944023],\n",
       "       [ 0.00100317, -0.0060065 ],\n",
       "       [ 0.00568094, -0.00070863],\n",
       "       [ 0.006575  , -0.00283663],\n",
       "       [ 0.00195738,  0.00871882],\n",
       "       [ 0.0051679 , -0.00715192],\n",
       "       [ 0.01136171,  0.00861109],\n",
       "       [ 0.00392051, -0.010827  ],\n",
       "       [-0.00224715,  0.00736448],\n",
       "       [-0.00815651, -0.00631816],\n",
       "       [-0.00376409, -0.00368507],\n",
       "       [ 0.00249377,  0.00758593],\n",
       "       [ 0.005772  , -0.00649926],\n",
       "       [-0.00109865,  0.00498505],\n",
       "       [-0.00260419,  0.00814076],\n",
       "       [ 0.00449551, -0.00786066],\n",
       "       [ 0.00568148, -0.00503886],\n",
       "       [ 0.00205642,  0.00773895],\n",
       "       [ 0.0042204 , -0.00023774]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_dummy.calculate_expectation(\n",
    "    x_filename=sample_dir + 'validation/x_validation.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbpresent": {
     "id": "07ca6a5c-68f8-4b09-aef8-b2bfc40d47e0"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_dummy.save(model_dir + 'sally_ensemble_dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c0e6b85f-dbf0-4145-826f-d27b57ebcd85"
    }
   },
   "source": [
    "## 1d toy study (resurrection phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbpresent": {
     "id": "7d39c984-d4a3-4f99-86d2-f5a6cf339bb4"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:35  Training 20 estimators in ensemble\n",
      "07:35  Training estimator 1 / 20 in ensemble\n",
      "07:35  Starting training\n",
      "07:35    Method:                 sally\n",
      "07:35    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "07:35                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "07:35    Features:               [29]\n",
      "07:35    Method:                 sally\n",
      "07:35    Hidden layers:          (100, 100)\n",
      "07:35    Activation function:    tanh\n",
      "07:35    Batch size:             128\n",
      "07:35    Epochs:                 20\n",
      "07:35    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "07:35    Validation split:       None\n",
      "07:35    Early stopping:         True\n",
      "07:35  Loading training data\n",
      "07:35  Found 1000000 samples with 2 parameters and 30 observables\n",
      "07:35  Only using 1 of 30 observables\n",
      "07:35  Creating model for method sally\n",
      "07:35  Training model\n",
      "07:36    Epoch 2: train loss 30.93 ([30.93250812])\n",
      "07:36    Epoch 4: train loss 30.93 ([30.93186505])\n",
      "07:37    Epoch 6: train loss 30.93 ([30.93152545])\n",
      "07:38    Epoch 8: train loss 30.93 ([30.93113358])\n",
      "07:38    Epoch 10: train loss 30.93 ([30.9310046])\n",
      "07:39    Epoch 12: train loss 30.93 ([30.93094216])\n",
      "07:39    Epoch 14: train loss 30.93 ([30.93172662])\n",
      "07:40    Epoch 16: train loss 30.93 ([30.93104394])\n",
      "07:41    Epoch 18: train loss 30.93 ([30.93323088])\n",
      "07:41    Epoch 20: train loss 30.93 ([30.93131133])\n",
      "07:41  Finished training\n",
      "07:41  Training estimator 2 / 20 in ensemble\n",
      "07:41  Starting training\n",
      "07:41    Method:                 sally\n",
      "07:41    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "07:41                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "07:41    Features:               [29]\n",
      "07:41    Method:                 sally\n",
      "07:41    Hidden layers:          (100, 100)\n",
      "07:41    Activation function:    tanh\n",
      "07:41    Batch size:             128\n",
      "07:41    Epochs:                 20\n",
      "07:41    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "07:41    Validation split:       None\n",
      "07:41    Early stopping:         True\n",
      "07:41  Loading training data\n",
      "07:41  Found 1000000 samples with 2 parameters and 30 observables\n",
      "07:41  Only using 1 of 30 observables\n",
      "07:41  Creating model for method sally\n",
      "07:41  Training model\n",
      "07:42    Epoch 2: train loss 34.08 ([34.07549024])\n",
      "07:42    Epoch 4: train loss 34.07 ([34.07476335])\n",
      "07:43    Epoch 6: train loss 34.07 ([34.07456903])\n",
      "07:44    Epoch 8: train loss 34.07 ([34.07427625])\n",
      "07:44    Epoch 10: train loss 34.07 ([34.07420893])\n",
      "07:45    Epoch 12: train loss 34.07 ([34.07416112])\n",
      "07:45    Epoch 14: train loss 34.07 ([34.07392257])\n",
      "07:46    Epoch 16: train loss 34.08 ([34.07561456])\n",
      "07:46    Epoch 18: train loss 34.07 ([34.07475601])\n",
      "07:47    Epoch 20: train loss 34.07 ([34.07383752])\n",
      "07:47  Finished training\n",
      "07:47  Training estimator 3 / 20 in ensemble\n",
      "07:47  Starting training\n",
      "07:47    Method:                 sally\n",
      "07:47    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "07:47                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "07:47    Features:               [29]\n",
      "07:47    Method:                 sally\n",
      "07:47    Hidden layers:          (100, 100)\n",
      "07:47    Activation function:    tanh\n",
      "07:47    Batch size:             128\n",
      "07:47    Epochs:                 20\n",
      "07:47    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "07:47    Validation split:       None\n",
      "07:47    Early stopping:         True\n",
      "07:47  Loading training data\n",
      "07:47  Found 1000000 samples with 2 parameters and 30 observables\n",
      "07:47  Only using 1 of 30 observables\n",
      "07:47  Creating model for method sally\n",
      "07:47  Training model\n",
      "07:48    Epoch 2: train loss 33.75 ([33.74846041])\n",
      "07:48    Epoch 4: train loss 33.75 ([33.74619079])\n",
      "07:49    Epoch 6: train loss 33.75 ([33.74597396])\n",
      "07:50    Epoch 8: train loss 33.75 ([33.74606455])\n",
      "07:50    Epoch 10: train loss 33.75 ([33.74600406])\n",
      "07:51    Epoch 12: train loss 33.75 ([33.74557888])\n",
      "07:51    Epoch 14: train loss 33.75 ([33.74565639])\n",
      "07:52    Epoch 16: train loss 33.75 ([33.74571221])\n",
      "07:52    Epoch 18: train loss 33.75 ([33.74557757])\n",
      "07:53    Epoch 20: train loss 33.75 ([33.74562295])\n",
      "07:53  Finished training\n",
      "07:53  Training estimator 4 / 20 in ensemble\n",
      "07:53  Starting training\n",
      "07:53    Method:                 sally\n",
      "07:53    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "07:53                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "07:53    Features:               [29]\n",
      "07:53    Method:                 sally\n",
      "07:53    Hidden layers:          (100, 100)\n",
      "07:53    Activation function:    tanh\n",
      "07:53    Batch size:             128\n",
      "07:53    Epochs:                 20\n",
      "07:53    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "07:53    Validation split:       None\n",
      "07:53    Early stopping:         True\n",
      "07:53  Loading training data\n",
      "07:53  Found 1000000 samples with 2 parameters and 30 observables\n",
      "07:53  Only using 1 of 30 observables\n",
      "07:53  Creating model for method sally\n",
      "07:53  Training model\n",
      "07:54    Epoch 2: train loss 30.59 ([30.59438053])\n",
      "07:54    Epoch 4: train loss 30.59 ([30.59288687])\n",
      "07:55    Epoch 6: train loss 30.59 ([30.59265678])\n",
      "07:55    Epoch 8: train loss 30.59 ([30.59283801])\n",
      "07:56    Epoch 10: train loss 30.59 ([30.59287918])\n",
      "07:57    Epoch 12: train loss 30.59 ([30.59236022])\n",
      "07:57    Epoch 14: train loss 30.59 ([30.59220216])\n",
      "07:58    Epoch 16: train loss 30.59 ([30.59233456])\n",
      "07:58    Epoch 18: train loss 30.59 ([30.59212208])\n",
      "07:59    Epoch 20: train loss 30.59 ([30.59207302])\n",
      "07:59  Finished training\n",
      "07:59  Training estimator 5 / 20 in ensemble\n",
      "07:59  Starting training\n",
      "07:59    Method:                 sally\n",
      "07:59    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "07:59                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "07:59    Features:               [29]\n",
      "07:59    Method:                 sally\n",
      "07:59    Hidden layers:          (100, 100)\n",
      "07:59    Activation function:    tanh\n",
      "07:59    Batch size:             128\n",
      "07:59    Epochs:                 20\n",
      "07:59    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "07:59    Validation split:       None\n",
      "07:59    Early stopping:         True\n",
      "07:59  Loading training data\n",
      "07:59  Found 1000000 samples with 2 parameters and 30 observables\n",
      "07:59  Only using 1 of 30 observables\n",
      "07:59  Creating model for method sally\n",
      "07:59  Training model\n",
      "08:00    Epoch 2: train loss 38.10 ([38.0979482])\n",
      "08:00    Epoch 4: train loss 38.10 ([38.09776195])\n",
      "08:01    Epoch 6: train loss 38.10 ([38.09701759])\n",
      "08:02    Epoch 8: train loss 38.10 ([38.09703634])\n",
      "08:02    Epoch 10: train loss 38.10 ([38.0968044])\n",
      "08:03    Epoch 12: train loss 38.10 ([38.09656668])\n",
      "08:03    Epoch 14: train loss 38.10 ([38.09659614])\n",
      "08:04    Epoch 16: train loss 38.10 ([38.09671092])\n",
      "08:04    Epoch 18: train loss 38.10 ([38.09641903])\n",
      "08:05    Epoch 20: train loss 38.10 ([38.09629968])\n",
      "08:05  Finished training\n",
      "08:05  Training estimator 6 / 20 in ensemble\n",
      "08:05  Starting training\n",
      "08:05    Method:                 sally\n",
      "08:05    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "08:05                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "08:05    Features:               [29]\n",
      "08:05    Method:                 sally\n",
      "08:05    Hidden layers:          (100, 100)\n",
      "08:05    Activation function:    tanh\n",
      "08:05    Batch size:             128\n",
      "08:05    Epochs:                 20\n",
      "08:05    Learning rate:          0.002 initially, decaying to 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:05    Validation split:       None\n",
      "08:05    Early stopping:         True\n",
      "08:05  Loading training data\n",
      "08:05  Found 1000000 samples with 2 parameters and 30 observables\n",
      "08:05  Only using 1 of 30 observables\n",
      "08:05  Creating model for method sally\n",
      "08:05  Training model\n",
      "08:06    Epoch 2: train loss 24.30 ([24.30287919])\n",
      "08:06    Epoch 4: train loss 24.30 ([24.30093839])\n",
      "08:07    Epoch 6: train loss 24.30 ([24.3011309])\n",
      "08:08    Epoch 8: train loss 24.30 ([24.30098106])\n",
      "08:08    Epoch 10: train loss 24.30 ([24.30100083])\n",
      "08:09    Epoch 12: train loss 24.30 ([24.30084711])\n",
      "08:09    Epoch 14: train loss 24.30 ([24.30056967])\n",
      "08:10    Epoch 16: train loss 24.30 ([24.30098907])\n",
      "08:11    Epoch 18: train loss 24.30 ([24.30038112])\n",
      "08:11    Epoch 20: train loss 24.30 ([24.30041937])\n",
      "08:11  Finished training\n",
      "08:11  Training estimator 7 / 20 in ensemble\n",
      "08:11  Starting training\n",
      "08:11    Method:                 sally\n",
      "08:11    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "08:11                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "08:11    Features:               [29]\n",
      "08:11    Method:                 sally\n",
      "08:11    Hidden layers:          (100, 100)\n",
      "08:11    Activation function:    tanh\n",
      "08:11    Batch size:             128\n",
      "08:11    Epochs:                 20\n",
      "08:11    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "08:11    Validation split:       None\n",
      "08:11    Early stopping:         True\n",
      "08:11  Loading training data\n",
      "08:11  Found 1000000 samples with 2 parameters and 30 observables\n",
      "08:11  Only using 1 of 30 observables\n",
      "08:11  Creating model for method sally\n",
      "08:11  Training model\n",
      "08:12    Epoch 2: train loss 19.88 ([19.87588721])\n",
      "08:13    Epoch 4: train loss 19.88 ([19.87767429])\n",
      "08:13    Epoch 6: train loss 19.88 ([19.87530744])\n",
      "08:14    Epoch 8: train loss 19.88 ([19.87522842])\n",
      "08:15    Epoch 10: train loss 19.88 ([19.87536041])\n",
      "08:15    Epoch 12: train loss 19.88 ([19.8751453])\n",
      "08:16    Epoch 14: train loss 19.88 ([19.87521752])\n",
      "08:17    Epoch 16: train loss 19.88 ([19.87524429])\n",
      "08:17    Epoch 18: train loss 19.88 ([19.87529356])\n",
      "08:18    Epoch 20: train loss 19.88 ([19.87507489])\n",
      "08:18  Finished training\n",
      "08:18  Training estimator 8 / 20 in ensemble\n",
      "08:18  Starting training\n",
      "08:18    Method:                 sally\n",
      "08:18    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "08:18                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "08:18    Features:               [29]\n",
      "08:18    Method:                 sally\n",
      "08:18    Hidden layers:          (100, 100)\n",
      "08:18    Activation function:    tanh\n",
      "08:18    Batch size:             128\n",
      "08:18    Epochs:                 20\n",
      "08:18    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "08:18    Validation split:       None\n",
      "08:18    Early stopping:         True\n",
      "08:18  Loading training data\n",
      "08:18  Found 1000000 samples with 2 parameters and 30 observables\n",
      "08:18  Only using 1 of 30 observables\n",
      "08:18  Creating model for method sally\n",
      "08:18  Training model\n",
      "08:19    Epoch 2: train loss 26.38 ([26.38327686])\n",
      "08:20    Epoch 4: train loss 26.38 ([26.38292555])\n",
      "08:20    Epoch 6: train loss 26.38 ([26.38272973])\n",
      "08:21    Epoch 8: train loss 26.38 ([26.38202409])\n",
      "08:22    Epoch 10: train loss 26.38 ([26.38241973])\n",
      "08:22    Epoch 12: train loss 26.38 ([26.38187815])\n",
      "08:23    Epoch 14: train loss 26.38 ([26.38189986])\n",
      "08:24    Epoch 16: train loss 26.38 ([26.3817995])\n",
      "08:24    Epoch 18: train loss 26.38 ([26.38219247])\n",
      "08:25    Epoch 20: train loss 26.38 ([26.38213712])\n",
      "08:25  Finished training\n",
      "08:25  Training estimator 9 / 20 in ensemble\n",
      "08:25  Starting training\n",
      "08:25    Method:                 sally\n",
      "08:25    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "08:25                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "08:25    Features:               [29]\n",
      "08:25    Method:                 sally\n",
      "08:25    Hidden layers:          (100, 100)\n",
      "08:25    Activation function:    tanh\n",
      "08:25    Batch size:             128\n",
      "08:25    Epochs:                 20\n",
      "08:25    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "08:25    Validation split:       None\n",
      "08:25    Early stopping:         True\n",
      "08:25  Loading training data\n",
      "08:25  Found 1000000 samples with 2 parameters and 30 observables\n",
      "08:25  Only using 1 of 30 observables\n",
      "08:25  Creating model for method sally\n",
      "08:25  Training model\n",
      "08:26    Epoch 2: train loss 26.46 ([26.45875353])\n",
      "08:26    Epoch 4: train loss 26.46 ([26.45810556])\n",
      "08:27    Epoch 6: train loss 26.46 ([26.45773783])\n",
      "08:28    Epoch 8: train loss 26.46 ([26.45763164])\n",
      "08:28    Epoch 10: train loss 26.46 ([26.45937944])\n",
      "08:29    Epoch 12: train loss 26.46 ([26.45762974])\n",
      "08:30    Epoch 14: train loss 26.46 ([26.45743114])\n",
      "08:30    Epoch 16: train loss 26.46 ([26.45734678])\n",
      "08:31    Epoch 18: train loss 26.46 ([26.45775856])\n",
      "08:31    Epoch 20: train loss 26.46 ([26.45811844])\n",
      "08:31  Finished training\n",
      "08:31  Training estimator 10 / 20 in ensemble\n",
      "08:31  Starting training\n",
      "08:31    Method:                 sally\n",
      "08:31    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "08:31                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "08:31    Features:               [29]\n",
      "08:31    Method:                 sally\n",
      "08:31    Hidden layers:          (100, 100)\n",
      "08:31    Activation function:    tanh\n",
      "08:31    Batch size:             128\n",
      "08:31    Epochs:                 20\n",
      "08:31    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "08:31    Validation split:       None\n",
      "08:31    Early stopping:         True\n",
      "08:31  Loading training data\n",
      "08:31  Found 1000000 samples with 2 parameters and 30 observables\n",
      "08:31  Only using 1 of 30 observables\n",
      "08:31  Creating model for method sally\n",
      "08:31  Training model\n",
      "08:32    Epoch 2: train loss 31.19 ([31.18919311])\n",
      "08:33    Epoch 4: train loss 31.19 ([31.19195366])\n",
      "08:34    Epoch 6: train loss 31.19 ([31.18824787])\n",
      "08:34    Epoch 8: train loss 31.19 ([31.18771417])\n",
      "08:35    Epoch 10: train loss 31.19 ([31.18752171])\n",
      "08:35    Epoch 12: train loss 31.19 ([31.18763273])\n",
      "08:45    Epoch 14: train loss 31.19 ([31.18752475])\n",
      "08:46    Epoch 16: train loss 31.21 ([31.21071427])\n",
      "08:46    Epoch 18: train loss 31.19 ([31.18785512])\n",
      "08:47    Epoch 20: train loss 31.19 ([31.1874161])\n",
      "08:47  Finished training\n",
      "08:47  Training estimator 11 / 20 in ensemble\n",
      "08:47  Starting training\n",
      "08:47    Method:                 sally\n",
      "08:47    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_10.npy\n",
      "08:47                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_10.npy\n",
      "08:47    Features:               [29]\n",
      "08:47    Method:                 sally\n",
      "08:47    Hidden layers:          (100, 100)\n",
      "08:47    Activation function:    tanh\n",
      "08:47    Batch size:             128\n",
      "08:47    Epochs:                 20\n",
      "08:47    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "08:47    Validation split:       None\n",
      "08:47    Early stopping:         True\n",
      "08:47  Loading training data\n",
      "08:47  Found 1000000 samples with 2 parameters and 30 observables\n",
      "08:47  Only using 1 of 30 observables\n",
      "08:47  Creating model for method sally\n",
      "08:47  Training model\n",
      "08:48    Epoch 2: train loss 28.46 ([28.45983461])\n",
      "08:49    Epoch 4: train loss 28.46 ([28.45927404])\n",
      "08:49    Epoch 6: train loss 28.46 ([28.46090934])\n",
      "08:50    Epoch 8: train loss 28.46 ([28.45869732])\n",
      "08:51    Epoch 10: train loss 28.46 ([28.45883858])\n",
      "08:51    Epoch 12: train loss 28.46 ([28.45863261])\n",
      "08:52    Epoch 14: train loss 28.46 ([28.4581745])\n",
      "08:53    Epoch 16: train loss 28.46 ([28.4581628])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:54    Epoch 18: train loss 28.46 ([28.45819304])\n",
      "08:54    Epoch 20: train loss 28.46 ([28.4581199])\n",
      "08:54  Finished training\n",
      "08:54  Training estimator 12 / 20 in ensemble\n",
      "08:54  Starting training\n",
      "08:54    Method:                 sally\n",
      "08:54    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_11.npy\n",
      "08:54                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_11.npy\n",
      "08:54    Features:               [29]\n",
      "08:54    Method:                 sally\n",
      "08:54    Hidden layers:          (100, 100)\n",
      "08:54    Activation function:    tanh\n",
      "08:54    Batch size:             128\n",
      "08:54    Epochs:                 20\n",
      "08:54    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "08:54    Validation split:       None\n",
      "08:54    Early stopping:         True\n",
      "08:54  Loading training data\n",
      "08:54  Found 1000000 samples with 2 parameters and 30 observables\n",
      "08:54  Only using 1 of 30 observables\n",
      "08:54  Creating model for method sally\n",
      "08:54  Training model\n",
      "08:55    Epoch 2: train loss 26.39 ([26.39404788])\n",
      "08:56    Epoch 4: train loss 26.39 ([26.39317731])\n",
      "08:56    Epoch 6: train loss 26.39 ([26.39284371])\n",
      "08:57    Epoch 8: train loss 26.39 ([26.39278142])\n",
      "08:58    Epoch 10: train loss 26.39 ([26.39404584])\n",
      "08:58    Epoch 12: train loss 26.39 ([26.39295364])\n",
      "08:59    Epoch 14: train loss 26.39 ([26.39245623])\n",
      "09:00    Epoch 16: train loss 26.39 ([26.39243552])\n",
      "09:01    Epoch 18: train loss 26.39 ([26.39241135])\n",
      "09:01    Epoch 20: train loss 26.39 ([26.39239861])\n",
      "09:01  Finished training\n",
      "09:01  Training estimator 13 / 20 in ensemble\n",
      "09:01  Starting training\n",
      "09:01    Method:                 sally\n",
      "09:01    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_12.npy\n",
      "09:01                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_12.npy\n",
      "09:01    Features:               [29]\n",
      "09:01    Method:                 sally\n",
      "09:01    Hidden layers:          (100, 100)\n",
      "09:01    Activation function:    tanh\n",
      "09:01    Batch size:             128\n",
      "09:01    Epochs:                 20\n",
      "09:01    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "09:01    Validation split:       None\n",
      "09:01    Early stopping:         True\n",
      "09:01  Loading training data\n",
      "09:01  Found 1000000 samples with 2 parameters and 30 observables\n",
      "09:01  Only using 1 of 30 observables\n",
      "09:01  Creating model for method sally\n",
      "09:01  Training model\n",
      "09:02    Epoch 2: train loss 36.72 ([36.71501501])\n",
      "09:03    Epoch 4: train loss 36.71 ([36.71121097])\n",
      "09:03    Epoch 6: train loss 36.71 ([36.71142076])\n",
      "09:04    Epoch 8: train loss 36.71 ([36.71169179])\n",
      "09:05    Epoch 10: train loss 36.71 ([36.71079905])\n",
      "09:05    Epoch 12: train loss 36.71 ([36.71089542])\n",
      "09:06    Epoch 14: train loss 36.71 ([36.71123982])\n",
      "09:07    Epoch 16: train loss 36.71 ([36.71058162])\n",
      "09:07    Epoch 18: train loss 36.71 ([36.7106471])\n",
      "09:08    Epoch 20: train loss 36.71 ([36.71048803])\n",
      "09:08  Finished training\n",
      "09:08  Training estimator 14 / 20 in ensemble\n",
      "09:08  Starting training\n",
      "09:08    Method:                 sally\n",
      "09:08    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_13.npy\n",
      "09:08                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_13.npy\n",
      "09:08    Features:               [29]\n",
      "09:08    Method:                 sally\n",
      "09:08    Hidden layers:          (100, 100)\n",
      "09:08    Activation function:    tanh\n",
      "09:08    Batch size:             128\n",
      "09:08    Epochs:                 20\n",
      "09:08    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "09:08    Validation split:       None\n",
      "09:08    Early stopping:         True\n",
      "09:08  Loading training data\n",
      "09:08  Found 1000000 samples with 2 parameters and 30 observables\n",
      "09:08  Only using 1 of 30 observables\n",
      "09:08  Creating model for method sally\n",
      "09:08  Training model\n",
      "09:09    Epoch 2: train loss 25.31 ([25.30959892])\n",
      "09:09    Epoch 4: train loss 25.31 ([25.3088747])\n",
      "09:10    Epoch 6: train loss 25.31 ([25.3093158])\n",
      "09:11    Epoch 8: train loss 25.31 ([25.30834772])\n",
      "09:11    Epoch 10: train loss 25.31 ([25.30848592])\n",
      "09:12    Epoch 12: train loss 25.31 ([25.3087663])\n",
      "09:13    Epoch 14: train loss 25.31 ([25.3085541])\n",
      "09:13    Epoch 16: train loss 25.31 ([25.30814629])\n",
      "09:14    Epoch 18: train loss 25.31 ([25.30833147])\n",
      "09:15    Epoch 20: train loss 25.31 ([25.30823667])\n",
      "09:15  Finished training\n",
      "09:15  Training estimator 15 / 20 in ensemble\n",
      "09:15  Starting training\n",
      "09:15    Method:                 sally\n",
      "09:15    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_14.npy\n",
      "09:15                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_14.npy\n",
      "09:15    Features:               [29]\n",
      "09:15    Method:                 sally\n",
      "09:15    Hidden layers:          (100, 100)\n",
      "09:15    Activation function:    tanh\n",
      "09:15    Batch size:             128\n",
      "09:15    Epochs:                 20\n",
      "09:15    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "09:15    Validation split:       None\n",
      "09:15    Early stopping:         True\n",
      "09:15  Loading training data\n",
      "09:15  Found 1000000 samples with 2 parameters and 30 observables\n",
      "09:15  Only using 1 of 30 observables\n",
      "09:15  Creating model for method sally\n",
      "09:15  Training model\n",
      "09:15    Epoch 2: train loss 65.16 ([65.16242656])\n",
      "09:16    Epoch 4: train loss 65.16 ([65.16156312])\n",
      "09:17    Epoch 6: train loss 65.16 ([65.16222702])\n",
      "09:17    Epoch 8: train loss 65.16 ([65.16043113])\n",
      "09:18    Epoch 10: train loss 65.16 ([65.16026958])\n",
      "09:18    Epoch 12: train loss 65.16 ([65.16022269])\n",
      "09:19    Epoch 14: train loss 65.16 ([65.16036414])\n",
      "09:19    Epoch 16: train loss 65.16 ([65.16034055])\n",
      "09:20    Epoch 18: train loss 65.16 ([65.16005834])\n",
      "09:21    Epoch 20: train loss 65.16 ([65.16031784])\n",
      "09:21  Finished training\n",
      "09:21  Training estimator 16 / 20 in ensemble\n",
      "09:21  Starting training\n",
      "09:21    Method:                 sally\n",
      "09:21    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_15.npy\n",
      "09:21                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_15.npy\n",
      "09:21    Features:               [29]\n",
      "09:21    Method:                 sally\n",
      "09:21    Hidden layers:          (100, 100)\n",
      "09:21    Activation function:    tanh\n",
      "09:21    Batch size:             128\n",
      "09:21    Epochs:                 20\n",
      "09:21    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "09:21    Validation split:       None\n",
      "09:21    Early stopping:         True\n",
      "09:21  Loading training data\n",
      "09:21  Found 1000000 samples with 2 parameters and 30 observables\n",
      "09:21  Only using 1 of 30 observables\n",
      "09:21  Creating model for method sally\n",
      "09:21  Training model\n",
      "09:21    Epoch 2: train loss 21.30 ([21.30115013])\n",
      "09:22    Epoch 4: train loss 21.30 ([21.30045112])\n",
      "09:23    Epoch 6: train loss 21.30 ([21.30048904])\n",
      "09:23    Epoch 8: train loss 21.30 ([21.30016027])\n",
      "09:24    Epoch 10: train loss 21.30 ([21.29977909])\n",
      "09:24    Epoch 12: train loss 21.30 ([21.30034721])\n",
      "09:25    Epoch 14: train loss 21.30 ([21.30001415])\n",
      "09:26    Epoch 16: train loss 21.30 ([21.29967723])\n",
      "09:26    Epoch 18: train loss 21.30 ([21.29980764])\n",
      "09:27    Epoch 20: train loss 21.30 ([21.29986165])\n",
      "09:27  Finished training\n",
      "09:27  Training estimator 17 / 20 in ensemble\n",
      "09:27  Starting training\n",
      "09:27    Method:                 sally\n",
      "09:27    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_16.npy\n",
      "09:27                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_16.npy\n",
      "09:27    Features:               [29]\n",
      "09:27    Method:                 sally\n",
      "09:27    Hidden layers:          (100, 100)\n",
      "09:27    Activation function:    tanh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:27    Batch size:             128\n",
      "09:27    Epochs:                 20\n",
      "09:27    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "09:27    Validation split:       None\n",
      "09:27    Early stopping:         True\n",
      "09:27  Loading training data\n",
      "09:27  Found 1000000 samples with 2 parameters and 30 observables\n",
      "09:27  Only using 1 of 30 observables\n",
      "09:27  Creating model for method sally\n",
      "09:27  Training model\n",
      "09:28    Epoch 2: train loss 20.84 ([20.83663648])\n",
      "09:28    Epoch 4: train loss 20.84 ([20.83580615])\n",
      "09:29    Epoch 6: train loss 20.84 ([20.83554541])\n",
      "09:29    Epoch 8: train loss 20.84 ([20.83553139])\n",
      "09:30    Epoch 10: train loss 20.84 ([20.83533335])\n",
      "09:31    Epoch 12: train loss 20.84 ([20.83519458])\n",
      "09:31    Epoch 14: train loss 20.84 ([20.83546944])\n",
      "09:32    Epoch 16: train loss 20.84 ([20.83511996])\n",
      "09:32    Epoch 18: train loss 20.84 ([20.83537666])\n",
      "09:33    Epoch 20: train loss 20.84 ([20.83511081])\n",
      "09:33  Finished training\n",
      "09:33  Training estimator 18 / 20 in ensemble\n",
      "09:33  Starting training\n",
      "09:33    Method:                 sally\n",
      "09:33    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_17.npy\n",
      "09:33                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_17.npy\n",
      "09:33    Features:               [29]\n",
      "09:33    Method:                 sally\n",
      "09:33    Hidden layers:          (100, 100)\n",
      "09:33    Activation function:    tanh\n",
      "09:33    Batch size:             128\n",
      "09:33    Epochs:                 20\n",
      "09:33    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "09:33    Validation split:       None\n",
      "09:33    Early stopping:         True\n",
      "09:33  Loading training data\n",
      "09:33  Found 1000000 samples with 2 parameters and 30 observables\n",
      "09:33  Only using 1 of 30 observables\n",
      "09:33  Creating model for method sally\n",
      "09:33  Training model\n",
      "09:34    Epoch 2: train loss 39.46 ([39.46274776])\n",
      "09:34    Epoch 4: train loss 39.46 ([39.46025042])\n",
      "09:35    Epoch 6: train loss 39.46 ([39.46151542])\n",
      "09:36    Epoch 8: train loss 39.46 ([39.45964425])\n",
      "09:36    Epoch 10: train loss 39.46 ([39.45957615])\n",
      "09:37    Epoch 12: train loss 39.46 ([39.45981032])\n",
      "09:37    Epoch 14: train loss 39.46 ([39.45938112])\n",
      "09:38    Epoch 16: train loss 39.46 ([39.45930525])\n",
      "09:38    Epoch 18: train loss 39.50 ([39.50432868])\n",
      "09:39    Epoch 20: train loss 39.46 ([39.45926262])\n",
      "09:39  Finished training\n",
      "09:39  Training estimator 19 / 20 in ensemble\n",
      "09:39  Starting training\n",
      "09:39    Method:                 sally\n",
      "09:39    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_18.npy\n",
      "09:39                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_18.npy\n",
      "09:39    Features:               [29]\n",
      "09:39    Method:                 sally\n",
      "09:39    Hidden layers:          (100, 100)\n",
      "09:39    Activation function:    tanh\n",
      "09:39    Batch size:             128\n",
      "09:39    Epochs:                 20\n",
      "09:39    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "09:39    Validation split:       None\n",
      "09:39    Early stopping:         True\n",
      "09:39  Loading training data\n",
      "09:39  Found 1000000 samples with 2 parameters and 30 observables\n",
      "09:39  Only using 1 of 30 observables\n",
      "09:39  Creating model for method sally\n",
      "09:39  Training model\n",
      "09:40    Epoch 2: train loss 26.16 ([26.15648138])\n",
      "09:40    Epoch 4: train loss 26.15 ([26.15352649])\n",
      "09:41    Epoch 6: train loss 26.15 ([26.15316916])\n",
      "09:42    Epoch 8: train loss 26.15 ([26.15370291])\n",
      "09:42    Epoch 10: train loss 26.19 ([26.19002303])\n",
      "09:43    Epoch 12: train loss 26.15 ([26.15285804])\n",
      "09:43    Epoch 14: train loss 26.15 ([26.15281581])\n",
      "09:44    Epoch 16: train loss 26.15 ([26.15273052])\n",
      "09:44    Epoch 18: train loss 26.15 ([26.15270884])\n",
      "09:45    Epoch 20: train loss 26.15 ([26.15333416])\n",
      "09:45  Finished training\n",
      "09:45  Training estimator 20 / 20 in ensemble\n",
      "09:45  Starting training\n",
      "09:45    Method:                 sally\n",
      "09:45    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_19.npy\n",
      "09:45                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_19.npy\n",
      "09:45    Features:               [29]\n",
      "09:45    Method:                 sally\n",
      "09:45    Hidden layers:          (100, 100)\n",
      "09:45    Activation function:    tanh\n",
      "09:45    Batch size:             128\n",
      "09:45    Epochs:                 20\n",
      "09:45    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "09:45    Validation split:       None\n",
      "09:45    Early stopping:         True\n",
      "09:45  Loading training data\n",
      "09:45  Found 1000000 samples with 2 parameters and 30 observables\n",
      "09:45  Only using 1 of 30 observables\n",
      "09:45  Creating model for method sally\n",
      "09:45  Training model\n",
      "09:46    Epoch 2: train loss 38.74 ([38.73778588])\n",
      "09:46    Epoch 4: train loss 38.74 ([38.7371789])\n",
      "09:47    Epoch 6: train loss 38.74 ([38.7368016])\n",
      "09:48    Epoch 8: train loss 38.74 ([38.73671854])\n",
      "09:48    Epoch 10: train loss 38.74 ([38.73654126])\n",
      "09:49    Epoch 12: train loss 38.74 ([38.73646902])\n",
      "09:49    Epoch 14: train loss 38.74 ([38.73683185])\n",
      "09:50    Epoch 16: train loss 38.74 ([38.73615225])\n",
      "09:51    Epoch 18: train loss 38.74 ([38.73619049])\n",
      "09:51    Epoch 20: train loss 38.74 ([38.73626232])\n",
      "09:51  Finished training\n"
     ]
    }
   ],
   "source": [
    "ensemble_res = EnsembleForge(n_estimators)\n",
    "\n",
    "ensemble_res.train_all(\n",
    "    features=[ [29] for _ in range(n_estimators)],\n",
    "    method='sally',\n",
    "    x_filename=[sample_dir + 'train_local/x_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    t_xz0_filename=[sample_dir + 'train_local/t_xz_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=None,\n",
    "    n_hidden=n_hidden\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbpresent": {
     "id": "92b1db79-876f-4d9d-9de5-1011c3d04633"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:51  Calculating expectation for 20 estimators in ensemble\n",
      "09:51  Starting evaluation for estimator 1 / 20 in ensemble\n",
      "09:51  Starting evaluation for estimator 2 / 20 in ensemble\n",
      "09:51  Starting evaluation for estimator 3 / 20 in ensemble\n",
      "09:52  Starting evaluation for estimator 4 / 20 in ensemble\n",
      "09:52  Starting evaluation for estimator 5 / 20 in ensemble\n",
      "09:52  Starting evaluation for estimator 6 / 20 in ensemble\n",
      "09:52  Starting evaluation for estimator 7 / 20 in ensemble\n",
      "09:52  Starting evaluation for estimator 8 / 20 in ensemble\n",
      "09:52  Starting evaluation for estimator 9 / 20 in ensemble\n",
      "09:52  Starting evaluation for estimator 10 / 20 in ensemble\n",
      "09:53  Starting evaluation for estimator 11 / 20 in ensemble\n",
      "09:53  Starting evaluation for estimator 12 / 20 in ensemble\n",
      "09:53  Starting evaluation for estimator 13 / 20 in ensemble\n",
      "09:53  Starting evaluation for estimator 14 / 20 in ensemble\n",
      "09:53  Starting evaluation for estimator 15 / 20 in ensemble\n",
      "09:53  Starting evaluation for estimator 16 / 20 in ensemble\n",
      "09:53  Starting evaluation for estimator 17 / 20 in ensemble\n",
      "09:53  Starting evaluation for estimator 18 / 20 in ensemble\n",
      "09:54  Starting evaluation for estimator 19 / 20 in ensemble\n",
      "09:54  Starting evaluation for estimator 20 / 20 in ensemble\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.0031571 , -0.00529903],\n",
       "       [-0.00704187, -0.01263378],\n",
       "       [ 0.00147036, -0.00874161],\n",
       "       [ 0.00024858,  0.00263365],\n",
       "       [ 0.00405722, -0.00343769],\n",
       "       [-0.00119947,  0.00644899],\n",
       "       [ 0.00158439, -0.00339344],\n",
       "       [ 0.00563954,  0.00829799],\n",
       "       [-0.00332506, -0.00051809],\n",
       "       [-0.00134955,  0.00435185],\n",
       "       [-0.00233581, -0.01089808],\n",
       "       [-0.00815972, -0.00128444],\n",
       "       [ 0.00607474,  0.00218514],\n",
       "       [ 0.00638331, -0.00159927],\n",
       "       [ 0.00032959,  0.00698967],\n",
       "       [-0.00381187,  0.00743328],\n",
       "       [ 0.01006584, -0.00946279],\n",
       "       [ 0.0073764 ,  0.00974631],\n",
       "       [ 0.00366369,  0.00336566],\n",
       "       [-0.00764222,  0.0042087 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_res.calculate_expectation(\n",
    "    x_filename=sample_dir + 'validation/x_validation.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbpresent": {
     "id": "77e6e176-ea25-4199-ac0e-e7fb37096e2d"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_res.save(model_dir + 'sally_ensemble_resurrection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7d171470-6c7e-4a74-8ca8-5589004e32d6"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
