{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=10, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=False)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.calculate_expectation(\n",
    "        x_filename=sample_dir + 'validation{}/x_validation.npy'.format(cut_label)\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:15  Training 10 estimators in ensemble\n",
      "19:15  Training estimator 1 / 10 in ensemble\n",
      "19:15  Starting training\n",
      "19:15    Method:                 sally\n",
      "19:15    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "19:15                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "19:15    Features:               all\n",
      "19:15    Method:                 sally\n",
      "19:15    Hidden layers:          (100, 100)\n",
      "19:15    Activation function:    tanh\n",
      "19:15    Batch size:             128\n",
      "19:15    Trainer:                amsgrad\n",
      "19:15    Epochs:                 50\n",
      "19:15    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "19:15    Validation split:       0.25\n",
      "19:15    Early stopping:         True\n",
      "19:15    Scale inputs:           True\n",
      "19:15  Loading training data\n",
      "19:15  Found 1000000 samples with 2 parameters and 26 observables\n",
      "19:15  Rescaling inputs\n",
      "19:15  Creating model for method sally\n",
      "19:15  Training model\n",
      "19:18    Epoch 5: train loss 22.29 ([22.29357544]), validation loss 38.61 ([38.60989698]) (*)\n",
      "19:21    Epoch 10: train loss 22.20 ([22.20207459]), validation loss 38.62 ([38.61886594])\n",
      "19:24    Epoch 15: train loss 22.01 ([22.01290315]), validation loss 38.64 ([38.64060991])\n",
      "19:26    Epoch 20: train loss 21.74 ([21.74305057]), validation loss 38.65 ([38.64650889])\n",
      "19:29    Epoch 25: train loss 21.42 ([21.422006]), validation loss 38.79 ([38.79194675])\n",
      "19:31  No improvement for 20 epochs, stopping training\n",
      "19:31  Early stopping after epoch 9, with loss 38.59 compared to final loss 38.78\n",
      "19:31  Finished training\n",
      "19:31  Training estimator 2 / 10 in ensemble\n",
      "19:31  Starting training\n",
      "19:31    Method:                 sally\n",
      "19:31    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "19:31                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "19:31    Features:               all\n",
      "19:31    Method:                 sally\n",
      "19:31    Hidden layers:          (100, 100)\n",
      "19:31    Activation function:    tanh\n",
      "19:31    Batch size:             128\n",
      "19:31    Trainer:                amsgrad\n",
      "19:31    Epochs:                 50\n",
      "19:31    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "19:31    Validation split:       0.25\n",
      "19:31    Early stopping:         True\n",
      "19:31    Scale inputs:           True\n",
      "19:31  Loading training data\n",
      "19:31  Found 1000000 samples with 2 parameters and 26 observables\n",
      "19:31  Rescaling inputs\n",
      "19:31  Creating model for method sally\n",
      "19:31  Training model\n",
      "19:34    Epoch 5: train loss 77.68 ([77.67711749]), validation loss 27.56 ([27.56337922]) (*)\n",
      "19:37    Epoch 10: train loss 77.50 ([77.49899722]), validation loss 27.51 ([27.5050464])\n",
      "19:40    Epoch 15: train loss 77.19 ([77.18967669]), validation loss 27.41 ([27.41281861]) (*)\n",
      "19:43    Epoch 20: train loss 76.79 ([76.79241497]), validation loss 27.30 ([27.30150075]) (*)\n",
      "19:46    Epoch 25: train loss 76.49 ([76.49107561]), validation loss 27.26 ([27.25521073]) (*)\n",
      "19:48    Epoch 30: train loss 76.19 ([76.188473]), validation loss 27.21 ([27.21297075]) (*)\n",
      "19:51    Epoch 35: train loss 75.95 ([75.95272451]), validation loss 27.20 ([27.2019033])\n",
      "19:54    Epoch 40: train loss 75.75 ([75.75038442]), validation loss 27.15 ([27.15448843]) (*)\n",
      "19:57    Epoch 45: train loss 75.59 ([75.59097958]), validation loss 27.14 ([27.1445783])\n",
      "20:00    Epoch 50: train loss 75.46 ([75.45591268]), validation loss 27.15 ([27.15207324])\n",
      "20:00  Early stopping after epoch 49, with loss 27.12 compared to final loss 27.15\n",
      "20:00  Finished training\n",
      "20:00  Training estimator 3 / 10 in ensemble\n",
      "20:00  Starting training\n",
      "20:00    Method:                 sally\n",
      "20:00    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "20:00                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "20:00    Features:               all\n",
      "20:00    Method:                 sally\n",
      "20:00    Hidden layers:          (100, 100)\n",
      "20:00    Activation function:    tanh\n",
      "20:00    Batch size:             128\n",
      "20:00    Trainer:                amsgrad\n",
      "20:00    Epochs:                 50\n",
      "20:00    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:00    Validation split:       0.25\n",
      "20:00    Early stopping:         True\n",
      "20:00    Scale inputs:           True\n",
      "20:00  Loading training data\n",
      "20:00  Found 1000000 samples with 2 parameters and 26 observables\n",
      "20:00  Rescaling inputs\n",
      "20:00  Creating model for method sally\n",
      "20:00  Training model\n",
      "20:03    Epoch 5: train loss 20.89 ([20.89499595]), validation loss 27.57 ([27.57028957])\n",
      "20:06    Epoch 10: train loss 20.81 ([20.80755088]), validation loss 27.53 ([27.53279413])\n",
      "20:09    Epoch 15: train loss 20.59 ([20.59372121]), validation loss 27.58 ([27.57992551])\n",
      "20:12    Epoch 20: train loss 20.23 ([20.2259945]), validation loss 27.60 ([27.59946052])\n",
      "20:15    Epoch 25: train loss 19.80 ([19.80221336]), validation loss 27.59 ([27.59333095])\n",
      "20:17    Epoch 30: train loss 19.41 ([19.41262075]), validation loss 27.76 ([27.76180426])\n",
      "20:20    Epoch 35: train loss 19.09 ([19.0858067]), validation loss 27.77 ([27.76780632])\n",
      "20:22  No improvement for 20 epochs, stopping training\n",
      "20:22  Early stopping after epoch 18, with loss 27.51 compared to final loss 27.78\n",
      "20:22  Finished training\n",
      "20:22  Training estimator 4 / 10 in ensemble\n",
      "20:22  Starting training\n",
      "20:22    Method:                 sally\n",
      "20:22    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "20:22                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "20:22    Features:               all\n",
      "20:22    Method:                 sally\n",
      "20:22    Hidden layers:          (100, 100)\n",
      "20:22    Activation function:    tanh\n",
      "20:22    Batch size:             128\n",
      "20:22    Trainer:                amsgrad\n",
      "20:22    Epochs:                 50\n",
      "20:22    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:22    Validation split:       0.25\n",
      "20:22    Early stopping:         True\n",
      "20:22    Scale inputs:           True\n",
      "20:22  Loading training data\n",
      "20:22  Found 1000000 samples with 2 parameters and 26 observables\n",
      "20:22  Rescaling inputs\n",
      "20:22  Creating model for method sally\n",
      "20:22  Training model\n",
      "20:25    Epoch 5: train loss 34.54 ([34.53927564]), validation loss 20.01 ([20.01313406])\n",
      "20:28    Epoch 10: train loss 34.43 ([34.4335254]), validation loss 20.06 ([20.06273238])\n",
      "20:31    Epoch 15: train loss 34.17 ([34.16913533]), validation loss 20.03 ([20.02614393])\n",
      "20:34    Epoch 20: train loss 33.76 ([33.76462306]), validation loss 20.21 ([20.21164841])\n",
      "20:37    Epoch 25: train loss 33.27 ([33.27288339]), validation loss 20.09 ([20.09250042])\n",
      "20:39  No improvement for 20 epochs, stopping training\n",
      "20:39  Early stopping after epoch 9, with loss 20.01 compared to final loss 20.28\n",
      "20:39  Finished training\n",
      "20:39  Training estimator 5 / 10 in ensemble\n",
      "20:39  Starting training\n",
      "20:39    Method:                 sally\n",
      "20:39    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "20:39                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "20:39    Features:               all\n",
      "20:39    Method:                 sally\n",
      "20:39    Hidden layers:          (100, 100)\n",
      "20:39    Activation function:    tanh\n",
      "20:39    Batch size:             128\n",
      "20:39    Trainer:                amsgrad\n",
      "20:39    Epochs:                 50\n",
      "20:39    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:39    Validation split:       0.25\n",
      "20:39    Early stopping:         True\n",
      "20:39    Scale inputs:           True\n",
      "20:39  Loading training data\n",
      "20:39  Found 1000000 samples with 2 parameters and 26 observables\n",
      "20:39  Rescaling inputs\n",
      "20:39  Creating model for method sally\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39  Training model\n",
      "20:42    Epoch 5: train loss 24.57 ([24.57243346]), validation loss 44.46 ([44.46122286])\n",
      "20:45    Epoch 10: train loss 24.50 ([24.50174225]), validation loss 44.42 ([44.41926205]) (*)\n",
      "20:48    Epoch 15: train loss 24.34 ([24.33919337]), validation loss 44.46 ([44.46456015])\n",
      "20:51    Epoch 20: train loss 24.07 ([24.06517427]), validation loss 44.65 ([44.65279397])\n",
      "20:54    Epoch 25: train loss 23.69 ([23.69379805]), validation loss 44.78 ([44.7771647])\n",
      "20:56    Epoch 30: train loss 23.34 ([23.33539544]), validation loss 44.98 ([44.9761648])\n",
      "20:57  No improvement for 20 epochs, stopping training\n",
      "20:57  Early stopping after epoch 11, with loss 44.41 compared to final loss 44.99\n",
      "20:57  Finished training\n",
      "20:57  Training estimator 6 / 10 in ensemble\n",
      "20:57  Starting training\n",
      "20:57    Method:                 sally\n",
      "20:57    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "20:57                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "20:57    Features:               all\n",
      "20:57    Method:                 sally\n",
      "20:57    Hidden layers:          (100, 100)\n",
      "20:57    Activation function:    tanh\n",
      "20:57    Batch size:             128\n",
      "20:57    Trainer:                amsgrad\n",
      "20:57    Epochs:                 50\n",
      "20:57    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:57    Validation split:       0.25\n",
      "20:57    Early stopping:         True\n",
      "20:57    Scale inputs:           True\n",
      "20:57  Loading training data\n",
      "20:57  Found 1000000 samples with 2 parameters and 26 observables\n",
      "20:57  Rescaling inputs\n",
      "20:57  Creating model for method sally\n",
      "20:57  Training model\n",
      "21:00    Epoch 5: train loss 52.01 ([52.01211249]), validation loss 14.29 ([14.29237695]) (*)\n",
      "21:03    Epoch 10: train loss 51.88 ([51.88484048]), validation loss 14.31 ([14.30726612])\n",
      "21:06    Epoch 15: train loss 51.63 ([51.62768508]), validation loss 14.34 ([14.3420644])\n",
      "21:09    Epoch 20: train loss 51.26 ([51.25974379]), validation loss 14.35 ([14.35290411])\n",
      "21:12    Epoch 25: train loss 50.90 ([50.90489479]), validation loss 14.46 ([14.46044315])\n",
      "21:13  No improvement for 20 epochs, stopping training\n",
      "21:13  Early stopping after epoch 8, with loss 14.29 compared to final loss 14.48\n",
      "21:13  Finished training\n",
      "21:13  Training estimator 7 / 10 in ensemble\n",
      "21:13  Starting training\n",
      "21:13    Method:                 sally\n",
      "21:13    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "21:13                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "21:13    Features:               all\n",
      "21:13    Method:                 sally\n",
      "21:13    Hidden layers:          (100, 100)\n",
      "21:13    Activation function:    tanh\n",
      "21:13    Batch size:             128\n",
      "21:13    Trainer:                amsgrad\n",
      "21:13    Epochs:                 50\n",
      "21:13    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:13    Validation split:       0.25\n",
      "21:13    Early stopping:         True\n",
      "21:13    Scale inputs:           True\n",
      "21:13  Loading training data\n",
      "21:13  Found 1000000 samples with 2 parameters and 26 observables\n",
      "21:13  Rescaling inputs\n",
      "21:13  Creating model for method sally\n",
      "21:13  Training model\n",
      "21:17    Epoch 5: train loss 19.05 ([19.04909518]), validation loss 26.44 ([26.44067398]) (*)\n",
      "21:19    Epoch 10: train loss 18.94 ([18.94461894]), validation loss 26.41 ([26.41459451]) (*)\n",
      "21:22    Epoch 15: train loss 18.73 ([18.72889494]), validation loss 26.42 ([26.4227578])\n",
      "21:25    Epoch 20: train loss 18.39 ([18.39189077]), validation loss 26.40 ([26.39835188]) (*)\n",
      "21:28    Epoch 25: train loss 17.98 ([17.9838636]), validation loss 26.34 ([26.3375137]) (*)\n",
      "21:31    Epoch 30: train loss 17.54 ([17.54324393]), validation loss 26.55 ([26.55420463])\n",
      "21:34    Epoch 35: train loss 17.17 ([17.17081612]), validation loss 26.89 ([26.88769411])\n",
      "21:36    Epoch 40: train loss 16.87 ([16.87003691]), validation loss 26.76 ([26.76332555])\n",
      "21:39    Epoch 45: train loss 16.62 ([16.61933863]), validation loss 26.65 ([26.65112822])\n",
      "21:39  No improvement for 20 epochs, stopping training\n",
      "21:39  Early stopping after epoch 25, with loss 26.34 compared to final loss 26.65\n",
      "21:39  Finished training\n",
      "21:39  Training estimator 8 / 10 in ensemble\n",
      "21:39  Starting training\n",
      "21:39    Method:                 sally\n",
      "21:39    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "21:39                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "21:39    Features:               all\n",
      "21:39    Method:                 sally\n",
      "21:39    Hidden layers:          (100, 100)\n",
      "21:39    Activation function:    tanh\n",
      "21:39    Batch size:             128\n",
      "21:39    Trainer:                amsgrad\n",
      "21:39    Epochs:                 50\n",
      "21:39    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:39    Validation split:       0.25\n",
      "21:39    Early stopping:         True\n",
      "21:39    Scale inputs:           True\n",
      "21:39  Loading training data\n",
      "21:39  Found 1000000 samples with 2 parameters and 26 observables\n",
      "21:39  Rescaling inputs\n",
      "21:39  Creating model for method sally\n",
      "21:39  Training model\n",
      "21:43    Epoch 5: train loss 27.74 ([27.74084474]), validation loss 55.21 ([55.21397043])\n",
      "21:45    Epoch 10: train loss 27.44 ([27.43578695]), validation loss 55.37 ([55.36882242])\n",
      "21:48    Epoch 15: train loss 27.04 ([27.03889254]), validation loss 55.48 ([55.47815144])\n",
      "21:51    Epoch 20: train loss 26.64 ([26.63809127]), validation loss 55.57 ([55.57243204])\n",
      "21:52  No improvement for 20 epochs, stopping training\n",
      "21:52  Early stopping after epoch 1, with loss 55.04 compared to final loss 55.44\n",
      "21:52  Finished training\n",
      "21:52  Training estimator 9 / 10 in ensemble\n",
      "21:52  Starting training\n",
      "21:52    Method:                 sally\n",
      "21:52    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "21:52                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "21:52    Features:               all\n",
      "21:52    Method:                 sally\n",
      "21:52    Hidden layers:          (100, 100)\n",
      "21:52    Activation function:    tanh\n",
      "21:52    Batch size:             128\n",
      "21:52    Trainer:                amsgrad\n",
      "21:52    Epochs:                 50\n",
      "21:52    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:52    Validation split:       0.25\n",
      "21:52    Early stopping:         True\n",
      "21:52    Scale inputs:           True\n",
      "21:52  Loading training data\n",
      "21:52  Found 1000000 samples with 2 parameters and 26 observables\n",
      "21:52  Rescaling inputs\n",
      "21:52  Creating model for method sally\n",
      "21:52  Training model\n",
      "21:55    Epoch 5: train loss 31.39 ([31.38798206]), validation loss 32.99 ([32.9866938]) (*)\n",
      "21:58    Epoch 10: train loss 31.25 ([31.25257498]), validation loss 32.99 ([32.98797742])\n",
      "22:01    Epoch 15: train loss 30.95 ([30.95361763]), validation loss 33.00 ([32.99748298])\n",
      "22:03    Epoch 20: train loss 30.57 ([30.56629104]), validation loss 33.02 ([33.02241013])\n",
      "22:06    Epoch 25: train loss 30.21 ([30.20686786]), validation loss 33.18 ([33.17607712])\n",
      "22:09    Epoch 30: train loss 29.87 ([29.86878419]), validation loss 33.18 ([33.17907727])\n",
      "22:11  No improvement for 20 epochs, stopping training\n",
      "22:11  Early stopping after epoch 13, with loss 32.97 compared to final loss 33.18\n",
      "22:11  Finished training\n",
      "22:11  Training estimator 10 / 10 in ensemble\n",
      "22:11  Starting training\n",
      "22:11    Method:                 sally\n",
      "22:11    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "22:11                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "22:11    Features:               all\n",
      "22:11    Method:                 sally\n",
      "22:11    Hidden layers:          (100, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:11    Activation function:    tanh\n",
      "22:11    Batch size:             128\n",
      "22:11    Trainer:                amsgrad\n",
      "22:11    Epochs:                 50\n",
      "22:11    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:11    Validation split:       0.25\n",
      "22:11    Early stopping:         True\n",
      "22:11    Scale inputs:           True\n",
      "22:11  Loading training data\n",
      "22:11  Found 1000000 samples with 2 parameters and 26 observables\n",
      "22:11  Rescaling inputs\n",
      "22:11  Creating model for method sally\n",
      "22:11  Training model\n",
      "22:14    Epoch 5: train loss 55.74 ([55.74330641]), validation loss 29.26 ([29.25619013])\n",
      "22:17    Epoch 10: train loss 55.57 ([55.56626205]), validation loss 29.21 ([29.21490525])\n",
      "22:20    Epoch 15: train loss 55.22 ([55.22291917]), validation loss 29.27 ([29.26824157])\n",
      "22:23    Epoch 20: train loss 54.82 ([54.8232569]), validation loss 29.36 ([29.35828795])\n",
      "22:25    Epoch 25: train loss 54.45 ([54.45019626]), validation loss 29.42 ([29.423762])\n",
      "22:28  No improvement for 20 epochs, stopping training\n",
      "22:28  Early stopping after epoch 9, with loss 29.21 compared to final loss 29.46\n",
      "22:28  Finished training\n",
      "22:28  Calculating expectation for 10 estimators in ensemble\n",
      "22:28  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "22:28  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "22:28  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "22:29  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "22:29  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "22:29  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "22:29  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "22:30  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "22:30  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "22:30  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble('all', use_tight_cuts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble('all_tight', use_tight_cuts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'all_tight_sgd',\n",
    "    use_tight_cuts=True,\n",
    "    trainer='sgd',\n",
    "    nesterov_momentum=0.9,\n",
    "    initial_lr=0.1,\n",
    "    final_lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble('resurrection_tight', use_tight_cuts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7d171470-6c7e-4a74-8ca8-5589004e32d6"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
