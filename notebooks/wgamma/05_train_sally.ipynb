{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9c4780e4-775d-4cbd-8f3d-64fbd2b45344"
    }
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "98a6ae38-08cd-4e31-a7a4-5a370df5c84e"
    }
   },
   "outputs": [],
   "source": [
    "n_estimators = 10\n",
    "n_hidden = (100,100)\n",
    "n_epochs = 20\n",
    "batch_size = 128\n",
    "initial_lr = 0.001\n",
    "final_lr = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables, no cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "be3db4db-cc81-42e8-8796-dfc021acf234"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:20  \n",
      "23:20  ------------------------------------------------------------\n",
      "23:20  |                                                          |\n",
      "23:20  |  MadMiner v2018.10.30                                    |\n",
      "23:20  |                                                          |\n",
      "23:20  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "23:20  |                                                          |\n",
      "23:20  ------------------------------------------------------------\n",
      "23:20  \n",
      "23:20  Training 10 estimators in ensemble\n",
      "23:20  Training estimator 1 / 10 in ensemble\n",
      "23:20  Starting training\n",
      "23:20    Method:                 sally\n",
      "23:20    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "23:20                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "23:20    Features:               all\n",
      "23:20    Method:                 sally\n",
      "23:20    Hidden layers:          (100, 100)\n",
      "23:20    Activation function:    tanh\n",
      "23:20    Batch size:             128\n",
      "23:20    Epochs:                 20\n",
      "23:20    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:20    Validation split:       None\n",
      "23:20    Early stopping:         True\n",
      "23:20  Loading training data\n",
      "23:20  Found 1000000 samples with 2 parameters and 30 observables\n",
      "23:20  Creating model for method sally\n",
      "23:20  Training model\n",
      "23:21    Epoch 2: train loss 23.91 ([23.91430155])\n",
      "23:21    Epoch 4: train loss 23.90 ([23.90254064])\n",
      "23:22    Epoch 6: train loss 23.89 ([23.89448383])\n",
      "23:23    Epoch 8: train loss 23.89 ([23.89442803])\n",
      "23:23    Epoch 10: train loss 23.88 ([23.88285281])\n",
      "23:24    Epoch 12: train loss 23.88 ([23.87631807])\n",
      "23:25    Epoch 14: train loss 23.87 ([23.87255636])\n",
      "23:25    Epoch 16: train loss 23.87 ([23.87088354])\n",
      "23:26    Epoch 18: train loss 23.87 ([23.86895975])\n",
      "23:27    Epoch 20: train loss 23.87 ([23.86871297])\n",
      "23:27  Finished training\n",
      "23:27  Training estimator 2 / 10 in ensemble\n",
      "23:27  Starting training\n",
      "23:27    Method:                 sally\n",
      "23:27    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "23:27                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "23:27    Features:               all\n",
      "23:27    Method:                 sally\n",
      "23:27    Hidden layers:          (100, 100)\n",
      "23:27    Activation function:    tanh\n",
      "23:27    Batch size:             128\n",
      "23:27    Epochs:                 20\n",
      "23:27    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:27    Validation split:       None\n",
      "23:27    Early stopping:         True\n",
      "23:27  Loading training data\n",
      "23:27  Found 1000000 samples with 2 parameters and 30 observables\n",
      "23:27  Creating model for method sally\n",
      "23:27  Training model\n",
      "23:27    Epoch 2: train loss 23.37 ([23.3740989])\n",
      "23:28    Epoch 4: train loss 23.36 ([23.36171114])\n",
      "23:29    Epoch 6: train loss 23.36 ([23.35556324])\n",
      "23:29    Epoch 8: train loss 23.35 ([23.34918097])\n",
      "23:30    Epoch 10: train loss 23.34 ([23.34470906])\n",
      "23:31    Epoch 12: train loss 23.34 ([23.3413177])\n",
      "23:31    Epoch 14: train loss 23.34 ([23.33611877])\n",
      "23:32    Epoch 16: train loss 23.33 ([23.3334888])\n",
      "23:32    Epoch 18: train loss 23.33 ([23.32980929])\n",
      "23:33    Epoch 20: train loss 23.33 ([23.3265937])\n",
      "23:33  Finished training\n",
      "23:33  Training estimator 3 / 10 in ensemble\n",
      "23:33  Starting training\n",
      "23:33    Method:                 sally\n",
      "23:33    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "23:33                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "23:33    Features:               all\n",
      "23:33    Method:                 sally\n",
      "23:33    Hidden layers:          (100, 100)\n",
      "23:33    Activation function:    tanh\n",
      "23:33    Batch size:             128\n",
      "23:33    Epochs:                 20\n",
      "23:33    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:33    Validation split:       None\n",
      "23:33    Early stopping:         True\n",
      "23:33  Loading training data\n",
      "23:33  Found 1000000 samples with 2 parameters and 30 observables\n",
      "23:33  Creating model for method sally\n",
      "23:33  Training model\n",
      "23:34    Epoch 2: train loss 36.52 ([36.52218379])\n",
      "23:35    Epoch 4: train loss 36.51 ([36.51174841])\n",
      "23:35    Epoch 6: train loss 36.50 ([36.50120531])\n",
      "23:36    Epoch 8: train loss 36.49 ([36.49325424])\n",
      "23:37    Epoch 10: train loss 36.49 ([36.48751633])\n",
      "23:37    Epoch 12: train loss 36.48 ([36.48050974])\n",
      "23:38    Epoch 14: train loss 36.48 ([36.47685042])\n",
      "23:39    Epoch 16: train loss 36.47 ([36.47189227])\n",
      "23:39    Epoch 18: train loss 36.47 ([36.46985499])\n",
      "23:40    Epoch 20: train loss 36.50 ([36.49825255])\n",
      "23:40  Finished training\n",
      "23:40  Training estimator 4 / 10 in ensemble\n",
      "23:40  Starting training\n",
      "23:40    Method:                 sally\n",
      "23:40    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "23:40                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "23:40    Features:               all\n",
      "23:40    Method:                 sally\n",
      "23:40    Hidden layers:          (100, 100)\n",
      "23:40    Activation function:    tanh\n",
      "23:40    Batch size:             128\n",
      "23:40    Epochs:                 20\n",
      "23:40    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:40    Validation split:       None\n",
      "23:40    Early stopping:         True\n",
      "23:40  Loading training data\n",
      "23:40  Found 1000000 samples with 2 parameters and 30 observables\n",
      "23:40  Creating model for method sally\n",
      "23:40  Training model\n",
      "23:41    Epoch 2: train loss 31.35 ([31.35111944])\n",
      "23:41    Epoch 4: train loss 31.34 ([31.33652749])\n",
      "23:42    Epoch 6: train loss 31.32 ([31.31865531])\n",
      "23:43    Epoch 8: train loss 31.32 ([31.32381157])\n",
      "23:43    Epoch 10: train loss 31.30 ([31.30496398])\n",
      "23:44    Epoch 12: train loss 31.29 ([31.2945453])\n",
      "23:45    Epoch 14: train loss 31.28 ([31.28439496])\n",
      "23:46    Epoch 16: train loss 31.30 ([31.29660163])\n",
      "23:46    Epoch 18: train loss 31.29 ([31.29340704])\n",
      "23:47    Epoch 20: train loss 31.28 ([31.28175111])\n",
      "23:47  Finished training\n",
      "23:47  Training estimator 5 / 10 in ensemble\n",
      "23:47  Starting training\n",
      "23:47    Method:                 sally\n",
      "23:47    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "23:47                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "23:47    Features:               all\n",
      "23:47    Method:                 sally\n",
      "23:47    Hidden layers:          (100, 100)\n",
      "23:47    Activation function:    tanh\n",
      "23:47    Batch size:             128\n",
      "23:47    Epochs:                 20\n",
      "23:47    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:47    Validation split:       None\n",
      "23:47    Early stopping:         True\n",
      "23:47  Loading training data\n",
      "23:47  Found 1000000 samples with 2 parameters and 30 observables\n",
      "23:47  Creating model for method sally\n",
      "23:47  Training model\n",
      "23:48    Epoch 2: train loss 29.83 ([29.83493623])\n",
      "23:48    Epoch 4: train loss 29.82 ([29.82222525])\n",
      "23:49    Epoch 6: train loss 29.81 ([29.81304476])\n",
      "23:50    Epoch 8: train loss 29.80 ([29.80472205])\n",
      "23:50    Epoch 10: train loss 29.81 ([29.80844994])\n",
      "23:51    Epoch 12: train loss 29.79 ([29.78975417])\n",
      "23:52    Epoch 14: train loss 29.78 ([29.78411203])\n",
      "23:52    Epoch 16: train loss 29.78 ([29.77694599])\n",
      "23:53    Epoch 18: train loss 29.78 ([29.77546938])\n",
      "23:54    Epoch 20: train loss 29.77 ([29.77273598])\n",
      "23:54  Finished training\n",
      "23:54  Training estimator 6 / 10 in ensemble\n",
      "23:54  Starting training\n",
      "23:54    Method:                 sally\n",
      "23:54    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "23:54                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:54    Features:               all\n",
      "23:54    Method:                 sally\n",
      "23:54    Hidden layers:          (100, 100)\n",
      "23:54    Activation function:    tanh\n",
      "23:54    Batch size:             128\n",
      "23:54    Epochs:                 20\n",
      "23:54    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:54    Validation split:       None\n",
      "23:54    Early stopping:         True\n",
      "23:54  Loading training data\n",
      "23:54  Found 1000000 samples with 2 parameters and 30 observables\n",
      "23:54  Creating model for method sally\n",
      "23:54  Training model\n",
      "23:55    Epoch 2: train loss 56.59 ([56.58926711])\n",
      "23:55    Epoch 4: train loss 56.60 ([56.59739756])\n",
      "23:56    Epoch 6: train loss 56.57 ([56.56795738])\n",
      "23:57    Epoch 8: train loss 56.56 ([56.56229127])\n",
      "23:57    Epoch 10: train loss 56.55 ([56.55433023])\n",
      "23:58    Epoch 12: train loss 56.55 ([56.55034327])\n",
      "23:59    Epoch 14: train loss 56.55 ([56.54904562])\n",
      "23:59    Epoch 16: train loss 56.54 ([56.54363289])\n",
      "00:00    Epoch 18: train loss 56.54 ([56.54154374])\n",
      "00:01    Epoch 20: train loss 56.54 ([56.53765809])\n",
      "00:01  Finished training\n",
      "00:01  Training estimator 7 / 10 in ensemble\n",
      "00:01  Starting training\n",
      "00:01    Method:                 sally\n",
      "00:01    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "00:01                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "00:01    Features:               all\n",
      "00:01    Method:                 sally\n",
      "00:01    Hidden layers:          (100, 100)\n",
      "00:01    Activation function:    tanh\n",
      "00:01    Batch size:             128\n",
      "00:01    Epochs:                 20\n",
      "00:01    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:01    Validation split:       None\n",
      "00:01    Early stopping:         True\n",
      "00:01  Loading training data\n",
      "00:01  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:01  Creating model for method sally\n",
      "00:01  Training model\n",
      "00:01    Epoch 2: train loss 32.60 ([32.5987147])\n",
      "00:02    Epoch 4: train loss 32.59 ([32.58772107])\n",
      "00:03    Epoch 6: train loss 32.58 ([32.57656023])\n",
      "00:03    Epoch 8: train loss 32.57 ([32.56851001])\n",
      "00:04    Epoch 10: train loss 32.56 ([32.56019248])\n",
      "00:05    Epoch 12: train loss 32.56 ([32.55858278])\n",
      "00:05    Epoch 14: train loss 32.55 ([32.55303161])\n",
      "00:06    Epoch 16: train loss 32.55 ([32.54983783])\n",
      "00:07    Epoch 18: train loss 32.55 ([32.54990596])\n",
      "00:07    Epoch 20: train loss 32.55 ([32.54679303])\n",
      "00:07  Finished training\n",
      "00:07  Training estimator 8 / 10 in ensemble\n",
      "00:07  Starting training\n",
      "00:07    Method:                 sally\n",
      "00:07    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "00:07                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "00:07    Features:               all\n",
      "00:07    Method:                 sally\n",
      "00:07    Hidden layers:          (100, 100)\n",
      "00:07    Activation function:    tanh\n",
      "00:07    Batch size:             128\n",
      "00:07    Epochs:                 20\n",
      "00:07    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:07    Validation split:       None\n",
      "00:07    Early stopping:         True\n",
      "00:07  Loading training data\n",
      "00:07  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:07  Creating model for method sally\n",
      "00:07  Training model\n",
      "00:08    Epoch 2: train loss 25.37 ([25.37290613])\n",
      "00:09    Epoch 4: train loss 25.36 ([25.36093766])\n",
      "00:09    Epoch 6: train loss 25.35 ([25.35231402])\n",
      "00:10    Epoch 8: train loss 25.34 ([25.34420565])\n",
      "00:11    Epoch 10: train loss 25.34 ([25.33819171])\n",
      "00:11    Epoch 12: train loss 25.33 ([25.33242534])\n",
      "00:12    Epoch 14: train loss 25.33 ([25.32670129])\n",
      "00:13    Epoch 16: train loss 25.32 ([25.32401085])\n",
      "00:13    Epoch 18: train loss 25.32 ([25.32264246])\n",
      "00:14    Epoch 20: train loss 25.32 ([25.31950051])\n",
      "00:14  Finished training\n",
      "00:14  Training estimator 9 / 10 in ensemble\n",
      "00:14  Starting training\n",
      "00:14    Method:                 sally\n",
      "00:14    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "00:14                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "00:14    Features:               all\n",
      "00:14    Method:                 sally\n",
      "00:14    Hidden layers:          (100, 100)\n",
      "00:14    Activation function:    tanh\n",
      "00:14    Batch size:             128\n",
      "00:14    Epochs:                 20\n",
      "00:14    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:14    Validation split:       None\n",
      "00:14    Early stopping:         True\n",
      "00:14  Loading training data\n",
      "00:14  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:14  Creating model for method sally\n",
      "00:14  Training model\n",
      "00:15    Epoch 2: train loss 47.71 ([47.70806964])\n",
      "00:16    Epoch 4: train loss 47.70 ([47.69728027])\n",
      "00:16    Epoch 6: train loss 47.68 ([47.68195466])\n",
      "00:17    Epoch 8: train loss 47.68 ([47.67551559])\n",
      "00:18    Epoch 10: train loss 47.67 ([47.66983866])\n",
      "00:18    Epoch 12: train loss 47.66 ([47.66345249])\n",
      "00:19    Epoch 14: train loss 47.66 ([47.66193809])\n",
      "00:20    Epoch 16: train loss 47.66 ([47.65573591])\n",
      "00:20    Epoch 18: train loss 47.66 ([47.65662615])\n",
      "00:21    Epoch 20: train loss 47.65 ([47.6548044])\n",
      "00:21  Finished training\n",
      "00:21  Training estimator 10 / 10 in ensemble\n",
      "00:21  Starting training\n",
      "00:21    Method:                 sally\n",
      "00:21    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "00:21                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "00:21    Features:               all\n",
      "00:21    Method:                 sally\n",
      "00:21    Hidden layers:          (100, 100)\n",
      "00:21    Activation function:    tanh\n",
      "00:21    Batch size:             128\n",
      "00:21    Epochs:                 20\n",
      "00:21    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:21    Validation split:       None\n",
      "00:21    Early stopping:         True\n",
      "00:21  Loading training data\n",
      "00:21  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:21  Creating model for method sally\n",
      "00:21  Training model\n",
      "00:22    Epoch 2: train loss 59.45 ([59.45181596])\n",
      "00:22    Epoch 4: train loss 59.44 ([59.43922117])\n",
      "00:23    Epoch 6: train loss 59.43 ([59.42749095])\n",
      "00:24    Epoch 8: train loss 59.42 ([59.41735108])\n",
      "00:24    Epoch 10: train loss 59.41 ([59.41024444])\n",
      "00:25    Epoch 12: train loss 59.40 ([59.40400973])\n",
      "00:26    Epoch 14: train loss 59.40 ([59.40087322])\n",
      "00:26    Epoch 16: train loss 59.40 ([59.39651577])\n",
      "00:27    Epoch 18: train loss 59.39 ([59.38706805])\n",
      "00:28    Epoch 20: train loss 59.38 ([59.38068952])\n",
      "00:28  Finished training\n",
      "00:28  Calculating expectation for 10 estimators in ensemble\n",
      "00:28  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "00:28  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "00:28  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "00:28  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "00:28  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "00:29  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "00:29  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "00:29  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "00:29  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "00:29  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "ensemble_all = EnsembleForge(n_estimators)\n",
    "\n",
    "ensemble_all.train_all(\n",
    "    method='sally',\n",
    "    x_filename=[sample_dir + 'train_local/x_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    t_xz0_filename=[sample_dir + 'train_local/t_xz_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=None,\n",
    "    n_hidden=n_hidden\n",
    ")\n",
    "\n",
    "ensemble_all.calculate_expectation(\n",
    "    x_filename=sample_dir + 'validation/x_validation.npy'\n",
    ")\n",
    "\n",
    "ensemble_all.save(model_dir + 'sally_ensemble_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All observables after cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "add9d876-2e73-4578-8d43-722df0c790d5"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:29  Training 10 estimators in ensemble\n",
      "00:29  Training estimator 1 / 10 in ensemble\n",
      "00:29  Starting training\n",
      "00:29    Method:                 sally\n",
      "00:29    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "00:29                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "00:29    Features:               all\n",
      "00:29    Method:                 sally\n",
      "00:29    Hidden layers:          (100, 100)\n",
      "00:29    Activation function:    tanh\n",
      "00:29    Batch size:             128\n",
      "00:29    Epochs:                 20\n",
      "00:29    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:29    Validation split:       None\n",
      "00:29    Early stopping:         True\n",
      "00:29  Loading training data\n",
      "00:29  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:29  Creating model for method sally\n",
      "00:29  Training model\n",
      "00:30    Epoch 2: train loss 6849.83 ([6849.83215127])\n",
      "00:31    Epoch 4: train loss 6849.76 ([6849.76489455])\n",
      "00:32    Epoch 6: train loss 6849.58 ([6849.57505885])\n",
      "00:32    Epoch 8: train loss 6849.67 ([6849.67358063])\n",
      "00:33    Epoch 10: train loss 6849.43 ([6849.43363735])\n",
      "00:34    Epoch 12: train loss 6854.74 ([6854.74048953])\n",
      "00:35    Epoch 14: train loss 6849.82 ([6849.82358434])\n",
      "00:35    Epoch 16: train loss 6849.32 ([6849.31668612])\n",
      "00:36    Epoch 18: train loss 6849.30 ([6849.30161898])\n",
      "00:37    Epoch 20: train loss 6849.31 ([6849.30846434])\n",
      "00:37  Finished training\n",
      "00:37  Training estimator 2 / 10 in ensemble\n",
      "00:37  Starting training\n",
      "00:37    Method:                 sally\n",
      "00:37    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "00:37                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "00:37    Features:               all\n",
      "00:37    Method:                 sally\n",
      "00:37    Hidden layers:          (100, 100)\n",
      "00:37    Activation function:    tanh\n",
      "00:37    Batch size:             128\n",
      "00:37    Epochs:                 20\n",
      "00:37    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:37    Validation split:       None\n",
      "00:37    Early stopping:         True\n",
      "00:37  Loading training data\n",
      "00:37  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:37  Creating model for method sally\n",
      "00:37  Training model\n",
      "00:38    Epoch 2: train loss 5428.06 ([5428.05617341])\n",
      "00:39    Epoch 4: train loss 5427.90 ([5427.89722787])\n",
      "00:39    Epoch 6: train loss 5427.78 ([5427.78317564])\n",
      "00:40    Epoch 8: train loss 5427.81 ([5427.81053977])\n",
      "00:41    Epoch 10: train loss 5427.69 ([5427.69136815])\n",
      "00:41    Epoch 12: train loss 5427.61 ([5427.61322453])\n",
      "00:42    Epoch 14: train loss 5427.54 ([5427.5387318])\n",
      "00:43    Epoch 16: train loss 5427.67 ([5427.67470404])\n",
      "00:44    Epoch 18: train loss 5427.55 ([5427.55211868])\n",
      "00:44    Epoch 20: train loss 5427.53 ([5427.52512804])\n",
      "00:44  Finished training\n",
      "00:44  Training estimator 3 / 10 in ensemble\n",
      "00:44  Starting training\n",
      "00:44    Method:                 sally\n",
      "00:44    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "00:44                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "00:44    Features:               all\n",
      "00:44    Method:                 sally\n",
      "00:44    Hidden layers:          (100, 100)\n",
      "00:44    Activation function:    tanh\n",
      "00:44    Batch size:             128\n",
      "00:44    Epochs:                 20\n",
      "00:44    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:44    Validation split:       None\n",
      "00:44    Early stopping:         True\n",
      "00:44  Loading training data\n",
      "00:44  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:44  Creating model for method sally\n",
      "00:44  Training model\n",
      "00:45    Epoch 2: train loss 5927.44 ([5927.44353791])\n",
      "00:46    Epoch 4: train loss 5927.79 ([5927.79276975])\n",
      "00:47    Epoch 6: train loss 5927.16 ([5927.15803852])\n",
      "00:47    Epoch 8: train loss 5927.21 ([5927.20995441])\n",
      "00:48    Epoch 10: train loss 5927.12 ([5927.12483855])\n",
      "00:49    Epoch 12: train loss 5926.99 ([5926.98658749])\n",
      "00:50    Epoch 14: train loss 5927.15 ([5927.14539828])\n",
      "00:50    Epoch 16: train loss 5926.98 ([5926.98063628])\n",
      "00:51    Epoch 18: train loss 5926.80 ([5926.79978968])\n",
      "00:52    Epoch 20: train loss 5926.69 ([5926.69325568])\n",
      "00:52  Finished training\n",
      "00:52  Training estimator 4 / 10 in ensemble\n",
      "00:52  Starting training\n",
      "00:52    Method:                 sally\n",
      "00:52    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_3.npy\n",
      "00:52                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_3.npy\n",
      "00:52    Features:               all\n",
      "00:52    Method:                 sally\n",
      "00:52    Hidden layers:          (100, 100)\n",
      "00:52    Activation function:    tanh\n",
      "00:52    Batch size:             128\n",
      "00:52    Epochs:                 20\n",
      "00:52    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:52    Validation split:       None\n",
      "00:52    Early stopping:         True\n",
      "00:52  Loading training data\n",
      "00:52  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:52  Creating model for method sally\n",
      "00:52  Training model\n",
      "00:53    Epoch 2: train loss 8716.91 ([8716.90599425])\n",
      "00:53    Epoch 4: train loss 8716.74 ([8716.74025515])\n",
      "00:54    Epoch 6: train loss 8716.71 ([8716.71239605])\n",
      "00:55    Epoch 8: train loss 8716.82 ([8716.82029472])\n",
      "00:55    Epoch 10: train loss 8716.49 ([8716.48582515])\n",
      "00:56    Epoch 12: train loss 8716.48 ([8716.48083302])\n",
      "00:57    Epoch 14: train loss 8716.31 ([8716.31202443])\n",
      "00:58    Epoch 16: train loss 8716.36 ([8716.35971289])\n",
      "00:58    Epoch 18: train loss 8716.25 ([8716.24989928])\n",
      "00:59    Epoch 20: train loss 8720.26 ([8720.25815034])\n",
      "00:59  Finished training\n",
      "00:59  Training estimator 5 / 10 in ensemble\n",
      "00:59  Starting training\n",
      "00:59    Method:                 sally\n",
      "00:59    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_4.npy\n",
      "00:59                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_4.npy\n",
      "00:59    Features:               all\n",
      "00:59    Method:                 sally\n",
      "00:59    Hidden layers:          (100, 100)\n",
      "00:59    Activation function:    tanh\n",
      "00:59    Batch size:             128\n",
      "00:59    Epochs:                 20\n",
      "00:59    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:59    Validation split:       None\n",
      "00:59    Early stopping:         True\n",
      "00:59  Loading training data\n",
      "00:59  Found 1000000 samples with 2 parameters and 30 observables\n",
      "00:59  Creating model for method sally\n",
      "00:59  Training model\n",
      "01:00    Epoch 2: train loss 7222.96 ([7222.95905969])\n",
      "01:01    Epoch 4: train loss 7222.96 ([7222.96177399])\n",
      "01:01    Epoch 6: train loss 7223.94 ([7223.93866666])\n",
      "01:02    Epoch 8: train loss 7222.64 ([7222.63780648])\n",
      "01:03    Epoch 10: train loss 7222.49 ([7222.48879516])\n",
      "01:04    Epoch 12: train loss 7222.46 ([7222.45928974])\n",
      "01:04    Epoch 14: train loss 7222.39 ([7222.39477747])\n",
      "01:05    Epoch 16: train loss 7222.40 ([7222.397761])\n",
      "01:06    Epoch 18: train loss 7223.21 ([7223.20775325])\n",
      "01:07    Epoch 20: train loss 7222.43 ([7222.42722751])\n",
      "01:07  Finished training\n",
      "01:07  Training estimator 6 / 10 in ensemble\n",
      "01:07  Starting training\n",
      "01:07    Method:                 sally\n",
      "01:07    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_5.npy\n",
      "01:07                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_5.npy\n",
      "01:07    Features:               all\n",
      "01:07    Method:                 sally\n",
      "01:07    Hidden layers:          (100, 100)\n",
      "01:07    Activation function:    tanh\n",
      "01:07    Batch size:             128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:07    Epochs:                 20\n",
      "01:07    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:07    Validation split:       None\n",
      "01:07    Early stopping:         True\n",
      "01:07  Loading training data\n",
      "01:07  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:07  Creating model for method sally\n",
      "01:07  Training model\n",
      "01:08    Epoch 2: train loss 5936.42 ([5936.42377793])\n",
      "01:08    Epoch 4: train loss 5936.33 ([5936.33406714])\n",
      "01:09    Epoch 6: train loss 5936.21 ([5936.20501747])\n",
      "01:10    Epoch 8: train loss 5936.12 ([5936.12423156])\n",
      "01:10    Epoch 10: train loss 5936.02 ([5936.01639746])\n",
      "01:11    Epoch 12: train loss 5935.95 ([5935.95193111])\n",
      "01:12    Epoch 14: train loss 5935.98 ([5935.97629038])\n",
      "01:13    Epoch 16: train loss 5935.89 ([5935.89336076])\n",
      "01:13    Epoch 18: train loss 5935.90 ([5935.89978681])\n",
      "01:14    Epoch 20: train loss 5935.92 ([5935.92130921])\n",
      "01:14  Finished training\n",
      "01:14  Training estimator 7 / 10 in ensemble\n",
      "01:14  Starting training\n",
      "01:14    Method:                 sally\n",
      "01:14    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_6.npy\n",
      "01:14                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_6.npy\n",
      "01:14    Features:               all\n",
      "01:14    Method:                 sally\n",
      "01:14    Hidden layers:          (100, 100)\n",
      "01:14    Activation function:    tanh\n",
      "01:14    Batch size:             128\n",
      "01:14    Epochs:                 20\n",
      "01:14    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:14    Validation split:       None\n",
      "01:14    Early stopping:         True\n",
      "01:14  Loading training data\n",
      "01:14  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:14  Creating model for method sally\n",
      "01:14  Training model\n",
      "01:15    Epoch 2: train loss 7521.90 ([7521.90067098])\n",
      "01:16    Epoch 4: train loss 7521.79 ([7521.79252925])\n",
      "01:16    Epoch 6: train loss 7521.64 ([7521.64440811])\n",
      "01:17    Epoch 8: train loss 7521.71 ([7521.70588986])\n",
      "01:18    Epoch 10: train loss 7521.52 ([7521.52257313])\n",
      "01:19    Epoch 12: train loss 7521.46 ([7521.46448556])\n",
      "01:20    Epoch 14: train loss 7521.50 ([7521.50347129])\n",
      "01:20    Epoch 16: train loss 7521.35 ([7521.35496699])\n",
      "01:21    Epoch 18: train loss 7521.36 ([7521.35927349])\n",
      "01:22    Epoch 20: train loss 7521.38 ([7521.38443253])\n",
      "01:22  Finished training\n",
      "01:22  Training estimator 8 / 10 in ensemble\n",
      "01:22  Starting training\n",
      "01:22    Method:                 sally\n",
      "01:22    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_7.npy\n",
      "01:22                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_7.npy\n",
      "01:22    Features:               all\n",
      "01:22    Method:                 sally\n",
      "01:22    Hidden layers:          (100, 100)\n",
      "01:22    Activation function:    tanh\n",
      "01:22    Batch size:             128\n",
      "01:22    Epochs:                 20\n",
      "01:22    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:22    Validation split:       None\n",
      "01:22    Early stopping:         True\n",
      "01:22  Loading training data\n",
      "01:22  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:22  Creating model for method sally\n",
      "01:22  Training model\n",
      "01:23    Epoch 2: train loss 6137.90 ([6137.90046025])\n",
      "01:23    Epoch 4: train loss 6137.64 ([6137.63621721])\n",
      "01:24    Epoch 6: train loss 6137.39 ([6137.38630374])\n",
      "01:25    Epoch 8: train loss 6137.41 ([6137.41437455])\n",
      "01:26    Epoch 10: train loss 6137.33 ([6137.33171471])\n",
      "01:26    Epoch 12: train loss 6137.65 ([6137.6470951])\n",
      "01:27    Epoch 14: train loss 6137.49 ([6137.48577742])\n",
      "01:28    Epoch 16: train loss 6137.29 ([6137.29324478])\n",
      "01:29    Epoch 18: train loss 6136.35 ([6136.35358365])\n",
      "01:29    Epoch 20: train loss 6135.72 ([6135.72091112])\n",
      "01:29  Finished training\n",
      "01:29  Training estimator 9 / 10 in ensemble\n",
      "01:29  Starting training\n",
      "01:29    Method:                 sally\n",
      "01:29    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_8.npy\n",
      "01:29                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_8.npy\n",
      "01:29    Features:               all\n",
      "01:29    Method:                 sally\n",
      "01:29    Hidden layers:          (100, 100)\n",
      "01:29    Activation function:    tanh\n",
      "01:29    Batch size:             128\n",
      "01:29    Epochs:                 20\n",
      "01:29    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:29    Validation split:       None\n",
      "01:29    Early stopping:         True\n",
      "01:29  Loading training data\n",
      "01:29  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:29  Creating model for method sally\n",
      "01:29  Training model\n",
      "01:30    Epoch 2: train loss 6386.97 ([6386.97077254])\n",
      "01:31    Epoch 4: train loss 6387.00 ([6387.00232189])\n",
      "01:31    Epoch 6: train loss 6386.52 ([6386.52499253])\n",
      "01:32    Epoch 8: train loss 6387.20 ([6387.19648703])\n",
      "01:33    Epoch 10: train loss 6386.36 ([6386.36365558])\n",
      "01:34    Epoch 12: train loss 6386.58 ([6386.58406568])\n",
      "01:34    Epoch 14: train loss 6421.15 ([6421.14852588])\n",
      "01:35    Epoch 16: train loss 6386.26 ([6386.25760174])\n",
      "01:36    Epoch 18: train loss 6386.23 ([6386.23109076])\n",
      "01:37    Epoch 20: train loss 6386.29 ([6386.29079947])\n",
      "01:37  Finished training\n",
      "01:37  Training estimator 10 / 10 in ensemble\n",
      "01:37  Starting training\n",
      "01:37    Method:                 sally\n",
      "01:37    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_9.npy\n",
      "01:37                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_9.npy\n",
      "01:37    Features:               all\n",
      "01:37    Method:                 sally\n",
      "01:37    Hidden layers:          (100, 100)\n",
      "01:37    Activation function:    tanh\n",
      "01:37    Batch size:             128\n",
      "01:37    Epochs:                 20\n",
      "01:37    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:37    Validation split:       None\n",
      "01:37    Early stopping:         True\n",
      "01:37  Loading training data\n",
      "01:37  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:37  Creating model for method sally\n",
      "01:37  Training model\n",
      "01:37    Epoch 2: train loss 5649.59 ([5649.5934308])\n",
      "01:38    Epoch 4: train loss 5649.43 ([5649.43489181])\n",
      "01:39    Epoch 6: train loss 5649.50 ([5649.50228594])\n",
      "01:40    Epoch 8: train loss 5649.21 ([5649.21156258])\n",
      "01:40    Epoch 10: train loss 5649.15 ([5649.14938446])\n",
      "01:41    Epoch 12: train loss 5649.17 ([5649.17499736])\n",
      "01:42    Epoch 14: train loss 5650.55 ([5650.55008643])\n",
      "01:42    Epoch 16: train loss 5649.93 ([5649.93063134])\n",
      "01:43    Epoch 18: train loss 5648.88 ([5648.87740747])\n",
      "01:44    Epoch 20: train loss 5648.94 ([5648.93609922])\n",
      "01:44  Finished training\n",
      "01:44  Calculating expectation for 10 estimators in ensemble\n",
      "01:44  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "01:44  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "01:44  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "01:45  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "01:45  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "01:45  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "01:45  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "01:45  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "01:45  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "01:46  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "ensemble_all_tight = EnsembleForge(n_estimators)\n",
    "\n",
    "ensemble_all_tight.train_all(\n",
    "    method='sally',\n",
    "    x_filename=[sample_dir + 'train_local_tight/x_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    t_xz0_filename=[sample_dir + 'train_local_tight/t_xz_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=None,\n",
    "    n_hidden=n_hidden\n",
    ")\n",
    "\n",
    "ensemble_all_tight.calculate_expectation(\n",
    "    x_filename=sample_dir + 'validation_tight/x_validation.npy'\n",
    ")\n",
    "\n",
    "ensemble_all_tight.save(model_dir + 'sally_ensemble_all_tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c0e6b85f-dbf0-4145-826f-d27b57ebcd85"
    }
   },
   "source": [
    "## Resurrection phi after cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "7d39c984-d4a3-4f99-86d2-f5a6cf339bb4"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:46  Training 10 estimators in ensemble\n",
      "01:46  Training estimator 1 / 10 in ensemble\n",
      "01:46  Starting training\n",
      "01:46    Method:                 sally\n",
      "01:46    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "01:46                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "01:46    Features:               [29]\n",
      "01:46    Method:                 sally\n",
      "01:46    Hidden layers:          (100, 100)\n",
      "01:46    Activation function:    tanh\n",
      "01:46    Batch size:             128\n",
      "01:46    Epochs:                 20\n",
      "01:46    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:46    Validation split:       None\n",
      "01:46    Early stopping:         True\n",
      "01:46  Loading training data\n",
      "01:46  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:46  Only using 1 of 30 observables\n",
      "01:46  Creating model for method sally\n",
      "01:46  Training model\n",
      "01:47    Epoch 2: train loss 6849.44 ([6849.43935489])\n",
      "01:47    Epoch 4: train loss 6848.74 ([6848.7440841])\n",
      "01:48    Epoch 6: train loss 6847.98 ([6847.98155059])\n",
      "01:48    Epoch 8: train loss 6847.35 ([6847.34606037])\n",
      "01:49    Epoch 10: train loss 6851.17 ([6851.17328619])\n",
      "01:49    Epoch 12: train loss 6846.65 ([6846.64935967])\n",
      "01:50    Epoch 14: train loss 6846.38 ([6846.38053931])\n",
      "01:50    Epoch 16: train loss 6846.16 ([6846.15800438])\n",
      "01:51    Epoch 18: train loss 6846.07 ([6846.07426746])\n",
      "01:52    Epoch 20: train loss 6845.99 ([6845.9930116])\n",
      "01:52  Finished training\n",
      "01:52  Training estimator 2 / 10 in ensemble\n",
      "01:52  Starting training\n",
      "01:52    Method:                 sally\n",
      "01:52    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "01:52                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "01:52    Features:               [29]\n",
      "01:52    Method:                 sally\n",
      "01:52    Hidden layers:          (100, 100)\n",
      "01:52    Activation function:    tanh\n",
      "01:52    Batch size:             128\n",
      "01:52    Epochs:                 20\n",
      "01:52    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:52    Validation split:       None\n",
      "01:52    Early stopping:         True\n",
      "01:52  Loading training data\n",
      "01:52  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:52  Only using 1 of 30 observables\n",
      "01:52  Creating model for method sally\n",
      "01:52  Training model\n",
      "01:52    Epoch 2: train loss 5427.42 ([5427.41801308])\n",
      "01:53    Epoch 4: train loss 5426.77 ([5426.77206716])\n",
      "01:54    Epoch 6: train loss 5426.03 ([5426.03215681])\n",
      "01:54    Epoch 8: train loss 5425.50 ([5425.50062375])\n",
      "01:55    Epoch 10: train loss 5425.11 ([5425.10589637])\n",
      "01:55    Epoch 12: train loss 5459.49 ([5459.48803083])\n",
      "01:56    Epoch 14: train loss 5424.91 ([5424.90852831])\n",
      "01:56    Epoch 16: train loss 5424.27 ([5424.27394087])\n",
      "01:57    Epoch 18: train loss 5424.13 ([5424.12604886])\n",
      "01:58    Epoch 20: train loss 5424.33 ([5424.33443174])\n",
      "01:58  Finished training\n",
      "01:58  Training estimator 3 / 10 in ensemble\n",
      "01:58  Starting training\n",
      "01:58    Method:                 sally\n",
      "01:58    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "01:58                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "01:58    Features:               [29]\n",
      "01:58    Method:                 sally\n",
      "01:58    Hidden layers:          (100, 100)\n",
      "01:58    Activation function:    tanh\n",
      "01:58    Batch size:             128\n",
      "01:58    Epochs:                 20\n",
      "01:58    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:58    Validation split:       None\n",
      "01:58    Early stopping:         True\n",
      "01:58  Loading training data\n",
      "01:58  Found 1000000 samples with 2 parameters and 30 observables\n",
      "01:58  Only using 1 of 30 observables\n",
      "01:58  Creating model for method sally\n",
      "01:58  Training model\n",
      "01:58    Epoch 2: train loss 5926.84 ([5926.83639142])\n",
      "01:59    Epoch 4: train loss 5925.44 ([5925.43626837])\n",
      "01:59    Epoch 6: train loss 5924.21 ([5924.20847231])\n",
      "02:00    Epoch 8: train loss 5923.81 ([5923.81091213])\n",
      "02:01    Epoch 10: train loss 5922.86 ([5922.86223431])\n",
      "02:01    Epoch 12: train loss 5922.66 ([5922.65733733])\n",
      "02:02    Epoch 14: train loss 5922.06 ([5922.06322616])\n",
      "02:02    Epoch 16: train loss 5922.00 ([5921.99583718])\n",
      "02:03    Epoch 18: train loss 5921.57 ([5921.56613259])\n",
      "02:04    Epoch 20: train loss 5921.42 ([5921.42355441])\n",
      "02:04  Finished training\n",
      "02:04  Training estimator 4 / 10 in ensemble\n",
      "02:04  Starting training\n",
      "02:04    Method:                 sally\n",
      "02:04    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_3.npy\n",
      "02:04                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_3.npy\n",
      "02:04    Features:               [29]\n",
      "02:04    Method:                 sally\n",
      "02:04    Hidden layers:          (100, 100)\n",
      "02:04    Activation function:    tanh\n",
      "02:04    Batch size:             128\n",
      "02:04    Epochs:                 20\n",
      "02:04    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:04    Validation split:       None\n",
      "02:04    Early stopping:         True\n",
      "02:04  Loading training data\n",
      "02:04  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:04  Only using 1 of 30 observables\n",
      "02:04  Creating model for method sally\n",
      "02:04  Training model\n",
      "02:04    Epoch 2: train loss 8716.06 ([8716.06402118])\n",
      "02:05    Epoch 4: train loss 8714.81 ([8714.80632983])\n",
      "02:05    Epoch 6: train loss 8713.35 ([8713.35109071])\n",
      "02:06    Epoch 8: train loss 8712.28 ([8712.27686661])\n",
      "02:07    Epoch 10: train loss 8711.53 ([8711.52903367])\n",
      "02:07    Epoch 12: train loss 8710.96 ([8710.96008547])\n",
      "02:08    Epoch 14: train loss 8711.03 ([8711.02746016])\n",
      "02:08    Epoch 16: train loss 8710.00 ([8709.99951371])\n",
      "02:09    Epoch 18: train loss 8709.74 ([8709.74249952])\n",
      "02:09    Epoch 20: train loss 8709.51 ([8709.51144842])\n",
      "02:09  Finished training\n",
      "02:09  Training estimator 5 / 10 in ensemble\n",
      "02:09  Starting training\n",
      "02:09    Method:                 sally\n",
      "02:09    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_4.npy\n",
      "02:09                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_4.npy\n",
      "02:09    Features:               [29]\n",
      "02:09    Method:                 sally\n",
      "02:09    Hidden layers:          (100, 100)\n",
      "02:09    Activation function:    tanh\n",
      "02:09    Batch size:             128\n",
      "02:09    Epochs:                 20\n",
      "02:09    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:09    Validation split:       None\n",
      "02:09    Early stopping:         True\n",
      "02:09  Loading training data\n",
      "02:10  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:10  Only using 1 of 30 observables\n",
      "02:10  Creating model for method sally\n",
      "02:10  Training model\n",
      "02:10    Epoch 2: train loss 7222.50 ([7222.49744492])\n",
      "02:11    Epoch 4: train loss 7221.58 ([7221.5783037])\n",
      "02:11    Epoch 6: train loss 7220.40 ([7220.39541846])\n",
      "02:12    Epoch 8: train loss 7219.21 ([7219.21420666])\n",
      "02:13    Epoch 10: train loss 7218.33 ([7218.33234893])\n",
      "02:13    Epoch 12: train loss 7217.70 ([7217.69576499])\n",
      "02:14    Epoch 14: train loss 7217.11 ([7217.10961358])\n",
      "02:14    Epoch 16: train loss 7216.69 ([7216.69299268])\n",
      "02:15    Epoch 18: train loss 7216.38 ([7216.38332239])\n",
      "02:15    Epoch 20: train loss 7216.12 ([7216.12290173])\n",
      "02:15  Finished training\n",
      "02:15  Training estimator 6 / 10 in ensemble\n",
      "02:15  Starting training\n",
      "02:15    Method:                 sally\n",
      "02:15    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_5.npy\n",
      "02:15                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_5.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:15    Features:               [29]\n",
      "02:15    Method:                 sally\n",
      "02:15    Hidden layers:          (100, 100)\n",
      "02:15    Activation function:    tanh\n",
      "02:15    Batch size:             128\n",
      "02:15    Epochs:                 20\n",
      "02:15    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:15    Validation split:       None\n",
      "02:15    Early stopping:         True\n",
      "02:15  Loading training data\n",
      "02:15  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:15  Only using 1 of 30 observables\n",
      "02:15  Creating model for method sally\n",
      "02:15  Training model\n",
      "02:16    Epoch 2: train loss 5935.29 ([5935.28893681])\n",
      "02:17    Epoch 4: train loss 5937.30 ([5937.29792206])\n",
      "02:17    Epoch 6: train loss 5932.55 ([5932.54936276])\n",
      "02:18    Epoch 8: train loss 5931.67 ([5931.66966473])\n",
      "02:19    Epoch 10: train loss 5930.83 ([5930.82557257])\n",
      "02:19    Epoch 12: train loss 5930.07 ([5930.07384384])\n",
      "02:20    Epoch 14: train loss 5929.57 ([5929.56852716])\n",
      "02:20    Epoch 16: train loss 5929.95 ([5929.95368134])\n",
      "02:21    Epoch 18: train loss 5928.84 ([5928.84381688])\n",
      "02:21    Epoch 20: train loss 5928.54 ([5928.53858027])\n",
      "02:21  Finished training\n",
      "02:21  Training estimator 7 / 10 in ensemble\n",
      "02:21  Starting training\n",
      "02:21    Method:                 sally\n",
      "02:21    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_6.npy\n",
      "02:21                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_6.npy\n",
      "02:21    Features:               [29]\n",
      "02:21    Method:                 sally\n",
      "02:21    Hidden layers:          (100, 100)\n",
      "02:21    Activation function:    tanh\n",
      "02:21    Batch size:             128\n",
      "02:21    Epochs:                 20\n",
      "02:21    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:21    Validation split:       None\n",
      "02:21    Early stopping:         True\n",
      "02:21  Loading training data\n",
      "02:21  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:21  Only using 1 of 30 observables\n",
      "02:21  Creating model for method sally\n",
      "02:21  Training model\n",
      "02:22    Epoch 2: train loss 7520.97 ([7520.96710201])\n",
      "02:23    Epoch 4: train loss 7519.79 ([7519.78541212])\n",
      "02:23    Epoch 6: train loss 7519.12 ([7519.12252161])\n",
      "02:24    Epoch 8: train loss 7518.48 ([7518.48109682])\n",
      "02:24    Epoch 10: train loss 7517.25 ([7517.24597613])\n",
      "02:25    Epoch 12: train loss 7517.19 ([7517.1871768])\n",
      "02:26    Epoch 14: train loss 7516.23 ([7516.23136396])\n",
      "02:26    Epoch 16: train loss 7516.02 ([7516.01548941])\n",
      "02:27    Epoch 18: train loss 7515.52 ([7515.51813911])\n",
      "02:27    Epoch 20: train loss 7515.39 ([7515.38776068])\n",
      "02:27  Finished training\n",
      "02:27  Training estimator 8 / 10 in ensemble\n",
      "02:27  Starting training\n",
      "02:27    Method:                 sally\n",
      "02:27    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_7.npy\n",
      "02:27                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_7.npy\n",
      "02:27    Features:               [29]\n",
      "02:27    Method:                 sally\n",
      "02:27    Hidden layers:          (100, 100)\n",
      "02:27    Activation function:    tanh\n",
      "02:27    Batch size:             128\n",
      "02:27    Epochs:                 20\n",
      "02:27    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:27    Validation split:       None\n",
      "02:27    Early stopping:         True\n",
      "02:27  Loading training data\n",
      "02:27  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:27  Only using 1 of 30 observables\n",
      "02:27  Creating model for method sally\n",
      "02:27  Training model\n",
      "02:28    Epoch 2: train loss 6137.09 ([6137.08840457])\n",
      "02:29    Epoch 4: train loss 6136.46 ([6136.46323371])\n",
      "02:29    Epoch 6: train loss 6135.97 ([6135.96917187])\n",
      "02:30    Epoch 8: train loss 6135.34 ([6135.33887965])\n",
      "02:30    Epoch 10: train loss 6136.15 ([6136.14942203])\n",
      "02:31    Epoch 12: train loss 6134.59 ([6134.5853825])\n",
      "02:32    Epoch 14: train loss 6134.31 ([6134.30582307])\n",
      "02:32    Epoch 16: train loss 6134.01 ([6134.01048942])\n",
      "02:33    Epoch 18: train loss 6133.89 ([6133.88512257])\n",
      "02:33    Epoch 20: train loss 6134.03 ([6134.03310646])\n",
      "02:33  Finished training\n",
      "02:33  Training estimator 9 / 10 in ensemble\n",
      "02:33  Starting training\n",
      "02:33    Method:                 sally\n",
      "02:33    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_8.npy\n",
      "02:33                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_8.npy\n",
      "02:33    Features:               [29]\n",
      "02:33    Method:                 sally\n",
      "02:33    Hidden layers:          (100, 100)\n",
      "02:33    Activation function:    tanh\n",
      "02:33    Batch size:             128\n",
      "02:33    Epochs:                 20\n",
      "02:33    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:33    Validation split:       None\n",
      "02:33    Early stopping:         True\n",
      "02:33  Loading training data\n",
      "02:33  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:33  Only using 1 of 30 observables\n",
      "02:33  Creating model for method sally\n",
      "02:33  Training model\n",
      "02:34    Epoch 2: train loss 6386.43 ([6386.42621045])\n",
      "02:35    Epoch 4: train loss 6385.70 ([6385.69936813])\n",
      "02:35    Epoch 6: train loss 6384.13 ([6384.12608398])\n",
      "02:36    Epoch 8: train loss 6382.60 ([6382.60426663])\n",
      "02:36    Epoch 10: train loss 6381.46 ([6381.45912741])\n",
      "02:37    Epoch 12: train loss 6380.34 ([6380.34152184])\n",
      "02:37    Epoch 14: train loss 6379.66 ([6379.66125367])\n",
      "02:38    Epoch 16: train loss 6379.05 ([6379.04917791])\n",
      "02:39    Epoch 18: train loss 6378.49 ([6378.49460789])\n",
      "02:39    Epoch 20: train loss 6378.11 ([6378.10911978])\n",
      "02:39  Finished training\n",
      "02:39  Training estimator 10 / 10 in ensemble\n",
      "02:39  Starting training\n",
      "02:39    Method:                 sally\n",
      "02:39    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_9.npy\n",
      "02:39                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_9.npy\n",
      "02:39    Features:               [29]\n",
      "02:39    Method:                 sally\n",
      "02:39    Hidden layers:          (100, 100)\n",
      "02:39    Activation function:    tanh\n",
      "02:39    Batch size:             128\n",
      "02:39    Epochs:                 20\n",
      "02:39    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:39    Validation split:       None\n",
      "02:39    Early stopping:         True\n",
      "02:39  Loading training data\n",
      "02:39  Found 1000000 samples with 2 parameters and 30 observables\n",
      "02:39  Only using 1 of 30 observables\n",
      "02:39  Creating model for method sally\n",
      "02:39  Training model\n",
      "02:40    Epoch 2: train loss 5648.98 ([5648.98490191])\n",
      "02:41    Epoch 4: train loss 5648.97 ([5648.96987292])\n",
      "02:41    Epoch 6: train loss 5647.35 ([5647.3464067])\n",
      "02:42    Epoch 8: train loss 5646.45 ([5646.44639213])\n",
      "02:42    Epoch 10: train loss 5645.74 ([5645.74435085])\n",
      "02:43    Epoch 12: train loss 5645.74 ([5645.73610786])\n",
      "02:43    Epoch 14: train loss 5645.35 ([5645.35228683])\n",
      "02:44    Epoch 16: train loss 5645.06 ([5645.06360421])\n",
      "02:45    Epoch 18: train loss 5644.67 ([5644.66720087])\n",
      "02:45    Epoch 20: train loss 5644.51 ([5644.51373172])\n",
      "02:45  Finished training\n",
      "02:45  Calculating expectation for 10 estimators in ensemble\n",
      "02:45  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "02:45  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "02:45  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "02:46  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "02:46  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "02:46  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "02:46  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "02:46  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "02:46  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "02:46  Starting evaluation for estimator 10 / 10 in ensemble\n"
     ]
    }
   ],
   "source": [
    "ensemble_res = EnsembleForge(n_estimators)\n",
    "\n",
    "ensemble_res.train_all(\n",
    "    features=[ [29] for _ in range(n_estimators)],\n",
    "    method='sally',\n",
    "    x_filename=[sample_dir + 'train_local_tight/x_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    t_xz0_filename=[sample_dir + 'train_local_tight/t_xz_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=None,\n",
    "    n_hidden=n_hidden\n",
    ")\n",
    "\n",
    "ensemble_res.calculate_expectation(\n",
    "    x_filename=sample_dir + 'validation_tight/x_validation.npy'\n",
    ")\n",
    "\n",
    "ensemble_res.save(model_dir + 'sally_ensemble_resurrection_tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7d171470-6c7e-4a74-8ca8-5589004e32d6"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
