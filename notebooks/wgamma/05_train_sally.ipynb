{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=10, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=False)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.calculate_expectation(\n",
    "        x_filename=sample_dir + 'validation{}/x_validation.npy'.format(cut_label)\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:36  Training 10 estimators in ensemble\n",
      "07:36  Training estimator 1 / 10 in ensemble\n",
      "07:36  Starting training\n",
      "07:36    Method:                 sally\n",
      "07:36    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "07:36                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "07:36    Features:               all\n",
      "07:36    Method:                 sally\n",
      "07:36    Hidden layers:          (100, 100)\n",
      "07:36    Activation function:    tanh\n",
      "07:36    Batch size:             128\n",
      "07:36    Trainer:                amsgrad\n",
      "07:36    Epochs:                 50\n",
      "07:36    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "07:36    Validation split:       0.25\n",
      "07:36    Early stopping:         True\n",
      "07:36    Scale inputs:           True\n",
      "07:36    Regularization:         None\n",
      "07:36  Loading training data\n",
      "07:36  Found 1000000 samples with 2 parameters and 27 observables\n",
      "07:36  Rescaling inputs\n",
      "07:36  Observable ranges:\n",
      "07:36    x_1: mean 2.440891933019884e-17, std 1.0000000000000644, range -1.2633651041265415 ... 50.04004645522849\n",
      "07:36    x_2: mean -1.7209345060109627e-17, std 0.9999999999999489, range -1.7299150019312344 ... 1.7315849725199286\n",
      "07:36    x_3: mean -5.0861320488593263e-14, std 0.999999999999989, range -0.6558830712075934 ... 27.1327820205914\n",
      "07:36    x_4: mean 3.744204946087848e-17, std 0.9999999999999581, range -5.239756038816252 ... 4.850858137397746\n",
      "07:36    x_5: mean -4.391154106997419e-17, std 0.9999999999999524, range -0.821785881383525 ... 42.92985214586265\n",
      "07:36    x_6: mean -5.620961651686685e-14, std 0.999999999999978, range -0.769099867487215 ... 34.160332242952585\n",
      "07:36    x_7: mean -1.2466472298910958e-17, std 0.9999999999999408, range -2.0477483462730484 ... 2.030028875960743\n",
      "07:36    x_8: mean 4.568079248201684e-17, std 1.0000000000000462, range -1.7413375378793845 ... 1.7264072380621693\n",
      "07:36    x_9: mean -6.980371836107224e-17, std 1.000000000000022, range -1.2069362392299257 ... 57.38882665882456\n",
      "07:36    x_10: mean -6.267031338325068e-17, std 0.9999999999999786, range -0.9342280598897849 ... 35.827897920033756\n",
      "07:36    x_11: mean -2.2176038783072727e-17, std 1.0000000000000817, range -1.96177579535038 ... 1.9619103379484242\n",
      "07:36    x_12: mean -3.5024427802454736e-17, std 0.9999999999999808, range -1.7253452148309152 ... 1.7347076848211749\n",
      "07:36    x_13: mean -4.2230219321481854e-17, std 0.9999999999942384, range -0.4326804393239117 ... 24.87244260380762\n",
      "07:36    x_14: mean -1.4535803671833493e-14, std 0.9999999999942837, range -0.262982297904677 ... 32.20949284912719\n",
      "07:36    x_15: mean -3.184652541676769e-17, std 1.0000000000078892, range -6.050132124332202 ... 5.960949936098563\n",
      "07:36    x_16: mean -1.899991275422508e-17, std 0.9999999999984803, range -3.903572617789749 ... 3.903333782851534\n",
      "07:36    x_17: mean 2.9331204132176934e-17, std 0.9999999999999665, range -2.433805215879062 ... 2.415342400165942\n",
      "07:36    x_18: mean -1.305622276959184e-17, std 1.000000000000015, range -2.0364210669633733 ... 2.0608045500184367\n",
      "07:36    x_19: mean 1.2052329090295188e-13, std 0.9999999999999788, range -1.5658821651235906 ... 44.592721116331425\n",
      "07:36    x_20: mean -8.001892837228297e-14, std 0.999999999999986, range -1.2169098021540412 ... 48.011515573806214\n",
      "07:36    x_21: mean 5.5345239502457845e-15, std 1.0000000000000286, range -0.967903993688896 ... 38.94081676678878\n",
      "07:36    x_22: mean -2.489031203367631e-17, std 0.999999999999971, range -3.960665246291044 ... 3.866114882553565\n",
      "07:36    x_23: mean 4.440536827132746e-17, std 0.9999999999999499, range -2.3578266551877785 ... 2.310225515714504\n",
      "07:36    x_24: mean -9.051114346902977e-14, std 1.000000000000025, range -1.308679741147821 ... 40.00231896888047\n",
      "07:36    x_25: mean 6.488510351232435e-14, std 1.0000000000000477, range -0.9746491218898782 ... 36.54101087960349\n",
      "07:36    x_26: mean 5.321539653380114e-15, std 0.9999999999999688, range -1.5283129918090783 ... 58.89033108894756\n",
      "07:36    x_27: mean -2.0602186623364105e-16, std 1.0000000000000084, range -1.5530044706623864 ... 1.5618463167035725\n",
      "07:36  Creating model for method sally\n",
      "07:36  Training model\n",
      "07:37    Epoch 1: train loss 46.7949 (mse_score: 46.7949)\n",
      "07:37              val. loss  23.5624 (mse_score: 23.5624) (*)\n",
      "07:37    Epoch 2: train loss 46.7668 (mse_score: 46.7668)\n",
      "07:37              val. loss  23.5582 (mse_score: 23.5582) (*)\n",
      "07:38    Epoch 3: train loss 46.7585 (mse_score: 46.7585)\n",
      "07:38              val. loss  23.5451 (mse_score: 23.5451) (*)\n",
      "07:38    Epoch 4: train loss 46.7194 (mse_score: 46.7194)\n",
      "07:38              val. loss  23.5313 (mse_score: 23.5313) (*)\n",
      "07:38    Epoch 5: train loss 46.7084 (mse_score: 46.7084)\n",
      "07:38              val. loss  23.5259 (mse_score: 23.5259) (*)\n",
      "07:39    Epoch 6: train loss 46.6660 (mse_score: 46.6660)\n",
      "07:39              val. loss  23.5537 (mse_score: 23.5537)\n",
      "07:39    Epoch 7: train loss 46.6298 (mse_score: 46.6298)\n",
      "07:39              val. loss  23.5257 (mse_score: 23.5257) (*)\n",
      "07:39    Epoch 8: train loss 46.5811 (mse_score: 46.5811)\n",
      "07:39              val. loss  23.5345 (mse_score: 23.5345)\n",
      "07:40    Epoch 9: train loss 46.5064 (mse_score: 46.5064)\n",
      "07:40              val. loss  23.5356 (mse_score: 23.5356)\n",
      "07:40    Epoch 10: train loss 46.4496 (mse_score: 46.4496)\n",
      "07:40              val. loss  23.6016 (mse_score: 23.6016)\n",
      "07:40    Epoch 11: train loss 46.3988 (mse_score: 46.3988)\n",
      "07:40              val. loss  23.5584 (mse_score: 23.5584)\n",
      "07:40    Epoch 12: train loss 46.3161 (mse_score: 46.3161)\n",
      "07:40              val. loss  23.6949 (mse_score: 23.6949)\n",
      "07:41    Epoch 13: train loss 46.2509 (mse_score: 46.2509)\n",
      "07:41              val. loss  23.6475 (mse_score: 23.6475)\n",
      "07:41    Epoch 14: train loss 46.1683 (mse_score: 46.1683)\n",
      "07:41              val. loss  23.8813 (mse_score: 23.8813)\n",
      "07:41    Epoch 15: train loss 46.0907 (mse_score: 46.0907)\n",
      "07:41              val. loss  23.6526 (mse_score: 23.6526)\n",
      "07:42    Epoch 16: train loss 45.9874 (mse_score: 45.9874)\n",
      "07:42              val. loss  23.6813 (mse_score: 23.6813)\n",
      "07:42    Epoch 17: train loss 45.9253 (mse_score: 45.9253)\n",
      "07:42              val. loss  23.7388 (mse_score: 23.7388)\n",
      "07:42    Epoch 18: train loss 45.8250 (mse_score: 45.8250)\n",
      "07:42              val. loss  23.8062 (mse_score: 23.8062)\n",
      "07:42    Epoch 19: train loss 45.7263 (mse_score: 45.7263)\n",
      "07:42              val. loss  23.7661 (mse_score: 23.7661)\n",
      "07:43    Epoch 20: train loss 45.6634 (mse_score: 45.6634)\n",
      "07:43              val. loss  23.7884 (mse_score: 23.7884)\n",
      "07:43    Epoch 21: train loss 45.5628 (mse_score: 45.5628)\n",
      "07:43              val. loss  23.8473 (mse_score: 23.8473)\n",
      "07:43    Epoch 22: train loss 45.4715 (mse_score: 45.4715)\n",
      "07:43              val. loss  23.8584 (mse_score: 23.8584)\n",
      "07:44    Epoch 23: train loss 45.3903 (mse_score: 45.3903)\n",
      "07:44              val. loss  23.9628 (mse_score: 23.9628)\n",
      "07:44    Epoch 24: train loss 45.2876 (mse_score: 45.2876)\n",
      "07:44              val. loss  24.0075 (mse_score: 24.0075)\n",
      "07:44    Epoch 25: train loss 45.2035 (mse_score: 45.2035)\n",
      "07:44              val. loss  23.9593 (mse_score: 23.9593)\n",
      "07:45    Epoch 26: train loss 45.1519 (mse_score: 45.1519)\n",
      "07:45              val. loss  24.0075 (mse_score: 24.0075)\n",
      "07:45    Epoch 27: train loss 45.0621 (mse_score: 45.0621)\n",
      "07:45              val. loss  24.0612 (mse_score: 24.0612)\n",
      "07:45  No improvement for 20 epochs, stopping training\n",
      "07:45  Early stopping after epoch 7, with loss 23.53 compared to final loss 24.06\n",
      "07:45  Finished training\n",
      "07:45  Training estimator 2 / 10 in ensemble\n",
      "07:45  Starting training\n",
      "07:45    Method:                 sally\n",
      "07:45    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "07:45                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "07:45    Features:               all\n",
      "07:45    Method:                 sally\n",
      "07:45    Hidden layers:          (100, 100)\n",
      "07:45    Activation function:    tanh\n",
      "07:45    Batch size:             128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:45    Trainer:                amsgrad\n",
      "07:45    Epochs:                 50\n",
      "07:45    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "07:45    Validation split:       0.25\n",
      "07:45    Early stopping:         True\n",
      "07:45    Scale inputs:           True\n",
      "07:45    Regularization:         None\n",
      "07:45  Loading training data\n",
      "07:45  Found 1000000 samples with 2 parameters and 27 observables\n",
      "07:45  Rescaling inputs\n",
      "07:45  Observable ranges:\n",
      "07:45    x_1: mean -9.239897735824343e-17, std 1.0000000000000209, range -1.2733982205951953 ... 46.967201549377855\n",
      "07:45    x_2: mean 3.892175470809889e-17, std 1.000000000000059, range -1.7275326350199396 ... 1.7301371229199798\n",
      "07:45    x_3: mean -2.99455340524446e-14, std 1.0000000000000244, range -0.6581110421560781 ... 29.45231630664146\n",
      "07:45    x_4: mean 3.6637715083998044e-16, std 1.0000000000000258, range -5.242219236491054 ... 4.710909019701929\n",
      "07:45    x_5: mean 4.166267331129347e-17, std 0.999999999999968, range -0.8279265579300497 ... 39.88214219327895\n",
      "07:45    x_6: mean -8.142222185369974e-14, std 1.000000000000021, range -0.770861545566321 ... 40.0050825133759\n",
      "07:45    x_7: mean -1.7323031897831243e-17, std 0.9999999999999982, range -2.0492539487167876 ... 2.030983011846563\n",
      "07:45    x_8: mean -8.764544645600836e-18, std 0.999999999999932, range -1.740904989377611 ... 1.725276915832211\n",
      "07:45    x_9: mean -3.005062865213404e-17, std 1.0000000000000637, range -1.2189277475224793 ... 46.64706543706861\n",
      "07:45    x_10: mean 3.952038696297677e-17, std 0.9999999999999761, range -0.9383071689648079 ... 43.139188880491425\n",
      "07:45    x_11: mean -1.0405898365206668e-17, std 1.0000000000000546, range -1.964031437685621 ... 1.9631553701968167\n",
      "07:45    x_12: mean 2.2257751197685137e-17, std 0.9999999999999746, range -1.7274767000695397 ... 1.737690868457324\n",
      "07:45    x_13: mean 5.865175012331747e-17, std 0.9999999999927396, range -0.4320998581528088 ... 26.728816680277692\n",
      "07:45    x_14: mean -1.4825824123931852e-14, std 0.9999999999925918, range -0.2626274770735227 ... 31.118065481248276\n",
      "07:45    x_15: mean -3.2507330161024584e-19, std 0.9999999999933021, range -6.051663285824439 ... 5.961345416647923\n",
      "07:45    x_16: mean -1.0587086762825493e-17, std 0.9999999999939838, range -3.912045378753528 ... 3.911120979213382\n",
      "07:45    x_17: mean 1.4654943925052066e-18, std 1.000000000000029, range -2.4372002058514703 ... 2.424305965751745\n",
      "07:45    x_18: mean -8.700595799382426e-18, std 0.999999999999995, range -2.035801905323955 ... 2.060105359831782\n",
      "07:45    x_19: mean 2.7462611740247668e-14, std 1.0000000000000087, range -1.5707779143363947 ... 43.83651603051942\n",
      "07:45    x_20: mean -4.880267567841656e-14, std 1.00000000000002, range -1.2276308354160534 ... 43.41287826381293\n",
      "07:45    x_21: mean 7.601990148486948e-16, std 0.999999999999943, range -0.9745627972172314 ... 46.47541815664069\n",
      "07:45    x_22: mean 1.3940848475613167e-17, std 1.0000000000000249, range -3.969836173619831 ... 3.8753094154719214\n",
      "07:45    x_23: mean -1.9959145447501213e-17, std 1.000000000000006, range -2.3583835363202494 ... 2.3097540544279953\n",
      "07:45    x_24: mean -6.496213700302177e-14, std 1.000000000000011, range -1.3461379226477652 ... 43.24022023261256\n",
      "07:45    x_25: mean 7.973153426377167e-14, std 0.9999999999999877, range -0.9782687100647366 ... 27.34550993194478\n",
      "07:45    x_26: mean -3.054736907870392e-14, std 0.9999999999999609, range -1.5322200063182019 ... 65.1383378628549\n",
      "07:45    x_27: mean -4.409059783938574e-16, std 1.0000000000000389, range -1.5528631657312995 ... 1.5607211166911348\n",
      "07:45  Creating model for method sally\n",
      "07:45  Training model\n",
      "07:45    Epoch 1: train loss 23.1654 (mse_score: 23.1654)\n",
      "07:45              val. loss  15.2243 (mse_score: 15.2243) (*)\n",
      "07:46    Epoch 2: train loss 23.1313 (mse_score: 23.1313)\n",
      "07:46              val. loss  15.2227 (mse_score: 15.2227) (*)\n",
      "07:46    Epoch 3: train loss 23.1000 (mse_score: 23.1000)\n",
      "07:46              val. loss  15.1841 (mse_score: 15.1841) (*)\n",
      "07:46    Epoch 4: train loss 23.0523 (mse_score: 23.0523)\n",
      "07:46              val. loss  15.1575 (mse_score: 15.1575) (*)\n",
      "07:47    Epoch 5: train loss 23.0025 (mse_score: 23.0025)\n",
      "07:47              val. loss  15.1205 (mse_score: 15.1205) (*)\n",
      "07:47    Epoch 6: train loss 22.9234 (mse_score: 22.9234)\n",
      "07:47              val. loss  15.1272 (mse_score: 15.1272)\n",
      "07:47    Epoch 7: train loss 22.8678 (mse_score: 22.8678)\n",
      "07:47              val. loss  15.0912 (mse_score: 15.0912) (*)\n",
      "07:48    Epoch 8: train loss 22.8120 (mse_score: 22.8120)\n",
      "07:48              val. loss  15.1435 (mse_score: 15.1435)\n",
      "07:48    Epoch 9: train loss 22.7308 (mse_score: 22.7308)\n",
      "07:48              val. loss  15.0517 (mse_score: 15.0517) (*)\n",
      "07:48    Epoch 10: train loss 22.6469 (mse_score: 22.6469)\n",
      "07:48              val. loss  15.0416 (mse_score: 15.0416) (*)\n",
      "07:48    Epoch 11: train loss 22.5724 (mse_score: 22.5724)\n",
      "07:48              val. loss  15.1158 (mse_score: 15.1158)\n",
      "07:49    Epoch 12: train loss 22.5154 (mse_score: 22.5154)\n",
      "07:49              val. loss  15.0578 (mse_score: 15.0578)\n",
      "07:49    Epoch 13: train loss 22.4370 (mse_score: 22.4370)\n",
      "07:49              val. loss  15.0188 (mse_score: 15.0188) (*)\n",
      "07:49    Epoch 14: train loss 22.3505 (mse_score: 22.3505)\n",
      "07:49              val. loss  15.1154 (mse_score: 15.1154)\n",
      "07:50    Epoch 15: train loss 22.2757 (mse_score: 22.2757)\n",
      "07:50              val. loss  15.0343 (mse_score: 15.0343)\n",
      "07:50    Epoch 16: train loss 22.2200 (mse_score: 22.2200)\n",
      "07:50              val. loss  15.0180 (mse_score: 15.0180) (*)\n",
      "07:50    Epoch 17: train loss 22.1144 (mse_score: 22.1144)\n",
      "07:50              val. loss  15.0899 (mse_score: 15.0899)\n",
      "07:51    Epoch 18: train loss 22.0700 (mse_score: 22.0700)\n",
      "07:51              val. loss  15.0583 (mse_score: 15.0583)\n",
      "07:51    Epoch 19: train loss 21.9744 (mse_score: 21.9744)\n",
      "07:51              val. loss  15.0595 (mse_score: 15.0595)\n",
      "07:51    Epoch 20: train loss 21.8876 (mse_score: 21.8876)\n",
      "07:51              val. loss  15.0866 (mse_score: 15.0866)\n",
      "07:52    Epoch 21: train loss 21.8095 (mse_score: 21.8095)\n",
      "07:52              val. loss  15.0427 (mse_score: 15.0427)\n",
      "07:52    Epoch 22: train loss 21.7804 (mse_score: 21.7804)\n",
      "07:52              val. loss  15.0898 (mse_score: 15.0898)\n",
      "07:52    Epoch 23: train loss 21.6786 (mse_score: 21.6786)\n",
      "07:52              val. loss  15.1185 (mse_score: 15.1185)\n",
      "07:52    Epoch 24: train loss 21.6047 (mse_score: 21.6047)\n",
      "07:52              val. loss  15.0416 (mse_score: 15.0416)\n",
      "07:53    Epoch 25: train loss 21.5323 (mse_score: 21.5323)\n",
      "07:53              val. loss  15.0103 (mse_score: 15.0103) (*)\n",
      "07:53    Epoch 26: train loss 21.4679 (mse_score: 21.4679)\n",
      "07:53              val. loss  15.1026 (mse_score: 15.1026)\n",
      "07:53    Epoch 27: train loss 21.4109 (mse_score: 21.4109)\n",
      "07:53              val. loss  15.0886 (mse_score: 15.0886)\n",
      "07:54    Epoch 28: train loss 21.3563 (mse_score: 21.3563)\n",
      "07:54              val. loss  15.0434 (mse_score: 15.0434)\n",
      "07:54    Epoch 29: train loss 21.2842 (mse_score: 21.2842)\n",
      "07:54              val. loss  15.0465 (mse_score: 15.0465)\n",
      "07:54    Epoch 30: train loss 21.2456 (mse_score: 21.2456)\n",
      "07:54              val. loss  15.1139 (mse_score: 15.1139)\n",
      "07:55    Epoch 31: train loss 21.1809 (mse_score: 21.1809)\n",
      "07:55              val. loss  15.0982 (mse_score: 15.0982)\n",
      "07:55    Epoch 32: train loss 21.1284 (mse_score: 21.1284)\n",
      "07:55              val. loss  15.0688 (mse_score: 15.0688)\n",
      "07:55    Epoch 33: train loss 21.0798 (mse_score: 21.0798)\n",
      "07:55              val. loss  15.0801 (mse_score: 15.0801)\n",
      "07:56    Epoch 34: train loss 21.0295 (mse_score: 21.0295)\n",
      "07:56              val. loss  15.0827 (mse_score: 15.0827)\n",
      "07:56    Epoch 35: train loss 20.9902 (mse_score: 20.9902)\n",
      "07:56              val. loss  15.1139 (mse_score: 15.1139)\n",
      "07:56    Epoch 36: train loss 20.9511 (mse_score: 20.9511)\n",
      "07:56              val. loss  15.0467 (mse_score: 15.0467)\n",
      "07:57    Epoch 37: train loss 20.9029 (mse_score: 20.9029)\n",
      "07:57              val. loss  15.0207 (mse_score: 15.0207)\n",
      "07:57    Epoch 38: train loss 20.8747 (mse_score: 20.8747)\n",
      "07:57              val. loss  15.0660 (mse_score: 15.0660)\n",
      "07:57    Epoch 39: train loss 20.8250 (mse_score: 20.8250)\n",
      "07:57              val. loss  15.1253 (mse_score: 15.1253)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:57    Epoch 40: train loss 20.7867 (mse_score: 20.7867)\n",
      "07:57              val. loss  15.0743 (mse_score: 15.0743)\n",
      "07:58    Epoch 41: train loss 20.7619 (mse_score: 20.7619)\n",
      "07:58              val. loss  15.0976 (mse_score: 15.0976)\n",
      "07:58    Epoch 42: train loss 20.7211 (mse_score: 20.7211)\n",
      "07:58              val. loss  15.0720 (mse_score: 15.0720)\n",
      "07:58    Epoch 43: train loss 20.6935 (mse_score: 20.6935)\n",
      "07:58              val. loss  15.0767 (mse_score: 15.0767)\n",
      "07:59    Epoch 44: train loss 20.6629 (mse_score: 20.6629)\n",
      "07:59              val. loss  15.0820 (mse_score: 15.0820)\n",
      "07:59    Epoch 45: train loss 20.6299 (mse_score: 20.6299)\n",
      "07:59              val. loss  15.0872 (mse_score: 15.0872)\n",
      "07:59  No improvement for 20 epochs, stopping training\n",
      "07:59  Early stopping after epoch 25, with loss 15.01 compared to final loss 15.09\n",
      "07:59  Finished training\n",
      "07:59  Training estimator 3 / 10 in ensemble\n",
      "07:59  Starting training\n",
      "07:59    Method:                 sally\n",
      "07:59    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "07:59                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "07:59    Features:               all\n",
      "07:59    Method:                 sally\n",
      "07:59    Hidden layers:          (100, 100)\n",
      "07:59    Activation function:    tanh\n",
      "07:59    Batch size:             128\n",
      "07:59    Trainer:                amsgrad\n",
      "07:59    Epochs:                 50\n",
      "07:59    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "07:59    Validation split:       0.25\n",
      "07:59    Early stopping:         True\n",
      "07:59    Scale inputs:           True\n",
      "07:59    Regularization:         None\n",
      "07:59  Loading training data\n",
      "07:59  Found 1000000 samples with 2 parameters and 27 observables\n",
      "07:59  Rescaling inputs\n",
      "07:59  Observable ranges:\n",
      "07:59    x_1: mean -5.834266403326182e-17, std 0.9999999999999535, range -1.2750688845767493 ... 50.056045428827474\n",
      "07:59    x_2: mean 2.6503244043851738e-18, std 0.999999999999915, range -1.728351040318163 ... 1.7332968576846517\n",
      "07:59    x_3: mean -4.3592688570015525e-14, std 1.0000000000000266, range -0.658347330581113 ... 31.279823118918905\n",
      "07:59    x_4: mean 2.588720349194773e-16, std 1.0000000000000142, range -4.3775376075656585 ... 4.70818362360733\n",
      "07:59    x_5: mean 3.086597644141875e-17, std 0.9999999999999847, range -0.8297924626435061 ... 40.21272053976962\n",
      "07:59    x_6: mean -6.049961598364462e-14, std 1.0000000000000213, range -0.7693711518762874 ... 53.17776067320257\n",
      "07:59    x_7: mean 1.0948575379643443e-17, std 1.0000000000000069, range -2.047286996254395 ... 2.0283312731880327\n",
      "07:59    x_8: mean -1.958611051122716e-17, std 0.9999999999999488, range -1.7416480313260756 ... 1.7265714769581528\n",
      "07:59    x_9: mean 1.1724665682777412e-16, std 1.0000000000000042, range -1.228637162840915 ... 45.19265702320945\n",
      "07:59    x_10: mean -2.1316282072803006e-20, std 0.9999999999999811, range -0.9390075464262925 ... 46.79091794640664\n",
      "07:59    x_11: mean -6.501466032204917e-18, std 1.0000000000000833, range -1.9615575216231107 ... 1.964085634497229\n",
      "07:59    x_12: mean 2.1582735598713042e-17, std 1.0000000000000184, range -1.7272070101915178 ... 1.7355413711789431\n",
      "07:59    x_13: mean -2.4499513529008255e-17, std 0.9999999999999036, range -0.4328025456686285 ... 28.88459971962945\n",
      "07:59    x_14: mean -1.7013977782198708e-14, std 1.0000000000028666, range -0.26304300333937763 ... 31.161346601475255\n",
      "07:59    x_15: mean -3.928235514649714e-17, std 1.000000000005604, range -6.054419229412923 ... 6.009686763481471\n",
      "07:59    x_16: mean 3.4040326113427e-17, std 0.9999999999997438, range -3.9018619924227576 ... 3.9024610575183507\n",
      "07:59    x_17: mean 2.9274360713316127e-18, std 0.9999999999999896, range -2.4360455118320807 ... 2.4268922997893427\n",
      "07:59    x_18: mean 4.7368331479447077e-17, std 1.0000000000000366, range -2.0448902046312822 ... 2.0600927716151856\n",
      "07:59    x_19: mean 5.905553379648155e-14, std 1.0000000000000242, range -1.5626823886749939 ... 52.5657767952931\n",
      "07:59    x_20: mean -2.0756580454417416e-14, std 0.9999999999999984, range -1.2325110438826659 ... 48.658225922684395\n",
      "07:59    x_21: mean 2.2551478906507327e-14, std 1.0000000000000138, range -0.9781721444833812 ... 37.36623077491638\n",
      "07:59    x_22: mean 2.2780000108468813e-17, std 0.9999999999999957, range -3.8750674317303977 ... 3.8669909806653933\n",
      "07:59    x_23: mean 4.25544044446724e-17, std 0.9999999999999698, range -2.3585394168988354 ... 2.3115513530638587\n",
      "07:59    x_24: mean -7.7641692186603e-14, std 1.000000000000039, range -1.322903478916447 ... 38.4837842043073\n",
      "07:59    x_25: mean 1.0064201916293313e-13, std 1.0000000000000462, range -0.9789072594489346 ... 32.80399391650988\n",
      "07:59    x_26: mean -4.389189456333042e-15, std 0.999999999999995, range -1.521514574728733 ... 50.26487459373224\n",
      "07:59    x_27: mean -3.7602632119160264e-16, std 0.9999999999999901, range -1.553667901750317 ... 1.5619495141771873\n",
      "07:59  Creating model for method sally\n",
      "07:59  Training model\n",
      "08:00    Epoch 1: train loss 23.9362 (mse_score: 23.9362)\n",
      "08:00              val. loss  17.6940 (mse_score: 17.6940) (*)\n",
      "08:00    Epoch 2: train loss 23.9077 (mse_score: 23.9077)\n",
      "08:00              val. loss  17.6794 (mse_score: 17.6794) (*)\n",
      "08:00    Epoch 3: train loss 23.8918 (mse_score: 23.8918)\n",
      "08:00              val. loss  17.6595 (mse_score: 17.6595) (*)\n",
      "08:01    Epoch 4: train loss 23.8790 (mse_score: 23.8790)\n",
      "08:01              val. loss  17.6272 (mse_score: 17.6272) (*)\n",
      "08:01    Epoch 5: train loss 23.8537 (mse_score: 23.8537)\n",
      "08:01              val. loss  17.6185 (mse_score: 17.6185) (*)\n",
      "08:01    Epoch 6: train loss 23.8177 (mse_score: 23.8177)\n",
      "08:01              val. loss  17.5819 (mse_score: 17.5819) (*)\n",
      "08:01    Epoch 7: train loss 23.7906 (mse_score: 23.7906)\n",
      "08:01              val. loss  17.6752 (mse_score: 17.6752)\n",
      "08:02    Epoch 8: train loss 23.7448 (mse_score: 23.7448)\n",
      "08:02              val. loss  17.5563 (mse_score: 17.5563) (*)\n",
      "08:02    Epoch 9: train loss 23.7136 (mse_score: 23.7136)\n",
      "08:02              val. loss  17.5572 (mse_score: 17.5572)\n",
      "08:02    Epoch 10: train loss 23.6586 (mse_score: 23.6586)\n",
      "08:02              val. loss  17.6046 (mse_score: 17.6046)\n",
      "08:03    Epoch 11: train loss 23.6015 (mse_score: 23.6015)\n",
      "08:03              val. loss  17.5563 (mse_score: 17.5563)\n",
      "08:03    Epoch 12: train loss 23.5445 (mse_score: 23.5445)\n",
      "08:03              val. loss  17.5416 (mse_score: 17.5416) (*)\n",
      "08:03    Epoch 13: train loss 23.4781 (mse_score: 23.4781)\n",
      "08:03              val. loss  17.5440 (mse_score: 17.5440)\n",
      "08:04    Epoch 14: train loss 23.3871 (mse_score: 23.3871)\n",
      "08:04              val. loss  17.5358 (mse_score: 17.5358) (*)\n",
      "08:04    Epoch 15: train loss 23.3527 (mse_score: 23.3527)\n",
      "08:04              val. loss  17.5977 (mse_score: 17.5977)\n",
      "08:04    Epoch 16: train loss 23.2711 (mse_score: 23.2711)\n",
      "08:04              val. loss  17.5215 (mse_score: 17.5215) (*)\n",
      "08:05    Epoch 17: train loss 23.1765 (mse_score: 23.1765)\n",
      "08:05              val. loss  17.5525 (mse_score: 17.5525)\n",
      "08:05    Epoch 18: train loss 23.0985 (mse_score: 23.0985)\n",
      "08:05              val. loss  17.6359 (mse_score: 17.6359)\n",
      "08:05    Epoch 19: train loss 23.0072 (mse_score: 23.0072)\n",
      "08:05              val. loss  17.5956 (mse_score: 17.5956)\n",
      "08:06    Epoch 20: train loss 22.9300 (mse_score: 22.9300)\n",
      "08:06              val. loss  17.6165 (mse_score: 17.6165)\n",
      "08:06    Epoch 21: train loss 22.8471 (mse_score: 22.8471)\n",
      "08:06              val. loss  17.6082 (mse_score: 17.6082)\n",
      "08:06    Epoch 22: train loss 22.7611 (mse_score: 22.7611)\n",
      "08:06              val. loss  17.6931 (mse_score: 17.6931)\n",
      "08:07    Epoch 23: train loss 22.6738 (mse_score: 22.6738)\n",
      "08:07              val. loss  17.7210 (mse_score: 17.7210)\n",
      "08:07    Epoch 24: train loss 22.5915 (mse_score: 22.5915)\n",
      "08:07              val. loss  17.7481 (mse_score: 17.7481)\n",
      "08:07    Epoch 25: train loss 22.5286 (mse_score: 22.5286)\n",
      "08:07              val. loss  17.6713 (mse_score: 17.6713)\n",
      "08:07    Epoch 26: train loss 22.4184 (mse_score: 22.4184)\n",
      "08:07              val. loss  17.8480 (mse_score: 17.8480)\n",
      "08:08    Epoch 27: train loss 22.3364 (mse_score: 22.3364)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:08              val. loss  17.7618 (mse_score: 17.7618)\n",
      "08:08    Epoch 28: train loss 22.2819 (mse_score: 22.2819)\n",
      "08:08              val. loss  17.8142 (mse_score: 17.8142)\n",
      "08:08    Epoch 29: train loss 22.2002 (mse_score: 22.2002)\n",
      "08:08              val. loss  17.7843 (mse_score: 17.7843)\n",
      "08:09    Epoch 30: train loss 22.1338 (mse_score: 22.1338)\n",
      "08:09              val. loss  17.8190 (mse_score: 17.8190)\n",
      "08:09    Epoch 31: train loss 22.0757 (mse_score: 22.0757)\n",
      "08:09              val. loss  17.7585 (mse_score: 17.7585)\n",
      "08:09    Epoch 32: train loss 22.0128 (mse_score: 22.0128)\n",
      "08:09              val. loss  17.8426 (mse_score: 17.8426)\n",
      "08:10    Epoch 33: train loss 21.9365 (mse_score: 21.9365)\n",
      "08:10              val. loss  17.7959 (mse_score: 17.7959)\n",
      "08:10    Epoch 34: train loss 21.8779 (mse_score: 21.8779)\n",
      "08:10              val. loss  17.8329 (mse_score: 17.8329)\n",
      "08:10    Epoch 35: train loss 21.8179 (mse_score: 21.8179)\n",
      "08:10              val. loss  17.8689 (mse_score: 17.8689)\n",
      "08:11    Epoch 36: train loss 21.7590 (mse_score: 21.7590)\n",
      "08:11              val. loss  17.9124 (mse_score: 17.9124)\n",
      "08:11  No improvement for 20 epochs, stopping training\n",
      "08:11  Early stopping after epoch 16, with loss 17.52 compared to final loss 17.91\n",
      "08:11  Finished training\n",
      "08:11  Training estimator 4 / 10 in ensemble\n",
      "08:11  Starting training\n",
      "08:11    Method:                 sally\n",
      "08:11    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "08:11                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "08:11    Features:               all\n",
      "08:11    Method:                 sally\n",
      "08:11    Hidden layers:          (100, 100)\n",
      "08:11    Activation function:    tanh\n",
      "08:11    Batch size:             128\n",
      "08:11    Trainer:                amsgrad\n",
      "08:11    Epochs:                 50\n",
      "08:11    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "08:11    Validation split:       0.25\n",
      "08:11    Early stopping:         True\n",
      "08:11    Scale inputs:           True\n",
      "08:11    Regularization:         None\n",
      "08:11  Loading training data\n",
      "08:11  Found 1000000 samples with 2 parameters and 27 observables\n",
      "08:11  Rescaling inputs\n",
      "08:11  Observable ranges:\n",
      "08:11    x_1: mean 1.2647216607319933e-16, std 0.9999999999999545, range -1.2747311543097688 ... 43.95560624996899\n",
      "08:11    x_2: mean 2.423838907361642e-18, std 1.0000000000000207, range -1.7318964674827522 ... 1.7305135890231413\n",
      "08:11    x_3: mean -2.356474482212434e-14, std 0.9999999999999858, range -0.6573040238127422 ... 25.269899681165423\n",
      "08:11    x_4: mean 4.147153731537401e-16, std 1.000000000000071, range -5.238110512936363 ... 4.706167743226751\n",
      "08:11    x_5: mean -5.4502180546478486e-17, std 1.0000000000000169, range -0.8240901634566871 ... 62.65108434807358\n",
      "08:11    x_6: mean -2.1522202686696802e-14, std 1.000000000000002, range -0.7688150566113531 ... 43.07492282386254\n",
      "08:11    x_7: mean -1.2118306358388509e-17, std 1.0000000000000384, range -2.048629819421327 ... 2.028804700112164\n",
      "08:11    x_8: mean 2.8556712550198426e-17, std 1.0000000000000027, range -1.7412162056055203 ... 1.7241091824106087\n",
      "08:11    x_9: mean -2.4121149522216002e-17, std 0.9999999999999475, range -1.2210712360473708 ... 49.612718784516446\n",
      "08:11    x_10: mean -3.388223035472038e-17, std 1.0000000000000167, range -0.9401683603105702 ... 35.42826293866068\n",
      "08:11    x_11: mean -3.961275751862559e-18, std 1.0000000000000238, range -1.9608620627359794 ... 1.9612227304987184\n",
      "08:11    x_12: mean 1.4281908988778014e-18, std 0.9999999999999661, range -1.7264633696545932 ... 1.735759286442455\n",
      "08:11    x_13: mean 1.715960706860642e-17, std 1.0000000000017715, range -0.4322582888902345 ... 34.55477027827278\n",
      "08:11    x_14: mean -1.3630947037768238e-14, std 1.000000000010175, range -0.26133441941464486 ... 30.888768997290697\n",
      "08:11    x_15: mean -6.406075669929123e-17, std 0.999999999989744, range -6.050122985573599 ... 5.959640670100675\n",
      "08:11    x_16: mean -4.632738637155853e-18, std 1.000000000004913, range -3.90542723227731 ... 3.9088075638129087\n",
      "08:11    x_17: mean -1.3677947663381928e-18, std 1.0000000000000056, range -2.4250155165225458 ... 2.415100969576658\n",
      "08:11    x_18: mean 7.148059921746608e-18, std 1.0000000000000235, range -2.0363799965092397 ... 2.062493368133421\n",
      "08:11    x_19: mean 8.517726435286476e-14, std 1.0000000000000078, range -1.567190407938899 ... 39.475276881813606\n",
      "08:11    x_20: mean -6.184937717534922e-14, std 0.9999999999999897, range -1.2292712475689087 ... 51.37515275434957\n",
      "08:11    x_21: mean -1.5590381430286015e-14, std 1.0000000000000278, range -0.9734906035208701 ... 36.913529577506125\n",
      "08:11    x_22: mean -1.3812950783176347e-17, std 1.0000000000000002, range -3.968866484061326 ... 3.8718496260640554\n",
      "08:11    x_23: mean 2.0477841644606087e-17, std 0.9999999999999609, range -2.3565179399931004 ... 2.3075172381699147\n",
      "08:11    x_24: mean -1.2425898887613585e-13, std 1.0000000000000255, range -1.3198112942165343 ... 37.2996251556932\n",
      "08:11    x_25: mean 4.858978286392812e-14, std 1.0000000000000344, range -0.9762666148022804 ... 52.98417600397193\n",
      "08:11    x_26: mean 3.0656366334369525e-16, std 0.9999999999999792, range -1.523654341803933 ... 45.73829797667318\n",
      "08:11    x_27: mean -9.482192808718536e-17, std 0.9999999999999815, range -1.5508349419199754 ... 1.5607168905475692\n",
      "08:11  Creating model for method sally\n",
      "08:11  Training model\n",
      "08:11    Epoch 1: train loss 36.5542 (mse_score: 36.5542)\n",
      "08:11              val. loss  30.8517 (mse_score: 30.8517) (*)\n",
      "08:12    Epoch 2: train loss 36.5349 (mse_score: 36.5349)\n",
      "08:12              val. loss  30.8507 (mse_score: 30.8507) (*)\n",
      "08:12    Epoch 3: train loss 36.5223 (mse_score: 36.5223)\n",
      "08:12              val. loss  30.8401 (mse_score: 30.8401) (*)\n",
      "08:12    Epoch 4: train loss 36.4972 (mse_score: 36.4972)\n",
      "08:12              val. loss  30.8744 (mse_score: 30.8744)\n",
      "08:13    Epoch 5: train loss 36.4824 (mse_score: 36.4824)\n",
      "08:13              val. loss  30.8048 (mse_score: 30.8048) (*)\n",
      "08:13    Epoch 6: train loss 36.4340 (mse_score: 36.4340)\n",
      "08:13              val. loss  30.8058 (mse_score: 30.8058)\n",
      "08:13    Epoch 7: train loss 36.4076 (mse_score: 36.4076)\n",
      "08:13              val. loss  30.8122 (mse_score: 30.8122)\n",
      "08:13    Epoch 8: train loss 36.3301 (mse_score: 36.3301)\n",
      "08:13              val. loss  30.8920 (mse_score: 30.8920)\n",
      "08:14    Epoch 9: train loss 36.2939 (mse_score: 36.2939)\n",
      "08:14              val. loss  30.7733 (mse_score: 30.7733) (*)\n",
      "08:14    Epoch 10: train loss 36.2110 (mse_score: 36.2110)\n",
      "08:14              val. loss  30.7582 (mse_score: 30.7582) (*)\n",
      "08:14    Epoch 11: train loss 36.1340 (mse_score: 36.1340)\n",
      "08:14              val. loss  30.7874 (mse_score: 30.7874)\n",
      "08:15    Epoch 12: train loss 36.0658 (mse_score: 36.0658)\n",
      "08:15              val. loss  30.7966 (mse_score: 30.7966)\n",
      "08:15    Epoch 13: train loss 35.9426 (mse_score: 35.9426)\n",
      "08:15              val. loss  30.7812 (mse_score: 30.7812)\n",
      "08:15    Epoch 14: train loss 35.8488 (mse_score: 35.8488)\n",
      "08:15              val. loss  30.7352 (mse_score: 30.7352) (*)\n",
      "08:16    Epoch 15: train loss 35.7728 (mse_score: 35.7728)\n",
      "08:16              val. loss  30.7953 (mse_score: 30.7953)\n",
      "08:16    Epoch 16: train loss 35.6655 (mse_score: 35.6655)\n",
      "08:16              val. loss  30.7611 (mse_score: 30.7611)\n",
      "08:16    Epoch 17: train loss 35.5942 (mse_score: 35.5942)\n",
      "08:16              val. loss  30.7787 (mse_score: 30.7787)\n",
      "08:17    Epoch 18: train loss 35.4821 (mse_score: 35.4821)\n",
      "08:17              val. loss  30.7613 (mse_score: 30.7613)\n",
      "08:27    Epoch 19: train loss 35.3855 (mse_score: 35.3855)\n",
      "08:27              val. loss  30.7819 (mse_score: 30.7819)\n",
      "08:27    Epoch 20: train loss 35.2660 (mse_score: 35.2660)\n",
      "08:27              val. loss  30.7764 (mse_score: 30.7764)\n",
      "08:28    Epoch 21: train loss 35.1730 (mse_score: 35.1730)\n",
      "08:28              val. loss  30.8506 (mse_score: 30.8506)\n",
      "08:28    Epoch 22: train loss 35.1064 (mse_score: 35.1064)\n",
      "08:28              val. loss  30.9144 (mse_score: 30.9144)\n",
      "08:28    Epoch 23: train loss 34.9921 (mse_score: 34.9921)\n",
      "08:28              val. loss  30.7656 (mse_score: 30.7656)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:29    Epoch 24: train loss 34.8782 (mse_score: 34.8782)\n",
      "08:29              val. loss  30.7689 (mse_score: 30.7689)\n",
      "08:29    Epoch 25: train loss 34.8256 (mse_score: 34.8256)\n",
      "08:29              val. loss  30.7974 (mse_score: 30.7974)\n",
      "08:29    Epoch 26: train loss 34.7229 (mse_score: 34.7229)\n",
      "08:29              val. loss  31.0003 (mse_score: 31.0003)\n",
      "08:30    Epoch 27: train loss 34.6452 (mse_score: 34.6452)\n",
      "08:30              val. loss  30.8496 (mse_score: 30.8496)\n",
      "08:30    Epoch 28: train loss 34.5561 (mse_score: 34.5561)\n",
      "08:30              val. loss  30.8253 (mse_score: 30.8253)\n",
      "08:30    Epoch 29: train loss 34.4853 (mse_score: 34.4853)\n",
      "08:30              val. loss  30.8868 (mse_score: 30.8868)\n",
      "08:30    Epoch 30: train loss 34.4126 (mse_score: 34.4126)\n",
      "08:30              val. loss  30.8629 (mse_score: 30.8629)\n",
      "08:31    Epoch 31: train loss 34.3180 (mse_score: 34.3180)\n",
      "08:31              val. loss  30.8576 (mse_score: 30.8576)\n",
      "08:31    Epoch 32: train loss 34.2919 (mse_score: 34.2919)\n",
      "08:31              val. loss  30.7949 (mse_score: 30.7949)\n",
      "08:31    Epoch 33: train loss 34.1925 (mse_score: 34.1925)\n",
      "08:31              val. loss  30.8535 (mse_score: 30.8535)\n",
      "08:32    Epoch 34: train loss 34.1126 (mse_score: 34.1126)\n",
      "08:32              val. loss  30.8276 (mse_score: 30.8276)\n",
      "08:32  No improvement for 20 epochs, stopping training\n",
      "08:32  Early stopping after epoch 14, with loss 30.74 compared to final loss 30.83\n",
      "08:32  Finished training\n",
      "08:32  Training estimator 5 / 10 in ensemble\n",
      "08:32  Starting training\n",
      "08:32    Method:                 sally\n",
      "08:32    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "08:32                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "08:32    Features:               all\n",
      "08:32    Method:                 sally\n",
      "08:32    Hidden layers:          (100, 100)\n",
      "08:32    Activation function:    tanh\n",
      "08:32    Batch size:             128\n",
      "08:32    Trainer:                amsgrad\n",
      "08:32    Epochs:                 50\n",
      "08:32    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "08:32    Validation split:       0.25\n",
      "08:32    Early stopping:         True\n",
      "08:32    Scale inputs:           True\n",
      "08:32    Regularization:         None\n",
      "08:32  Loading training data\n",
      "08:32  Found 1000000 samples with 2 parameters and 27 observables\n",
      "08:32  Rescaling inputs\n",
      "08:32  Observable ranges:\n",
      "08:32    x_1: mean -1.645048541831784e-16, std 1.0000000000000366, range -1.2694017013127752 ... 44.46516963259183\n",
      "08:32    x_2: mean -3.0127011996228247e-17, std 1.0000000000000142, range -1.7272043568409865 ... 1.7328423452738477\n",
      "08:32    x_3: mean -8.159319264677834e-14, std 1.0000000000000124, range -0.6564398979202711 ... 23.524800174587885\n",
      "08:32    x_4: mean 5.638156608256395e-18, std 0.9999999999999992, range -5.242337683031748 ... 4.712282350160786\n",
      "08:32    x_5: mean 6.210498781911156e-17, std 1.0000000000000124, range -0.8270119358684379 ... 26.987556830390748\n",
      "08:32    x_6: mean -2.7727374174446594e-14, std 0.9999999999999929, range -0.7669839954753288 ... 34.23130882752661\n",
      "08:32    x_7: mean 8.039791055125533e-18, std 0.9999999999999836, range -2.045533545981855 ... 2.0269689485224345\n",
      "08:32    x_8: mean 1.830180451634078e-17, std 1.0, range -1.7384111602771233 ... 1.7274791303930708\n",
      "08:32    x_9: mean -2.347277927583491e-17, std 0.9999999999999816, range -1.217217911975517 ... 41.030593574716434\n",
      "08:32    x_10: mean 9.78257475026112e-17, std 0.9999999999999915, range -0.9330656575543836 ... 33.988412686424056\n",
      "08:32    x_11: mean -2.970956813896919e-17, std 1.0000000000000435, range -1.9605304179024197 ... 1.9624609879715398\n",
      "08:32    x_12: mean -1.6001422409317457e-17, std 1.0000000000000846, range -1.7272612010406385 ... 1.7356366102646217\n",
      "08:32    x_13: mean 1.9239720927544113e-17, std 1.0000000000025036, range -0.43289438000113384 ... 26.70995987848743\n",
      "08:32    x_14: mean -1.7635347404620916e-14, std 0.9999999999997563, range -0.2624494039073763 ... 31.229733889985766\n",
      "08:32    x_15: mean 8.341771717823576e-18, std 1.000000000001868, range -6.064801469810021 ... 6.006619080490245\n",
      "08:32    x_16: mean 3.517897084748256e-17, std 1.0000000000009033, range -3.905432447961606 ... 3.9072974163722405\n",
      "08:32    x_17: mean -4.1804781858445493e-17, std 1.000000000000003, range -2.436320349990198 ... 2.41820715299139\n",
      "08:32    x_18: mean 3.0119906568870645e-17, std 1.0000000000000127, range -2.038166520538321 ... 2.0595530563522035\n",
      "08:32    x_19: mean 1.1821818901580627e-13, std 1.0000000000000073, range -1.5651589882747237 ... 37.51388057908033\n",
      "08:32    x_20: mean 5.551967774408695e-15, std 0.999999999999982, range -1.2242582348151647 ... 34.989542799849694\n",
      "08:32    x_21: mean -1.537193483613919e-14, std 0.9999999999999866, range -0.9696213854576534 ... 36.81370688491352\n",
      "08:32    x_22: mean 4.192202140984591e-18, std 0.9999999999999644, range -3.9647894859716204 ... 3.8674911866578805\n",
      "08:32    x_23: mean 2.6087576543432077e-17, std 1.0000000000000298, range -2.357053524095637 ... 2.313654335339666\n",
      "08:32    x_24: mean -1.1236277686066388e-13, std 0.9999999999999605, range -1.3411245996408327 ... 38.09529874996471\n",
      "08:32    x_25: mean 3.6297143068964035e-14, std 1.000000000000037, range -0.9787917636776305 ... 28.931717015366985\n",
      "08:32    x_26: mean -2.420843259187677e-14, std 1.0000000000000209, range -1.5306789147472402 ... 54.92540020158324\n",
      "08:32    x_27: mean -1.2518341918621446e-16, std 0.9999999999999324, range -1.5552793659258795 ... 1.5587925039243369\n",
      "08:32  Creating model for method sally\n",
      "08:32  Training model\n",
      "08:32    Epoch 1: train loss 26.5792 (mse_score: 26.5792)\n",
      "08:32              val. loss  27.4308 (mse_score: 27.4308) (*)\n",
      "08:32    Epoch 2: train loss 26.5615 (mse_score: 26.5615)\n",
      "08:32              val. loss  27.4088 (mse_score: 27.4088) (*)\n",
      "08:33    Epoch 3: train loss 26.5398 (mse_score: 26.5398)\n",
      "08:33              val. loss  27.3857 (mse_score: 27.3857) (*)\n",
      "08:33    Epoch 4: train loss 26.4937 (mse_score: 26.4937)\n",
      "08:33              val. loss  27.3249 (mse_score: 27.3249) (*)\n",
      "08:33    Epoch 5: train loss 26.4494 (mse_score: 26.4494)\n",
      "08:33              val. loss  27.2666 (mse_score: 27.2666) (*)\n",
      "08:34    Epoch 6: train loss 26.3580 (mse_score: 26.3580)\n",
      "08:34              val. loss  27.2248 (mse_score: 27.2248) (*)\n",
      "08:34    Epoch 7: train loss 26.2871 (mse_score: 26.2871)\n",
      "08:34              val. loss  27.2160 (mse_score: 27.2160) (*)\n",
      "08:35    Epoch 8: train loss 26.2162 (mse_score: 26.2162)\n",
      "08:35              val. loss  27.1672 (mse_score: 27.1672) (*)\n",
      "08:35    Epoch 9: train loss 26.0828 (mse_score: 26.0828)\n",
      "08:35              val. loss  27.0616 (mse_score: 27.0616) (*)\n",
      "08:35    Epoch 10: train loss 25.9915 (mse_score: 25.9915)\n",
      "08:35              val. loss  27.0076 (mse_score: 27.0076) (*)\n",
      "08:35    Epoch 11: train loss 25.8692 (mse_score: 25.8692)\n",
      "08:35              val. loss  27.0643 (mse_score: 27.0643)\n",
      "08:36    Epoch 12: train loss 25.7542 (mse_score: 25.7542)\n",
      "08:36              val. loss  27.2350 (mse_score: 27.2350)\n",
      "08:36    Epoch 13: train loss 25.6413 (mse_score: 25.6413)\n",
      "08:36              val. loss  26.9804 (mse_score: 26.9804) (*)\n",
      "08:37    Epoch 14: train loss 25.5526 (mse_score: 25.5526)\n",
      "08:37              val. loss  26.9583 (mse_score: 26.9583) (*)\n",
      "08:37    Epoch 15: train loss 25.4215 (mse_score: 25.4215)\n",
      "08:37              val. loss  27.0401 (mse_score: 27.0401)\n",
      "08:37    Epoch 16: train loss 25.3638 (mse_score: 25.3638)\n",
      "08:37              val. loss  26.9428 (mse_score: 26.9428) (*)\n",
      "08:37    Epoch 17: train loss 25.1881 (mse_score: 25.1881)\n",
      "08:37              val. loss  27.2881 (mse_score: 27.2881)\n",
      "08:38    Epoch 18: train loss 25.0980 (mse_score: 25.0980)\n",
      "08:38              val. loss  27.0469 (mse_score: 27.0469)\n",
      "08:38    Epoch 19: train loss 25.0100 (mse_score: 25.0100)\n",
      "08:38              val. loss  27.0943 (mse_score: 27.0943)\n",
      "08:38    Epoch 20: train loss 24.9136 (mse_score: 24.9136)\n",
      "08:38              val. loss  27.1336 (mse_score: 27.1336)\n",
      "08:39    Epoch 21: train loss 24.8176 (mse_score: 24.8176)\n",
      "08:39              val. loss  27.1159 (mse_score: 27.1159)\n",
      "08:39    Epoch 22: train loss 24.7634 (mse_score: 24.7634)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:39              val. loss  27.1545 (mse_score: 27.1545)\n",
      "08:40    Epoch 23: train loss 24.6633 (mse_score: 24.6633)\n",
      "08:40              val. loss  26.9766 (mse_score: 26.9766)\n",
      "08:40    Epoch 24: train loss 24.6330 (mse_score: 24.6330)\n",
      "08:40              val. loss  27.0788 (mse_score: 27.0788)\n",
      "08:41    Epoch 25: train loss 24.5185 (mse_score: 24.5185)\n",
      "08:41              val. loss  27.3031 (mse_score: 27.3031)\n",
      "08:41    Epoch 26: train loss 24.4316 (mse_score: 24.4316)\n",
      "08:41              val. loss  27.1269 (mse_score: 27.1269)\n",
      "08:41    Epoch 27: train loss 24.3671 (mse_score: 24.3671)\n",
      "08:41              val. loss  27.2349 (mse_score: 27.2349)\n",
      "08:42    Epoch 28: train loss 24.2896 (mse_score: 24.2896)\n",
      "08:42              val. loss  27.1004 (mse_score: 27.1004)\n",
      "08:42    Epoch 29: train loss 24.2326 (mse_score: 24.2326)\n",
      "08:42              val. loss  27.3237 (mse_score: 27.3237)\n",
      "08:42    Epoch 30: train loss 24.1806 (mse_score: 24.1806)\n",
      "08:42              val. loss  26.9859 (mse_score: 26.9859)\n",
      "08:42    Epoch 31: train loss 24.1004 (mse_score: 24.1004)\n",
      "08:42              val. loss  27.6451 (mse_score: 27.6451)\n",
      "08:43    Epoch 32: train loss 24.0726 (mse_score: 24.0726)\n",
      "08:43              val. loss  27.1581 (mse_score: 27.1581)\n",
      "08:43    Epoch 33: train loss 23.9956 (mse_score: 23.9956)\n",
      "08:43              val. loss  27.2776 (mse_score: 27.2776)\n",
      "08:43    Epoch 34: train loss 23.9536 (mse_score: 23.9536)\n",
      "08:43              val. loss  27.0246 (mse_score: 27.0246)\n",
      "08:44    Epoch 35: train loss 23.9114 (mse_score: 23.9114)\n",
      "08:44              val. loss  27.4141 (mse_score: 27.4141)\n",
      "08:44    Epoch 36: train loss 23.8498 (mse_score: 23.8498)\n",
      "08:44              val. loss  27.2616 (mse_score: 27.2616)\n",
      "08:44  No improvement for 20 epochs, stopping training\n",
      "08:44  Early stopping after epoch 16, with loss 26.94 compared to final loss 27.26\n",
      "08:44  Finished training\n",
      "08:44  Training estimator 6 / 10 in ensemble\n",
      "08:44  Starting training\n",
      "08:44    Method:                 sally\n",
      "08:44    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "08:44                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "08:44    Features:               all\n",
      "08:44    Method:                 sally\n",
      "08:44    Hidden layers:          (100, 100)\n",
      "08:44    Activation function:    tanh\n",
      "08:44    Batch size:             128\n",
      "08:44    Trainer:                amsgrad\n",
      "08:44    Epochs:                 50\n",
      "08:44    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "08:44    Validation split:       0.25\n",
      "08:44    Early stopping:         True\n",
      "08:44    Scale inputs:           True\n",
      "08:44    Regularization:         None\n",
      "08:44  Loading training data\n",
      "08:44  Found 1000000 samples with 2 parameters and 27 observables\n",
      "08:44  Rescaling inputs\n",
      "08:44  Observable ranges:\n",
      "08:44    x_1: mean -1.1693401802403968e-16, std 0.9999999999999911, range -1.266964591735942 ... 44.86733640357409\n",
      "08:44    x_2: mean -4.1957548546633915e-18, std 0.9999999999999607, range -1.731440400152219 ... 1.7292736231771333\n",
      "08:44    x_3: mean -8.018520958330556e-15, std 1.0000000000000124, range -0.6540470024440289 ... 23.446558788959447\n",
      "08:44    x_4: mean 1.346673883517724e-16, std 0.9999999999999573, range -5.241951282576335 ... 4.8499856580743765\n",
      "08:44    x_5: mean 7.179856709171872e-17, std 1.0000000000000526, range -0.8261101458955066 ... 37.65625564822212\n",
      "08:44    x_6: mean -8.140122531585803e-14, std 1.0000000000000187, range -0.7667978192520314 ... 31.780821889809573\n",
      "08:44    x_7: mean -5.410782932813163e-18, std 1.0000000000000309, range -2.0481590385743638 ... 2.029060363961561\n",
      "08:44    x_8: mean -8.913758620110457e-18, std 1.0000000000000715, range -1.7404693568951453 ... 1.7232783369274611\n",
      "08:44    x_9: mean 8.043343768804334e-17, std 1.0000000000000449, range -1.2102940586462212 ... 40.71655921000905\n",
      "08:44    x_10: mean -5.983480377835803e-17, std 0.9999999999999895, range -0.9359606075238633 ... 35.23987081508553\n",
      "08:44    x_11: mean 1.7188028778036825e-17, std 0.9999999999999943, range -1.9610540836475168 ... 1.9601363547417103\n",
      "08:44    x_12: mean -4.284572696633404e-18, std 1.0000000000000178, range -1.7260887947426673 ... 1.737593125068786\n",
      "08:44    x_13: mean 4.7691628424217925e-17, std 1.0000000000060045, range -0.4327587808646808 ... 29.919510999823665\n",
      "08:44    x_14: mean -1.9774105908254568e-14, std 0.9999999999957377, range -0.26194191142815926 ... 24.723670448942254\n",
      "08:44    x_15: mean -6.104627914282901e-17, std 0.9999999999998038, range -6.044830359558977 ... 5.952918923409367\n",
      "08:44    x_16: mean 6.612843606035313e-17, std 1.0000000000023739, range -3.904324036095306 ... 3.9078447319559273\n",
      "08:44    x_17: mean 3.861799768856145e-18, std 0.9999999999999784, range -2.431120131213534 ... 2.4169445889741663\n",
      "08:44    x_18: mean 3.299760464869905e-17, std 0.9999999999999859, range -2.042401346973843 ... 2.0638035213004406\n",
      "08:44    x_19: mean 1.556363145027717e-13, std 1.0000000000000175, range -1.563739072772997 ... 42.109981527305195\n",
      "08:44    x_20: mean -5.958095883329406e-14, std 0.9999999999999919, range -1.2236022213052011 ... 40.73812889397913\n",
      "08:44    x_21: mean 4.1349730750539494e-14, std 1.0000000000000069, range -0.9692924507174442 ... 35.36265469786166\n",
      "08:44    x_22: mean -5.6736837450443996e-18, std 0.9999999999999977, range -3.9626861148996317 ... 3.867558429803777\n",
      "08:44    x_23: mean 1.6704859717719954e-17, std 1.0000000000000187, range -2.358782823305283 ... 2.3081977544441874\n",
      "08:44    x_24: mean -9.019851532343637e-14, std 1.0000000000000426, range -1.3382398778577098 ... 37.68918292547416\n",
      "08:44    x_25: mean 8.773512050197496e-14, std 0.9999999999999875, range -0.977751355103022 ... 27.64667266343014\n",
      "08:44    x_26: mean -7.77555904107885e-14, std 1.000000000000013, range -1.5309704613346902 ... 50.576929853213684\n",
      "08:44    x_27: mean -4.726565805412974e-16, std 0.9999999999999484, range -1.553061630195205 ... 1.5598924169479842\n",
      "08:44  Creating model for method sally\n",
      "08:44  Training model\n",
      "08:45    Epoch 1: train loss 22.3492 (mse_score: 22.3492)\n",
      "08:45              val. loss  23.2644 (mse_score: 23.2644) (*)\n",
      "08:45    Epoch 2: train loss 22.3386 (mse_score: 22.3386)\n",
      "08:45              val. loss  23.2468 (mse_score: 23.2468) (*)\n",
      "08:45    Epoch 3: train loss 22.3271 (mse_score: 22.3271)\n",
      "08:45              val. loss  23.2652 (mse_score: 23.2652)\n",
      "08:46    Epoch 4: train loss 22.2969 (mse_score: 22.2969)\n",
      "08:46              val. loss  23.2632 (mse_score: 23.2632)\n",
      "08:46    Epoch 5: train loss 22.2646 (mse_score: 22.2646)\n",
      "08:46              val. loss  23.2406 (mse_score: 23.2406) (*)\n",
      "08:46    Epoch 6: train loss 22.2392 (mse_score: 22.2392)\n",
      "08:46              val. loss  23.2079 (mse_score: 23.2079) (*)\n",
      "08:47    Epoch 7: train loss 22.2027 (mse_score: 22.2027)\n",
      "08:47              val. loss  23.1907 (mse_score: 23.1907) (*)\n",
      "08:47    Epoch 8: train loss 22.1782 (mse_score: 22.1782)\n",
      "08:47              val. loss  23.2113 (mse_score: 23.2113)\n",
      "08:47    Epoch 9: train loss 22.1388 (mse_score: 22.1388)\n",
      "08:47              val. loss  23.1403 (mse_score: 23.1403) (*)\n",
      "08:48    Epoch 10: train loss 22.1120 (mse_score: 22.1120)\n",
      "08:48              val. loss  23.1698 (mse_score: 23.1698)\n",
      "08:48    Epoch 11: train loss 22.0699 (mse_score: 22.0699)\n",
      "08:48              val. loss  23.1999 (mse_score: 23.1999)\n",
      "08:48    Epoch 12: train loss 22.0296 (mse_score: 22.0296)\n",
      "08:48              val. loss  23.1842 (mse_score: 23.1842)\n",
      "08:49    Epoch 13: train loss 21.9939 (mse_score: 21.9939)\n",
      "08:49              val. loss  23.2778 (mse_score: 23.2778)\n",
      "08:49    Epoch 14: train loss 21.9418 (mse_score: 21.9418)\n",
      "08:49              val. loss  23.2042 (mse_score: 23.2042)\n",
      "08:49    Epoch 15: train loss 21.9058 (mse_score: 21.9058)\n",
      "08:49              val. loss  23.2097 (mse_score: 23.2097)\n",
      "08:50    Epoch 16: train loss 21.8379 (mse_score: 21.8379)\n",
      "08:50              val. loss  23.2160 (mse_score: 23.2160)\n",
      "08:50    Epoch 17: train loss 21.8173 (mse_score: 21.8173)\n",
      "08:50              val. loss  23.2027 (mse_score: 23.2027)\n",
      "08:50    Epoch 18: train loss 21.7135 (mse_score: 21.7135)\n",
      "08:50              val. loss  23.2142 (mse_score: 23.2142)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:51    Epoch 19: train loss 21.6782 (mse_score: 21.6782)\n",
      "08:51              val. loss  23.2659 (mse_score: 23.2659)\n",
      "08:51    Epoch 20: train loss 21.6140 (mse_score: 21.6140)\n",
      "08:51              val. loss  23.1706 (mse_score: 23.1706)\n",
      "08:51    Epoch 21: train loss 21.5636 (mse_score: 21.5636)\n",
      "08:51              val. loss  23.2177 (mse_score: 23.2177)\n",
      "08:52    Epoch 22: train loss 21.5038 (mse_score: 21.5038)\n",
      "08:52              val. loss  23.3174 (mse_score: 23.3174)\n",
      "08:52    Epoch 23: train loss 21.4381 (mse_score: 21.4381)\n",
      "08:52              val. loss  23.3032 (mse_score: 23.3032)\n",
      "08:52    Epoch 24: train loss 21.3905 (mse_score: 21.3905)\n",
      "08:52              val. loss  23.2532 (mse_score: 23.2532)\n",
      "08:53    Epoch 25: train loss 21.3317 (mse_score: 21.3317)\n",
      "08:53              val. loss  23.2724 (mse_score: 23.2724)\n",
      "08:53    Epoch 26: train loss 21.2841 (mse_score: 21.2841)\n",
      "08:53              val. loss  23.3404 (mse_score: 23.3404)\n",
      "08:53    Epoch 27: train loss 21.2191 (mse_score: 21.2191)\n",
      "08:53              val. loss  23.2672 (mse_score: 23.2672)\n",
      "08:54    Epoch 28: train loss 21.1583 (mse_score: 21.1583)\n",
      "08:54              val. loss  23.3925 (mse_score: 23.3925)\n",
      "08:54    Epoch 29: train loss 21.1354 (mse_score: 21.1354)\n",
      "08:54              val. loss  23.4407 (mse_score: 23.4407)\n",
      "08:54  No improvement for 20 epochs, stopping training\n",
      "08:54  Early stopping after epoch 9, with loss 23.14 compared to final loss 23.44\n",
      "08:54  Finished training\n",
      "08:54  Training estimator 7 / 10 in ensemble\n",
      "08:54  Starting training\n",
      "08:54    Method:                 sally\n",
      "08:54    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "08:54                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "08:54    Features:               all\n",
      "08:54    Method:                 sally\n",
      "08:54    Hidden layers:          (100, 100)\n",
      "08:54    Activation function:    tanh\n",
      "08:54    Batch size:             128\n",
      "08:54    Trainer:                amsgrad\n",
      "08:54    Epochs:                 50\n",
      "08:54    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "08:54    Validation split:       0.25\n",
      "08:54    Early stopping:         True\n",
      "08:54    Scale inputs:           True\n",
      "08:54    Regularization:         None\n",
      "08:54  Loading training data\n",
      "08:54  Found 1000000 samples with 2 parameters and 27 observables\n",
      "08:54  Rescaling inputs\n",
      "08:54  Observable ranges:\n",
      "08:54    x_1: mean -1.226823087563389e-16, std 0.9999999999999855, range -1.2675300373388676 ... 49.2422947539703\n",
      "08:54    x_2: mean -6.252776074688882e-18, std 1.000000000000025, range -1.729340576192757 ... 1.73041380811319\n",
      "08:54    x_3: mean -5.702747429836563e-14, std 0.9999999999999848, range -0.6566956359278177 ... 23.59280364245447\n",
      "08:54    x_4: mean 2.0832402469750378e-16, std 0.9999999999999851, range -5.237870167098847 ... 4.709758899590016\n",
      "08:54    x_5: mean 5.931255486757437e-17, std 1.0000000000000437, range -0.8258372037188274 ... 43.81401519749193\n",
      "08:54    x_6: mean -1.968915697148077e-14, std 0.9999999999999744, range -0.7690215396443201 ... 32.579991076740754\n",
      "08:54    x_7: mean 1.8058443629342946e-17, std 0.9999999999999856, range -2.0449709853988014 ... 2.0289453330208604\n",
      "08:54    x_8: mean 3.015188099197985e-17, std 1.0000000000000384, range -1.7436413948147456 ... 1.7265176514050968\n",
      "08:54    x_9: mean -1.4949819160392508e-16, std 1.0000000000000249, range -1.2123582888389464 ... 48.94113641335624\n",
      "08:54    x_10: mean -1.2072121080564102e-17, std 1.0000000000000346, range -0.9376290949948569 ... 39.9027542807414\n",
      "08:54    x_11: mean -3.139888349323883e-17, std 1.0000000000000462, range -1.9600139874049205 ... 1.9618010779487836\n",
      "08:54    x_12: mean -4.0106584719978856e-17, std 0.9999999999999992, range -1.7254443880623704 ... 1.7358884230406604\n",
      "08:54    x_13: mean -2.3980817331903383e-18, std 0.999999999996481, range -0.4335512198605967 ... 20.127434337603606\n",
      "08:54    x_14: mean -1.6818956893871472e-14, std 0.9999999999921831, range -0.2631600564682608 ... 31.191537154506342\n",
      "08:54    x_15: mean 3.5669245335157026e-18, std 0.9999999999950279, range -6.052366143065699 ... 5.961726047438901\n",
      "08:54    x_16: mean -1.1386447340555605e-17, std 0.9999999999939289, range -3.9017270482198487 ... 3.9023422838959387\n",
      "08:54    x_17: mean 3.5011993304578934e-17, std 0.9999999999999887, range -2.438287497615349 ... 2.4296364264289942\n",
      "08:54    x_18: mean -5.821476634082501e-17, std 1.00000000000006, range -2.035704825153409 ... 2.0610224286667655\n",
      "08:54    x_19: mean 7.95773367201491e-14, std 0.9999999999999336, range -1.559515830022484 ... 41.99620196402866\n",
      "08:54    x_20: mean -5.2475222389603005e-14, std 0.9999999999999858, range -1.2260921869593506 ... 53.34268709514964\n",
      "08:54    x_21: mean 2.1437671193780262e-14, std 1.0000000000000004, range -0.9709504635301797 ... 35.35825739366308\n",
      "08:54    x_22: mean -1.6189716234293884e-17, std 1.0000000000000095, range -3.9590037356746812 ... 3.8644545632967544\n",
      "08:54    x_23: mean 5.808686864838819e-18, std 0.999999999999972, range -2.3604241687979775 ... 2.3105726458333686\n",
      "08:54    x_24: mean -1.025849698521597e-13, std 0.9999999999999689, range -1.3487587127285716 ... 38.793441234594845\n",
      "08:54    x_25: mean 5.579165573976752e-14, std 0.9999999999999678, range -0.9810287160983326 ... 33.76125110348822\n",
      "08:54    x_26: mean 3.082463706505223e-14, std 0.9999999999999926, range -1.5257087659615036 ... 50.44598114371946\n",
      "08:54    x_27: mean -1.8127543910395616e-16, std 1.0000000000000004, range -1.5548215960089962 ... 1.5587779506930877\n",
      "08:54  Creating model for method sally\n",
      "08:54  Training model\n",
      "08:55    Epoch 1: train loss 21.2782 (mse_score: 21.2782)\n",
      "08:55              val. loss  34.4255 (mse_score: 34.4255) (*)\n",
      "08:55    Epoch 2: train loss 21.2548 (mse_score: 21.2548)\n",
      "08:55              val. loss  34.4107 (mse_score: 34.4107) (*)\n",
      "08:55    Epoch 3: train loss 21.2362 (mse_score: 21.2362)\n",
      "08:55              val. loss  34.3878 (mse_score: 34.3878) (*)\n",
      "08:56    Epoch 4: train loss 21.2037 (mse_score: 21.2037)\n",
      "08:56              val. loss  34.3810 (mse_score: 34.3810) (*)\n",
      "08:56    Epoch 5: train loss 21.1531 (mse_score: 21.1531)\n",
      "08:56              val. loss  34.3810 (mse_score: 34.3810) (*)\n",
      "08:56    Epoch 6: train loss 21.1166 (mse_score: 21.1166)\n",
      "08:56              val. loss  34.2855 (mse_score: 34.2855) (*)\n",
      "08:57    Epoch 7: train loss 21.0595 (mse_score: 21.0595)\n",
      "08:57              val. loss  34.2481 (mse_score: 34.2481) (*)\n",
      "08:57    Epoch 8: train loss 21.0021 (mse_score: 21.0021)\n",
      "08:57              val. loss  34.3834 (mse_score: 34.3834)\n",
      "08:57    Epoch 9: train loss 20.9338 (mse_score: 20.9338)\n",
      "08:57              val. loss  34.2063 (mse_score: 34.2063) (*)\n",
      "08:58    Epoch 10: train loss 20.8729 (mse_score: 20.8729)\n",
      "08:58              val. loss  34.1795 (mse_score: 34.1795) (*)\n",
      "08:58    Epoch 11: train loss 20.8011 (mse_score: 20.8011)\n",
      "08:58              val. loss  34.2490 (mse_score: 34.2490)\n",
      "08:59    Epoch 12: train loss 20.7413 (mse_score: 20.7413)\n",
      "08:59              val. loss  34.1650 (mse_score: 34.1650) (*)\n",
      "08:59    Epoch 13: train loss 20.6851 (mse_score: 20.6851)\n",
      "08:59              val. loss  34.1464 (mse_score: 34.1464) (*)\n",
      "08:59    Epoch 14: train loss 20.5841 (mse_score: 20.5841)\n",
      "08:59              val. loss  34.1029 (mse_score: 34.1029) (*)\n",
      "09:00    Epoch 15: train loss 20.4870 (mse_score: 20.4870)\n",
      "09:00              val. loss  34.1293 (mse_score: 34.1293)\n",
      "09:00    Epoch 16: train loss 20.4747 (mse_score: 20.4747)\n",
      "09:00              val. loss  34.1251 (mse_score: 34.1251)\n",
      "09:00    Epoch 17: train loss 20.3341 (mse_score: 20.3341)\n",
      "09:00              val. loss  34.4195 (mse_score: 34.4195)\n",
      "09:01    Epoch 18: train loss 20.2620 (mse_score: 20.2620)\n",
      "09:01              val. loss  34.1534 (mse_score: 34.1534)\n",
      "09:01    Epoch 19: train loss 20.1238 (mse_score: 20.1238)\n",
      "09:01              val. loss  34.0718 (mse_score: 34.0718) (*)\n",
      "09:01    Epoch 20: train loss 20.1042 (mse_score: 20.1042)\n",
      "09:01              val. loss  34.2006 (mse_score: 34.2006)\n",
      "09:02    Epoch 21: train loss 19.9734 (mse_score: 19.9734)\n",
      "09:02              val. loss  34.2920 (mse_score: 34.2920)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:02    Epoch 22: train loss 19.8801 (mse_score: 19.8801)\n",
      "09:02              val. loss  34.1598 (mse_score: 34.1598)\n",
      "09:02    Epoch 23: train loss 19.8040 (mse_score: 19.8040)\n",
      "09:02              val. loss  34.1401 (mse_score: 34.1401)\n",
      "09:03    Epoch 24: train loss 19.7144 (mse_score: 19.7144)\n",
      "09:03              val. loss  34.1752 (mse_score: 34.1752)\n",
      "09:03    Epoch 25: train loss 19.6179 (mse_score: 19.6179)\n",
      "09:03              val. loss  34.1667 (mse_score: 34.1667)\n",
      "09:04    Epoch 26: train loss 19.5295 (mse_score: 19.5295)\n",
      "09:04              val. loss  34.2261 (mse_score: 34.2261)\n",
      "09:04    Epoch 27: train loss 19.4490 (mse_score: 19.4490)\n",
      "09:04              val. loss  34.1560 (mse_score: 34.1560)\n",
      "09:04    Epoch 28: train loss 19.3671 (mse_score: 19.3671)\n",
      "09:04              val. loss  34.2720 (mse_score: 34.2720)\n",
      "09:05    Epoch 29: train loss 19.2803 (mse_score: 19.2803)\n",
      "09:05              val. loss  34.2573 (mse_score: 34.2573)\n",
      "09:05    Epoch 30: train loss 19.2050 (mse_score: 19.2050)\n",
      "09:05              val. loss  34.2214 (mse_score: 34.2214)\n",
      "09:06    Epoch 31: train loss 19.0976 (mse_score: 19.0976)\n",
      "09:06              val. loss  34.3711 (mse_score: 34.3711)\n",
      "09:06    Epoch 32: train loss 19.0389 (mse_score: 19.0389)\n",
      "09:06              val. loss  34.2424 (mse_score: 34.2424)\n",
      "09:06    Epoch 33: train loss 18.9536 (mse_score: 18.9536)\n",
      "09:06              val. loss  34.3392 (mse_score: 34.3392)\n",
      "09:07    Epoch 34: train loss 18.8973 (mse_score: 18.8973)\n",
      "09:07              val. loss  34.3957 (mse_score: 34.3957)\n",
      "09:07    Epoch 35: train loss 18.8333 (mse_score: 18.8333)\n",
      "09:07              val. loss  34.2687 (mse_score: 34.2687)\n",
      "09:07    Epoch 36: train loss 18.7501 (mse_score: 18.7501)\n",
      "09:07              val. loss  34.2189 (mse_score: 34.2189)\n",
      "09:08    Epoch 37: train loss 18.6887 (mse_score: 18.6887)\n",
      "09:08              val. loss  34.3897 (mse_score: 34.3897)\n",
      "09:08    Epoch 38: train loss 18.6369 (mse_score: 18.6369)\n",
      "09:08              val. loss  34.4057 (mse_score: 34.4057)\n",
      "09:08    Epoch 39: train loss 18.5733 (mse_score: 18.5733)\n",
      "09:08              val. loss  34.2999 (mse_score: 34.2999)\n",
      "09:08  No improvement for 20 epochs, stopping training\n",
      "09:08  Early stopping after epoch 19, with loss 34.07 compared to final loss 34.30\n",
      "09:08  Finished training\n",
      "09:08  Training estimator 8 / 10 in ensemble\n",
      "09:08  Starting training\n",
      "09:08    Method:                 sally\n",
      "09:08    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "09:08                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "09:08    Features:               all\n",
      "09:08    Method:                 sally\n",
      "09:08    Hidden layers:          (100, 100)\n",
      "09:08    Activation function:    tanh\n",
      "09:08    Batch size:             128\n",
      "09:08    Trainer:                amsgrad\n",
      "09:08    Epochs:                 50\n",
      "09:08    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "09:08    Validation split:       0.25\n",
      "09:08    Early stopping:         True\n",
      "09:08    Scale inputs:           True\n",
      "09:08    Regularization:         None\n",
      "09:08  Loading training data\n",
      "09:08  Found 1000000 samples with 2 parameters and 27 observables\n",
      "09:08  Rescaling inputs\n",
      "09:08  Observable ranges:\n",
      "09:08    x_1: mean 1.1285550272077672e-16, std 1.0000000000000262, range -1.263014950268326 ... 62.092020094529154\n",
      "09:08    x_2: mean 1.9284129848529118e-17, std 1.0000000000000255, range -1.7276684024836306 ... 1.7316403448464144\n",
      "09:08    x_3: mean -5.5151176070467045e-14, std 1.0000000000000484, range -0.6573195572786572 ... 24.344915924905674\n",
      "09:08    x_4: mean 2.35628405675925e-16, std 1.0000000000000027, range -5.240076407514792 ... 4.896946710576869\n",
      "09:08    x_5: mean -3.240074875066057e-17, std 1.0000000000000127, range -0.8267958128715763 ... 46.80626051453837\n",
      "09:08    x_6: mean -6.649237604960945e-14, std 0.9999999999999736, range -0.772006744714475 ... 27.43245289119744\n",
      "09:08    x_7: mean 0.0, std 0.9999999999999788, range -2.04604857569667 ... 2.0298026236265754\n",
      "09:08    x_8: mean 4.9176662741956534e-17, std 1.0000000000000333, range -1.741545903087977 ... 1.7259672947973537\n",
      "09:08    x_9: mean 6.991740519879386e-18, std 1.0000000000000027, range -1.209536455057799 ... 58.557976009519145\n",
      "09:08    x_10: mean -3.140598892059643e-17, std 1.0000000000000113, range -0.9335627592865869 ... 39.055561231737876\n",
      "09:08    x_11: mean -3.340616672176111e-17, std 1.0000000000000553, range -1.9593173650502032 ... 1.9628254601123836\n",
      "09:08    x_12: mean -3.841904572254862e-17, std 0.9999999999999712, range -1.7273454696862813 ... 1.735327833965476\n",
      "09:08    x_13: mean 1.2963852213943028e-17, std 0.9999999999954102, range -0.4332803065647515 ... 24.020920283058302\n",
      "09:08    x_14: mean -1.777247149448158e-14, std 1.00000000000075, range -0.2620220424302744 ... 30.923440382379646\n",
      "09:08    x_15: mean 2.5626611943607713e-17, std 1.0000000000028348, range -6.04932165066537 ... 5.958029332571511\n",
      "09:08    x_16: mean -1.326228016296227e-17, std 0.9999999999903754, range -3.8976094213601864 ... 3.9019224548856672\n",
      "09:08    x_17: mean -6.089351245464059e-18, std 0.9999999999999692, range -2.434477507756869 ... 2.4132271234209752\n",
      "09:08    x_18: mean 4.916955731459893e-17, std 1.0000000000000195, range -2.021836426599964 ... 2.0597940158144796\n",
      "09:08    x_19: mean 8.033614307123571e-14, std 1.0000000000000118, range -1.5722487073771108 ... 34.23895365879293\n",
      "09:08    x_20: mean -1.142757390937277e-13, std 0.9999999999999919, range -1.2216860657516078 ... 62.03191816160907\n",
      "09:08    x_21: mean 3.896629152677633e-14, std 0.99999999999991, range -0.9667510574374653 ... 43.474996698123405\n",
      "09:08    x_22: mean -2.1138646388862982e-17, std 0.9999999999999665, range -3.958529082888796 ... 3.8618503400408293\n",
      "09:08    x_23: mean 6.888711823194171e-18, std 0.9999999999999952, range -2.357912496725581 ... 2.31080066030204\n",
      "09:08    x_24: mean -6.215856274138787e-14, std 1.000000000000038, range -1.344510671859706 ... 53.037775005555915\n",
      "09:08    x_25: mean 1.2873154631165563e-13, std 0.9999999999999918, range -0.9761880046520519 ... 38.010586009931004\n",
      "09:08    x_26: mean -2.948449662198982e-14, std 1.0000000000000198, range -1.5343567580461612 ... 46.04941017902791\n",
      "09:08    x_27: mean -2.970281798297947e-16, std 0.9999999999999702, range -1.5541272542204252 ... 1.5619766999739748\n",
      "09:08  Creating model for method sally\n",
      "09:08  Training model\n",
      "09:09    Epoch 1: train loss 35.6791 (mse_score: 35.6791)\n",
      "09:09              val. loss  28.2500 (mse_score: 28.2500) (*)\n",
      "09:10    Epoch 2: train loss 35.6752 (mse_score: 35.6752)\n",
      "09:10              val. loss  28.2201 (mse_score: 28.2201) (*)\n",
      "09:10    Epoch 3: train loss 35.6520 (mse_score: 35.6520)\n",
      "09:10              val. loss  28.2083 (mse_score: 28.2083) (*)\n",
      "09:10    Epoch 4: train loss 35.6151 (mse_score: 35.6151)\n",
      "09:10              val. loss  28.1904 (mse_score: 28.1904) (*)\n",
      "09:11    Epoch 5: train loss 35.5723 (mse_score: 35.5723)\n",
      "09:11              val. loss  28.1716 (mse_score: 28.1716) (*)\n",
      "09:11    Epoch 6: train loss 35.5123 (mse_score: 35.5123)\n",
      "09:11              val. loss  28.1515 (mse_score: 28.1515) (*)\n",
      "09:11    Epoch 7: train loss 35.4327 (mse_score: 35.4327)\n",
      "09:11              val. loss  28.3500 (mse_score: 28.3500)\n",
      "09:12    Epoch 8: train loss 35.3768 (mse_score: 35.3768)\n",
      "09:12              val. loss  28.1384 (mse_score: 28.1384) (*)\n",
      "09:12    Epoch 9: train loss 35.3117 (mse_score: 35.3117)\n",
      "09:12              val. loss  28.0493 (mse_score: 28.0493) (*)\n",
      "09:12    Epoch 10: train loss 35.2057 (mse_score: 35.2057)\n",
      "09:12              val. loss  28.0310 (mse_score: 28.0310) (*)\n",
      "09:13    Epoch 11: train loss 35.1138 (mse_score: 35.1138)\n",
      "09:13              val. loss  28.0489 (mse_score: 28.0489)\n",
      "09:13    Epoch 12: train loss 35.0660 (mse_score: 35.0660)\n",
      "09:13              val. loss  27.9994 (mse_score: 27.9994) (*)\n",
      "09:13    Epoch 13: train loss 34.9613 (mse_score: 34.9613)\n",
      "09:13              val. loss  28.4867 (mse_score: 28.4867)\n",
      "09:14    Epoch 14: train loss 34.8787 (mse_score: 34.8787)\n",
      "09:14              val. loss  28.0504 (mse_score: 28.0504)\n",
      "09:14    Epoch 15: train loss 34.8015 (mse_score: 34.8015)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:14              val. loss  28.0520 (mse_score: 28.0520)\n",
      "09:15    Epoch 16: train loss 34.6748 (mse_score: 34.6748)\n",
      "09:15              val. loss  28.1940 (mse_score: 28.1940)\n",
      "09:15    Epoch 17: train loss 34.6431 (mse_score: 34.6431)\n",
      "09:15              val. loss  27.9827 (mse_score: 27.9827) (*)\n",
      "09:16    Epoch 18: train loss 34.5325 (mse_score: 34.5325)\n",
      "09:16              val. loss  27.9971 (mse_score: 27.9971)\n",
      "09:16    Epoch 19: train loss 34.4299 (mse_score: 34.4299)\n",
      "09:16              val. loss  28.0854 (mse_score: 28.0854)\n",
      "09:16    Epoch 20: train loss 34.3365 (mse_score: 34.3365)\n",
      "09:16              val. loss  27.9781 (mse_score: 27.9781) (*)\n",
      "09:17    Epoch 21: train loss 34.2600 (mse_score: 34.2600)\n",
      "09:17              val. loss  28.0037 (mse_score: 28.0037)\n",
      "09:17    Epoch 22: train loss 34.2231 (mse_score: 34.2231)\n",
      "09:17              val. loss  28.0515 (mse_score: 28.0515)\n",
      "09:18    Epoch 23: train loss 34.1220 (mse_score: 34.1220)\n",
      "09:18              val. loss  28.0279 (mse_score: 28.0279)\n",
      "09:18    Epoch 24: train loss 34.0492 (mse_score: 34.0492)\n",
      "09:18              val. loss  27.9934 (mse_score: 27.9934)\n",
      "09:18    Epoch 25: train loss 33.9597 (mse_score: 33.9597)\n",
      "09:18              val. loss  28.0130 (mse_score: 28.0130)\n",
      "09:19    Epoch 26: train loss 33.9313 (mse_score: 33.9313)\n",
      "09:19              val. loss  28.0100 (mse_score: 28.0100)\n",
      "09:19    Epoch 27: train loss 33.8337 (mse_score: 33.8337)\n",
      "09:19              val. loss  28.0360 (mse_score: 28.0360)\n",
      "09:19    Epoch 28: train loss 33.7989 (mse_score: 33.7989)\n",
      "09:19              val. loss  28.0366 (mse_score: 28.0366)\n",
      "09:20    Epoch 29: train loss 33.7023 (mse_score: 33.7023)\n",
      "09:20              val. loss  28.0269 (mse_score: 28.0269)\n",
      "09:20    Epoch 30: train loss 33.6404 (mse_score: 33.6404)\n",
      "09:20              val. loss  28.0120 (mse_score: 28.0120)\n",
      "09:21    Epoch 31: train loss 33.5917 (mse_score: 33.5917)\n",
      "09:21              val. loss  28.0438 (mse_score: 28.0438)\n",
      "09:21    Epoch 32: train loss 33.5628 (mse_score: 33.5628)\n",
      "09:21              val. loss  28.0563 (mse_score: 28.0563)\n",
      "09:22    Epoch 33: train loss 33.4879 (mse_score: 33.4879)\n",
      "09:22              val. loss  28.0313 (mse_score: 28.0313)\n",
      "09:22    Epoch 34: train loss 33.4406 (mse_score: 33.4406)\n",
      "09:22              val. loss  28.3204 (mse_score: 28.3204)\n",
      "09:22    Epoch 35: train loss 33.3901 (mse_score: 33.3901)\n",
      "09:22              val. loss  28.0678 (mse_score: 28.0678)\n",
      "09:23    Epoch 36: train loss 33.3516 (mse_score: 33.3516)\n",
      "09:23              val. loss  28.0042 (mse_score: 28.0042)\n",
      "09:23    Epoch 37: train loss 33.2987 (mse_score: 33.2987)\n",
      "09:23              val. loss  28.1070 (mse_score: 28.1070)\n",
      "09:23    Epoch 38: train loss 33.2514 (mse_score: 33.2514)\n",
      "09:23              val. loss  28.0606 (mse_score: 28.0606)\n",
      "09:24    Epoch 39: train loss 33.1970 (mse_score: 33.1970)\n",
      "09:24              val. loss  28.0471 (mse_score: 28.0471)\n",
      "09:24    Epoch 40: train loss 33.1652 (mse_score: 33.1652)\n",
      "09:24              val. loss  28.0273 (mse_score: 28.0273)\n",
      "09:24  No improvement for 20 epochs, stopping training\n",
      "09:24  Early stopping after epoch 20, with loss 27.98 compared to final loss 28.03\n",
      "09:24  Finished training\n",
      "09:24  Training estimator 9 / 10 in ensemble\n",
      "09:24  Starting training\n",
      "09:24    Method:                 sally\n",
      "09:24    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "09:24                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "09:24    Features:               all\n",
      "09:24    Method:                 sally\n",
      "09:24    Hidden layers:          (100, 100)\n",
      "09:24    Activation function:    tanh\n",
      "09:24    Batch size:             128\n",
      "09:24    Trainer:                amsgrad\n",
      "09:24    Epochs:                 50\n",
      "09:24    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "09:24    Validation split:       0.25\n",
      "09:24    Early stopping:         True\n",
      "09:24    Scale inputs:           True\n",
      "09:24    Regularization:         None\n",
      "09:24  Loading training data\n",
      "09:24  Found 1000000 samples with 2 parameters and 27 observables\n",
      "09:24  Rescaling inputs\n",
      "09:24  Observable ranges:\n",
      "09:24    x_1: mean 1.3173462320992257e-17, std 1.0000000000000093, range -1.2701827725145973 ... 65.47490755666993\n",
      "09:24    x_2: mean 2.2741364347211855e-17, std 0.9999999999999994, range -1.7304929901027402 ... 1.7317947054577878\n",
      "09:24    x_3: mean -4.375053919147831e-14, std 0.9999999999999769, range -0.6573215738197409 ... 18.854030192052427\n",
      "09:24    x_4: mean -2.2872370664117626e-17, std 0.9999999999999997, range -5.233486760416762 ... 4.845069232738428\n",
      "09:24    x_5: mean -9.169554004984093e-18, std 0.9999999999999811, range -0.8255174955371318 ... 42.30463910863708\n",
      "09:24    x_6: mean -4.989716906322883e-14, std 1.000000000000007, range -0.7682004265329869 ... 31.489765352487357\n",
      "09:24    x_7: mean -8.174794174919953e-18, std 1.0000000000000075, range -2.0474337495673782 ... 2.0286787863159885\n",
      "09:24    x_8: mean -2.1671553440683057e-17, std 0.9999999999999919, range -1.7417802763495933 ... 1.7235805723333875\n",
      "09:24    x_9: mean 7.641176580364117e-17, std 1.0000000000000349, range -1.2148141295495354 ... 65.05957899418532\n",
      "09:24    x_10: mean -2.7458035845029373e-17, std 1.0000000000000349, range -0.9317802077915046 ... 54.26340325373221\n",
      "09:24    x_11: mean 4.298783551348606e-18, std 1.0000000000000713, range -1.9625952620574705 ... 1.9621143665502847\n",
      "09:24    x_12: mean 2.2740032079582307e-17, std 0.999999999999981, range -1.7242990465389434 ... 1.7347736052481146\n",
      "09:24    x_13: mean -1.0842882147699129e-17, std 1.0000000000005402, range -0.4335589942143912 ... 22.63536815950553\n",
      "09:24    x_14: mean -1.3119560549057496e-14, std 1.0000000000084788, range -0.2632167631144203 ... 30.957138881281995\n",
      "09:24    x_15: mean -1.1660006293823245e-17, std 0.9999999999977083, range -6.033775172386598 ... 5.975153904586959\n",
      "09:24    x_16: mean -4.9624304665485395e-17, std 1.0000000000078926, range -3.892722944772129 ... 3.897393776808792\n",
      "09:24    x_17: mean -1.311839525897085e-18, std 0.9999999999999416, range -2.427613637631435 ... 2.425295234658436\n",
      "09:24    x_18: mean -4.9316106753849455e-18, std 0.9999999999999849, range -2.035545333306438 ... 2.0608178175895673\n",
      "09:24    x_19: mean 9.070191353544032e-14, std 0.9999999999999813, range -1.5695300741276466 ... 34.17339753106205\n",
      "09:24    x_20: mean 3.978924389969052e-14, std 1.0000000000000462, range -1.2268186600309592 ... 58.28169009257525\n",
      "09:24    x_21: mean 2.7516691147866368e-14, std 0.999999999999972, range -0.9692651312147041 ... 53.26855065836473\n",
      "09:24    x_22: mean -1.9895196601282805e-19, std 0.999999999999988, range -3.8750050736822863 ... 3.8704430125317786\n",
      "09:24    x_23: mean 1.155520124029863e-18, std 0.9999999999999923, range -2.358988654148817 ... 2.3084372418689574\n",
      "09:24    x_24: mean -8.680651575332377e-14, std 0.9999999999999851, range -1.3400504738085424 ... 55.466442319479285\n",
      "09:24    x_25: mean 6.599095669912458e-14, std 0.9999999999999425, range -0.9777958704596136 ... 27.51191945602314\n",
      "09:24    x_26: mean 3.0564180519832007e-14, std 1.0000000000000264, range -1.5308401150066486 ... 45.97760036983417\n",
      "09:24    x_27: mean -4.713314183391049e-16, std 1.000000000000011, range -1.5535399788304356 ... 1.5613519194048613\n",
      "09:24  Creating model for method sally\n",
      "09:24  Training model\n",
      "09:25    Epoch 1: train loss 28.5763 (mse_score: 28.5763)\n",
      "09:25              val. loss  39.3968 (mse_score: 39.3968) (*)\n",
      "09:25    Epoch 2: train loss 28.5466 (mse_score: 28.5466)\n",
      "09:25              val. loss  39.4450 (mse_score: 39.4450)\n",
      "09:25    Epoch 3: train loss 28.5423 (mse_score: 28.5423)\n",
      "09:25              val. loss  39.3518 (mse_score: 39.3518) (*)\n",
      "09:26    Epoch 4: train loss 28.5133 (mse_score: 28.5133)\n",
      "09:26              val. loss  39.3395 (mse_score: 39.3395) (*)\n",
      "09:26    Epoch 5: train loss 28.4840 (mse_score: 28.4840)\n",
      "09:26              val. loss  39.3257 (mse_score: 39.3257) (*)\n",
      "09:26    Epoch 6: train loss 28.4509 (mse_score: 28.4509)\n",
      "09:26              val. loss  39.2976 (mse_score: 39.2976) (*)\n",
      "09:27    Epoch 7: train loss 28.4266 (mse_score: 28.4266)\n",
      "09:27              val. loss  39.2989 (mse_score: 39.2989)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:27    Epoch 8: train loss 28.3879 (mse_score: 28.3879)\n",
      "09:27              val. loss  39.2800 (mse_score: 39.2800) (*)\n",
      "09:27    Epoch 9: train loss 28.3546 (mse_score: 28.3546)\n",
      "09:27              val. loss  39.2493 (mse_score: 39.2493) (*)\n",
      "09:28    Epoch 10: train loss 28.3089 (mse_score: 28.3089)\n",
      "09:28              val. loss  39.3130 (mse_score: 39.3130)\n",
      "09:28    Epoch 11: train loss 28.2299 (mse_score: 28.2299)\n",
      "09:28              val. loss  39.2809 (mse_score: 39.2809)\n",
      "09:28    Epoch 12: train loss 28.1924 (mse_score: 28.1924)\n",
      "09:28              val. loss  39.2140 (mse_score: 39.2140) (*)\n",
      "09:29    Epoch 13: train loss 28.1880 (mse_score: 28.1880)\n",
      "09:29              val. loss  39.2489 (mse_score: 39.2489)\n",
      "09:29    Epoch 14: train loss 28.1081 (mse_score: 28.1081)\n",
      "09:29              val. loss  39.3116 (mse_score: 39.3116)\n",
      "09:29    Epoch 15: train loss 28.0222 (mse_score: 28.0222)\n",
      "09:29              val. loss  39.2371 (mse_score: 39.2371)\n",
      "09:30    Epoch 16: train loss 27.9888 (mse_score: 27.9888)\n",
      "09:30              val. loss  39.3979 (mse_score: 39.3979)\n",
      "09:30    Epoch 17: train loss 27.8927 (mse_score: 27.8927)\n",
      "09:30              val. loss  39.2709 (mse_score: 39.2709)\n",
      "09:30    Epoch 18: train loss 27.8077 (mse_score: 27.8077)\n",
      "09:30              val. loss  39.1936 (mse_score: 39.1936) (*)\n",
      "09:31    Epoch 19: train loss 27.7532 (mse_score: 27.7532)\n",
      "09:31              val. loss  39.3033 (mse_score: 39.3033)\n",
      "09:31    Epoch 20: train loss 27.6637 (mse_score: 27.6637)\n",
      "09:31              val. loss  39.2229 (mse_score: 39.2229)\n",
      "09:31    Epoch 21: train loss 27.6007 (mse_score: 27.6007)\n",
      "09:31              val. loss  39.2763 (mse_score: 39.2763)\n",
      "09:32    Epoch 22: train loss 27.5830 (mse_score: 27.5830)\n",
      "09:32              val. loss  39.3260 (mse_score: 39.3260)\n",
      "09:32    Epoch 23: train loss 27.4308 (mse_score: 27.4308)\n",
      "09:32              val. loss  39.2824 (mse_score: 39.2824)\n",
      "09:32    Epoch 24: train loss 27.3411 (mse_score: 27.3411)\n",
      "09:32              val. loss  39.2983 (mse_score: 39.2983)\n",
      "09:33    Epoch 25: train loss 27.2463 (mse_score: 27.2463)\n",
      "09:33              val. loss  39.2877 (mse_score: 39.2877)\n",
      "09:33    Epoch 26: train loss 27.1819 (mse_score: 27.1819)\n",
      "09:33              val. loss  39.3646 (mse_score: 39.3646)\n",
      "09:34    Epoch 27: train loss 27.1109 (mse_score: 27.1109)\n",
      "09:34              val. loss  39.3069 (mse_score: 39.3069)\n",
      "09:34    Epoch 28: train loss 27.0592 (mse_score: 27.0592)\n",
      "09:34              val. loss  39.3304 (mse_score: 39.3304)\n",
      "09:34    Epoch 29: train loss 26.9394 (mse_score: 26.9394)\n",
      "09:34              val. loss  39.3946 (mse_score: 39.3946)\n",
      "09:35    Epoch 30: train loss 26.8658 (mse_score: 26.8658)\n",
      "09:35              val. loss  39.4587 (mse_score: 39.4587)\n",
      "09:35    Epoch 31: train loss 26.8102 (mse_score: 26.8102)\n",
      "09:35              val. loss  39.4434 (mse_score: 39.4434)\n",
      "09:35    Epoch 32: train loss 26.7498 (mse_score: 26.7498)\n",
      "09:35              val. loss  39.4259 (mse_score: 39.4259)\n",
      "09:36    Epoch 33: train loss 26.6507 (mse_score: 26.6507)\n",
      "09:36              val. loss  39.3494 (mse_score: 39.3494)\n",
      "09:36    Epoch 34: train loss 26.6040 (mse_score: 26.6040)\n",
      "09:36              val. loss  39.3880 (mse_score: 39.3880)\n",
      "09:37    Epoch 35: train loss 26.5351 (mse_score: 26.5351)\n",
      "09:37              val. loss  39.4642 (mse_score: 39.4642)\n",
      "09:37    Epoch 36: train loss 26.4783 (mse_score: 26.4783)\n",
      "09:37              val. loss  39.5328 (mse_score: 39.5328)\n",
      "09:37    Epoch 37: train loss 26.4239 (mse_score: 26.4239)\n",
      "09:37              val. loss  39.4420 (mse_score: 39.4420)\n",
      "09:38    Epoch 38: train loss 26.3684 (mse_score: 26.3684)\n",
      "09:38              val. loss  39.5265 (mse_score: 39.5265)\n",
      "09:38  No improvement for 20 epochs, stopping training\n",
      "09:38  Early stopping after epoch 18, with loss 39.19 compared to final loss 39.53\n",
      "09:38  Finished training\n",
      "09:38  Training estimator 10 / 10 in ensemble\n",
      "09:38  Starting training\n",
      "09:38    Method:                 sally\n",
      "09:38    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "09:38                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "09:38    Features:               all\n",
      "09:38    Method:                 sally\n",
      "09:38    Hidden layers:          (100, 100)\n",
      "09:38    Activation function:    tanh\n",
      "09:38    Batch size:             128\n",
      "09:38    Trainer:                amsgrad\n",
      "09:38    Epochs:                 50\n",
      "09:38    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "09:38    Validation split:       0.25\n",
      "09:38    Early stopping:         True\n",
      "09:38    Scale inputs:           True\n",
      "09:38    Regularization:         None\n",
      "09:38  Loading training data\n",
      "09:38  Found 1000000 samples with 2 parameters and 27 observables\n",
      "09:38  Rescaling inputs\n",
      "09:38  Observable ranges:\n",
      "09:38    x_1: mean -7.903722121227474e-17, std 0.9999999999999958, range -1.2701867837754552 ... 52.48096580433534\n",
      "09:38    x_2: mean 3.7765346405649327e-17, std 0.9999999999999837, range -1.7298381727143874 ... 1.7324501682950677\n",
      "09:38    x_3: mean -1.819479678033531e-15, std 0.9999999999999669, range -0.6548578790604961 ... 23.47225538701434\n",
      "09:38    x_4: mean 3.659792469079548e-16, std 1.0000000000000728, range -4.868151371515192 ... 4.711549274704801\n",
      "09:38    x_5: mean -4.82103246213228e-17, std 1.0000000000000349, range -0.8222410989812411 ... 37.99572730889747\n",
      "09:38    x_6: mean -8.098951553847655e-14, std 1.0000000000000235, range -0.7671009973808339 ... 39.54428884863763\n",
      "09:38    x_7: mean -3.964784056620374e-17, std 0.9999999999999841, range -2.0479937337993093 ... 2.0291495762564637\n",
      "09:38    x_8: mean 4.9205084451386936e-18, std 1.0000000000000564, range -1.7409869097867265 ... 1.7259989234761874\n",
      "09:38    x_9: mean 6.2705396430828844e-18, std 1.0000000000000522, range -1.205882803808728 ... 48.18214632189676\n",
      "09:38    x_10: mean -5.657696533489798e-17, std 0.9999999999999672, range -0.9329019117179335 ... 46.46613992037505\n",
      "09:38    x_11: mean -4.520828156273637e-18, std 0.9999999999999651, range -1.9591981263711535 ... 1.961682063679482\n",
      "09:38    x_12: mean 8.270717444247567e-18, std 0.9999999999999877, range -1.725190373367953 ... 1.7340685809224567\n",
      "09:38    x_13: mean 4.301625722291647e-17, std 1.0000000000100508, range -0.43314015425897007 ... 26.89596349693933\n",
      "09:38    x_14: mean -1.3551375133147302e-14, std 0.9999999999937429, range -0.2623784915684279 ... 30.999688791100823\n",
      "09:38    x_15: mean 6.515676886920118e-18, std 0.9999999999978842, range -6.05113870471591 ... 6.007542123709075\n",
      "09:38    x_16: mean 2.2513546582558774e-17, std 1.0000000000044085, range -3.900831671374452 ... 3.899588133659138\n",
      "09:38    x_17: mean -4.2312819914513966e-18, std 0.9999999999999996, range -2.428231559299007 ... 2.428813899416292\n",
      "09:38    x_18: mean -2.0420998225745278e-17, std 0.9999999999999725, range -2.0205770855567686 ... 2.0599047667504036\n",
      "09:38    x_19: mean 1.0362176183775773e-13, std 1.0000000000000244, range -1.5711399381059625 ... 36.914977204735074\n",
      "09:38    x_20: mean -1.213693323620646e-14, std 0.9999999999999788, range -1.2220314993680548 ... 48.85614145286286\n",
      "09:38    x_21: mean 3.0531481343132325e-14, std 1.0000000000000109, range -0.9652835878798619 ... 43.587403127674364\n",
      "09:38    x_22: mean 1.6704859717719954e-17, std 1.000000000000008, range -3.965109909363884 ... 3.930213958442664\n",
      "09:38    x_23: mean -2.3590018827235326e-18, std 1.000000000000036, range -2.356467821562849 ... 2.309177614630513\n",
      "09:38    x_24: mean -8.763853998061677e-14, std 1.0000000000000235, range -1.33724067018088 ... 40.01778608895739\n",
      "09:38    x_25: mean 1.6515848244580412e-14, std 0.9999999999999896, range -0.9766621505204776 ... 37.74395957951772\n",
      "09:38    x_26: mean -1.4465683761955007e-14, std 1.0000000000000084, range -1.532466268569568 ... 46.024641513524934\n",
      "09:38    x_27: mean -3.267182080435305e-16, std 1.0000000000000169, range -1.5544217940178215 ... 1.559913332509844\n",
      "09:38  Creating model for method sally\n",
      "09:38  Training model\n",
      "09:38    Epoch 1: train loss 37.4882 (mse_score: 37.4882)\n",
      "09:38              val. loss  20.3275 (mse_score: 20.3275) (*)\n",
      "09:38    Epoch 2: train loss 37.4728 (mse_score: 37.4728)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:38              val. loss  20.2789 (mse_score: 20.2789) (*)\n",
      "09:39    Epoch 3: train loss 37.4595 (mse_score: 37.4595)\n",
      "09:39              val. loss  20.2430 (mse_score: 20.2430) (*)\n",
      "09:39    Epoch 4: train loss 37.4496 (mse_score: 37.4496)\n",
      "09:39              val. loss  20.2440 (mse_score: 20.2440)\n",
      "09:39    Epoch 5: train loss 37.4252 (mse_score: 37.4252)\n",
      "09:39              val. loss  20.2490 (mse_score: 20.2490)\n",
      "09:40    Epoch 6: train loss 37.3908 (mse_score: 37.3908)\n",
      "09:40              val. loss  20.2153 (mse_score: 20.2153) (*)\n",
      "09:40    Epoch 7: train loss 37.3233 (mse_score: 37.3233)\n",
      "09:40              val. loss  20.3200 (mse_score: 20.3200)\n",
      "09:40    Epoch 8: train loss 37.2923 (mse_score: 37.2923)\n",
      "09:40              val. loss  20.1848 (mse_score: 20.1848) (*)\n",
      "09:41    Epoch 9: train loss 37.2333 (mse_score: 37.2333)\n",
      "09:41              val. loss  20.1681 (mse_score: 20.1681) (*)\n",
      "09:41    Epoch 10: train loss 37.1669 (mse_score: 37.1669)\n",
      "09:41              val. loss  20.4343 (mse_score: 20.4343)\n",
      "09:41    Epoch 11: train loss 37.1285 (mse_score: 37.1285)\n",
      "09:41              val. loss  20.1399 (mse_score: 20.1399) (*)\n",
      "09:42    Epoch 12: train loss 37.0494 (mse_score: 37.0494)\n",
      "09:42              val. loss  20.2148 (mse_score: 20.2148)\n",
      "09:42    Epoch 13: train loss 36.9656 (mse_score: 36.9656)\n",
      "09:42              val. loss  20.5251 (mse_score: 20.5251)\n",
      "09:42    Epoch 14: train loss 36.9013 (mse_score: 36.9013)\n",
      "09:42              val. loss  20.3149 (mse_score: 20.3149)\n",
      "09:43    Epoch 15: train loss 36.8268 (mse_score: 36.8268)\n",
      "09:43              val. loss  20.1433 (mse_score: 20.1433)\n",
      "09:43    Epoch 16: train loss 36.7362 (mse_score: 36.7362)\n",
      "09:43              val. loss  20.1983 (mse_score: 20.1983)\n",
      "09:43    Epoch 17: train loss 36.6691 (mse_score: 36.6691)\n",
      "09:43              val. loss  20.1978 (mse_score: 20.1978)\n",
      "09:44    Epoch 18: train loss 36.6032 (mse_score: 36.6032)\n",
      "09:44              val. loss  20.1705 (mse_score: 20.1705)\n",
      "09:44    Epoch 19: train loss 36.4992 (mse_score: 36.4992)\n",
      "09:44              val. loss  20.1635 (mse_score: 20.1635)\n",
      "09:44    Epoch 20: train loss 36.4245 (mse_score: 36.4245)\n",
      "09:44              val. loss  20.2326 (mse_score: 20.2326)\n",
      "09:45    Epoch 21: train loss 36.3573 (mse_score: 36.3573)\n",
      "09:45              val. loss  20.1588 (mse_score: 20.1588)\n",
      "09:45    Epoch 22: train loss 36.3230 (mse_score: 36.3230)\n",
      "09:45              val. loss  20.2131 (mse_score: 20.2131)\n",
      "09:45    Epoch 23: train loss 36.2136 (mse_score: 36.2136)\n",
      "09:45              val. loss  20.3370 (mse_score: 20.3370)\n",
      "09:46    Epoch 24: train loss 36.1700 (mse_score: 36.1700)\n",
      "09:46              val. loss  20.3401 (mse_score: 20.3401)\n",
      "09:46    Epoch 25: train loss 36.0739 (mse_score: 36.0739)\n",
      "09:46              val. loss  20.2761 (mse_score: 20.2761)\n",
      "09:46    Epoch 26: train loss 36.0425 (mse_score: 36.0425)\n",
      "09:46              val. loss  20.2233 (mse_score: 20.2233)\n",
      "09:47    Epoch 27: train loss 35.9737 (mse_score: 35.9737)\n",
      "09:47              val. loss  20.1820 (mse_score: 20.1820)\n",
      "09:47    Epoch 28: train loss 35.9377 (mse_score: 35.9377)\n",
      "09:47              val. loss  20.2148 (mse_score: 20.2148)\n",
      "09:47    Epoch 29: train loss 35.8625 (mse_score: 35.8625)\n",
      "09:47              val. loss  20.2861 (mse_score: 20.2861)\n",
      "09:48    Epoch 30: train loss 35.8083 (mse_score: 35.8083)\n",
      "09:48              val. loss  20.2439 (mse_score: 20.2439)\n",
      "09:48    Epoch 31: train loss 35.7444 (mse_score: 35.7444)\n",
      "09:48              val. loss  20.2747 (mse_score: 20.2747)\n",
      "09:48  No improvement for 20 epochs, stopping training\n",
      "09:48  Early stopping after epoch 11, with loss 20.14 compared to final loss 20.27\n",
      "09:48  Finished training\n",
      "09:48  Calculating expectation for 10 estimators in ensemble\n",
      "09:48  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "09:48  Loading evaluation data\n",
      "09:48  Starting score evaluation\n",
      "09:48  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "09:48  Loading evaluation data\n",
      "09:48  Starting score evaluation\n",
      "09:49  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "09:49  Loading evaluation data\n",
      "09:49  Starting score evaluation\n",
      "09:49  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "09:49  Loading evaluation data\n",
      "09:49  Starting score evaluation\n",
      "09:49  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "09:49  Loading evaluation data\n",
      "09:49  Starting score evaluation\n",
      "09:49  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "09:49  Loading evaluation data\n",
      "09:49  Starting score evaluation\n",
      "09:49  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "09:49  Loading evaluation data\n",
      "09:49  Starting score evaluation\n",
      "09:49  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "09:49  Loading evaluation data\n",
      "09:49  Starting score evaluation\n",
      "09:50  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "09:50  Loading evaluation data\n",
      "09:50  Starting score evaluation\n",
      "09:50  Starting evaluation for estimator 10 / 10 in ensemble\n",
      "09:50  Loading evaluation data\n",
      "09:50  Starting score evaluation\n",
      "09:50  Saving ensemble setup to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/ensemble.json\n",
      "09:50  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_0_settings.json\n",
      "09:50  Saving input scaling information to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_0_x_means.npy and /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_0_x_stds.npy\n",
      "09:50  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_0_state_dict.pt\n",
      "09:50  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_1_settings.json\n",
      "09:50  Saving input scaling information to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_1_x_means.npy and /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_1_x_stds.npy\n",
      "09:50  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_1_state_dict.pt\n",
      "09:50  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_2_settings.json\n",
      "09:50  Saving input scaling information to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_2_x_means.npy and /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_2_x_stds.npy\n",
      "09:50  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_2_state_dict.pt\n",
      "09:50  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_3_settings.json\n",
      "09:50  Saving input scaling information to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_3_x_means.npy and /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_3_x_stds.npy\n",
      "09:50  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_3_state_dict.pt\n",
      "09:50  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_4_settings.json\n",
      "09:50  Saving input scaling information to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_4_x_means.npy and /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_4_x_stds.npy\n",
      "09:50  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_4_state_dict.pt\n",
      "09:50  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_5_settings.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:50  Saving input scaling information to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_5_x_means.npy and /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_5_x_stds.npy\n",
      "09:50  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_5_state_dict.pt\n",
      "09:50  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_6_settings.json\n",
      "09:50  Saving input scaling information to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_6_x_means.npy and /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_6_x_stds.npy\n",
      "09:50  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_6_state_dict.pt\n",
      "09:50  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_7_settings.json\n",
      "09:50  Saving input scaling information to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_7_x_means.npy and /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_7_x_stds.npy\n",
      "09:50  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_7_state_dict.pt\n",
      "09:50  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_8_settings.json\n",
      "09:50  Saving input scaling information to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_8_x_means.npy and /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_8_x_stds.npy\n",
      "09:50  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_8_state_dict.pt\n",
      "09:50  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_9_settings.json\n",
      "09:50  Saving input scaling information to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_9_x_means.npy and /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_9_x_stds.npy\n",
      "09:50  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_9_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "train_ensemble('all', use_tight_cuts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:11  Training 10 estimators in ensemble\n",
      "10:11  Training estimator 1 / 10 in ensemble\n",
      "10:11  Starting training\n",
      "10:11    Method:                 sally\n",
      "10:11    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_0.npy\n",
      "10:11                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_0.npy\n",
      "10:11    Features:               all\n",
      "10:11    Method:                 sally\n",
      "10:11    Hidden layers:          (100, 100)\n",
      "10:11    Activation function:    tanh\n",
      "10:11    Batch size:             128\n",
      "10:11    Trainer:                amsgrad\n",
      "10:11    Epochs:                 50\n",
      "10:11    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:11    Validation split:       0.25\n",
      "10:11    Early stopping:         True\n",
      "10:11    Scale inputs:           True\n",
      "10:11    Regularization:         None\n",
      "10:11  Loading training data\n",
      "10:11  Found 1000000 samples with 2 parameters and 27 observables\n",
      "10:11  Rescaling inputs\n",
      "10:11  Creating model for method sally\n",
      "10:11  Training model\n",
      "10:13    Epoch 5: train loss 5343.7254 (mse_score: 5343.7254)\n",
      "10:13              val. loss  5588.3700 (mse_score: 5588.3700) (*)\n",
      "10:16    Epoch 10: train loss 5232.1742 (mse_score: 5232.1742)\n",
      "10:16              val. loss  5498.4135 (mse_score: 5498.4135) (*)\n",
      "10:19    Epoch 15: train loss 5137.9287 (mse_score: 5137.9287)\n",
      "10:19              val. loss  5425.2348 (mse_score: 5425.2348) (*)\n",
      "10:23    Epoch 20: train loss 5061.9748 (mse_score: 5061.9748)\n",
      "10:23              val. loss  5381.9773 (mse_score: 5381.9773) (*)\n",
      "10:28    Epoch 25: train loss 5004.1352 (mse_score: 5004.1352)\n",
      "10:28              val. loss  5346.5284 (mse_score: 5346.5284) (*)\n",
      "10:32    Epoch 30: train loss 4956.8540 (mse_score: 4956.8540)\n",
      "10:32              val. loss  5311.6575 (mse_score: 5311.6575) (*)\n",
      "10:37    Epoch 35: train loss 4920.7419 (mse_score: 4920.7419)\n",
      "10:37              val. loss  5291.7103 (mse_score: 5291.7103) (*)\n",
      "10:42    Epoch 40: train loss 4891.9121 (mse_score: 4891.9121)\n",
      "10:42              val. loss  5274.2547 (mse_score: 5274.2547) (*)\n",
      "10:46    Epoch 45: train loss 4868.9418 (mse_score: 4868.9418)\n",
      "10:46              val. loss  5266.0065 (mse_score: 5266.0065) (*)\n",
      "10:50    Epoch 50: train loss 4850.6105 (mse_score: 4850.6105)\n",
      "10:50              val. loss  5256.5883 (mse_score: 5256.5883) (*)\n",
      "10:50  Early stopping did not improve performance\n",
      "10:50  Finished training\n",
      "10:50  Training estimator 2 / 10 in ensemble\n",
      "10:50  Starting training\n",
      "10:50    Method:                 sally\n",
      "10:50    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_1.npy\n",
      "10:50                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_1.npy\n",
      "10:50    Features:               all\n",
      "10:50    Method:                 sally\n",
      "10:50    Hidden layers:          (100, 100)\n",
      "10:50    Activation function:    tanh\n",
      "10:50    Batch size:             128\n",
      "10:50    Trainer:                amsgrad\n",
      "10:50    Epochs:                 50\n",
      "10:50    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:50    Validation split:       0.25\n",
      "10:50    Early stopping:         True\n",
      "10:50    Scale inputs:           True\n",
      "10:50    Regularization:         None\n",
      "10:50  Loading training data\n",
      "10:50  Found 1000000 samples with 2 parameters and 27 observables\n",
      "10:50  Rescaling inputs\n",
      "10:50  Creating model for method sally\n",
      "10:50  Training model\n",
      "10:54    Epoch 5: train loss 4780.7458 (mse_score: 4780.7458)\n",
      "10:54              val. loss  7894.9362 (mse_score: 7894.9362) (*)\n",
      "10:58    Epoch 10: train loss 4650.2174 (mse_score: 4650.2174)\n",
      "10:58              val. loss  7777.5857 (mse_score: 7777.5857) (*)\n",
      "11:03    Epoch 15: train loss 4536.3594 (mse_score: 4536.3594)\n",
      "11:03              val. loss  7692.3348 (mse_score: 7692.3348) (*)\n",
      "11:07    Epoch 20: train loss 4443.9659 (mse_score: 4443.9659)\n",
      "11:07              val. loss  7609.2597 (mse_score: 7609.2597) (*)\n",
      "11:11    Epoch 25: train loss 4373.8638 (mse_score: 4373.8638)\n",
      "11:11              val. loss  7558.1608 (mse_score: 7558.1608) (*)\n",
      "11:16    Epoch 30: train loss 4317.1477 (mse_score: 4317.1477)\n",
      "11:16              val. loss  7517.9804 (mse_score: 7517.9804) (*)\n",
      "11:21    Epoch 35: train loss 4274.2401 (mse_score: 4274.2401)\n",
      "11:21              val. loss  7487.2505 (mse_score: 7487.2505) (*)\n",
      "11:26    Epoch 40: train loss 4239.7104 (mse_score: 4239.7104)\n",
      "11:26              val. loss  7464.4987 (mse_score: 7464.4987) (*)\n",
      "11:30    Epoch 45: train loss 4213.9197 (mse_score: 4213.9197)\n",
      "11:30              val. loss  7481.6487 (mse_score: 7481.6487)\n",
      "11:33    Epoch 50: train loss 4193.0206 (mse_score: 4193.0206)\n",
      "11:33              val. loss  7433.5103 (mse_score: 7433.5103) (*)\n",
      "11:33  Early stopping did not improve performance\n",
      "11:33  Finished training\n",
      "11:33  Training estimator 3 / 10 in ensemble\n",
      "11:33  Starting training\n",
      "11:33    Method:                 sally\n",
      "11:33    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_2.npy\n",
      "11:33                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_2.npy\n",
      "11:33    Features:               all\n",
      "11:33    Method:                 sally\n",
      "11:33    Hidden layers:          (100, 100)\n",
      "11:33    Activation function:    tanh\n",
      "11:33    Batch size:             128\n",
      "11:33    Trainer:                amsgrad\n",
      "11:33    Epochs:                 50\n",
      "11:33    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "11:33    Validation split:       0.25\n",
      "11:33    Early stopping:         True\n",
      "11:33    Scale inputs:           True\n",
      "11:33    Regularization:         None\n",
      "11:33  Loading training data\n",
      "11:33  Found 1000000 samples with 2 parameters and 27 observables\n",
      "11:33  Rescaling inputs\n",
      "11:33  Creating model for method sally\n",
      "11:33  Training model\n",
      "11:38    Epoch 5: train loss 4697.4815 (mse_score: 4697.4815)\n",
      "11:38              val. loss  5354.4332 (mse_score: 5354.4332) (*)\n",
      "11:41    Epoch 10: train loss 4601.0513 (mse_score: 4601.0513)\n",
      "11:41              val. loss  5289.1386 (mse_score: 5289.1386) (*)\n",
      "11:45    Epoch 15: train loss 4509.8094 (mse_score: 4509.8094)\n",
      "11:45              val. loss  5235.1782 (mse_score: 5235.1782) (*)\n",
      "11:48    Epoch 20: train loss 4429.2847 (mse_score: 4429.2847)\n",
      "11:48              val. loss  5211.5080 (mse_score: 5211.5080) (*)\n",
      "11:52    Epoch 25: train loss 4367.3848 (mse_score: 4367.3848)\n",
      "11:52              val. loss  5190.3175 (mse_score: 5190.3175) (*)\n",
      "11:55    Epoch 30: train loss 4313.8122 (mse_score: 4313.8122)\n",
      "11:55              val. loss  5178.3327 (mse_score: 5178.3327)\n",
      "11:58    Epoch 35: train loss 4274.6593 (mse_score: 4274.6593)\n",
      "11:58              val. loss  5177.9089 (mse_score: 5177.9089)\n",
      "12:01    Epoch 40: train loss 4243.6189 (mse_score: 4243.6189)\n",
      "12:01              val. loss  5151.3516 (mse_score: 5151.3516) (*)\n",
      "12:05    Epoch 45: train loss 4216.8135 (mse_score: 4216.8135)\n",
      "12:05              val. loss  5144.5212 (mse_score: 5144.5212)\n",
      "12:08    Epoch 50: train loss 4195.0567 (mse_score: 4195.0567)\n",
      "12:08              val. loss  5140.3921 (mse_score: 5140.3921)\n",
      "12:08  Early stopping after epoch 48, with loss 5137.75 compared to final loss 5140.39\n",
      "12:08  Finished training\n",
      "12:08  Training estimator 4 / 10 in ensemble\n",
      "12:08  Starting training\n",
      "12:08    Method:                 sally\n",
      "12:08    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_3.npy\n",
      "12:08                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_3.npy\n",
      "12:08    Features:               all\n",
      "12:08    Method:                 sally\n",
      "12:08    Hidden layers:          (100, 100)\n",
      "12:08    Activation function:    tanh\n",
      "12:08    Batch size:             128\n",
      "12:08    Trainer:                amsgrad\n",
      "12:08    Epochs:                 50\n",
      "12:08    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "12:08    Validation split:       0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:08    Early stopping:         True\n",
      "12:08    Scale inputs:           True\n",
      "12:08    Regularization:         None\n",
      "12:08  Loading training data\n",
      "12:08  Found 1000000 samples with 2 parameters and 27 observables\n",
      "12:08  Rescaling inputs\n",
      "12:08  Creating model for method sally\n",
      "12:08  Training model\n",
      "12:12    Epoch 5: train loss 6261.0580 (mse_score: 6261.0580)\n",
      "12:12              val. loss  5305.8347 (mse_score: 5305.8347) (*)\n",
      "12:15    Epoch 10: train loss 6181.0251 (mse_score: 6181.0251)\n",
      "12:15              val. loss  5245.4023 (mse_score: 5245.4023) (*)\n",
      "12:18    Epoch 15: train loss 6113.7660 (mse_score: 6113.7660)\n",
      "12:18              val. loss  5203.8410 (mse_score: 5203.8410) (*)\n",
      "12:21    Epoch 20: train loss 6056.1493 (mse_score: 6056.1493)\n",
      "12:21              val. loss  5168.8501 (mse_score: 5168.8501) (*)\n",
      "12:25    Epoch 25: train loss 6007.4099 (mse_score: 6007.4099)\n",
      "12:25              val. loss  5147.7889 (mse_score: 5147.7889) (*)\n",
      "12:28    Epoch 30: train loss 5968.0462 (mse_score: 5968.0462)\n",
      "12:28              val. loss  5124.9446 (mse_score: 5124.9446) (*)\n",
      "12:31    Epoch 35: train loss 5936.0120 (mse_score: 5936.0120)\n",
      "12:31              val. loss  5110.6228 (mse_score: 5110.6228) (*)\n",
      "12:35    Epoch 40: train loss 5910.0929 (mse_score: 5910.0929)\n",
      "12:35              val. loss  5098.1351 (mse_score: 5098.1351) (*)\n",
      "12:38    Epoch 45: train loss 5890.0514 (mse_score: 5890.0514)\n",
      "12:38              val. loss  5087.6038 (mse_score: 5087.6038) (*)\n",
      "12:41    Epoch 50: train loss 5872.4699 (mse_score: 5872.4699)\n",
      "12:41              val. loss  5083.3862 (mse_score: 5083.3862)\n",
      "12:41  Early stopping after epoch 48, with loss 5082.12 compared to final loss 5083.39\n",
      "12:41  Finished training\n",
      "12:41  Training estimator 5 / 10 in ensemble\n",
      "12:41  Starting training\n",
      "12:41    Method:                 sally\n",
      "12:41    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_4.npy\n",
      "12:41                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_4.npy\n",
      "12:41    Features:               all\n",
      "12:41    Method:                 sally\n",
      "12:41    Hidden layers:          (100, 100)\n",
      "12:41    Activation function:    tanh\n",
      "12:41    Batch size:             128\n",
      "12:41    Trainer:                amsgrad\n",
      "12:41    Epochs:                 50\n",
      "12:41    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "12:41    Validation split:       0.25\n",
      "12:41    Early stopping:         True\n",
      "12:41    Scale inputs:           True\n",
      "12:41    Regularization:         None\n",
      "12:41  Loading training data\n",
      "12:41  Found 1000000 samples with 2 parameters and 27 observables\n",
      "12:41  Rescaling inputs\n",
      "12:41  Creating model for method sally\n",
      "12:41  Training model\n",
      "12:45    Epoch 5: train loss 8789.7990 (mse_score: 8789.7990)\n",
      "12:45              val. loss  5812.9785 (mse_score: 5812.9785) (*)\n",
      "12:48    Epoch 10: train loss 8691.7043 (mse_score: 8691.7043)\n",
      "12:48              val. loss  5726.5728 (mse_score: 5726.5728) (*)\n",
      "12:52    Epoch 15: train loss 8601.3454 (mse_score: 8601.3454)\n",
      "12:52              val. loss  5667.1796 (mse_score: 5667.1796) (*)\n",
      "12:56    Epoch 20: train loss 8525.9788 (mse_score: 8525.9788)\n",
      "12:56              val. loss  5607.6113 (mse_score: 5607.6113) (*)\n",
      "13:00    Epoch 25: train loss 8466.1125 (mse_score: 8466.1125)\n",
      "13:00              val. loss  5570.5145 (mse_score: 5570.5145) (*)\n",
      "13:04    Epoch 30: train loss 8416.2240 (mse_score: 8416.2240)\n",
      "13:04              val. loss  5546.7991 (mse_score: 5546.7991) (*)\n",
      "13:08    Epoch 35: train loss 8377.3553 (mse_score: 8377.3553)\n",
      "13:08              val. loss  5524.7986 (mse_score: 5524.7986)\n",
      "13:11    Epoch 40: train loss 8345.5811 (mse_score: 8345.5811)\n",
      "13:11              val. loss  5505.0278 (mse_score: 5505.0278) (*)\n",
      "13:15    Epoch 45: train loss 8320.1601 (mse_score: 8320.1601)\n",
      "13:15              val. loss  5488.8823 (mse_score: 5488.8823) (*)\n",
      "13:18    Epoch 50: train loss 8300.2656 (mse_score: 8300.2656)\n",
      "13:18              val. loss  5479.8091 (mse_score: 5479.8091) (*)\n",
      "13:18  Early stopping did not improve performance\n",
      "13:18  Finished training\n",
      "13:18  Training estimator 6 / 10 in ensemble\n",
      "13:18  Starting training\n",
      "13:18    Method:                 sally\n",
      "13:18    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_5.npy\n",
      "13:18                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_5.npy\n",
      "13:18    Features:               all\n",
      "13:18    Method:                 sally\n",
      "13:18    Hidden layers:          (100, 100)\n",
      "13:18    Activation function:    tanh\n",
      "13:18    Batch size:             128\n",
      "13:18    Trainer:                amsgrad\n",
      "13:18    Epochs:                 50\n",
      "13:18    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:18    Validation split:       0.25\n",
      "13:18    Early stopping:         True\n",
      "13:18    Scale inputs:           True\n",
      "13:18    Regularization:         None\n",
      "13:18  Loading training data\n",
      "13:18  Found 1000000 samples with 2 parameters and 27 observables\n",
      "13:18  Rescaling inputs\n",
      "13:18  Creating model for method sally\n",
      "13:18  Training model\n",
      "13:22    Epoch 5: train loss 6063.5414 (mse_score: 6063.5414)\n",
      "13:22              val. loss  5000.9141 (mse_score: 5000.9141) (*)\n",
      "13:25    Epoch 10: train loss 5940.1848 (mse_score: 5940.1848)\n",
      "13:25              val. loss  4909.7657 (mse_score: 4909.7657) (*)\n",
      "13:28    Epoch 15: train loss 5829.7683 (mse_score: 5829.7683)\n",
      "13:28              val. loss  4835.5739 (mse_score: 4835.5739) (*)\n",
      "13:32    Epoch 20: train loss 5735.4431 (mse_score: 5735.4431)\n",
      "13:32              val. loss  4776.3801 (mse_score: 4776.3801) (*)\n",
      "13:35    Epoch 25: train loss 5654.3251 (mse_score: 5654.3251)\n",
      "13:35              val. loss  4725.1540 (mse_score: 4725.1540) (*)\n",
      "13:38    Epoch 30: train loss 5592.9660 (mse_score: 5592.9660)\n",
      "13:38              val. loss  4693.2101 (mse_score: 4693.2101) (*)\n",
      "13:42    Epoch 35: train loss 5548.2102 (mse_score: 5548.2102)\n",
      "13:42              val. loss  4672.3419 (mse_score: 4672.3419) (*)\n",
      "13:45    Epoch 40: train loss 5512.8288 (mse_score: 5512.8288)\n",
      "13:45              val. loss  4656.0113 (mse_score: 4656.0113) (*)\n",
      "13:49    Epoch 45: train loss 5484.1789 (mse_score: 5484.1789)\n",
      "13:49              val. loss  4640.8047 (mse_score: 4640.8047) (*)\n",
      "13:52    Epoch 50: train loss 5462.9058 (mse_score: 5462.9058)\n",
      "13:52              val. loss  4628.7817 (mse_score: 4628.7817) (*)\n",
      "13:52  Early stopping did not improve performance\n",
      "13:52  Finished training\n",
      "13:52  Training estimator 7 / 10 in ensemble\n",
      "13:52  Starting training\n",
      "13:52    Method:                 sally\n",
      "13:52    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_6.npy\n",
      "13:52                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_6.npy\n",
      "13:52    Features:               all\n",
      "13:52    Method:                 sally\n",
      "13:52    Hidden layers:          (100, 100)\n",
      "13:52    Activation function:    tanh\n",
      "13:52    Batch size:             128\n",
      "13:52    Trainer:                amsgrad\n",
      "13:52    Epochs:                 50\n",
      "13:52    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:52    Validation split:       0.25\n",
      "13:52    Early stopping:         True\n",
      "13:52    Scale inputs:           True\n",
      "13:52    Regularization:         None\n",
      "13:52  Loading training data\n",
      "13:52  Found 1000000 samples with 2 parameters and 27 observables\n",
      "13:52  Rescaling inputs\n",
      "13:52  Creating model for method sally\n",
      "13:52  Training model\n",
      "13:56    Epoch 5: train loss 5685.2775 (mse_score: 5685.2775)\n",
      "13:56              val. loss  5346.9769 (mse_score: 5346.9769) (*)\n",
      "13:59    Epoch 10: train loss 5604.3068 (mse_score: 5604.3068)\n",
      "13:59              val. loss  5281.2510 (mse_score: 5281.2510) (*)\n",
      "14:03    Epoch 15: train loss 5518.1698 (mse_score: 5518.1698)\n",
      "14:03              val. loss  5235.9088 (mse_score: 5235.9088) (*)\n",
      "14:07    Epoch 20: train loss 5445.6373 (mse_score: 5445.6373)\n",
      "14:07              val. loss  5183.5720 (mse_score: 5183.5720) (*)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:11    Epoch 25: train loss 5389.5380 (mse_score: 5389.5380)\n",
      "14:11              val. loss  5155.6717 (mse_score: 5155.6717) (*)\n",
      "14:15    Epoch 30: train loss 5342.6621 (mse_score: 5342.6621)\n",
      "14:15              val. loss  5131.3902 (mse_score: 5131.3902) (*)\n",
      "14:19    Epoch 35: train loss 5306.8620 (mse_score: 5306.8620)\n",
      "14:19              val. loss  5118.8099 (mse_score: 5118.8099) (*)\n",
      "14:23    Epoch 40: train loss 5279.2168 (mse_score: 5279.2168)\n",
      "14:23              val. loss  5105.3275 (mse_score: 5105.3275) (*)\n",
      "14:26    Epoch 45: train loss 5257.6220 (mse_score: 5257.6220)\n",
      "14:26              val. loss  5092.5217 (mse_score: 5092.5217) (*)\n",
      "14:30    Epoch 50: train loss 5240.3143 (mse_score: 5240.3143)\n",
      "14:30              val. loss  5085.7543 (mse_score: 5085.7543) (*)\n",
      "14:30  Early stopping did not improve performance\n",
      "14:30  Finished training\n",
      "14:30  Training estimator 8 / 10 in ensemble\n",
      "14:30  Starting training\n",
      "14:30    Method:                 sally\n",
      "14:30    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_7.npy\n",
      "14:30                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_7.npy\n",
      "14:30    Features:               all\n",
      "14:30    Method:                 sally\n",
      "14:30    Hidden layers:          (100, 100)\n",
      "14:30    Activation function:    tanh\n",
      "14:30    Batch size:             128\n",
      "14:30    Trainer:                amsgrad\n",
      "14:30    Epochs:                 50\n",
      "14:30    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:30    Validation split:       0.25\n",
      "14:30    Early stopping:         True\n",
      "14:30    Scale inputs:           True\n",
      "14:30    Regularization:         None\n",
      "14:30  Loading training data\n",
      "14:30  Found 1000000 samples with 2 parameters and 27 observables\n",
      "14:30  Rescaling inputs\n",
      "14:30  Creating model for method sally\n",
      "14:30  Training model\n",
      "14:34    Epoch 5: train loss 5666.3531 (mse_score: 5666.3531)\n",
      "14:34              val. loss  5558.7441 (mse_score: 5558.7441) (*)\n",
      "14:37    Epoch 10: train loss 5534.5442 (mse_score: 5534.5442)\n",
      "14:37              val. loss  5466.8851 (mse_score: 5466.8851) (*)\n",
      "14:41    Epoch 15: train loss 5426.5087 (mse_score: 5426.5087)\n",
      "14:41              val. loss  5398.8838 (mse_score: 5398.8838) (*)\n",
      "14:46    Epoch 20: train loss 5337.8773 (mse_score: 5337.8773)\n",
      "14:46              val. loss  5352.3686 (mse_score: 5352.3686) (*)\n",
      "14:49    Epoch 25: train loss 5265.4634 (mse_score: 5265.4634)\n",
      "14:49              val. loss  5317.4926 (mse_score: 5317.4926) (*)\n",
      "14:53    Epoch 30: train loss 5210.9238 (mse_score: 5210.9238)\n",
      "14:53              val. loss  5297.3424 (mse_score: 5297.3424) (*)\n",
      "14:56    Epoch 35: train loss 5169.0718 (mse_score: 5169.0718)\n",
      "14:56              val. loss  5265.7892 (mse_score: 5265.7892) (*)\n",
      "15:00    Epoch 40: train loss 5134.5859 (mse_score: 5134.5859)\n",
      "15:00              val. loss  5251.9859 (mse_score: 5251.9859) (*)\n",
      "15:03    Epoch 45: train loss 5108.9610 (mse_score: 5108.9610)\n",
      "15:03              val. loss  5240.2631 (mse_score: 5240.2631) (*)\n",
      "15:07    Epoch 50: train loss 5088.9648 (mse_score: 5088.9648)\n",
      "15:07              val. loss  5230.7603 (mse_score: 5230.7603) (*)\n",
      "15:07  Early stopping did not improve performance\n",
      "15:07  Finished training\n",
      "15:07  Training estimator 9 / 10 in ensemble\n",
      "15:07  Starting training\n",
      "15:07    Method:                 sally\n",
      "15:07    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_8.npy\n",
      "15:07                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_8.npy\n",
      "15:07    Features:               all\n",
      "15:07    Method:                 sally\n",
      "15:07    Hidden layers:          (100, 100)\n",
      "15:07    Activation function:    tanh\n",
      "15:07    Batch size:             128\n",
      "15:07    Trainer:                amsgrad\n",
      "15:07    Epochs:                 50\n",
      "15:07    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:07    Validation split:       0.25\n",
      "15:07    Early stopping:         True\n",
      "15:07    Scale inputs:           True\n",
      "15:07    Regularization:         None\n",
      "15:07  Loading training data\n",
      "15:07  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:07  Rescaling inputs\n",
      "15:07  Creating model for method sally\n",
      "15:07  Training model\n",
      "15:12    Epoch 5: train loss 5200.6270 (mse_score: 5200.6270)\n",
      "15:12              val. loss  5885.8518 (mse_score: 5885.8518) (*)\n",
      "15:16    Epoch 10: train loss 5066.5499 (mse_score: 5066.5499)\n",
      "15:16              val. loss  5803.8463 (mse_score: 5803.8463) (*)\n",
      "15:20    Epoch 15: train loss 4953.7314 (mse_score: 4953.7314)\n",
      "15:20              val. loss  5741.9424 (mse_score: 5741.9424) (*)\n",
      "15:23    Epoch 20: train loss 4862.8002 (mse_score: 4862.8002)\n",
      "15:23              val. loss  5705.7900 (mse_score: 5705.7900)\n",
      "15:27    Epoch 25: train loss 4790.8302 (mse_score: 4790.8302)\n",
      "15:27              val. loss  5662.1464 (mse_score: 5662.1464) (*)\n",
      "15:30    Epoch 30: train loss 4734.8167 (mse_score: 4734.8167)\n",
      "15:30              val. loss  5641.6713 (mse_score: 5641.6713) (*)\n",
      "15:34    Epoch 35: train loss 4691.6371 (mse_score: 4691.6371)\n",
      "15:34              val. loss  5619.5876 (mse_score: 5619.5876) (*)\n",
      "15:37    Epoch 40: train loss 4656.6574 (mse_score: 4656.6574)\n",
      "15:37              val. loss  5609.0763 (mse_score: 5609.0763) (*)\n",
      "15:40    Epoch 45: train loss 4629.5580 (mse_score: 4629.5580)\n",
      "15:40              val. loss  5602.4476 (mse_score: 5602.4476)\n",
      "15:43    Epoch 50: train loss 4609.8977 (mse_score: 4609.8977)\n",
      "15:43              val. loss  5586.1252 (mse_score: 5586.1252) (*)\n",
      "15:43  Early stopping did not improve performance\n",
      "15:43  Finished training\n",
      "15:43  Training estimator 10 / 10 in ensemble\n",
      "15:43  Starting training\n",
      "15:43    Method:                 sally\n",
      "15:43    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/x_train_9.npy\n",
      "15:43                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local_tight/t_xz_train_9.npy\n",
      "15:43    Features:               all\n",
      "15:43    Method:                 sally\n",
      "15:43    Hidden layers:          (100, 100)\n",
      "15:43    Activation function:    tanh\n",
      "15:43    Batch size:             128\n",
      "15:43    Trainer:                amsgrad\n",
      "15:43    Epochs:                 50\n",
      "15:43    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:43    Validation split:       0.25\n",
      "15:43    Early stopping:         True\n",
      "15:43    Scale inputs:           True\n",
      "15:43    Regularization:         None\n",
      "15:43  Loading training data\n",
      "15:43  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:43  Rescaling inputs\n",
      "15:43  Creating model for method sally\n",
      "15:43  Training model\n",
      "15:47    Epoch 5: train loss 5771.6627 (mse_score: 5771.6627)\n",
      "15:47              val. loss  5964.1376 (mse_score: 5964.1376) (*)\n",
      "15:50    Epoch 10: train loss 5621.9759 (mse_score: 5621.9759)\n",
      "15:50              val. loss  5869.6052 (mse_score: 5869.6052) (*)\n"
     ]
    }
   ],
   "source": [
    "train_ensemble('all_tight', use_tight_cuts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal observable basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal',\n",
    "    use_tight_cuts=False,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_reg',\n",
    "    use_tight_cuts=False,\n",
    "    grad_x_regularization=0.1,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight_reg',\n",
    "    use_tight_cuts=True,\n",
    "    grad_x_regularization=0.1,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just resurrection phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'resurrection',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[26] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
