{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SALLY\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forge instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:53  \n",
      "15:53  ------------------------------------------------------------\n",
      "15:53  |                                                          |\n",
      "15:53  |  MadMiner v2018.10.12                                    |\n",
      "15:53  |                                                          |\n",
      "15:53  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "15:53  |                                                          |\n",
      "15:53  ------------------------------------------------------------\n",
      "15:53  \n"
     ]
    }
   ],
   "source": [
    "forge = MLForge(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SALLY on all observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:32  Starting training\n",
      "14:32    Method:                 sally\n",
      "14:32    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_local.npy\n",
      "14:32                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_local.npy\n",
      "14:32    Features:               all\n",
      "14:32    Method:                 sally\n",
      "14:32    Hidden layers:          (100, 100, 100)\n",
      "14:32    Activation function:    tanh\n",
      "14:32    Batch size:             256\n",
      "14:32    Epochs:                 5\n",
      "14:32    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:32    Validation split:       0.5\n",
      "14:32    Early stopping:         True\n",
      "14:32  Loading training data\n",
      "14:32  Found 1000000 samples with 2 parameters and 27 observables\n",
      "14:32  Creating model for method sally\n",
      "14:32  Training model\n",
      "14:33    Epoch 1: train loss 25.10 ([25.09802981]), validation loss 52.64 ([52.6414458]) (*)\n",
      "14:33    Epoch 2: train loss 25.08 ([25.0779512]), validation loss 52.64 ([52.64285311])\n",
      "14:34    Epoch 3: train loss 25.07 ([25.06971891]), validation loss 52.62 ([52.61562332]) (*)\n",
      "14:34    Epoch 4: train loss 25.06 ([25.05518795]), validation loss 52.61 ([52.60951653]) (*)\n",
      "14:34    Epoch 5: train loss 25.04 ([25.03807093]), validation loss 52.61 ([52.60777459]) (*)\n",
      "14:34  Early stopping did not improve performance\n",
      "14:34  Finished training\n",
      "14:34  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_all_settings.json\n",
      "14:34  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_all_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "forge.train(\n",
    "    method='sally',\n",
    "    x_filename=sample_dir + 'train_local/x_train_local.npy',\n",
    "    t_xz0_filename=sample_dir + 'train_local/t_xz_train_local.npy',\n",
    "    n_epochs=5,\n",
    "    batch_size=256,\n",
    "    validation_split=0.5\n",
    ")\n",
    "\n",
    "forge.save(model_dir + 'sally_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train just on individual observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:34  Starting training\n",
      "14:34    Method:                 sally\n",
      "14:34    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_local.npy\n",
      "14:34                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_local.npy\n",
      "14:34    Features:               [0]\n",
      "14:34    Method:                 sally\n",
      "14:34    Hidden layers:          (100, 100, 100)\n",
      "14:34    Activation function:    tanh\n",
      "14:34    Batch size:             256\n",
      "14:34    Epochs:                 5\n",
      "14:34    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:34    Validation split:       0.5\n",
      "14:34    Early stopping:         True\n",
      "14:34  Loading training data\n",
      "14:34  Found 1000000 samples with 2 parameters and 27 observables\n",
      "14:34  Only using 1 of 27 observables\n",
      "14:34  Creating model for method sally\n",
      "14:34  Training model\n",
      "14:35    Epoch 1: train loss 21.92 ([21.9249765]), validation loss 55.82 ([55.81708338]) (*)\n",
      "14:35    Epoch 2: train loss 21.92 ([21.92078588]), validation loss 55.81 ([55.81058214]) (*)\n",
      "14:36    Epoch 3: train loss 21.92 ([21.91934415]), validation loss 55.81 ([55.8106995])\n",
      "14:36    Epoch 4: train loss 21.92 ([21.91965576]), validation loss 55.81 ([55.81165715])\n",
      "14:37    Epoch 5: train loss 21.92 ([21.91940753]), validation loss 55.81 ([55.8101264]) (*)\n",
      "14:37  Early stopping did not improve performance\n",
      "14:37  Finished training\n",
      "14:37  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_met_settings.json\n",
      "14:37  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_met_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "forge.train(\n",
    "    method='sally',\n",
    "    x_filename=sample_dir + 'train_local/x_train_local.npy',\n",
    "    t_xz0_filename=sample_dir + 'train_local/t_xz_train_local.npy',\n",
    "    features=[0],\n",
    "    n_epochs=5,\n",
    "    batch_size=256,\n",
    "    validation_split=0.5\n",
    ")\n",
    "\n",
    "forge.save(model_dir + 'sally_met')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:37  Starting training\n",
      "14:37    Method:                 sally\n",
      "14:37    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_local.npy\n",
      "14:37                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_local.npy\n",
      "14:37    Features:               [4]\n",
      "14:37    Method:                 sally\n",
      "14:37    Hidden layers:          (100, 100, 100)\n",
      "14:37    Activation function:    tanh\n",
      "14:37    Batch size:             256\n",
      "14:37    Epochs:                 5\n",
      "14:37    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:37    Validation split:       0.5\n",
      "14:37    Early stopping:         True\n",
      "14:37  Loading training data\n",
      "14:37  Found 1000000 samples with 2 parameters and 27 observables\n",
      "14:37  Only using 1 of 27 observables\n",
      "14:37  Creating model for method sally\n",
      "14:37  Training model\n",
      "14:38    Epoch 1: train loss 17.10 ([17.10155995]), validation loss 60.63 ([60.62747632]) (*)\n",
      "14:38    Epoch 2: train loss 17.10 ([17.10170878]), validation loss 60.63 ([60.62778533])\n",
      "14:39    Epoch 3: train loss 17.09 ([17.09077977]), validation loss 60.63 ([60.62707357]) (*)\n",
      "14:40    Epoch 4: train loss 17.09 ([17.08932019]), validation loss 60.63 ([60.62867677])\n",
      "14:40    Epoch 5: train loss 17.09 ([17.08770772]), validation loss 60.63 ([60.62894096])\n",
      "14:40  Early stopping after epoch 3, with loss 60.63 compared to final loss 60.63\n",
      "14:40  Finished training\n",
      "14:40  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ptl_settings.json\n",
      "14:40  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ptl_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "forge.train(\n",
    "    method='sally',\n",
    "    x_filename=sample_dir + 'train_local/x_train_local.npy',\n",
    "    t_xz0_filename=sample_dir + 'train_local/t_xz_train_local.npy',\n",
    "    features=[4],\n",
    "    n_epochs=5,\n",
    "    batch_size=256,\n",
    "    validation_split=0.5\n",
    ")\n",
    "\n",
    "forge.save(model_dir + 'sally_ptl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:26  Starting training\n",
      "11:26    Method:                 sally\n",
      "11:26    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_local.npy\n",
      "11:26                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_local.npy\n",
      "11:26    Features:               [9]\n",
      "11:26    Method:                 sally\n",
      "11:26    Hidden layers:          (100, 100, 100)\n",
      "11:26    Activation function:    tanh\n",
      "11:26    Batch size:             256\n",
      "11:26    Epochs:                 10\n",
      "11:26    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "11:26    Validation split:       None\n",
      "11:26    Early stopping:         True\n",
      "11:26  Loading training data\n",
      "11:26  Found 1000000 samples with 2 parameters and 27 observables\n",
      "11:26  Only using 1 of 27 observables\n",
      "11:26  Creating model for method sally\n",
      "11:26  Training model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b586c5b719a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/projects/madminer/madminer/madminer/ml.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, method, x_filename, y_filename, theta0_filename, theta1_filename, r_xz_filename, t_xz0_filename, t_xz1_filename, features, nde_type, n_hidden, activation, maf_n_mades, maf_batch_norm, maf_batch_norm_alpha, maf_mog_n_components, alpha, n_epochs, batch_size, initial_lr, final_lr, validation_split, early_stopping)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0mfinal_learning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_lr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                 \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m             )\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'nde'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scandal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/projects/madminer/madminer/madminer/utils/ml/score_trainer.py\u001b[0m in \u001b[0;36mtrain_local_score_model\u001b[0;34m(model, loss_functions, xs, t_xzs, loss_weights, loss_labels, batch_size, initial_learning_rate, final_learning_rate, n_epochs, clip_gradient, run_on_gpu, double_precision, validation_split, early_stopping, early_stopping_patience, learning_curve_folder, learning_curve_filename, verbose)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# Convert to Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mt_xzs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt_xzs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "forge.train(\n",
    "    method='sally',\n",
    "    x_filename=sample_dir + 'train_local/x_train_local.npy',\n",
    "    t_xz0_filename=sample_dir + 'train_local/t_xz_train_local.npy',\n",
    "    features=[9],\n",
    "    n_epochs=10,\n",
    "    batch_size=256,\n",
    "    validation_split=0.5\n",
    ")\n",
    "\n",
    "forge.save(model_dir + 'sally_pta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:42  Starting training\n",
      "14:42    Method:                 sally\n",
      "14:42    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_local.npy\n",
      "14:42                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_local.npy\n",
      "14:42    Features:               [0, 4, 9]\n",
      "14:42    Method:                 sally\n",
      "14:42    Hidden layers:          (100, 100, 100)\n",
      "14:42    Activation function:    tanh\n",
      "14:42    Batch size:             256\n",
      "14:42    Epochs:                 5\n",
      "14:42    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:42    Validation split:       0.5\n",
      "14:42    Early stopping:         True\n",
      "14:42  Loading training data\n",
      "14:42  Found 1000000 samples with 2 parameters and 27 observables\n",
      "14:42  Only using 3 of 27 observables\n",
      "14:42  Creating model for method sally\n",
      "14:42  Training model\n",
      "14:43    Epoch 1: train loss 58.98 ([58.98083916]), validation loss 18.76 ([18.76097091]) (*)\n",
      "14:43    Epoch 2: train loss 59.00 ([59.00022014]), validation loss 18.75 ([18.75186464]) (*)\n",
      "14:43    Epoch 3: train loss 58.97 ([58.96821565]), validation loss 18.75 ([18.75050095]) (*)\n",
      "14:43    Epoch 4: train loss 58.96 ([58.95972964]), validation loss 18.76 ([18.76033507])\n",
      "14:44    Epoch 5: train loss 58.96 ([58.9562463]), validation loss 18.75 ([18.74759404]) (*)\n",
      "14:44  Early stopping did not improve performance\n",
      "14:44  Finished training\n",
      "14:44  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_pts_settings.json\n",
      "14:44  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_pts_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "forge.train(\n",
    "    method='sally',\n",
    "    x_filename=sample_dir + 'train_local/x_train_local.npy',\n",
    "    t_xz0_filename=sample_dir + 'train_local/t_xz_train_local.npy',\n",
    "    features=[0,4,9],\n",
    "    n_epochs=5,\n",
    "    batch_size=256,\n",
    "    validation_split=0.5\n",
    ")\n",
    "\n",
    "forge.save(model_dir + 'sally_pts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:52  Starting training\n",
      "15:52    Method:                 sally\n",
      "15:52    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_local.npy\n",
      "15:52                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_local.npy\n",
      "15:52    Features:               [25]\n",
      "15:52    Method:                 sally\n",
      "15:52    Hidden layers:          (100, 100, 100)\n",
      "15:52    Activation function:    tanh\n",
      "15:52    Batch size:             256\n",
      "15:52    Epochs:                 5\n",
      "15:52    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:52    Validation split:       0.5\n",
      "15:52    Early stopping:         True\n",
      "15:52  Loading training data\n",
      "15:52  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:52  Only using 1 of 27 observables\n",
      "15:52  Creating model for method sally\n",
      "15:52  Training model\n",
      "15:52    Epoch 1: train loss 50.68 ([50.6792069]), validation loss 27.06 ([27.05993742]) (*)\n",
      "15:52    Epoch 2: train loss 50.68 ([50.67625708]), validation loss 27.05 ([27.05222861]) (*)\n",
      "15:53    Epoch 3: train loss 50.67 ([50.67483762]), validation loss 27.05 ([27.05367671])\n",
      "15:53    Epoch 4: train loss 50.67 ([50.67413753]), validation loss 27.06 ([27.05850027])\n",
      "15:53    Epoch 5: train loss 50.67 ([50.67312604]), validation loss 27.05 ([27.05024461]) (*)\n",
      "15:53  Early stopping did not improve performance\n",
      "15:53  Finished training\n",
      "15:53  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_deltaphi_al_settings.json\n",
      "15:53  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_deltaphi_al_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "forge.train(\n",
    "    method='sally',\n",
    "    x_filename=sample_dir + 'train_local/x_train_local.npy',\n",
    "    t_xz0_filename=sample_dir + 'train_local/t_xz_train_local.npy',\n",
    "    features=[25],\n",
    "    n_epochs=5,\n",
    "    batch_size=256,\n",
    "    validation_split=0.5\n",
    ")\n",
    "\n",
    "forge.save(model_dir + 'sally_deltaphi_al')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:44  Starting training\n",
      "14:44    Method:                 sally\n",
      "14:44    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_local.npy\n",
      "14:44                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_local.npy\n",
      "14:44    Features:               [21, 22, 25]\n",
      "14:44    Method:                 sally\n",
      "14:44    Hidden layers:          (100, 100, 100)\n",
      "14:44    Activation function:    tanh\n",
      "14:44    Batch size:             256\n",
      "14:44    Epochs:                 5\n",
      "14:44    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:44    Validation split:       0.5\n",
      "14:44    Early stopping:         True\n",
      "14:44  Loading training data\n",
      "14:44  Found 1000000 samples with 2 parameters and 27 observables\n",
      "14:44  Only using 3 of 27 observables\n",
      "14:44  Creating model for method sally\n",
      "14:44  Training model\n",
      "14:44    Epoch 1: train loss 21.51 ([21.50864471]), validation loss 56.22 ([56.21998136]) (*)\n",
      "14:44    Epoch 2: train loss 21.50 ([21.50163493]), validation loss 56.22 ([56.21912735]) (*)\n",
      "14:49    Epoch 3: train loss 21.50 ([21.50166144]), validation loss 56.22 ([56.21600726]) (*)\n",
      "14:49    Epoch 4: train loss 21.50 ([21.49834785]), validation loss 56.22 ([56.21687595])\n",
      "14:49    Epoch 5: train loss 21.50 ([21.49967024]), validation loss 56.22 ([56.21566451]) (*)\n",
      "14:49  Early stopping did not improve performance\n",
      "14:49  Finished training\n",
      "14:49  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_deltaphis_settings.json\n",
      "14:49  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_deltaphis_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "forge.train(\n",
    "    method='sally',\n",
    "    x_filename=sample_dir + 'train_local/x_train_local.npy',\n",
    "    t_xz0_filename=sample_dir + 'train_local/t_xz_train_local.npy',\n",
    "    features=[21,22,25],\n",
    "    n_epochs=5,\n",
    "    batch_size=256,\n",
    "    validation_split=0.5\n",
    ")\n",
    "\n",
    "forge.save(model_dir + 'sally_deltaphis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delta phi variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:54  Starting training\n",
      "15:54    Method:                 sally\n",
      "15:54    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train.npy\n",
      "15:54                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train.npy\n",
      "15:54    Features:               [25]\n",
      "15:54    Method:                 sally\n",
      "15:54    Hidden layers:          (100, 100, 100)\n",
      "15:54    Activation function:    tanh\n",
      "15:54    Batch size:             256\n",
      "15:54    Epochs:                 10\n",
      "15:54    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "15:54    Validation split:       None\n",
      "15:54    Early stopping:         True\n",
      "15:54  Loading training data\n",
      "15:54  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:54  Only using 1 of 27 observables\n",
      "15:54  Creating model for method sally\n",
      "15:54  Training model\n",
      "15:54    Epoch 1: train loss 24.94 ([24.94450307])\n",
      "15:54    Epoch 2: train loss 24.90 ([24.89811951])\n",
      "15:55    Epoch 3: train loss 24.88 ([24.88289192])\n",
      "15:55    Epoch 4: train loss 24.86 ([24.86286096])\n",
      "15:56    Epoch 5: train loss 24.85 ([24.85235801])\n",
      "15:56    Epoch 6: train loss 24.84 ([24.84341593])\n",
      "15:56    Epoch 7: train loss 24.84 ([24.84005492])\n",
      "15:57    Epoch 8: train loss 24.84 ([24.83661095])\n",
      "15:57    Epoch 9: train loss 24.84 ([24.83503318])\n",
      "15:57    Epoch 10: train loss 24.84 ([24.83674081])\n",
      "15:57  Finished training\n",
      "15:57  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_deltaphi_la_1_settings.json\n",
      "15:57  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_deltaphi_la_1_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "forge.train(\n",
    "    method='sally',\n",
    "    x_filename=sample_dir + 'train_local/x_train.npy',\n",
    "    t_xz0_filename=sample_dir + 'train_local/t_xz_train.npy',\n",
    "    features=[25],\n",
    "    n_epochs=10,\n",
    "    batch_size=256,\n",
    "    validation_split=None,\n",
    "    initial_lr=0.01,\n",
    "    final_lr=0.0001\n",
    ")\n",
    "\n",
    "forge.save(model_dir + 'sally_deltaphi_la_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:06  Starting training\n",
      "16:06    Method:                 sally\n",
      "16:06    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train.npy\n",
      "16:06                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train.npy\n",
      "16:06    Features:               [25]\n",
      "16:06    Method:                 sally\n",
      "16:06    Hidden layers:          (100, 100, 100)\n",
      "16:06    Activation function:    tanh\n",
      "16:06    Batch size:             256\n",
      "16:06    Epochs:                 10\n",
      "16:06    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "16:06    Validation split:       None\n",
      "16:06    Early stopping:         True\n",
      "16:06  Loading training data\n",
      "16:06  Found 1000000 samples with 2 parameters and 27 observables\n",
      "16:06  Only using 1 of 27 observables\n",
      "16:06  Creating model for method sally\n",
      "16:06  Training model\n",
      "16:07    Epoch 1: train loss 24.94 ([24.93807698])\n",
      "16:07    Epoch 2: train loss 24.91 ([24.90790476])\n",
      "16:08    Epoch 3: train loss 24.88 ([24.87926488])\n",
      "16:08    Epoch 4: train loss 24.86 ([24.86232993])\n",
      "16:08    Epoch 5: train loss 24.85 ([24.85156556])\n",
      "16:09    Epoch 6: train loss 24.86 ([24.86230912])\n",
      "16:09    Epoch 7: train loss 24.84 ([24.83979924])\n",
      "16:09    Epoch 8: train loss 24.83 ([24.83204032])\n",
      "16:10    Epoch 9: train loss 24.83 ([24.83227242])\n",
      "16:10    Epoch 10: train loss 24.83 ([24.82952907])\n",
      "16:10  Finished training\n",
      "16:10  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_deltaphi_la_2_settings.json\n",
      "16:10  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_deltaphi_la_2_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "forge.train(\n",
    "    method='sally',\n",
    "    x_filename=sample_dir + 'train_local/x_train.npy',\n",
    "    t_xz0_filename=sample_dir + 'train_local/t_xz_train.npy',\n",
    "    features=[25],\n",
    "    n_epochs=10,\n",
    "    batch_size=256,\n",
    "    validation_split=None,\n",
    "    initial_lr=0.01,\n",
    "    final_lr=0.0001\n",
    ")\n",
    "\n",
    "forge.save(model_dir + 'sally_deltaphi_la_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:58  Starting training\n",
      "15:58    Method:                 sally\n",
      "15:58    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/all_local/x_all.npy\n",
      "15:58                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/all_local/t_xz_all.npy\n",
      "15:58    Features:               [25]\n",
      "15:58    Method:                 sally\n",
      "15:58    Hidden layers:          (100, 100, 100)\n",
      "15:58    Activation function:    tanh\n",
      "15:58    Batch size:             256\n",
      "15:58    Epochs:                 10\n",
      "15:58    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "15:58    Validation split:       None\n",
      "15:58    Early stopping:         True\n",
      "15:58  Loading training data\n",
      "15:58  Found 1000000 samples with 2 parameters and 27 observables\n",
      "15:58  Only using 1 of 27 observables\n",
      "15:58  Creating model for method sally\n",
      "15:58  Training model\n",
      "15:58    Epoch 1: train loss 25.02 ([25.02128876])\n",
      "15:58    Epoch 2: train loss 24.99 ([24.98898835])\n",
      "15:59    Epoch 3: train loss 24.96 ([24.96494446])\n",
      "15:59    Epoch 4: train loss 24.95 ([24.94922362])\n",
      "16:00    Epoch 5: train loss 24.95 ([24.95025364])\n",
      "16:00    Epoch 6: train loss 24.93 ([24.9309041])\n",
      "16:00    Epoch 7: train loss 24.92 ([24.92314869])\n",
      "16:01    Epoch 8: train loss 24.92 ([24.9172167])\n",
      "16:01    Epoch 9: train loss 24.92 ([24.91519571])\n",
      "16:02    Epoch 10: train loss 24.91 ([24.91376029])\n",
      "16:02  Finished training\n",
      "16:02  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_deltaphi_la_big_1_settings.json\n",
      "16:02  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_deltaphi_la_big_1_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "forge.train(\n",
    "    method='sally',\n",
    "    x_filename=sample_dir + 'all_local/x_all.npy',\n",
    "    t_xz0_filename=sample_dir + 'all_local/t_xz_all.npy',\n",
    "    features=[25],\n",
    "    n_epochs=10,\n",
    "    batch_size=256,\n",
    "    validation_split=None,\n",
    "    initial_lr=0.01,\n",
    "    final_lr=0.0001\n",
    ")\n",
    "\n",
    "forge.save(model_dir + 'sally_deltaphi_la_big_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:02  Starting training\n",
      "16:02    Method:                 sally\n",
      "16:02    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/all_local/x_all.npy\n",
      "16:02                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/all_local/t_xz_all.npy\n",
      "16:02    Features:               [25]\n",
      "16:02    Method:                 sally\n",
      "16:02    Hidden layers:          (100, 100, 100)\n",
      "16:02    Activation function:    tanh\n",
      "16:02    Batch size:             256\n",
      "16:02    Epochs:                 10\n",
      "16:02    Learning rate:          0.01 initially, decaying to 0.0001\n",
      "16:02    Validation split:       None\n",
      "16:02    Early stopping:         True\n",
      "16:02  Loading training data\n",
      "16:02  Found 1000000 samples with 2 parameters and 27 observables\n",
      "16:02  Only using 1 of 27 observables\n",
      "16:02  Creating model for method sally\n",
      "16:02  Training model\n",
      "16:02    Epoch 1: train loss 25.03 ([25.02658608])\n",
      "16:03    Epoch 2: train loss 24.98 ([24.98288769])\n",
      "16:03    Epoch 3: train loss 24.97 ([24.96530231])\n",
      "16:03    Epoch 4: train loss 24.95 ([24.94717665])\n",
      "16:04    Epoch 5: train loss 24.94 ([24.93664335])\n",
      "16:04    Epoch 6: train loss 24.93 ([24.92835338])\n",
      "16:04    Epoch 7: train loss 24.92 ([24.92018679])\n",
      "16:05    Epoch 8: train loss 24.92 ([24.91783207])\n",
      "16:05    Epoch 9: train loss 24.91 ([24.91458891])\n",
      "16:05    Epoch 10: train loss 24.92 ([24.91945381])\n",
      "16:05  Finished training\n",
      "16:05  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_deltaphi_la_big_2_settings.json\n",
      "16:05  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_deltaphi_la_big_2_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "forge.train(\n",
    "    method='sally',\n",
    "    x_filename=sample_dir + 'all_local/x_all.npy',\n",
    "    t_xz0_filename=sample_dir + 'all_local/t_xz_all.npy',\n",
    "    features=[25],\n",
    "    n_epochs=10,\n",
    "    batch_size=256,\n",
    "    validation_split=None,\n",
    "    initial_lr=0.01,\n",
    "    final_lr=0.0001\n",
    ")\n",
    "\n",
    "forge.save(model_dir + 'sally_deltaphi_la_big_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
