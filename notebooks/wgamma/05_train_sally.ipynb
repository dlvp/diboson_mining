{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 20\n",
    "n_hidden = (100,100)\n",
    "n_epochs = 20\n",
    "batch_size = 128\n",
    "initial_lr = 0.001\n",
    "final_lr = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SALLY on all observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:40  \n",
      "21:40  ------------------------------------------------------------\n",
      "21:40  |                                                          |\n",
      "21:40  |  MadMiner v2018.10.26                                    |\n",
      "21:40  |                                                          |\n",
      "21:40  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "21:40  |                                                          |\n",
      "21:40  ------------------------------------------------------------\n",
      "21:40  \n",
      "21:40  Training 20 estimators in ensemble\n",
      "21:40  Training estimator 1 / 20 in ensemble\n",
      "21:40  Starting training\n",
      "21:40    Method:                 sally\n",
      "21:40    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "21:40                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "21:40    Features:               all\n",
      "21:40    Method:                 sally\n",
      "21:40    Hidden layers:          (100, 100)\n",
      "21:40    Activation function:    tanh\n",
      "21:40    Batch size:             128\n",
      "21:40    Epochs:                 20\n",
      "21:40    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "21:40    Validation split:       None\n",
      "21:40    Early stopping:         True\n",
      "21:40  Loading training data\n",
      "21:40  Found 1000000 samples with 2 parameters and 29 observables\n",
      "21:40  Creating model for method sally\n",
      "21:40  Training model\n",
      "21:41    Epoch 2: train loss 18.54 ([18.53997622])\n",
      "21:42    Epoch 4: train loss 18.53 ([18.53280304])\n",
      "21:42    Epoch 6: train loss 18.53 ([18.52691334])\n",
      "21:43    Epoch 8: train loss 18.52 ([18.51905806])\n",
      "21:44    Epoch 10: train loss 18.51 ([18.51020231])\n",
      "21:44    Epoch 12: train loss 18.51 ([18.50682093])\n",
      "21:45    Epoch 14: train loss 18.50 ([18.50124716])\n",
      "21:46    Epoch 16: train loss 18.50 ([18.49917512])\n",
      "21:46    Epoch 18: train loss 18.50 ([18.49706615])\n",
      "21:47    Epoch 20: train loss 18.49 ([18.49456744])\n",
      "21:47  Finished training\n",
      "21:47  Training estimator 2 / 20 in ensemble\n",
      "21:47  Starting training\n",
      "21:47    Method:                 sally\n",
      "21:47    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "21:47                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "21:47    Features:               all\n",
      "21:47    Method:                 sally\n",
      "21:47    Hidden layers:          (100, 100)\n",
      "21:47    Activation function:    tanh\n",
      "21:47    Batch size:             128\n",
      "21:47    Epochs:                 20\n",
      "21:47    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "21:47    Validation split:       None\n",
      "21:47    Early stopping:         True\n",
      "21:47  Loading training data\n",
      "21:47  Found 1000000 samples with 2 parameters and 29 observables\n",
      "21:47  Creating model for method sally\n",
      "21:47  Training model\n",
      "21:48    Epoch 2: train loss 15.97 ([15.97143462])\n",
      "21:49    Epoch 4: train loss 15.96 ([15.96093121])\n",
      "21:49    Epoch 6: train loss 15.95 ([15.9540416])\n",
      "21:50    Epoch 8: train loss 15.95 ([15.94821725])\n",
      "21:51    Epoch 10: train loss 15.94 ([15.94123144])\n",
      "21:51    Epoch 12: train loss 15.94 ([15.93554094])\n",
      "21:52    Epoch 14: train loss 15.93 ([15.93284162])\n",
      "21:53    Epoch 16: train loss 15.93 ([15.93015294])\n",
      "21:53    Epoch 18: train loss 15.93 ([15.92546342])\n",
      "21:54    Epoch 20: train loss 15.92 ([15.92220818])\n",
      "21:54  Finished training\n",
      "21:54  Training estimator 3 / 20 in ensemble\n",
      "21:54  Starting training\n",
      "21:54    Method:                 sally\n",
      "21:54    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "21:54                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "21:54    Features:               all\n",
      "21:54    Method:                 sally\n",
      "21:54    Hidden layers:          (100, 100)\n",
      "21:54    Activation function:    tanh\n",
      "21:54    Batch size:             128\n",
      "21:54    Epochs:                 20\n",
      "21:54    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "21:54    Validation split:       None\n",
      "21:54    Early stopping:         True\n",
      "21:54  Loading training data\n",
      "21:54  Found 1000000 samples with 2 parameters and 29 observables\n",
      "21:54  Creating model for method sally\n",
      "21:54  Training model\n",
      "21:55    Epoch 2: train loss 53.49 ([53.49182212])\n",
      "21:55    Epoch 4: train loss 53.48 ([53.47673802])\n",
      "21:56    Epoch 6: train loss 53.47 ([53.46869327])\n",
      "21:57    Epoch 8: train loss 53.46 ([53.46223839])\n",
      "21:57    Epoch 10: train loss 53.45 ([53.45390505])\n",
      "21:58    Epoch 12: train loss 53.45 ([53.44915965])\n",
      "21:59    Epoch 14: train loss 53.45 ([53.44632091])\n",
      "22:00    Epoch 16: train loss 53.44 ([53.44284723])\n",
      "22:00    Epoch 18: train loss 53.44 ([53.43724069])\n",
      "22:01    Epoch 20: train loss 53.44 ([53.43514347])\n",
      "22:01  Finished training\n",
      "22:01  Training estimator 4 / 20 in ensemble\n",
      "22:01  Starting training\n",
      "22:01    Method:                 sally\n",
      "22:01    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "22:01                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "22:01    Features:               all\n",
      "22:01    Method:                 sally\n",
      "22:01    Hidden layers:          (100, 100)\n",
      "22:01    Activation function:    tanh\n",
      "22:01    Batch size:             128\n",
      "22:01    Epochs:                 20\n",
      "22:01    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "22:01    Validation split:       None\n",
      "22:01    Early stopping:         True\n",
      "22:01  Loading training data\n",
      "22:01  Found 1000000 samples with 2 parameters and 29 observables\n",
      "22:01  Creating model for method sally\n",
      "22:01  Training model\n",
      "22:02    Epoch 2: train loss 27.77 ([27.76500807])\n",
      "22:03    Epoch 4: train loss 27.76 ([27.75622174])\n",
      "22:03    Epoch 6: train loss 27.75 ([27.74689005])\n",
      "22:04    Epoch 8: train loss 27.74 ([27.74199241])\n",
      "22:05    Epoch 10: train loss 27.74 ([27.73890009])\n",
      "22:05    Epoch 12: train loss 27.73 ([27.73199269])\n",
      "22:06    Epoch 14: train loss 27.73 ([27.72856823])\n",
      "22:07    Epoch 16: train loss 27.73 ([27.72599238])\n",
      "22:08    Epoch 18: train loss 27.72 ([27.72405726])\n",
      "22:08    Epoch 20: train loss 27.72 ([27.72232999])\n",
      "22:08  Finished training\n",
      "22:08  Training estimator 5 / 20 in ensemble\n",
      "22:08  Starting training\n",
      "22:08    Method:                 sally\n",
      "22:08    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "22:08                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "22:08    Features:               all\n",
      "22:08    Method:                 sally\n",
      "22:08    Hidden layers:          (100, 100)\n",
      "22:08    Activation function:    tanh\n",
      "22:08    Batch size:             128\n",
      "22:08    Epochs:                 20\n",
      "22:08    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "22:08    Validation split:       None\n",
      "22:08    Early stopping:         True\n",
      "22:08  Loading training data\n",
      "22:08  Found 1000000 samples with 2 parameters and 29 observables\n",
      "22:08  Creating model for method sally\n",
      "22:08  Training model\n",
      "22:09    Epoch 2: train loss 17.97 ([17.96858536])\n",
      "22:10    Epoch 4: train loss 17.96 ([17.96151795])\n",
      "22:11    Epoch 6: train loss 17.95 ([17.95282848])\n",
      "22:11    Epoch 8: train loss 17.94 ([17.94490914])\n",
      "22:12    Epoch 10: train loss 17.94 ([17.94315451])\n",
      "22:13    Epoch 12: train loss 17.94 ([17.93829382])\n",
      "22:13    Epoch 14: train loss 17.93 ([17.93337585])\n",
      "22:14    Epoch 16: train loss 17.93 ([17.92954603])\n",
      "22:15    Epoch 18: train loss 17.93 ([17.92570636])\n",
      "22:16    Epoch 20: train loss 17.92 ([17.92282071])\n",
      "22:16  Finished training\n",
      "22:16  Training estimator 6 / 20 in ensemble\n",
      "22:16  Starting training\n",
      "22:16    Method:                 sally\n",
      "22:16    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "22:16                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:16    Features:               all\n",
      "22:16    Method:                 sally\n",
      "22:16    Hidden layers:          (100, 100)\n",
      "22:16    Activation function:    tanh\n",
      "22:16    Batch size:             128\n",
      "22:16    Epochs:                 20\n",
      "22:16    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "22:16    Validation split:       None\n",
      "22:16    Early stopping:         True\n",
      "22:16  Loading training data\n",
      "22:16  Found 1000000 samples with 2 parameters and 29 observables\n",
      "22:16  Creating model for method sally\n",
      "22:16  Training model\n",
      "22:16    Epoch 2: train loss 21.21 ([21.2104542])\n",
      "22:17    Epoch 4: train loss 21.20 ([21.20129979])\n",
      "22:18    Epoch 6: train loss 21.19 ([21.1927511])\n",
      "22:18    Epoch 8: train loss 21.19 ([21.18890209])\n",
      "22:19    Epoch 10: train loss 21.18 ([21.18146434])\n",
      "22:20    Epoch 12: train loss 21.17 ([21.17402208])\n",
      "22:20    Epoch 14: train loss 21.17 ([21.17002762])\n",
      "22:21    Epoch 16: train loss 21.16 ([21.16466516])\n",
      "22:22    Epoch 18: train loss 21.16 ([21.16441933])\n",
      "22:22    Epoch 20: train loss 21.16 ([21.16185344])\n",
      "22:22  Finished training\n",
      "22:22  Training estimator 7 / 20 in ensemble\n",
      "22:22  Starting training\n",
      "22:22    Method:                 sally\n",
      "22:22    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "22:22                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "22:22    Features:               all\n",
      "22:22    Method:                 sally\n",
      "22:22    Hidden layers:          (100, 100)\n",
      "22:22    Activation function:    tanh\n",
      "22:22    Batch size:             128\n",
      "22:22    Epochs:                 20\n",
      "22:22    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "22:22    Validation split:       None\n",
      "22:22    Early stopping:         True\n",
      "22:22  Loading training data\n",
      "22:22  Found 1000000 samples with 2 parameters and 29 observables\n",
      "22:22  Creating model for method sally\n",
      "22:22  Training model\n",
      "22:23    Epoch 2: train loss 15.94 ([15.93878818])\n",
      "22:24    Epoch 4: train loss 15.93 ([15.93113033])\n",
      "22:24    Epoch 6: train loss 15.93 ([15.92570057])\n",
      "22:25    Epoch 8: train loss 15.92 ([15.9192629])\n",
      "22:25    Epoch 10: train loss 15.91 ([15.91470512])\n",
      "22:26    Epoch 12: train loss 15.91 ([15.91048691])\n",
      "22:27    Epoch 14: train loss 15.91 ([15.90585397])\n",
      "22:27    Epoch 16: train loss 15.90 ([15.90183405])\n",
      "22:28    Epoch 18: train loss 15.90 ([15.89910213])\n",
      "22:29    Epoch 20: train loss 15.90 ([15.8980792])\n",
      "22:29  Finished training\n",
      "22:29  Training estimator 8 / 20 in ensemble\n",
      "22:29  Starting training\n",
      "22:29    Method:                 sally\n",
      "22:29    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "22:29                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "22:29    Features:               all\n",
      "22:29    Method:                 sally\n",
      "22:29    Hidden layers:          (100, 100)\n",
      "22:29    Activation function:    tanh\n",
      "22:29    Batch size:             128\n",
      "22:29    Epochs:                 20\n",
      "22:29    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "22:29    Validation split:       None\n",
      "22:29    Early stopping:         True\n",
      "22:29  Loading training data\n",
      "22:29  Found 1000000 samples with 2 parameters and 29 observables\n",
      "22:29  Creating model for method sally\n",
      "22:29  Training model\n",
      "22:29    Epoch 2: train loss 27.65 ([27.64729615])\n",
      "22:30    Epoch 4: train loss 27.64 ([27.63840953])\n",
      "22:31    Epoch 6: train loss 27.63 ([27.62767673])\n",
      "22:31    Epoch 8: train loss 27.62 ([27.62078021])\n",
      "22:32    Epoch 10: train loss 27.62 ([27.61571055])\n",
      "22:33    Epoch 12: train loss 27.61 ([27.61126418])\n",
      "22:33    Epoch 14: train loss 27.61 ([27.60513237])\n",
      "22:34    Epoch 16: train loss 27.60 ([27.6018656])\n",
      "22:35    Epoch 18: train loss 27.60 ([27.5959201])\n",
      "22:35    Epoch 20: train loss 27.59 ([27.59423068])\n",
      "22:35  Finished training\n",
      "22:35  Training estimator 9 / 20 in ensemble\n",
      "22:35  Starting training\n",
      "22:35    Method:                 sally\n",
      "22:35    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "22:35                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "22:35    Features:               all\n",
      "22:35    Method:                 sally\n",
      "22:35    Hidden layers:          (100, 100)\n",
      "22:35    Activation function:    tanh\n",
      "22:35    Batch size:             128\n",
      "22:35    Epochs:                 20\n",
      "22:35    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "22:35    Validation split:       None\n",
      "22:35    Early stopping:         True\n",
      "22:35  Loading training data\n",
      "22:35  Found 1000000 samples with 2 parameters and 29 observables\n",
      "22:35  Creating model for method sally\n",
      "22:35  Training model\n",
      "22:36    Epoch 2: train loss 21.01 ([21.01476139])\n",
      "22:37    Epoch 4: train loss 21.01 ([21.00945454])\n",
      "22:37    Epoch 6: train loss 21.00 ([20.99906929])\n",
      "22:38    Epoch 8: train loss 20.99 ([20.99290038])\n",
      "22:39    Epoch 10: train loss 20.99 ([20.98624045])\n",
      "22:39    Epoch 12: train loss 20.98 ([20.98329075])\n",
      "22:40    Epoch 14: train loss 20.98 ([20.97716671])\n",
      "22:41    Epoch 16: train loss 20.97 ([20.97349454])\n",
      "22:42    Epoch 18: train loss 20.97 ([20.97153204])\n",
      "22:42    Epoch 20: train loss 20.97 ([20.96931386])\n",
      "22:42  Finished training\n",
      "22:42  Training estimator 10 / 20 in ensemble\n",
      "22:42  Starting training\n",
      "22:42    Method:                 sally\n",
      "22:42    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "22:42                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "22:42    Features:               all\n",
      "22:42    Method:                 sally\n",
      "22:42    Hidden layers:          (100, 100)\n",
      "22:42    Activation function:    tanh\n",
      "22:42    Batch size:             128\n",
      "22:42    Epochs:                 20\n",
      "22:42    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "22:42    Validation split:       None\n",
      "22:42    Early stopping:         True\n",
      "22:42  Loading training data\n",
      "22:42  Found 1000000 samples with 2 parameters and 29 observables\n",
      "22:42  Creating model for method sally\n",
      "22:42  Training model\n",
      "22:43    Epoch 2: train loss 47.36 ([47.35907764])\n",
      "22:44    Epoch 4: train loss 47.35 ([47.35270826])\n",
      "22:44    Epoch 6: train loss 47.35 ([47.34677442])\n",
      "22:45    Epoch 8: train loss 47.34 ([47.33843559])\n",
      "22:46    Epoch 10: train loss 47.33 ([47.32933499])\n",
      "22:46    Epoch 12: train loss 47.33 ([47.32529957])\n",
      "22:47    Epoch 14: train loss 47.32 ([47.31763775])\n",
      "22:48    Epoch 16: train loss 47.32 ([47.31853758])\n",
      "22:48    Epoch 18: train loss 47.32 ([47.31596206])\n",
      "22:49    Epoch 20: train loss 47.31 ([47.31213108])\n",
      "22:49  Finished training\n",
      "22:49  Training estimator 11 / 20 in ensemble\n",
      "22:49  Starting training\n",
      "22:49    Method:                 sally\n",
      "22:49    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_10.npy\n",
      "22:49                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_10.npy\n",
      "22:49    Features:               all\n",
      "22:49    Method:                 sally\n",
      "22:49    Hidden layers:          (100, 100)\n",
      "22:49    Activation function:    tanh\n",
      "22:49    Batch size:             128\n",
      "22:49    Epochs:                 20\n",
      "22:49    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "22:49    Validation split:       None\n",
      "22:49    Early stopping:         True\n",
      "22:49  Loading training data\n",
      "22:49  Found 1000000 samples with 2 parameters and 29 observables\n",
      "22:49  Creating model for method sally\n",
      "22:49  Training model\n",
      "22:50    Epoch 2: train loss 17.00 ([17.00397915])\n",
      "22:50    Epoch 4: train loss 17.00 ([16.99605601])\n",
      "22:51    Epoch 6: train loss 16.98 ([16.98166286])\n",
      "22:52    Epoch 8: train loss 16.98 ([16.97726691])\n",
      "22:52    Epoch 10: train loss 16.97 ([16.97210476])\n",
      "22:53    Epoch 12: train loss 16.97 ([16.96779716])\n",
      "22:54    Epoch 14: train loss 16.96 ([16.96212219])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:54    Epoch 16: train loss 16.96 ([16.96042802])\n",
      "22:55    Epoch 18: train loss 16.96 ([16.95803412])\n",
      "22:56    Epoch 20: train loss 16.95 ([16.95380618])\n",
      "22:56  Finished training\n",
      "22:56  Training estimator 12 / 20 in ensemble\n",
      "22:56  Starting training\n",
      "22:56    Method:                 sally\n",
      "22:56    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_11.npy\n",
      "22:56                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_11.npy\n",
      "22:56    Features:               all\n",
      "22:56    Method:                 sally\n",
      "22:56    Hidden layers:          (100, 100)\n",
      "22:56    Activation function:    tanh\n",
      "22:56    Batch size:             128\n",
      "22:56    Epochs:                 20\n",
      "22:56    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "22:56    Validation split:       None\n",
      "22:56    Early stopping:         True\n",
      "22:56  Loading training data\n",
      "22:56  Found 1000000 samples with 2 parameters and 29 observables\n",
      "22:56  Creating model for method sally\n",
      "22:56  Training model\n",
      "22:57    Epoch 2: train loss 17.48 ([17.48494715])\n",
      "22:57    Epoch 4: train loss 17.48 ([17.47623745])\n",
      "22:58    Epoch 6: train loss 17.47 ([17.46808966])\n",
      "22:58    Epoch 8: train loss 17.46 ([17.46300834])\n",
      "22:59    Epoch 10: train loss 17.46 ([17.4567822])\n",
      "23:00    Epoch 12: train loss 17.45 ([17.45270383])\n",
      "23:00    Epoch 14: train loss 17.45 ([17.44694929])\n",
      "23:01    Epoch 16: train loss 17.44 ([17.44253658])\n",
      "23:02    Epoch 18: train loss 17.44 ([17.439902])\n",
      "23:02    Epoch 20: train loss 17.44 ([17.43621449])\n",
      "23:02  Finished training\n",
      "23:02  Training estimator 13 / 20 in ensemble\n",
      "23:02  Starting training\n",
      "23:02    Method:                 sally\n",
      "23:02    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_12.npy\n",
      "23:02                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_12.npy\n",
      "23:02    Features:               all\n",
      "23:02    Method:                 sally\n",
      "23:02    Hidden layers:          (100, 100)\n",
      "23:02    Activation function:    tanh\n",
      "23:02    Batch size:             128\n",
      "23:02    Epochs:                 20\n",
      "23:02    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:02    Validation split:       None\n",
      "23:02    Early stopping:         True\n",
      "23:02  Loading training data\n",
      "23:02  Found 1000000 samples with 2 parameters and 29 observables\n",
      "23:02  Creating model for method sally\n",
      "23:02  Training model\n",
      "23:03    Epoch 2: train loss 18.14 ([18.14216097])\n",
      "23:04    Epoch 4: train loss 18.13 ([18.13448695])\n",
      "23:04    Epoch 6: train loss 18.13 ([18.12568251])\n",
      "23:05    Epoch 8: train loss 18.12 ([18.11824526])\n",
      "23:06    Epoch 10: train loss 18.11 ([18.11374484])\n",
      "23:06    Epoch 12: train loss 18.11 ([18.11076554])\n",
      "23:07    Epoch 14: train loss 18.10 ([18.10466304])\n",
      "23:08    Epoch 16: train loss 18.10 ([18.10340984])\n",
      "23:08    Epoch 18: train loss 18.10 ([18.09742794])\n",
      "23:09    Epoch 20: train loss 18.09 ([18.09283413])\n",
      "23:09  Finished training\n",
      "23:09  Training estimator 14 / 20 in ensemble\n",
      "23:09  Starting training\n",
      "23:09    Method:                 sally\n",
      "23:09    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_13.npy\n",
      "23:09                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_13.npy\n",
      "23:09    Features:               all\n",
      "23:09    Method:                 sally\n",
      "23:09    Hidden layers:          (100, 100)\n",
      "23:09    Activation function:    tanh\n",
      "23:09    Batch size:             128\n",
      "23:09    Epochs:                 20\n",
      "23:09    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:09    Validation split:       None\n",
      "23:09    Early stopping:         True\n",
      "23:09  Loading training data\n",
      "23:09  Found 1000000 samples with 2 parameters and 29 observables\n",
      "23:09  Creating model for method sally\n",
      "23:09  Training model\n",
      "23:10    Epoch 2: train loss 12.51 ([12.5085008])\n",
      "23:11    Epoch 4: train loss 12.50 ([12.50119754])\n",
      "23:11    Epoch 6: train loss 12.49 ([12.49484698])\n",
      "23:12    Epoch 8: train loss 12.49 ([12.49017936])\n",
      "23:12    Epoch 10: train loss 12.48 ([12.48493602])\n",
      "23:13    Epoch 12: train loss 12.48 ([12.47921251])\n",
      "23:14    Epoch 14: train loss 12.48 ([12.47538279])\n",
      "23:14    Epoch 16: train loss 12.47 ([12.47139209])\n",
      "23:15    Epoch 18: train loss 12.47 ([12.46701817])\n",
      "23:16    Epoch 20: train loss 12.46 ([12.46413422])\n",
      "23:16  Finished training\n",
      "23:16  Training estimator 15 / 20 in ensemble\n",
      "23:16  Starting training\n",
      "23:16    Method:                 sally\n",
      "23:16    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_14.npy\n",
      "23:16                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_14.npy\n",
      "23:16    Features:               all\n",
      "23:16    Method:                 sally\n",
      "23:16    Hidden layers:          (100, 100)\n",
      "23:16    Activation function:    tanh\n",
      "23:16    Batch size:             128\n",
      "23:16    Epochs:                 20\n",
      "23:16    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:16    Validation split:       None\n",
      "23:16    Early stopping:         True\n",
      "23:16  Loading training data\n",
      "23:16  Found 1000000 samples with 2 parameters and 29 observables\n",
      "23:16  Creating model for method sally\n",
      "23:16  Training model\n",
      "23:17    Epoch 2: train loss 22.64 ([22.63780624])\n",
      "23:17    Epoch 4: train loss 22.63 ([22.63117791])\n",
      "23:18    Epoch 6: train loss 22.62 ([22.6208167])\n",
      "23:18    Epoch 8: train loss 22.62 ([22.61677926])\n",
      "23:19    Epoch 10: train loss 22.61 ([22.60961901])\n",
      "23:20    Epoch 12: train loss 22.61 ([22.60645756])\n",
      "23:20    Epoch 14: train loss 22.60 ([22.60191437])\n",
      "23:21    Epoch 16: train loss 22.60 ([22.59755003])\n",
      "23:22    Epoch 18: train loss 22.59 ([22.59331884])\n",
      "23:22    Epoch 20: train loss 22.59 ([22.58849063])\n",
      "23:22  Finished training\n",
      "23:22  Training estimator 16 / 20 in ensemble\n",
      "23:22  Starting training\n",
      "23:22    Method:                 sally\n",
      "23:22    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_15.npy\n",
      "23:22                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_15.npy\n",
      "23:22    Features:               all\n",
      "23:22    Method:                 sally\n",
      "23:22    Hidden layers:          (100, 100)\n",
      "23:22    Activation function:    tanh\n",
      "23:22    Batch size:             128\n",
      "23:22    Epochs:                 20\n",
      "23:22    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:22    Validation split:       None\n",
      "23:22    Early stopping:         True\n",
      "23:22  Loading training data\n",
      "23:22  Found 1000000 samples with 2 parameters and 29 observables\n",
      "23:22  Creating model for method sally\n",
      "23:22  Training model\n",
      "23:23    Epoch 2: train loss 15.57 ([15.57323211])\n",
      "23:24    Epoch 4: train loss 15.56 ([15.56477186])\n",
      "23:24    Epoch 6: train loss 15.56 ([15.55840452])\n",
      "23:25    Epoch 8: train loss 15.55 ([15.5523735])\n",
      "23:26    Epoch 10: train loss 15.55 ([15.54697476])\n",
      "23:26    Epoch 12: train loss 15.54 ([15.54328647])\n",
      "23:27    Epoch 14: train loss 15.54 ([15.53905627])\n",
      "23:28    Epoch 16: train loss 15.54 ([15.53993294])\n",
      "23:28    Epoch 18: train loss 15.53 ([15.53320473])\n",
      "23:29    Epoch 20: train loss 15.53 ([15.53119835])\n",
      "23:29  Finished training\n",
      "23:29  Training estimator 17 / 20 in ensemble\n",
      "23:29  Starting training\n",
      "23:29    Method:                 sally\n",
      "23:29    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_16.npy\n",
      "23:29                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_16.npy\n",
      "23:29    Features:               all\n",
      "23:29    Method:                 sally\n",
      "23:29    Hidden layers:          (100, 100)\n",
      "23:29    Activation function:    tanh\n",
      "23:29    Batch size:             128\n",
      "23:29    Epochs:                 20\n",
      "23:29    Learning rate:          0.002 initially, decaying to 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:29    Validation split:       None\n",
      "23:29    Early stopping:         True\n",
      "23:29  Loading training data\n",
      "23:29  Found 1000000 samples with 2 parameters and 29 observables\n",
      "23:29  Creating model for method sally\n",
      "23:29  Training model\n",
      "23:30    Epoch 2: train loss 17.45 ([17.45280555])\n",
      "23:31    Epoch 4: train loss 17.44 ([17.44440517])\n",
      "23:31    Epoch 6: train loss 17.44 ([17.43552967])\n",
      "23:32    Epoch 8: train loss 17.43 ([17.42725864])\n",
      "23:33    Epoch 10: train loss 17.42 ([17.42293276])\n",
      "23:33    Epoch 12: train loss 17.42 ([17.41857858])\n",
      "23:34    Epoch 14: train loss 17.41 ([17.41264606])\n",
      "23:35    Epoch 16: train loss 17.43 ([17.42605152])\n",
      "23:35    Epoch 18: train loss 17.41 ([17.40529531])\n",
      "23:36    Epoch 20: train loss 17.40 ([17.40176731])\n",
      "23:36  Finished training\n",
      "23:36  Training estimator 18 / 20 in ensemble\n",
      "23:36  Starting training\n",
      "23:36    Method:                 sally\n",
      "23:36    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_17.npy\n",
      "23:36                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_17.npy\n",
      "23:36    Features:               all\n",
      "23:36    Method:                 sally\n",
      "23:36    Hidden layers:          (100, 100)\n",
      "23:36    Activation function:    tanh\n",
      "23:36    Batch size:             128\n",
      "23:36    Epochs:                 20\n",
      "23:36    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:36    Validation split:       None\n",
      "23:36    Early stopping:         True\n",
      "23:36  Loading training data\n",
      "23:36  Found 1000000 samples with 2 parameters and 29 observables\n",
      "23:36  Creating model for method sally\n",
      "23:36  Training model\n",
      "23:37    Epoch 2: train loss 18.11 ([18.1131668])\n",
      "23:37    Epoch 4: train loss 18.09 ([18.09193689])\n",
      "23:38    Epoch 6: train loss 18.09 ([18.08699616])\n",
      "23:39    Epoch 8: train loss 18.08 ([18.07854742])\n",
      "23:39    Epoch 10: train loss 18.07 ([18.07437644])\n",
      "23:40    Epoch 12: train loss 18.07 ([18.06891618])\n",
      "23:41    Epoch 14: train loss 18.06 ([18.06462356])\n",
      "23:41    Epoch 16: train loss 18.06 ([18.06258479])\n",
      "23:42    Epoch 18: train loss 18.06 ([18.05788503])\n",
      "23:43    Epoch 20: train loss 18.05 ([18.05423959])\n",
      "23:43  Finished training\n",
      "23:43  Training estimator 19 / 20 in ensemble\n",
      "23:43  Starting training\n",
      "23:43    Method:                 sally\n",
      "23:43    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_18.npy\n",
      "23:43                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_18.npy\n",
      "23:43    Features:               all\n",
      "23:43    Method:                 sally\n",
      "23:43    Hidden layers:          (100, 100)\n",
      "23:43    Activation function:    tanh\n",
      "23:43    Batch size:             128\n",
      "23:43    Epochs:                 20\n",
      "23:43    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:43    Validation split:       None\n",
      "23:43    Early stopping:         True\n",
      "23:43  Loading training data\n",
      "23:43  Found 1000000 samples with 2 parameters and 29 observables\n",
      "23:43  Creating model for method sally\n",
      "23:43  Training model\n",
      "23:43    Epoch 2: train loss 44.30 ([44.29977437])\n",
      "23:44    Epoch 4: train loss 44.29 ([44.28929589])\n",
      "23:45    Epoch 6: train loss 44.30 ([44.300526])\n",
      "23:45    Epoch 8: train loss 44.28 ([44.27665384])\n",
      "23:46    Epoch 10: train loss 44.28 ([44.27518881])\n",
      "23:47    Epoch 12: train loss 44.27 ([44.26592058])\n",
      "23:47    Epoch 14: train loss 44.26 ([44.26206125])\n",
      "23:48    Epoch 16: train loss 44.26 ([44.25523219])\n",
      "23:49    Epoch 18: train loss 44.25 ([44.25168483])\n",
      "23:49    Epoch 20: train loss 44.25 ([44.25069816])\n",
      "23:49  Finished training\n",
      "23:49  Training estimator 20 / 20 in ensemble\n",
      "23:49  Starting training\n",
      "23:49    Method:                 sally\n",
      "23:49    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_19.npy\n",
      "23:49                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_19.npy\n",
      "23:49    Features:               all\n",
      "23:49    Method:                 sally\n",
      "23:49    Hidden layers:          (100, 100)\n",
      "23:49    Activation function:    tanh\n",
      "23:49    Batch size:             128\n",
      "23:49    Epochs:                 20\n",
      "23:49    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:49    Validation split:       None\n",
      "23:49    Early stopping:         True\n",
      "23:49  Loading training data\n",
      "23:49  Found 1000000 samples with 2 parameters and 29 observables\n",
      "23:49  Creating model for method sally\n",
      "23:49  Training model\n",
      "23:50    Epoch 2: train loss 15.79 ([15.78828895])\n",
      "23:51    Epoch 4: train loss 15.78 ([15.7801021])\n",
      "23:51    Epoch 6: train loss 15.77 ([15.77288242])\n",
      "23:52    Epoch 8: train loss 15.77 ([15.7678721])\n",
      "23:53    Epoch 10: train loss 15.76 ([15.7630499])\n",
      "23:53    Epoch 12: train loss 15.76 ([15.7574686])\n",
      "23:54    Epoch 14: train loss 15.75 ([15.75453064])\n",
      "23:55    Epoch 16: train loss 15.75 ([15.74885321])\n",
      "23:55    Epoch 18: train loss 15.75 ([15.74539614])\n",
      "23:56    Epoch 20: train loss 15.74 ([15.74296019])\n",
      "23:56  Finished training\n"
     ]
    }
   ],
   "source": [
    "ensemble_all = EnsembleForge(n_estimators)\n",
    "\n",
    "ensemble_all.train_all(\n",
    "    method='sally',\n",
    "    x_filename=[sample_dir + 'train_local/x_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    t_xz0_filename=[sample_dir + 'train_local/t_xz_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=None,\n",
    "    n_hidden=n_hidden\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:56  Calculating expectation for 20 estimators in ensemble\n",
      "23:56  Starting evaluation for estimator 1 / 20 in ensemble\n",
      "23:56  Loading evaluation data\n",
      "23:56  Starting score evaluation\n",
      "23:56  Starting evaluation for estimator 2 / 20 in ensemble\n",
      "23:56  Loading evaluation data\n",
      "23:56  Starting score evaluation\n",
      "23:56  Starting evaluation for estimator 3 / 20 in ensemble\n",
      "23:56  Loading evaluation data\n",
      "23:56  Starting score evaluation\n",
      "23:57  Starting evaluation for estimator 4 / 20 in ensemble\n",
      "23:57  Loading evaluation data\n",
      "23:57  Starting score evaluation\n",
      "23:57  Starting evaluation for estimator 5 / 20 in ensemble\n",
      "23:57  Loading evaluation data\n",
      "23:57  Starting score evaluation\n",
      "23:57  Starting evaluation for estimator 6 / 20 in ensemble\n",
      "23:57  Loading evaluation data\n",
      "23:57  Starting score evaluation\n",
      "23:57  Starting evaluation for estimator 7 / 20 in ensemble\n",
      "23:57  Loading evaluation data\n",
      "23:57  Starting score evaluation\n",
      "23:57  Starting evaluation for estimator 8 / 20 in ensemble\n",
      "23:57  Loading evaluation data\n",
      "23:57  Starting score evaluation\n",
      "23:57  Starting evaluation for estimator 9 / 20 in ensemble\n",
      "23:57  Loading evaluation data\n",
      "23:57  Starting score evaluation\n",
      "23:58  Starting evaluation for estimator 10 / 20 in ensemble\n",
      "23:58  Loading evaluation data\n",
      "23:58  Starting score evaluation\n",
      "23:58  Starting evaluation for estimator 11 / 20 in ensemble\n",
      "23:58  Loading evaluation data\n",
      "23:58  Starting score evaluation\n",
      "23:58  Starting evaluation for estimator 12 / 20 in ensemble\n",
      "23:58  Loading evaluation data\n",
      "23:58  Starting score evaluation\n",
      "23:58  Starting evaluation for estimator 13 / 20 in ensemble\n",
      "23:58  Loading evaluation data\n",
      "23:58  Starting score evaluation\n",
      "23:58  Starting evaluation for estimator 14 / 20 in ensemble\n",
      "23:58  Loading evaluation data\n",
      "23:58  Starting score evaluation\n",
      "23:58  Starting evaluation for estimator 15 / 20 in ensemble\n",
      "23:58  Loading evaluation data\n",
      "23:58  Starting score evaluation\n",
      "23:58  Starting evaluation for estimator 16 / 20 in ensemble\n",
      "23:58  Loading evaluation data\n",
      "23:59  Starting score evaluation\n",
      "23:59  Starting evaluation for estimator 17 / 20 in ensemble\n",
      "23:59  Loading evaluation data\n",
      "23:59  Starting score evaluation\n",
      "23:59  Starting evaluation for estimator 18 / 20 in ensemble\n",
      "23:59  Loading evaluation data\n",
      "23:59  Starting score evaluation\n",
      "23:59  Starting evaluation for estimator 19 / 20 in ensemble\n",
      "23:59  Loading evaluation data\n",
      "23:59  Starting score evaluation\n",
      "23:59  Starting evaluation for estimator 20 / 20 in ensemble\n",
      "23:59  Loading evaluation data\n",
      "23:59  Starting score evaluation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.01433866, -0.00798974],\n",
       "       [-0.04412725,  0.02471908],\n",
       "       [-0.06542948,  0.0835643 ],\n",
       "       [-0.02332889, -0.00558355],\n",
       "       [ 0.02657468,  0.0145047 ],\n",
       "       [ 0.11239411, -0.03549213],\n",
       "       [ 0.00559056, -0.00027227],\n",
       "       [-0.07079547,  0.10741747],\n",
       "       [-0.00762126,  0.01911212],\n",
       "       [ 0.00617709,  0.01502894],\n",
       "       [ 0.00371833, -0.03548462],\n",
       "       [ 0.01541766, -0.01456542],\n",
       "       [ 0.06796021,  0.00998996],\n",
       "       [ 0.02345792, -0.02461899],\n",
       "       [-0.00599976,  0.01418786],\n",
       "       [ 0.01237708, -0.0282448 ],\n",
       "       [-0.02025609,  0.01516213],\n",
       "       [-0.00509163,  0.00717454],\n",
       "       [ 0.00775625, -0.04099903],\n",
       "       [-0.02500463, -0.03304806]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_all.calculate_expectation(\n",
    "    x_filename=sample_dir + 'validation/x_validation.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:59  Saving ensemble setup to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/ensemble.json\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_0_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_0_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_1_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_1_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_2_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_2_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_3_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_3_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_4_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_4_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_5_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_5_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_6_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_6_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_7_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_7_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_8_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_8_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_9_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_9_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_10_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_10_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_11_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_11_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_12_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_12_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_13_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_13_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_14_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_14_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_15_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_15_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_16_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_16_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_17_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_17_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_18_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_18_state_dict.pt\n",
      "23:59  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_19_settings.json\n",
      "23:59  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all/estimator_19_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "ensemble_all.save(model_dir + 'sally_ensemble_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d toy study (delta phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module madminer.ml:\n",
      "\n",
      "train(self, method, x_filename, y_filename=None, theta0_filename=None, theta1_filename=None, r_xz_filename=None, t_xz0_filename=None, t_xz1_filename=None, features=None, nde_type='maf', n_hidden=(100, 100, 100), activation='tanh', maf_n_mades=3, maf_batch_norm=True, maf_batch_norm_alpha=0.1, maf_mog_n_components=10, alpha=1.0, n_epochs=20, batch_size=128, initial_lr=0.002, final_lr=0.0001, validation_split=0.2, early_stopping=True)\n",
      "    Trains a neural network to estimate either the likelihood ratio or, if method is 'sally' or 'sallino', the\n",
      "    score.\n",
      "    \n",
      "    The keyword method determines the structure of the estimator that an instance of this class represents:\n",
      "    \n",
      "    * For 'alice', 'alices', 'carl', 'nde', 'rascal', 'rolr', and 'scandal', the neural network models\n",
      "      the likelihood ratio as a function of the observables `x` and the numerator hypothesis `theta0`, while\n",
      "      the denominator hypothesis is kept at a fixed reference value (\"single-parameterized likelihood ratio\n",
      "      estimator\"). In addition to the likelihood ratio, the estimator allows to estimate the score at `theta0`.\n",
      "    * For 'alice2', 'alices2', 'carl2', 'rascal2', and 'rolr2', the neural network models\n",
      "      the likelihood ratio as a function of the observables `x`, the numerator hypothesis `theta0`, and the\n",
      "      denominator hypothesis `theta1` (\"doubly parameterized likelihood ratio estimator\"). The score at `theta0`\n",
      "      and `theta1` can also be evaluated.\n",
      "    * For 'sally' and 'sallino', the neural networks models the score evaluated at some reference hypothesis\n",
      "      (\"local score regression\"). The likelihood ratio cannot be estimated directly from the neural network, but\n",
      "      can be estimated in a second step through density estimation in the estimated score space.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    method : str\n",
      "        The inference method used. Allows values are 'alice', 'alices', 'carl', 'nde', 'rascal', 'rolr', and\n",
      "        'scandal' for a single-parameterized likelihood ratio estimator; 'alice2', 'alices2', 'carl2', 'rascal2',\n",
      "        and 'rolr2' for a doubly-parameterized likelihood ratio estimator; and 'sally' and 'sallino' for local\n",
      "        score regression.\n",
      "        \n",
      "    x_filename : str\n",
      "        Path to an unweighted sample of observations, as saved by the `madminer.sampling.SampleAugmenter` functions.\n",
      "        Required for all inference methods.\n",
      "        \n",
      "    y_filename : str or None, optional\n",
      "        Path to an unweighted sample of class labels, as saved by the `madminer.sampling.SampleAugmenter` functions.\n",
      "        Required for the 'alice', 'alice2', 'alices', 'alices2', 'carl', 'carl2', 'rascal', 'rascal2', 'rolr',\n",
      "        and 'rolr2' methods. Default value: None.\n",
      "    \n",
      "    theta0_filename : str or None, optional\n",
      "        Path to an unweighted sample of numerator parameters, as saved by the `madminer.sampling.SampleAugmenter`\n",
      "        functions. Required for the 'alice', 'alice2', 'alices', 'alices2', 'carl', 'carl2', 'nde', 'rascal',\n",
      "        'rascal2', 'rolr', 'rolr2', and 'scandal' methods. Default value: None.\n",
      "    \n",
      "    theta1_filename : str or None, optional\n",
      "        Path to an unweighted sample of denominator parameters, as saved by the `madminer.sampling.SampleAugmenter`\n",
      "        functions. Required for the 'alice2', 'alices2', 'carl2', 'rascal2', and 'rolr2' methods. Default value:\n",
      "        None.\n",
      "    \n",
      "    r_xz_filename : str or None, optional\n",
      "        Path to an unweighted sample of joint likelihood ratios, as saved by the `madminer.sampling.SampleAugmenter`\n",
      "        functions. Required for the 'alice', 'alice2', 'alices', 'alices2', 'rascal', 'rascal2', 'rolr', and 'rolr2'\n",
      "        methods. Default value: None.\n",
      "    \n",
      "    t_xz0_filename : str or None, optional\n",
      "        Path to an unweighted sample of joint scores at theta0, as saved by the `madminer.sampling.SampleAugmenter`\n",
      "        functions. Required for the 'alices', 'alices2', 'rascal', 'rascal2', 'sallino', 'sally', and 'scandal'\n",
      "        methods. Default value: None.\n",
      "    \n",
      "    t_xz1_filename : str or None, optional\n",
      "        Path to an unweighted sample of joint scores at theta1, as saved by the `madminer.sampling.SampleAugmenter`\n",
      "        functions. Required for the 'rascal2' and 'alices2' methods. Default value: None.\n",
      "    \n",
      "    features : list of int or None, optional\n",
      "        Indices of observables (features) that are used as input to the neural networks. If None, all observables\n",
      "        are used. Default value: None.\n",
      "    \n",
      "    nde_type : {'maf', 'mafmog'}, optional\n",
      "        If the method is 'nde' or 'scandal', nde_type determines the architecture used in the neural density\n",
      "        estimator. Currently supported are 'maf' for a Masked Autoregressive Flow with a Gaussian base density, or\n",
      "        'mafmog' for a Masked Autoregressive Flow with a mixture of Gaussian base densities. Default value: 'maf'.\n",
      "    \n",
      "    n_hidden : int, optional\n",
      "        Number of hidden layers in the neural networks. If method is 'nde' or 'scandal', this refers to the number\n",
      "        of hidden layers in each individual MADE layer. Default value: 100.\n",
      "        \n",
      "    activation : {'tanh', 'sigmoid', 'relu'}, optional\n",
      "        Activation function. Default value: 'tanh'\n",
      "    \n",
      "    maf_n_mades : int, optional\n",
      "        If method is 'nde' or 'scandal', this sets the number of MADE layers. Default value: 3.\n",
      "    \n",
      "    maf_batch_norm : bool, optional\n",
      "        If method is 'nde' or 'scandal', switches batch normalization layers after each MADE layer on or off.\n",
      "        Default: True.\n",
      "    \n",
      "    maf_batch_norm_alpha : float, optional\n",
      "        If method is 'nde' or 'scandal' and maf_batch_norm is True, this sets the alpha parameter in the calculation\n",
      "        of the running average of the mean and variance. Default value: 0.1.\n",
      "    \n",
      "    maf_mog_n_components : int, optional\n",
      "        If method is 'nde' or 'scandal' and nde_type is 'mafmog', this sets the number of Gaussian base components.\n",
      "        Default value: 10.\n",
      "    \n",
      "    alpha : float, optional\n",
      "        Hyperparameter weighting the score error in the loss function of the 'alices', 'alices2', 'rascal',\n",
      "        'rascal2', and 'scandal' methods.\n",
      "    \n",
      "    n_epochs : int, optional\n",
      "        Number of epochs. Default value: 20.\n",
      "    \n",
      "    batch_size : int, optional\n",
      "        Batch size. Default value: 128.\n",
      "    \n",
      "    initial_lr : float, optional\n",
      "        Learning rate during the first epoch, after which it exponentially decays to final_lr. Default value:\n",
      "        0.002.\n",
      "    \n",
      "    final_lr : float, optional\n",
      "        Learning rate during the last epoch. Default value: 0.0001.\n",
      "    \n",
      "    validation_split : float or None, optional\n",
      "        Fraction of samples used  for validation and early stopping (if early_stopping is True). If None, the entire\n",
      "        sample is used for training and early stopping is deactivated. Default value: 0.2.\n",
      "    \n",
      "    early_stopping : bool, optional\n",
      "        Activates early stopping based on the validation loss (only if validation_split is not None).\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "        None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MLForge.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:59  Training 20 estimators in ensemble\n",
      "23:59  Training estimator 1 / 20 in ensemble\n",
      "23:59  Starting training\n",
      "23:59    Method:                 sally\n",
      "23:59    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "23:59                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "23:59    Features:               [20]\n",
      "23:59    Method:                 sally\n",
      "23:59    Hidden layers:          (100, 100)\n",
      "23:59    Activation function:    tanh\n",
      "23:59    Batch size:             128\n",
      "23:59    Epochs:                 20\n",
      "23:59    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "23:59    Validation split:       None\n",
      "23:59    Early stopping:         True\n",
      "23:59  Loading training data\n",
      "23:59  Found 1000000 samples with 2 parameters and 29 observables\n",
      "23:59  Only using 1 of 29 observables\n",
      "23:59  Creating model for method sally\n",
      "23:59  Training model\n",
      "00:00    Epoch 2: train loss 18.53 ([18.53116525])\n",
      "00:01    Epoch 4: train loss 18.53 ([18.52501775])\n",
      "00:01    Epoch 6: train loss 18.52 ([18.52420072])\n",
      "00:02    Epoch 8: train loss 18.52 ([18.52275387])\n",
      "00:03    Epoch 10: train loss 18.52 ([18.52155758])\n",
      "00:03    Epoch 12: train loss 18.52 ([18.52197478])\n",
      "00:04    Epoch 14: train loss 18.52 ([18.52087975])\n",
      "00:04    Epoch 16: train loss 18.52 ([18.52101896])\n",
      "00:05    Epoch 18: train loss 18.52 ([18.52048455])\n",
      "00:05    Epoch 20: train loss 18.52 ([18.5205545])\n",
      "00:05  Finished training\n",
      "00:05  Training estimator 2 / 20 in ensemble\n",
      "00:05  Starting training\n",
      "00:05    Method:                 sally\n",
      "00:05    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "00:05                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "00:05    Features:               [20]\n",
      "00:05    Method:                 sally\n",
      "00:05    Hidden layers:          (100, 100)\n",
      "00:05    Activation function:    tanh\n",
      "00:05    Batch size:             128\n",
      "00:05    Epochs:                 20\n",
      "00:05    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:05    Validation split:       None\n",
      "00:05    Early stopping:         True\n",
      "00:05  Loading training data\n",
      "00:05  Found 1000000 samples with 2 parameters and 29 observables\n",
      "00:05  Only using 1 of 29 observables\n",
      "00:05  Creating model for method sally\n",
      "00:05  Training model\n",
      "00:06    Epoch 2: train loss 15.96 ([15.95518416])\n",
      "00:07    Epoch 4: train loss 15.95 ([15.95460588])\n",
      "00:07    Epoch 6: train loss 15.95 ([15.95423545])\n",
      "00:08    Epoch 8: train loss 15.96 ([15.95506164])\n",
      "00:08    Epoch 10: train loss 15.95 ([15.95380211])\n",
      "00:09    Epoch 12: train loss 15.95 ([15.95398711])\n",
      "00:10    Epoch 14: train loss 15.95 ([15.95386588])\n",
      "00:10    Epoch 16: train loss 15.95 ([15.95381451])\n",
      "00:11    Epoch 18: train loss 15.95 ([15.9536025])\n",
      "00:11    Epoch 20: train loss 15.95 ([15.95361918])\n",
      "00:11  Finished training\n",
      "00:11  Training estimator 3 / 20 in ensemble\n",
      "00:11  Starting training\n",
      "00:11    Method:                 sally\n",
      "00:11    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "00:11                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "00:11    Features:               [20]\n",
      "00:11    Method:                 sally\n",
      "00:11    Hidden layers:          (100, 100)\n",
      "00:11    Activation function:    tanh\n",
      "00:11    Batch size:             128\n",
      "00:11    Epochs:                 20\n",
      "00:11    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:11    Validation split:       None\n",
      "00:11    Early stopping:         True\n",
      "00:11  Loading training data\n",
      "00:11  Found 1000000 samples with 2 parameters and 29 observables\n",
      "00:11  Only using 1 of 29 observables\n",
      "00:11  Creating model for method sally\n",
      "00:11  Training model\n",
      "00:12    Epoch 2: train loss 53.49 ([53.48723374])\n",
      "00:13    Epoch 4: train loss 53.49 ([53.48758101])\n",
      "00:13    Epoch 6: train loss 53.48 ([53.48105027])\n",
      "00:14    Epoch 8: train loss 53.47 ([53.47314554])\n",
      "00:14    Epoch 10: train loss 53.47 ([53.47112789])\n",
      "00:15    Epoch 12: train loss 53.47 ([53.46847444])\n",
      "00:16    Epoch 14: train loss 53.47 ([53.46959862])\n",
      "00:16    Epoch 16: train loss 53.47 ([53.46562604])\n",
      "00:17    Epoch 18: train loss 53.46 ([53.46410091])\n",
      "00:17    Epoch 20: train loss 53.46 ([53.46313132])\n",
      "00:17  Finished training\n",
      "00:17  Training estimator 4 / 20 in ensemble\n",
      "00:17  Starting training\n",
      "00:17    Method:                 sally\n",
      "00:17    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "00:17                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "00:17    Features:               [20]\n",
      "00:17    Method:                 sally\n",
      "00:17    Hidden layers:          (100, 100)\n",
      "00:17    Activation function:    tanh\n",
      "00:17    Batch size:             128\n",
      "00:17    Epochs:                 20\n",
      "00:17    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:17    Validation split:       None\n",
      "00:17    Early stopping:         True\n",
      "00:17  Loading training data\n",
      "00:17  Found 1000000 samples with 2 parameters and 29 observables\n",
      "00:17  Only using 1 of 29 observables\n",
      "00:17  Creating model for method sally\n",
      "00:17  Training model\n",
      "00:18    Epoch 2: train loss 27.76 ([27.76236253])\n",
      "00:19    Epoch 4: train loss 27.76 ([27.76071841])\n",
      "00:19    Epoch 6: train loss 27.76 ([27.75552098])\n",
      "00:20    Epoch 8: train loss 27.75 ([27.75075157])\n",
      "00:20    Epoch 10: train loss 27.75 ([27.74740176])\n",
      "00:21    Epoch 12: train loss 27.75 ([27.74587898])\n",
      "00:21    Epoch 14: train loss 27.74 ([27.74390567])\n",
      "00:22    Epoch 16: train loss 27.74 ([27.7431447])\n",
      "00:23    Epoch 18: train loss 27.74 ([27.74206585])\n",
      "00:23    Epoch 20: train loss 27.74 ([27.74080825])\n",
      "00:23  Finished training\n",
      "00:23  Training estimator 5 / 20 in ensemble\n",
      "00:23  Starting training\n",
      "00:23    Method:                 sally\n",
      "00:23    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "00:23                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "00:23    Features:               [20]\n",
      "00:23    Method:                 sally\n",
      "00:23    Hidden layers:          (100, 100)\n",
      "00:23    Activation function:    tanh\n",
      "00:23    Batch size:             128\n",
      "00:23    Epochs:                 20\n",
      "00:23    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:23    Validation split:       None\n",
      "00:23    Early stopping:         True\n",
      "00:23  Loading training data\n",
      "00:23  Found 1000000 samples with 2 parameters and 29 observables\n",
      "00:23  Only using 1 of 29 observables\n",
      "00:23  Creating model for method sally\n",
      "00:23  Training model\n",
      "00:24    Epoch 2: train loss 17.96 ([17.96492218])\n",
      "00:25    Epoch 4: train loss 17.95 ([17.95150186])\n",
      "00:25    Epoch 6: train loss 17.96 ([17.96148812])\n",
      "00:26    Epoch 8: train loss 17.95 ([17.94979456])\n",
      "00:26    Epoch 10: train loss 17.95 ([17.94966713])\n",
      "00:27    Epoch 12: train loss 17.95 ([17.94930806])\n",
      "00:27    Epoch 14: train loss 17.95 ([17.9494573])\n",
      "00:28    Epoch 16: train loss 17.95 ([17.94914624])\n",
      "00:29    Epoch 18: train loss 17.95 ([17.94895465])\n",
      "00:29    Epoch 20: train loss 17.95 ([17.94966475])\n",
      "00:29  Finished training\n",
      "00:29  Training estimator 6 / 20 in ensemble\n",
      "00:29  Starting training\n",
      "00:29    Method:                 sally\n",
      "00:29    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "00:29                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "00:29    Features:               [20]\n",
      "00:29    Method:                 sally\n",
      "00:29    Hidden layers:          (100, 100)\n",
      "00:29    Activation function:    tanh\n",
      "00:29    Batch size:             128\n",
      "00:29    Epochs:                 20\n",
      "00:29    Learning rate:          0.002 initially, decaying to 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:29    Validation split:       None\n",
      "00:29    Early stopping:         True\n",
      "00:29  Loading training data\n",
      "00:29  Found 1000000 samples with 2 parameters and 29 observables\n",
      "00:29  Only using 1 of 29 observables\n",
      "00:29  Creating model for method sally\n",
      "00:29  Training model\n",
      "00:30    Epoch 2: train loss 21.21 ([21.20873806])\n",
      "00:31    Epoch 4: train loss 21.20 ([21.20120494])\n",
      "00:31    Epoch 6: train loss 21.19 ([21.19467101])\n",
      "00:32    Epoch 8: train loss 21.19 ([21.19351106])\n",
      "00:32    Epoch 10: train loss 21.19 ([21.19176383])\n",
      "00:33    Epoch 12: train loss 21.19 ([21.19122556])\n",
      "00:33    Epoch 14: train loss 21.19 ([21.18980352])\n",
      "00:34    Epoch 16: train loss 21.19 ([21.18935256])\n",
      "00:35    Epoch 18: train loss 21.19 ([21.18934567])\n",
      "00:35    Epoch 20: train loss 21.19 ([21.18909334])\n",
      "00:35  Finished training\n",
      "00:35  Training estimator 7 / 20 in ensemble\n",
      "00:35  Starting training\n",
      "00:35    Method:                 sally\n",
      "00:35    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "00:35                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "00:35    Features:               [20]\n",
      "00:35    Method:                 sally\n",
      "00:35    Hidden layers:          (100, 100)\n",
      "00:35    Activation function:    tanh\n",
      "00:35    Batch size:             128\n",
      "00:35    Epochs:                 20\n",
      "00:35    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:35    Validation split:       None\n",
      "00:35    Early stopping:         True\n",
      "00:35  Loading training data\n",
      "00:35  Found 1000000 samples with 2 parameters and 29 observables\n",
      "00:35  Only using 1 of 29 observables\n",
      "00:35  Creating model for method sally\n",
      "00:35  Training model\n",
      "00:36    Epoch 2: train loss 15.93 ([15.92743416])\n",
      "00:36    Epoch 4: train loss 15.92 ([15.92141564])\n",
      "00:37    Epoch 6: train loss 15.92 ([15.9194445])\n",
      "00:38    Epoch 8: train loss 15.92 ([15.91884629])\n",
      "00:38    Epoch 10: train loss 15.92 ([15.91842145])\n",
      "00:39    Epoch 12: train loss 15.92 ([15.91824599])\n",
      "00:39    Epoch 14: train loss 15.92 ([15.91790298])\n",
      "00:40    Epoch 16: train loss 15.92 ([15.9176102])\n",
      "00:40    Epoch 18: train loss 15.92 ([15.91761499])\n",
      "00:41    Epoch 20: train loss 15.92 ([15.91746784])\n",
      "00:41  Finished training\n",
      "00:41  Training estimator 8 / 20 in ensemble\n",
      "00:41  Starting training\n",
      "00:41    Method:                 sally\n",
      "00:41    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "00:41                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "00:41    Features:               [20]\n",
      "00:41    Method:                 sally\n",
      "00:41    Hidden layers:          (100, 100)\n",
      "00:41    Activation function:    tanh\n",
      "00:41    Batch size:             128\n",
      "00:41    Epochs:                 20\n",
      "00:41    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:41    Validation split:       None\n",
      "00:41    Early stopping:         True\n",
      "00:41  Loading training data\n",
      "00:41  Found 1000000 samples with 2 parameters and 29 observables\n",
      "00:41  Only using 1 of 29 observables\n",
      "00:41  Creating model for method sally\n",
      "00:41  Training model\n",
      "00:42    Epoch 2: train loss 27.64 ([27.64439145])\n",
      "00:42    Epoch 4: train loss 27.64 ([27.63735373])\n",
      "00:43    Epoch 6: train loss 27.63 ([27.63283289])\n",
      "00:44    Epoch 8: train loss 27.63 ([27.62595885])\n",
      "00:44    Epoch 10: train loss 27.62 ([27.623762])\n",
      "00:45    Epoch 12: train loss 27.62 ([27.62206022])\n",
      "00:45    Epoch 14: train loss 27.62 ([27.62146678])\n",
      "00:46    Epoch 16: train loss 27.63 ([27.63021678])\n",
      "00:46    Epoch 18: train loss 27.62 ([27.62232256])\n",
      "00:47    Epoch 20: train loss 27.62 ([27.62031952])\n",
      "00:47  Finished training\n",
      "00:47  Training estimator 9 / 20 in ensemble\n",
      "00:47  Starting training\n",
      "00:47    Method:                 sally\n",
      "00:47    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "00:47                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "00:47    Features:               [20]\n",
      "00:47    Method:                 sally\n",
      "00:47    Hidden layers:          (100, 100)\n",
      "00:47    Activation function:    tanh\n",
      "00:47    Batch size:             128\n",
      "00:47    Epochs:                 20\n",
      "00:47    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:47    Validation split:       None\n",
      "00:47    Early stopping:         True\n",
      "00:47  Loading training data\n",
      "00:47  Found 1000000 samples with 2 parameters and 29 observables\n",
      "00:47  Only using 1 of 29 observables\n",
      "00:47  Creating model for method sally\n",
      "00:47  Training model\n",
      "00:48    Epoch 2: train loss 21.02 ([21.01687632])\n",
      "00:48    Epoch 4: train loss 21.00 ([20.99953164])\n",
      "00:49    Epoch 6: train loss 20.99 ([20.99495819])\n",
      "00:49    Epoch 8: train loss 21.00 ([20.99508813])\n",
      "00:50    Epoch 10: train loss 21.00 ([20.99534097])\n",
      "00:51    Epoch 12: train loss 21.00 ([20.9952548])\n",
      "00:51    Epoch 14: train loss 20.99 ([20.99467621])\n",
      "00:52    Epoch 16: train loss 20.99 ([20.99468009])\n",
      "00:52    Epoch 18: train loss 20.99 ([20.9944837])\n",
      "00:53    Epoch 20: train loss 20.99 ([20.99448287])\n",
      "00:53  Finished training\n",
      "00:53  Training estimator 10 / 20 in ensemble\n",
      "00:53  Starting training\n",
      "00:53    Method:                 sally\n",
      "00:53    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "00:53                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "00:53    Features:               [20]\n",
      "00:53    Method:                 sally\n",
      "00:53    Hidden layers:          (100, 100)\n",
      "00:53    Activation function:    tanh\n",
      "00:53    Batch size:             128\n",
      "00:53    Epochs:                 20\n",
      "00:53    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:53    Validation split:       None\n",
      "00:53    Early stopping:         True\n",
      "00:53  Loading training data\n",
      "00:53  Found 1000000 samples with 2 parameters and 29 observables\n",
      "00:53  Only using 1 of 29 observables\n",
      "00:53  Creating model for method sally\n",
      "00:53  Training model\n",
      "00:54    Epoch 2: train loss 47.36 ([47.35638501])\n",
      "00:54    Epoch 4: train loss 47.35 ([47.34555437])\n",
      "00:55    Epoch 6: train loss 47.34 ([47.3431456])\n",
      "00:55    Epoch 8: train loss 47.34 ([47.3409281])\n",
      "00:56    Epoch 10: train loss 47.34 ([47.33997278])\n",
      "00:57    Epoch 12: train loss 47.34 ([47.33905047])\n",
      "00:57    Epoch 14: train loss 47.34 ([47.33892851])\n",
      "00:58    Epoch 16: train loss 47.34 ([47.33854158])\n",
      "00:58    Epoch 18: train loss 47.34 ([47.33839984])\n",
      "00:59    Epoch 20: train loss 47.34 ([47.33811409])\n",
      "00:59  Finished training\n",
      "00:59  Training estimator 11 / 20 in ensemble\n",
      "00:59  Starting training\n",
      "00:59    Method:                 sally\n",
      "00:59    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_10.npy\n",
      "00:59                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_10.npy\n",
      "00:59    Features:               [20]\n",
      "00:59    Method:                 sally\n",
      "00:59    Hidden layers:          (100, 100)\n",
      "00:59    Activation function:    tanh\n",
      "00:59    Batch size:             128\n",
      "00:59    Epochs:                 20\n",
      "00:59    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "00:59    Validation split:       None\n",
      "00:59    Early stopping:         True\n",
      "00:59  Loading training data\n",
      "00:59  Found 1000000 samples with 2 parameters and 29 observables\n",
      "00:59  Only using 1 of 29 observables\n",
      "00:59  Creating model for method sally\n",
      "00:59  Training model\n",
      "01:00    Epoch 2: train loss 16.99 ([16.98752939])\n",
      "01:00    Epoch 4: train loss 16.98 ([16.98223565])\n",
      "01:01    Epoch 6: train loss 16.98 ([16.97966992])\n",
      "01:01    Epoch 8: train loss 16.98 ([16.97868289])\n",
      "01:02    Epoch 10: train loss 16.98 ([16.97811449])\n",
      "01:03    Epoch 12: train loss 16.98 ([16.97749578])\n",
      "01:03    Epoch 14: train loss 16.98 ([16.97731455])\n",
      "01:04    Epoch 16: train loss 16.98 ([16.97704964])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:04    Epoch 18: train loss 16.98 ([16.97736391])\n",
      "01:05    Epoch 20: train loss 16.98 ([16.97683994])\n",
      "01:05  Finished training\n",
      "01:05  Training estimator 12 / 20 in ensemble\n",
      "01:05  Starting training\n",
      "01:05    Method:                 sally\n",
      "01:05    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_11.npy\n",
      "01:05                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_11.npy\n",
      "01:05    Features:               [20]\n",
      "01:05    Method:                 sally\n",
      "01:05    Hidden layers:          (100, 100)\n",
      "01:05    Activation function:    tanh\n",
      "01:05    Batch size:             128\n",
      "01:05    Epochs:                 20\n",
      "01:05    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:05    Validation split:       None\n",
      "01:05    Early stopping:         True\n",
      "01:05  Loading training data\n",
      "01:05  Found 1000000 samples with 2 parameters and 29 observables\n",
      "01:05  Only using 1 of 29 observables\n",
      "01:05  Creating model for method sally\n",
      "01:05  Training model\n",
      "01:06    Epoch 2: train loss 17.48 ([17.47726915])\n",
      "01:06    Epoch 4: train loss 17.47 ([17.47314735])\n",
      "01:07    Epoch 6: train loss 17.47 ([17.4681754])\n",
      "01:07    Epoch 8: train loss 17.47 ([17.46537085])\n",
      "01:08    Epoch 10: train loss 17.46 ([17.46465068])\n",
      "01:08    Epoch 12: train loss 17.46 ([17.463889])\n",
      "01:09    Epoch 14: train loss 17.46 ([17.46344164])\n",
      "01:10    Epoch 16: train loss 17.46 ([17.46294422])\n",
      "01:10    Epoch 18: train loss 17.47 ([17.46595074])\n",
      "01:11    Epoch 20: train loss 17.46 ([17.46386202])\n",
      "01:11  Finished training\n",
      "01:11  Training estimator 13 / 20 in ensemble\n",
      "01:11  Starting training\n",
      "01:11    Method:                 sally\n",
      "01:11    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_12.npy\n",
      "01:11                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_12.npy\n",
      "01:11    Features:               [20]\n",
      "01:11    Method:                 sally\n",
      "01:11    Hidden layers:          (100, 100)\n",
      "01:11    Activation function:    tanh\n",
      "01:11    Batch size:             128\n",
      "01:11    Epochs:                 20\n",
      "01:11    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:11    Validation split:       None\n",
      "01:11    Early stopping:         True\n",
      "01:11  Loading training data\n",
      "01:11  Found 1000000 samples with 2 parameters and 29 observables\n",
      "01:11  Only using 1 of 29 observables\n",
      "01:11  Creating model for method sally\n",
      "01:11  Training model\n",
      "01:12    Epoch 2: train loss 18.13 ([18.1344122])\n",
      "01:12    Epoch 4: train loss 18.13 ([18.12675121])\n",
      "01:13    Epoch 6: train loss 18.12 ([18.12491839])\n",
      "01:13    Epoch 8: train loss 18.13 ([18.12649202])\n",
      "01:14    Epoch 10: train loss 18.12 ([18.12419583])\n",
      "01:14    Epoch 12: train loss 18.12 ([18.12406296])\n",
      "01:15    Epoch 14: train loss 18.12 ([18.12393017])\n",
      "01:16    Epoch 16: train loss 18.12 ([18.12372436])\n",
      "01:16    Epoch 18: train loss 18.12 ([18.12357993])\n",
      "01:17    Epoch 20: train loss 18.12 ([18.12363781])\n",
      "01:17  Finished training\n",
      "01:17  Training estimator 14 / 20 in ensemble\n",
      "01:17  Starting training\n",
      "01:17    Method:                 sally\n",
      "01:17    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_13.npy\n",
      "01:17                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_13.npy\n",
      "01:17    Features:               [20]\n",
      "01:17    Method:                 sally\n",
      "01:17    Hidden layers:          (100, 100)\n",
      "01:17    Activation function:    tanh\n",
      "01:17    Batch size:             128\n",
      "01:17    Epochs:                 20\n",
      "01:17    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:17    Validation split:       None\n",
      "01:17    Early stopping:         True\n",
      "01:17  Loading training data\n",
      "01:17  Found 1000000 samples with 2 parameters and 29 observables\n",
      "01:17  Only using 1 of 29 observables\n",
      "01:17  Creating model for method sally\n",
      "01:17  Training model\n",
      "01:17    Epoch 2: train loss 12.49 ([12.49454145])\n",
      "01:18    Epoch 4: train loss 12.49 ([12.49352787])\n",
      "01:19    Epoch 6: train loss 12.49 ([12.49296262])\n",
      "01:19    Epoch 8: train loss 12.49 ([12.4926301])\n",
      "01:20    Epoch 10: train loss 12.49 ([12.4922927])\n",
      "01:20    Epoch 12: train loss 12.49 ([12.49222591])\n",
      "01:21    Epoch 14: train loss 12.49 ([12.49198954])\n",
      "01:21    Epoch 16: train loss 12.49 ([12.49212789])\n",
      "01:22    Epoch 18: train loss 12.49 ([12.49196305])\n",
      "01:23    Epoch 20: train loss 12.49 ([12.49191693])\n",
      "01:23  Finished training\n",
      "01:23  Training estimator 15 / 20 in ensemble\n",
      "01:23  Starting training\n",
      "01:23    Method:                 sally\n",
      "01:23    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_14.npy\n",
      "01:23                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_14.npy\n",
      "01:23    Features:               [20]\n",
      "01:23    Method:                 sally\n",
      "01:23    Hidden layers:          (100, 100)\n",
      "01:23    Activation function:    tanh\n",
      "01:23    Batch size:             128\n",
      "01:23    Epochs:                 20\n",
      "01:23    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:23    Validation split:       None\n",
      "01:23    Early stopping:         True\n",
      "01:23  Loading training data\n",
      "01:23  Found 1000000 samples with 2 parameters and 29 observables\n",
      "01:23  Only using 1 of 29 observables\n",
      "01:23  Creating model for method sally\n",
      "01:23  Training model\n",
      "01:23    Epoch 2: train loss 22.62 ([22.62115529])\n",
      "01:24    Epoch 4: train loss 22.62 ([22.61564614])\n",
      "01:25    Epoch 6: train loss 22.62 ([22.61510734])\n",
      "01:25    Epoch 8: train loss 22.61 ([22.61449443])\n",
      "01:26    Epoch 10: train loss 22.61 ([22.61381094])\n",
      "01:26    Epoch 12: train loss 22.61 ([22.6137074])\n",
      "01:27    Epoch 14: train loss 22.62 ([22.61943655])\n",
      "01:27    Epoch 16: train loss 22.61 ([22.61359388])\n",
      "01:28    Epoch 18: train loss 22.61 ([22.61316192])\n",
      "01:29    Epoch 20: train loss 22.61 ([22.61346771])\n",
      "01:29  Finished training\n",
      "01:29  Training estimator 16 / 20 in ensemble\n",
      "01:29  Starting training\n",
      "01:29    Method:                 sally\n",
      "01:29    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_15.npy\n",
      "01:29                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_15.npy\n",
      "01:29    Features:               [20]\n",
      "01:29    Method:                 sally\n",
      "01:29    Hidden layers:          (100, 100)\n",
      "01:29    Activation function:    tanh\n",
      "01:29    Batch size:             128\n",
      "01:29    Epochs:                 20\n",
      "01:29    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:29    Validation split:       None\n",
      "01:29    Early stopping:         True\n",
      "01:29  Loading training data\n",
      "01:29  Found 1000000 samples with 2 parameters and 29 observables\n",
      "01:29  Only using 1 of 29 observables\n",
      "01:29  Creating model for method sally\n",
      "01:29  Training model\n",
      "01:29    Epoch 2: train loss 15.56 ([15.55867889])\n",
      "01:30    Epoch 4: train loss 15.56 ([15.55540352])\n",
      "01:30    Epoch 6: train loss 15.55 ([15.5548168])\n",
      "01:31    Epoch 8: train loss 15.55 ([15.55410739])\n",
      "01:32    Epoch 10: train loss 15.55 ([15.55393557])\n",
      "01:32    Epoch 12: train loss 15.55 ([15.55381889])\n",
      "01:33    Epoch 14: train loss 15.56 ([15.5550375])\n",
      "01:33    Epoch 16: train loss 15.55 ([15.55339665])\n",
      "01:34    Epoch 18: train loss 15.55 ([15.55470926])\n",
      "01:34    Epoch 20: train loss 15.55 ([15.5531485])\n",
      "01:34  Finished training\n",
      "01:34  Training estimator 17 / 20 in ensemble\n",
      "01:34  Starting training\n",
      "01:34    Method:                 sally\n",
      "01:34    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_16.npy\n",
      "01:34                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_16.npy\n",
      "01:34    Features:               [20]\n",
      "01:34    Method:                 sally\n",
      "01:34    Hidden layers:          (100, 100)\n",
      "01:34    Activation function:    tanh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:34    Batch size:             128\n",
      "01:34    Epochs:                 20\n",
      "01:34    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:34    Validation split:       None\n",
      "01:34    Early stopping:         True\n",
      "01:34  Loading training data\n",
      "01:35  Found 1000000 samples with 2 parameters and 29 observables\n",
      "01:35  Only using 1 of 29 observables\n",
      "01:35  Creating model for method sally\n",
      "01:35  Training model\n",
      "01:35    Epoch 2: train loss 17.44 ([17.44425149])\n",
      "01:36    Epoch 4: train loss 17.44 ([17.43526646])\n",
      "01:36    Epoch 6: train loss 17.43 ([17.43436998])\n",
      "01:37    Epoch 8: train loss 17.43 ([17.43389831])\n",
      "01:38    Epoch 10: train loss 17.43 ([17.43356307])\n",
      "01:38    Epoch 12: train loss 17.43 ([17.43346514])\n",
      "01:39    Epoch 14: train loss 17.43 ([17.43315392])\n",
      "01:39    Epoch 16: train loss 17.43 ([17.43298645])\n",
      "01:40    Epoch 18: train loss 17.43 ([17.43345226])\n",
      "01:40    Epoch 20: train loss 17.43 ([17.4325266])\n",
      "01:40  Finished training\n",
      "01:40  Training estimator 18 / 20 in ensemble\n",
      "01:40  Starting training\n",
      "01:40    Method:                 sally\n",
      "01:40    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_17.npy\n",
      "01:40                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_17.npy\n",
      "01:40    Features:               [20]\n",
      "01:40    Method:                 sally\n",
      "01:40    Hidden layers:          (100, 100)\n",
      "01:40    Activation function:    tanh\n",
      "01:40    Batch size:             128\n",
      "01:40    Epochs:                 20\n",
      "01:40    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:40    Validation split:       None\n",
      "01:40    Early stopping:         True\n",
      "01:40  Loading training data\n",
      "01:40  Found 1000000 samples with 2 parameters and 29 observables\n",
      "01:40  Only using 1 of 29 observables\n",
      "01:40  Creating model for method sally\n",
      "01:40  Training model\n",
      "01:41    Epoch 2: train loss 18.10 ([18.10165393])\n",
      "01:42    Epoch 4: train loss 18.09 ([18.08982837])\n",
      "01:42    Epoch 6: train loss 18.09 ([18.08802791])\n",
      "01:43    Epoch 8: train loss 18.09 ([18.08611013])\n",
      "01:44    Epoch 10: train loss 18.09 ([18.08540843])\n",
      "01:44    Epoch 12: train loss 18.08 ([18.08454867])\n",
      "01:45    Epoch 14: train loss 18.08 ([18.08371402])\n",
      "01:45    Epoch 16: train loss 18.08 ([18.08345869])\n",
      "01:46    Epoch 18: train loss 18.08 ([18.08373459])\n",
      "01:46    Epoch 20: train loss 18.08 ([18.08290279])\n",
      "01:46  Finished training\n",
      "01:46  Training estimator 19 / 20 in ensemble\n",
      "01:46  Starting training\n",
      "01:46    Method:                 sally\n",
      "01:46    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_18.npy\n",
      "01:46                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_18.npy\n",
      "01:46    Features:               [20]\n",
      "01:46    Method:                 sally\n",
      "01:46    Hidden layers:          (100, 100)\n",
      "01:46    Activation function:    tanh\n",
      "01:46    Batch size:             128\n",
      "01:46    Epochs:                 20\n",
      "01:46    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:46    Validation split:       None\n",
      "01:46    Early stopping:         True\n",
      "01:46  Loading training data\n",
      "01:46  Found 1000000 samples with 2 parameters and 29 observables\n",
      "01:46  Only using 1 of 29 observables\n",
      "01:46  Creating model for method sally\n",
      "01:46  Training model\n",
      "01:47    Epoch 2: train loss 44.30 ([44.30308732])\n",
      "01:48    Epoch 4: train loss 44.30 ([44.29723955])\n",
      "01:48    Epoch 6: train loss 44.29 ([44.29078281])\n",
      "01:49    Epoch 8: train loss 44.28 ([44.28399371])\n",
      "01:49    Epoch 10: train loss 44.28 ([44.28189048])\n",
      "01:50    Epoch 12: train loss 44.28 ([44.28150978])\n",
      "01:51    Epoch 14: train loss 44.28 ([44.27813394])\n",
      "01:51    Epoch 16: train loss 44.28 ([44.2755434])\n",
      "01:52    Epoch 18: train loss 44.27 ([44.27497634])\n",
      "01:52    Epoch 20: train loss 44.28 ([44.28088766])\n",
      "01:52  Finished training\n",
      "01:52  Training estimator 20 / 20 in ensemble\n",
      "01:52  Starting training\n",
      "01:52    Method:                 sally\n",
      "01:52    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_19.npy\n",
      "01:52                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_19.npy\n",
      "01:52    Features:               [20]\n",
      "01:52    Method:                 sally\n",
      "01:52    Hidden layers:          (100, 100)\n",
      "01:52    Activation function:    tanh\n",
      "01:52    Batch size:             128\n",
      "01:52    Epochs:                 20\n",
      "01:52    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "01:52    Validation split:       None\n",
      "01:52    Early stopping:         True\n",
      "01:52  Loading training data\n",
      "01:52  Found 1000000 samples with 2 parameters and 29 observables\n",
      "01:52  Only using 1 of 29 observables\n",
      "01:52  Creating model for method sally\n",
      "01:52  Training model\n",
      "01:53    Epoch 2: train loss 15.77 ([15.77486401])\n",
      "01:54    Epoch 4: train loss 15.77 ([15.77209969])\n",
      "01:54    Epoch 6: train loss 15.77 ([15.7709254])\n",
      "01:55    Epoch 8: train loss 15.77 ([15.77034252])\n",
      "01:56    Epoch 10: train loss 15.77 ([15.76995142])\n",
      "01:56    Epoch 12: train loss 15.77 ([15.76977279])\n",
      "01:57    Epoch 14: train loss 15.77 ([15.7691733])\n",
      "01:57    Epoch 16: train loss 15.77 ([15.77219176])\n",
      "01:58    Epoch 18: train loss 15.77 ([15.76879358])\n",
      "01:58    Epoch 20: train loss 15.77 ([15.76872118])\n",
      "01:58  Finished training\n"
     ]
    }
   ],
   "source": [
    "ensemble_deltaphi = EnsembleForge(n_estimators)\n",
    "\n",
    "ensemble_deltaphi.train_all(\n",
    "    features=[ [20] for _ in range(n_estimators)],\n",
    "    method='sally',\n",
    "    x_filename=[sample_dir + 'train_local/x_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    t_xz0_filename=[sample_dir + 'train_local/t_xz_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=None,\n",
    "    n_hidden=n_hidden\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:58  Calculating expectation for 20 estimators in ensemble\n",
      "01:58  Starting evaluation for estimator 1 / 20 in ensemble\n",
      "01:58  Loading evaluation data\n",
      "01:58  Starting score evaluation\n",
      "01:59  Starting evaluation for estimator 2 / 20 in ensemble\n",
      "01:59  Loading evaluation data\n",
      "01:59  Starting score evaluation\n",
      "01:59  Starting evaluation for estimator 3 / 20 in ensemble\n",
      "01:59  Loading evaluation data\n",
      "01:59  Starting score evaluation\n",
      "01:59  Starting evaluation for estimator 4 / 20 in ensemble\n",
      "01:59  Loading evaluation data\n",
      "01:59  Starting score evaluation\n",
      "01:59  Starting evaluation for estimator 5 / 20 in ensemble\n",
      "01:59  Loading evaluation data\n",
      "01:59  Starting score evaluation\n",
      "01:59  Starting evaluation for estimator 6 / 20 in ensemble\n",
      "01:59  Loading evaluation data\n",
      "01:59  Starting score evaluation\n",
      "01:59  Starting evaluation for estimator 7 / 20 in ensemble\n",
      "01:59  Loading evaluation data\n",
      "01:59  Starting score evaluation\n",
      "01:59  Starting evaluation for estimator 8 / 20 in ensemble\n",
      "01:59  Loading evaluation data\n",
      "01:59  Starting score evaluation\n",
      "01:59  Starting evaluation for estimator 9 / 20 in ensemble\n",
      "01:59  Loading evaluation data\n",
      "01:59  Starting score evaluation\n",
      "02:00  Starting evaluation for estimator 10 / 20 in ensemble\n",
      "02:00  Loading evaluation data\n",
      "02:00  Starting score evaluation\n",
      "02:00  Starting evaluation for estimator 11 / 20 in ensemble\n",
      "02:00  Loading evaluation data\n",
      "02:00  Starting score evaluation\n",
      "02:00  Starting evaluation for estimator 12 / 20 in ensemble\n",
      "02:00  Loading evaluation data\n",
      "02:00  Starting score evaluation\n",
      "02:00  Starting evaluation for estimator 13 / 20 in ensemble\n",
      "02:00  Loading evaluation data\n",
      "02:00  Starting score evaluation\n",
      "02:00  Starting evaluation for estimator 14 / 20 in ensemble\n",
      "02:00  Loading evaluation data\n",
      "02:00  Starting score evaluation\n",
      "02:00  Starting evaluation for estimator 15 / 20 in ensemble\n",
      "02:00  Loading evaluation data\n",
      "02:00  Starting score evaluation\n",
      "02:00  Starting evaluation for estimator 16 / 20 in ensemble\n",
      "02:00  Loading evaluation data\n",
      "02:00  Starting score evaluation\n",
      "02:01  Starting evaluation for estimator 17 / 20 in ensemble\n",
      "02:01  Loading evaluation data\n",
      "02:01  Starting score evaluation\n",
      "02:01  Starting evaluation for estimator 18 / 20 in ensemble\n",
      "02:01  Loading evaluation data\n",
      "02:01  Starting score evaluation\n",
      "02:01  Starting evaluation for estimator 19 / 20 in ensemble\n",
      "02:01  Loading evaluation data\n",
      "02:01  Starting score evaluation\n",
      "02:01  Starting evaluation for estimator 20 / 20 in ensemble\n",
      "02:01  Loading evaluation data\n",
      "02:01  Starting score evaluation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00875856,  0.00459889],\n",
       "       [-0.0003045 ,  0.00149938],\n",
       "       [ 0.00439043, -0.00443414],\n",
       "       [-0.0059252 ,  0.01023495],\n",
       "       [-0.00315193, -0.00054421],\n",
       "       [ 0.00304334,  0.00724456],\n",
       "       [ 0.00237222, -0.00301501],\n",
       "       [ 0.0007224 ,  0.00481131],\n",
       "       [-0.00355629,  0.00685876],\n",
       "       [ 0.00090163, -0.00755925],\n",
       "       [ 0.0026124 , -0.01008341],\n",
       "       [ 0.00877647,  0.00730935],\n",
       "       [ 0.00096675, -0.0042282 ],\n",
       "       [-0.00017658, -0.00820993],\n",
       "       [-0.00691816, -0.00035836],\n",
       "       [-0.0018715 ,  0.0006327 ],\n",
       "       [ 0.00078935, -0.00366219],\n",
       "       [ 0.00461002,  0.0142154 ],\n",
       "       [ 0.00450602,  0.0049802 ],\n",
       "       [ 0.00160791,  0.00054987]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_deltaphi.calculate_expectation(\n",
    "    x_filename=sample_dir + 'validation/x_validation.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:01  Saving ensemble setup to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/ensemble.json\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_0_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_0_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_1_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_1_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_2_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_2_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_3_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_3_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_4_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_4_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_5_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_5_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_6_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_6_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_7_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_7_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_8_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_8_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_9_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_9_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_10_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_10_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_11_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_11_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_12_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_12_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_13_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_13_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_14_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_14_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_15_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_15_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_16_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_16_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_17_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_17_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_18_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_18_state_dict.pt\n",
      "02:01  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_19_settings.json\n",
      "02:01  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_deltaphi/estimator_19_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "ensemble_deltaphi.save(model_dir + 'sally_ensemble_deltaphi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d toy study (MET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:01  Training 20 estimators in ensemble\n",
      "02:01  Training estimator 1 / 20 in ensemble\n",
      "02:01  Starting training\n",
      "02:01    Method:                 sally\n",
      "02:01    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "02:01                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "02:01    Features:               [0]\n",
      "02:01    Method:                 sally\n",
      "02:01    Hidden layers:          (100, 100)\n",
      "02:01    Activation function:    tanh\n",
      "02:01    Batch size:             128\n",
      "02:01    Epochs:                 20\n",
      "02:01    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:01    Validation split:       None\n",
      "02:01    Early stopping:         True\n",
      "02:01  Loading training data\n",
      "02:01  Found 1000000 samples with 2 parameters and 29 observables\n",
      "02:01  Only using 1 of 29 observables\n",
      "02:01  Creating model for method sally\n",
      "02:01  Training model\n",
      "02:02    Epoch 2: train loss 18.55 ([18.55081943])\n",
      "02:02    Epoch 4: train loss 18.54 ([18.5441461])\n",
      "02:03    Epoch 6: train loss 18.54 ([18.53655158])\n",
      "02:04    Epoch 8: train loss 18.53 ([18.53373033])\n",
      "02:04    Epoch 10: train loss 18.53 ([18.52976536])\n",
      "02:05    Epoch 12: train loss 18.53 ([18.52846132])\n",
      "02:06    Epoch 14: train loss 18.53 ([18.52849869])\n",
      "02:06    Epoch 16: train loss 18.53 ([18.52577819])\n",
      "02:07    Epoch 18: train loss 18.53 ([18.53276829])\n",
      "02:08    Epoch 20: train loss 18.52 ([18.52285248])\n",
      "02:08  Finished training\n",
      "02:08  Training estimator 2 / 20 in ensemble\n",
      "02:08  Starting training\n",
      "02:08    Method:                 sally\n",
      "02:08    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "02:08                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "02:08    Features:               [0]\n",
      "02:08    Method:                 sally\n",
      "02:08    Hidden layers:          (100, 100)\n",
      "02:08    Activation function:    tanh\n",
      "02:08    Batch size:             128\n",
      "02:08    Epochs:                 20\n",
      "02:08    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:08    Validation split:       None\n",
      "02:08    Early stopping:         True\n",
      "02:08  Loading training data\n",
      "02:08  Found 1000000 samples with 2 parameters and 29 observables\n",
      "02:08  Only using 1 of 29 observables\n",
      "02:08  Creating model for method sally\n",
      "02:08  Training model\n",
      "02:08    Epoch 2: train loss 15.98 ([15.98145184])\n",
      "02:09    Epoch 4: train loss 15.97 ([15.97405149])\n",
      "02:10    Epoch 6: train loss 15.97 ([15.96927649])\n",
      "02:10    Epoch 8: train loss 15.96 ([15.96444112])\n",
      "02:11    Epoch 10: train loss 15.96 ([15.96201655])\n",
      "02:12    Epoch 12: train loss 15.96 ([15.95952577])\n",
      "02:12    Epoch 14: train loss 15.96 ([15.9580597])\n",
      "02:13    Epoch 16: train loss 15.96 ([15.95720158])\n",
      "02:14    Epoch 18: train loss 15.96 ([15.95618877])\n",
      "02:14    Epoch 20: train loss 15.96 ([15.95627781])\n",
      "02:14  Finished training\n",
      "02:14  Training estimator 3 / 20 in ensemble\n",
      "02:14  Starting training\n",
      "02:14    Method:                 sally\n",
      "02:14    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "02:14                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "02:14    Features:               [0]\n",
      "02:14    Method:                 sally\n",
      "02:14    Hidden layers:          (100, 100)\n",
      "02:14    Activation function:    tanh\n",
      "02:14    Batch size:             128\n",
      "02:14    Epochs:                 20\n",
      "02:14    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:14    Validation split:       None\n",
      "02:14    Early stopping:         True\n",
      "02:14  Loading training data\n",
      "02:14  Found 1000000 samples with 2 parameters and 29 observables\n",
      "02:14  Only using 1 of 29 observables\n",
      "02:14  Creating model for method sally\n",
      "02:14  Training model\n",
      "02:15    Epoch 2: train loss 53.50 ([53.49804174])\n",
      "02:16    Epoch 4: train loss 53.49 ([53.48868457])\n",
      "02:16    Epoch 6: train loss 53.48 ([53.48380358])\n",
      "02:17    Epoch 8: train loss 53.48 ([53.47716726])\n",
      "02:18    Epoch 10: train loss 53.47 ([53.47364848])\n",
      "02:18    Epoch 12: train loss 53.47 ([53.47117406])\n",
      "02:19    Epoch 14: train loss 53.47 ([53.46928667])\n",
      "02:20    Epoch 16: train loss 53.47 ([53.46686503])\n",
      "02:20    Epoch 18: train loss 53.47 ([53.46618964])\n",
      "02:21    Epoch 20: train loss 53.47 ([53.46953094])\n",
      "02:21  Finished training\n",
      "02:21  Training estimator 4 / 20 in ensemble\n",
      "02:21  Starting training\n",
      "02:21    Method:                 sally\n",
      "02:21    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "02:21                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "02:21    Features:               [0]\n",
      "02:21    Method:                 sally\n",
      "02:21    Hidden layers:          (100, 100)\n",
      "02:21    Activation function:    tanh\n",
      "02:21    Batch size:             128\n",
      "02:21    Epochs:                 20\n",
      "02:21    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:21    Validation split:       None\n",
      "02:21    Early stopping:         True\n",
      "02:21  Loading training data\n",
      "02:21  Found 1000000 samples with 2 parameters and 29 observables\n",
      "02:21  Only using 1 of 29 observables\n",
      "02:21  Creating model for method sally\n",
      "02:21  Training model\n",
      "02:22    Epoch 2: train loss 27.77 ([27.77411721])\n",
      "02:22    Epoch 4: train loss 27.77 ([27.76531809])\n",
      "02:23    Epoch 6: train loss 27.76 ([27.76044602])\n",
      "02:23    Epoch 8: train loss 27.75 ([27.75411302])\n",
      "02:24    Epoch 10: train loss 27.75 ([27.7508961])\n",
      "02:25    Epoch 12: train loss 27.75 ([27.74868734])\n",
      "02:25    Epoch 14: train loss 27.75 ([27.74669])\n",
      "02:26    Epoch 16: train loss 27.74 ([27.74474368])\n",
      "02:26    Epoch 18: train loss 27.74 ([27.74398343])\n",
      "02:27    Epoch 20: train loss 27.74 ([27.74221645])\n",
      "02:27  Finished training\n",
      "02:27  Training estimator 5 / 20 in ensemble\n",
      "02:27  Starting training\n",
      "02:27    Method:                 sally\n",
      "02:27    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "02:27                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "02:27    Features:               [0]\n",
      "02:27    Method:                 sally\n",
      "02:27    Hidden layers:          (100, 100)\n",
      "02:27    Activation function:    tanh\n",
      "02:27    Batch size:             128\n",
      "02:27    Epochs:                 20\n",
      "02:27    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:27    Validation split:       None\n",
      "02:27    Early stopping:         True\n",
      "02:27  Loading training data\n",
      "02:27  Found 1000000 samples with 2 parameters and 29 observables\n",
      "02:27  Only using 1 of 29 observables\n",
      "02:27  Creating model for method sally\n",
      "02:27  Training model\n",
      "02:28    Epoch 2: train loss 17.98 ([17.97890919])\n",
      "02:28    Epoch 4: train loss 17.97 ([17.96929158])\n",
      "02:29    Epoch 6: train loss 17.97 ([17.96616769])\n",
      "02:30    Epoch 8: train loss 17.96 ([17.96176166])\n",
      "02:30    Epoch 10: train loss 17.96 ([17.95907741])\n",
      "02:31    Epoch 12: train loss 17.96 ([17.95611225])\n",
      "02:32    Epoch 14: train loss 17.95 ([17.95445472])\n",
      "02:32    Epoch 16: train loss 17.95 ([17.95338399])\n",
      "02:33    Epoch 18: train loss 17.95 ([17.95239616])\n",
      "02:34    Epoch 20: train loss 17.95 ([17.95166992])\n",
      "02:34  Finished training\n",
      "02:34  Training estimator 6 / 20 in ensemble\n",
      "02:34  Starting training\n",
      "02:34    Method:                 sally\n",
      "02:34    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "02:34                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "02:34    Features:               [0]\n",
      "02:34    Method:                 sally\n",
      "02:34    Hidden layers:          (100, 100)\n",
      "02:34    Activation function:    tanh\n",
      "02:34    Batch size:             128\n",
      "02:34    Epochs:                 20\n",
      "02:34    Learning rate:          0.002 initially, decaying to 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:34    Validation split:       None\n",
      "02:34    Early stopping:         True\n",
      "02:34  Loading training data\n",
      "02:34  Found 1000000 samples with 2 parameters and 29 observables\n",
      "02:34  Only using 1 of 29 observables\n",
      "02:34  Creating model for method sally\n",
      "02:34  Training model\n",
      "02:34    Epoch 2: train loss 21.22 ([21.21900682])\n",
      "02:35    Epoch 4: train loss 21.21 ([21.2132529])\n",
      "02:36    Epoch 6: train loss 21.21 ([21.20515026])\n",
      "02:36    Epoch 8: train loss 21.20 ([21.20273598])\n",
      "02:37    Epoch 10: train loss 21.20 ([21.20007542])\n",
      "02:38    Epoch 12: train loss 21.20 ([21.19698787])\n",
      "02:38    Epoch 14: train loss 21.20 ([21.19551774])\n",
      "02:39    Epoch 16: train loss 21.19 ([21.19478414])\n",
      "02:40    Epoch 18: train loss 21.19 ([21.19325749])\n",
      "02:40    Epoch 20: train loss 21.19 ([21.19233094])\n",
      "02:40  Finished training\n",
      "02:40  Training estimator 7 / 20 in ensemble\n",
      "02:40  Starting training\n",
      "02:40    Method:                 sally\n",
      "02:40    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "02:40                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "02:40    Features:               [0]\n",
      "02:40    Method:                 sally\n",
      "02:40    Hidden layers:          (100, 100)\n",
      "02:40    Activation function:    tanh\n",
      "02:40    Batch size:             128\n",
      "02:40    Epochs:                 20\n",
      "02:40    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:40    Validation split:       None\n",
      "02:40    Early stopping:         True\n",
      "02:40  Loading training data\n",
      "02:40  Found 1000000 samples with 2 parameters and 29 observables\n",
      "02:40  Only using 1 of 29 observables\n",
      "02:40  Creating model for method sally\n",
      "02:40  Training model\n",
      "02:41    Epoch 2: train loss 15.94 ([15.94484607])\n",
      "02:42    Epoch 4: train loss 15.94 ([15.9403804])\n",
      "02:42    Epoch 6: train loss 15.93 ([15.93412137])\n",
      "02:43    Epoch 8: train loss 15.93 ([15.92992154])\n",
      "02:43    Epoch 10: train loss 15.93 ([15.93130006])\n",
      "02:44    Epoch 12: train loss 15.93 ([15.92546221])\n",
      "02:45    Epoch 14: train loss 15.92 ([15.92352967])\n",
      "02:45    Epoch 16: train loss 15.92 ([15.92307422])\n",
      "02:46    Epoch 18: train loss 15.92 ([15.92218773])\n",
      "02:47    Epoch 20: train loss 15.92 ([15.92097233])\n",
      "02:47  Finished training\n",
      "02:47  Training estimator 8 / 20 in ensemble\n",
      "02:47  Starting training\n",
      "02:47    Method:                 sally\n",
      "02:47    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "02:47                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "02:47    Features:               [0]\n",
      "02:47    Method:                 sally\n",
      "02:47    Hidden layers:          (100, 100)\n",
      "02:47    Activation function:    tanh\n",
      "02:47    Batch size:             128\n",
      "02:47    Epochs:                 20\n",
      "02:47    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:47    Validation split:       None\n",
      "02:47    Early stopping:         True\n",
      "02:47  Loading training data\n",
      "02:47  Found 1000000 samples with 2 parameters and 29 observables\n",
      "02:47  Only using 1 of 29 observables\n",
      "02:47  Creating model for method sally\n",
      "02:47  Training model\n",
      "02:47    Epoch 2: train loss 27.65 ([27.65302088])\n",
      "02:48    Epoch 4: train loss 27.65 ([27.64675166])\n",
      "02:49    Epoch 6: train loss 27.64 ([27.63939726])\n",
      "02:49    Epoch 8: train loss 27.64 ([27.63501177])\n",
      "02:50    Epoch 10: train loss 27.63 ([27.6327795])\n",
      "02:51    Epoch 12: train loss 27.63 ([27.62918962])\n",
      "02:51    Epoch 14: train loss 27.63 ([27.62751869])\n",
      "02:52    Epoch 16: train loss 27.63 ([27.62623655])\n",
      "02:53    Epoch 18: train loss 27.63 ([27.62538362])\n",
      "02:53    Epoch 20: train loss 27.62 ([27.6249558])\n",
      "02:53  Finished training\n",
      "02:53  Training estimator 9 / 20 in ensemble\n",
      "02:53  Starting training\n",
      "02:53    Method:                 sally\n",
      "02:53    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "02:53                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "02:53    Features:               [0]\n",
      "02:53    Method:                 sally\n",
      "02:53    Hidden layers:          (100, 100)\n",
      "02:53    Activation function:    tanh\n",
      "02:53    Batch size:             128\n",
      "02:53    Epochs:                 20\n",
      "02:53    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "02:53    Validation split:       None\n",
      "02:53    Early stopping:         True\n",
      "02:53  Loading training data\n",
      "02:53  Found 1000000 samples with 2 parameters and 29 observables\n",
      "02:53  Only using 1 of 29 observables\n",
      "02:53  Creating model for method sally\n",
      "02:53  Training model\n",
      "02:54    Epoch 2: train loss 21.03 ([21.02529514])\n",
      "02:55    Epoch 4: train loss 21.02 ([21.01717082])\n",
      "02:55    Epoch 6: train loss 21.01 ([21.01115401])\n",
      "02:56    Epoch 8: train loss 21.01 ([21.0071865])\n",
      "02:57    Epoch 10: train loss 21.00 ([21.00323823])\n",
      "02:57    Epoch 12: train loss 21.00 ([21.00046043])\n",
      "02:58    Epoch 14: train loss 21.00 ([20.99929357])\n",
      "02:58    Epoch 16: train loss 21.00 ([20.9977636])\n",
      "02:59    Epoch 18: train loss 21.00 ([20.99695395])\n",
      "03:00    Epoch 20: train loss 21.00 ([20.99551285])\n",
      "03:00  Finished training\n",
      "03:00  Training estimator 10 / 20 in ensemble\n",
      "03:00  Starting training\n",
      "03:00    Method:                 sally\n",
      "03:00    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "03:00                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "03:00    Features:               [0]\n",
      "03:00    Method:                 sally\n",
      "03:00    Hidden layers:          (100, 100)\n",
      "03:00    Activation function:    tanh\n",
      "03:00    Batch size:             128\n",
      "03:00    Epochs:                 20\n",
      "03:00    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:00    Validation split:       None\n",
      "03:00    Early stopping:         True\n",
      "03:00  Loading training data\n",
      "03:00  Found 1000000 samples with 2 parameters and 29 observables\n",
      "03:00  Only using 1 of 29 observables\n",
      "03:00  Creating model for method sally\n",
      "03:00  Training model\n",
      "03:01    Epoch 2: train loss 47.38 ([47.38432633])\n",
      "03:01    Epoch 4: train loss 47.37 ([47.36617329])\n",
      "03:02    Epoch 6: train loss 47.36 ([47.35866297])\n",
      "03:02    Epoch 8: train loss 47.35 ([47.35482153])\n",
      "03:03    Epoch 10: train loss 47.35 ([47.35014018])\n",
      "03:04    Epoch 12: train loss 47.35 ([47.34776695])\n",
      "03:04    Epoch 14: train loss 47.35 ([47.34631396])\n",
      "03:05    Epoch 16: train loss 47.34 ([47.34459339])\n",
      "03:06    Epoch 18: train loss 47.34 ([47.344035])\n",
      "03:06    Epoch 20: train loss 47.34 ([47.34321391])\n",
      "03:06  Finished training\n",
      "03:06  Training estimator 11 / 20 in ensemble\n",
      "03:06  Starting training\n",
      "03:06    Method:                 sally\n",
      "03:06    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_10.npy\n",
      "03:06                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_10.npy\n",
      "03:06    Features:               [0]\n",
      "03:06    Method:                 sally\n",
      "03:06    Hidden layers:          (100, 100)\n",
      "03:06    Activation function:    tanh\n",
      "03:06    Batch size:             128\n",
      "03:06    Epochs:                 20\n",
      "03:06    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:06    Validation split:       None\n",
      "03:06    Early stopping:         True\n",
      "03:06  Loading training data\n",
      "03:06  Found 1000000 samples with 2 parameters and 29 observables\n",
      "03:06  Only using 1 of 29 observables\n",
      "03:06  Creating model for method sally\n",
      "03:06  Training model\n",
      "03:07    Epoch 2: train loss 17.01 ([17.00638405])\n",
      "03:08    Epoch 4: train loss 17.00 ([17.00025718])\n",
      "03:08    Epoch 6: train loss 16.99 ([16.99426362])\n",
      "03:09    Epoch 8: train loss 16.99 ([16.99027278])\n",
      "03:10    Epoch 10: train loss 16.99 ([16.98977997])\n",
      "03:10    Epoch 12: train loss 16.99 ([16.98551056])\n",
      "03:11    Epoch 14: train loss 16.99 ([16.98644539])\n",
      "03:12    Epoch 16: train loss 17.00 ([16.99990563])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:12    Epoch 18: train loss 16.98 ([16.98204427])\n",
      "03:13    Epoch 20: train loss 16.98 ([16.98071614])\n",
      "03:13  Finished training\n",
      "03:13  Training estimator 12 / 20 in ensemble\n",
      "03:13  Starting training\n",
      "03:13    Method:                 sally\n",
      "03:13    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_11.npy\n",
      "03:13                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_11.npy\n",
      "03:13    Features:               [0]\n",
      "03:13    Method:                 sally\n",
      "03:13    Hidden layers:          (100, 100)\n",
      "03:13    Activation function:    tanh\n",
      "03:13    Batch size:             128\n",
      "03:13    Epochs:                 20\n",
      "03:13    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:13    Validation split:       None\n",
      "03:13    Early stopping:         True\n",
      "03:13  Loading training data\n",
      "03:13  Found 1000000 samples with 2 parameters and 29 observables\n",
      "03:13  Only using 1 of 29 observables\n",
      "03:13  Creating model for method sally\n",
      "03:13  Training model\n",
      "03:14    Epoch 2: train loss 17.49 ([17.49339947])\n",
      "03:14    Epoch 4: train loss 17.48 ([17.48482124])\n",
      "03:15    Epoch 6: train loss 17.48 ([17.47921034])\n",
      "03:15    Epoch 8: train loss 17.48 ([17.47623748])\n",
      "03:16    Epoch 10: train loss 17.47 ([17.47344811])\n",
      "03:17    Epoch 12: train loss 17.47 ([17.47156966])\n",
      "03:17    Epoch 14: train loss 17.47 ([17.47143688])\n",
      "03:18    Epoch 16: train loss 17.47 ([17.46864874])\n",
      "03:19    Epoch 18: train loss 17.47 ([17.46758602])\n",
      "03:19    Epoch 20: train loss 17.47 ([17.46733254])\n",
      "03:19  Finished training\n",
      "03:19  Training estimator 13 / 20 in ensemble\n",
      "03:19  Starting training\n",
      "03:19    Method:                 sally\n",
      "03:19    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_12.npy\n",
      "03:19                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_12.npy\n",
      "03:19    Features:               [0]\n",
      "03:19    Method:                 sally\n",
      "03:19    Hidden layers:          (100, 100)\n",
      "03:19    Activation function:    tanh\n",
      "03:19    Batch size:             128\n",
      "03:19    Epochs:                 20\n",
      "03:19    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:19    Validation split:       None\n",
      "03:19    Early stopping:         True\n",
      "03:19  Loading training data\n",
      "03:19  Found 1000000 samples with 2 parameters and 29 observables\n",
      "03:19  Only using 1 of 29 observables\n",
      "03:19  Creating model for method sally\n",
      "03:19  Training model\n",
      "03:20    Epoch 2: train loss 18.15 ([18.15059358])\n",
      "03:21    Epoch 4: train loss 18.15 ([18.14535368])\n",
      "03:21    Epoch 6: train loss 18.14 ([18.14047447])\n",
      "03:22    Epoch 8: train loss 18.14 ([18.13556005])\n",
      "03:23    Epoch 10: train loss 18.13 ([18.13383179])\n",
      "03:23    Epoch 12: train loss 18.13 ([18.13018606])\n",
      "03:24    Epoch 14: train loss 18.13 ([18.12855793])\n",
      "03:25    Epoch 16: train loss 18.13 ([18.12696375])\n",
      "03:25    Epoch 18: train loss 18.13 ([18.12632032])\n",
      "03:26    Epoch 20: train loss 18.13 ([18.12593153])\n",
      "03:26  Finished training\n",
      "03:26  Training estimator 14 / 20 in ensemble\n",
      "03:26  Starting training\n",
      "03:26    Method:                 sally\n",
      "03:26    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_13.npy\n",
      "03:26                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_13.npy\n",
      "03:26    Features:               [0]\n",
      "03:26    Method:                 sally\n",
      "03:26    Hidden layers:          (100, 100)\n",
      "03:26    Activation function:    tanh\n",
      "03:26    Batch size:             128\n",
      "03:26    Epochs:                 20\n",
      "03:26    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:26    Validation split:       None\n",
      "03:26    Early stopping:         True\n",
      "03:26  Loading training data\n",
      "03:26  Found 1000000 samples with 2 parameters and 29 observables\n",
      "03:26  Only using 1 of 29 observables\n",
      "03:26  Creating model for method sally\n",
      "03:26  Training model\n",
      "03:27    Epoch 2: train loss 12.52 ([12.51707204])\n",
      "03:27    Epoch 4: train loss 12.51 ([12.51235854])\n",
      "03:28    Epoch 6: train loss 12.51 ([12.50755416])\n",
      "03:29    Epoch 8: train loss 12.50 ([12.50300607])\n",
      "03:29    Epoch 10: train loss 12.50 ([12.5005925])\n",
      "03:30    Epoch 12: train loss 12.50 ([12.49876924])\n",
      "03:30    Epoch 14: train loss 12.50 ([12.49695743])\n",
      "03:31    Epoch 16: train loss 12.50 ([12.4957988])\n",
      "03:32    Epoch 18: train loss 12.49 ([12.49394878])\n",
      "03:32    Epoch 20: train loss 12.49 ([12.49353345])\n",
      "03:32  Finished training\n",
      "03:32  Training estimator 15 / 20 in ensemble\n",
      "03:32  Starting training\n",
      "03:32    Method:                 sally\n",
      "03:32    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_14.npy\n",
      "03:32                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_14.npy\n",
      "03:32    Features:               [0]\n",
      "03:32    Method:                 sally\n",
      "03:32    Hidden layers:          (100, 100)\n",
      "03:32    Activation function:    tanh\n",
      "03:32    Batch size:             128\n",
      "03:32    Epochs:                 20\n",
      "03:32    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:32    Validation split:       None\n",
      "03:32    Early stopping:         True\n",
      "03:32  Loading training data\n",
      "03:32  Found 1000000 samples with 2 parameters and 29 observables\n",
      "03:32  Only using 1 of 29 observables\n",
      "03:32  Creating model for method sally\n",
      "03:32  Training model\n",
      "03:33    Epoch 2: train loss 22.64 ([22.64360541])\n",
      "03:34    Epoch 4: train loss 22.64 ([22.63804203])\n",
      "03:34    Epoch 6: train loss 22.63 ([22.63245455])\n",
      "03:35    Epoch 8: train loss 22.63 ([22.6275017])\n",
      "03:36    Epoch 10: train loss 22.62 ([22.6245602])\n",
      "03:36    Epoch 12: train loss 22.62 ([22.62146153])\n",
      "03:37    Epoch 14: train loss 22.62 ([22.61944879])\n",
      "03:38    Epoch 16: train loss 22.62 ([22.61861294])\n",
      "03:38    Epoch 18: train loss 22.62 ([22.61884443])\n",
      "03:39    Epoch 20: train loss 22.62 ([22.61528721])\n",
      "03:39  Finished training\n",
      "03:39  Training estimator 16 / 20 in ensemble\n",
      "03:39  Starting training\n",
      "03:39    Method:                 sally\n",
      "03:39    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_15.npy\n",
      "03:39                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_15.npy\n",
      "03:39    Features:               [0]\n",
      "03:39    Method:                 sally\n",
      "03:39    Hidden layers:          (100, 100)\n",
      "03:39    Activation function:    tanh\n",
      "03:39    Batch size:             128\n",
      "03:39    Epochs:                 20\n",
      "03:39    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:39    Validation split:       None\n",
      "03:39    Early stopping:         True\n",
      "03:39  Loading training data\n",
      "03:39  Found 1000000 samples with 2 parameters and 29 observables\n",
      "03:39  Only using 1 of 29 observables\n",
      "03:39  Creating model for method sally\n",
      "03:39  Training model\n",
      "03:40    Epoch 2: train loss 15.58 ([15.57991865])\n",
      "03:40    Epoch 4: train loss 15.57 ([15.57231432])\n",
      "03:41    Epoch 6: train loss 15.57 ([15.56846868])\n",
      "03:41    Epoch 8: train loss 15.56 ([15.5639621])\n",
      "03:42    Epoch 10: train loss 15.56 ([15.56205652])\n",
      "03:43    Epoch 12: train loss 15.56 ([15.56021675])\n",
      "03:43    Epoch 14: train loss 15.57 ([15.56525279])\n",
      "03:44    Epoch 16: train loss 15.56 ([15.55716491])\n",
      "03:45    Epoch 18: train loss 15.56 ([15.55627902])\n",
      "03:45    Epoch 20: train loss 15.56 ([15.55551904])\n",
      "03:45  Finished training\n",
      "03:45  Training estimator 17 / 20 in ensemble\n",
      "03:45  Starting training\n",
      "03:45    Method:                 sally\n",
      "03:45    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_16.npy\n",
      "03:45                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_16.npy\n",
      "03:45    Features:               [0]\n",
      "03:45    Method:                 sally\n",
      "03:45    Hidden layers:          (100, 100)\n",
      "03:45    Activation function:    tanh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:45    Batch size:             128\n",
      "03:45    Epochs:                 20\n",
      "03:45    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:45    Validation split:       None\n",
      "03:45    Early stopping:         True\n",
      "03:45  Loading training data\n",
      "03:45  Found 1000000 samples with 2 parameters and 29 observables\n",
      "03:45  Only using 1 of 29 observables\n",
      "03:45  Creating model for method sally\n",
      "03:45  Training model\n",
      "03:46    Epoch 2: train loss 17.46 ([17.46170746])\n",
      "03:47    Epoch 4: train loss 17.45 ([17.45390543])\n",
      "03:47    Epoch 6: train loss 17.45 ([17.44887938])\n",
      "03:48    Epoch 8: train loss 17.45 ([17.44596141])\n",
      "03:49    Epoch 10: train loss 17.44 ([17.44220913])\n",
      "03:49    Epoch 12: train loss 17.44 ([17.43906627])\n",
      "03:50    Epoch 14: train loss 17.44 ([17.43859459])\n",
      "03:51    Epoch 16: train loss 17.44 ([17.43705118])\n",
      "03:51    Epoch 18: train loss 17.44 ([17.43639376])\n",
      "03:52    Epoch 20: train loss 17.44 ([17.43535924])\n",
      "03:52  Finished training\n",
      "03:52  Training estimator 18 / 20 in ensemble\n",
      "03:52  Starting training\n",
      "03:52    Method:                 sally\n",
      "03:52    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_17.npy\n",
      "03:52                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_17.npy\n",
      "03:52    Features:               [0]\n",
      "03:52    Method:                 sally\n",
      "03:52    Hidden layers:          (100, 100)\n",
      "03:52    Activation function:    tanh\n",
      "03:52    Batch size:             128\n",
      "03:52    Epochs:                 20\n",
      "03:52    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:52    Validation split:       None\n",
      "03:52    Early stopping:         True\n",
      "03:52  Loading training data\n",
      "03:52  Found 1000000 samples with 2 parameters and 29 observables\n",
      "03:52  Only using 1 of 29 observables\n",
      "03:52  Creating model for method sally\n",
      "03:52  Training model\n",
      "03:53    Epoch 2: train loss 18.11 ([18.11047351])\n",
      "03:53    Epoch 4: train loss 18.11 ([18.10966332])\n",
      "03:54    Epoch 6: train loss 18.10 ([18.09986899])\n",
      "03:55    Epoch 8: train loss 18.10 ([18.0953984])\n",
      "03:55    Epoch 10: train loss 18.09 ([18.0927457])\n",
      "03:56    Epoch 12: train loss 18.09 ([18.09127525])\n",
      "03:57    Epoch 14: train loss 18.09 ([18.08937895])\n",
      "03:57    Epoch 16: train loss 18.09 ([18.08807185])\n",
      "03:58    Epoch 18: train loss 18.09 ([18.08708543])\n",
      "03:58    Epoch 20: train loss 18.09 ([18.08626901])\n",
      "03:58  Finished training\n",
      "03:58  Training estimator 19 / 20 in ensemble\n",
      "03:58  Starting training\n",
      "03:58    Method:                 sally\n",
      "03:58    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_18.npy\n",
      "03:58                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_18.npy\n",
      "03:58    Features:               [0]\n",
      "03:58    Method:                 sally\n",
      "03:58    Hidden layers:          (100, 100)\n",
      "03:58    Activation function:    tanh\n",
      "03:58    Batch size:             128\n",
      "03:58    Epochs:                 20\n",
      "03:58    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "03:58    Validation split:       None\n",
      "03:58    Early stopping:         True\n",
      "03:58  Loading training data\n",
      "03:58  Found 1000000 samples with 2 parameters and 29 observables\n",
      "03:58  Only using 1 of 29 observables\n",
      "03:58  Creating model for method sally\n",
      "03:58  Training model\n",
      "03:59    Epoch 2: train loss 44.31 ([44.31006154])\n",
      "04:00    Epoch 4: train loss 44.30 ([44.30166791])\n",
      "04:00    Epoch 6: train loss 44.29 ([44.2925957])\n",
      "04:01    Epoch 8: train loss 44.29 ([44.28938493])\n",
      "04:02    Epoch 10: train loss 44.29 ([44.28631306])\n",
      "04:02    Epoch 12: train loss 44.28 ([44.2818447])\n",
      "04:03    Epoch 14: train loss 44.28 ([44.28019582])\n",
      "04:04    Epoch 16: train loss 44.28 ([44.27811862])\n",
      "04:04    Epoch 18: train loss 44.28 ([44.27724756])\n",
      "04:05    Epoch 20: train loss 44.28 ([44.27654939])\n",
      "04:05  Finished training\n",
      "04:05  Training estimator 20 / 20 in ensemble\n",
      "04:05  Starting training\n",
      "04:05    Method:                 sally\n",
      "04:05    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_19.npy\n",
      "04:05                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_19.npy\n",
      "04:05    Features:               [0]\n",
      "04:05    Method:                 sally\n",
      "04:05    Hidden layers:          (100, 100)\n",
      "04:05    Activation function:    tanh\n",
      "04:05    Batch size:             128\n",
      "04:05    Epochs:                 20\n",
      "04:05    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "04:05    Validation split:       None\n",
      "04:05    Early stopping:         True\n",
      "04:05  Loading training data\n",
      "04:05  Found 1000000 samples with 2 parameters and 29 observables\n",
      "04:05  Only using 1 of 29 observables\n",
      "04:05  Creating model for method sally\n",
      "04:05  Training model\n",
      "04:06    Epoch 2: train loss 15.80 ([15.79823871])\n",
      "04:06    Epoch 4: train loss 15.79 ([15.79110857])\n",
      "04:07    Epoch 6: train loss 15.78 ([15.78442591])\n",
      "04:08    Epoch 8: train loss 15.78 ([15.78045291])\n",
      "04:08    Epoch 10: train loss 15.78 ([15.77806229])\n",
      "04:09    Epoch 12: train loss 15.78 ([15.77657866])\n",
      "04:10    Epoch 14: train loss 15.77 ([15.7740116])\n",
      "04:10    Epoch 16: train loss 15.77 ([15.77318167])\n",
      "04:11    Epoch 18: train loss 15.77 ([15.772741])\n",
      "04:12    Epoch 20: train loss 15.77 ([15.77231683])\n",
      "04:12  Finished training\n"
     ]
    }
   ],
   "source": [
    "ensemble_met = EnsembleForge(n_estimators)\n",
    "\n",
    "ensemble_met.train_all(\n",
    "    features=[ [0] for _ in range(n_estimators)],\n",
    "    method='sally',\n",
    "    x_filename=[sample_dir + 'train_local/x_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    t_xz0_filename=[sample_dir + 'train_local/t_xz_train_{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=None,\n",
    "    n_hidden=n_hidden\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:12  Calculating expectation for 20 estimators in ensemble\n",
      "04:12  Starting evaluation for estimator 1 / 20 in ensemble\n",
      "04:12  Loading evaluation data\n",
      "04:12  Starting score evaluation\n",
      "04:12  Starting evaluation for estimator 2 / 20 in ensemble\n",
      "04:12  Loading evaluation data\n",
      "04:12  Starting score evaluation\n",
      "04:12  Starting evaluation for estimator 3 / 20 in ensemble\n",
      "04:12  Loading evaluation data\n",
      "04:12  Starting score evaluation\n",
      "04:12  Starting evaluation for estimator 4 / 20 in ensemble\n",
      "04:12  Loading evaluation data\n",
      "04:12  Starting score evaluation\n",
      "04:12  Starting evaluation for estimator 5 / 20 in ensemble\n",
      "04:12  Loading evaluation data\n",
      "04:12  Starting score evaluation\n",
      "04:12  Starting evaluation for estimator 6 / 20 in ensemble\n",
      "04:12  Loading evaluation data\n",
      "04:12  Starting score evaluation\n",
      "04:13  Starting evaluation for estimator 7 / 20 in ensemble\n",
      "04:13  Loading evaluation data\n",
      "04:13  Starting score evaluation\n",
      "04:13  Starting evaluation for estimator 8 / 20 in ensemble\n",
      "04:13  Loading evaluation data\n",
      "04:13  Starting score evaluation\n",
      "04:13  Starting evaluation for estimator 9 / 20 in ensemble\n",
      "04:13  Loading evaluation data\n",
      "04:13  Starting score evaluation\n",
      "04:13  Starting evaluation for estimator 10 / 20 in ensemble\n",
      "04:13  Loading evaluation data\n",
      "04:13  Starting score evaluation\n",
      "04:13  Starting evaluation for estimator 11 / 20 in ensemble\n",
      "04:13  Loading evaluation data\n",
      "04:13  Starting score evaluation\n",
      "04:13  Starting evaluation for estimator 12 / 20 in ensemble\n",
      "04:13  Loading evaluation data\n",
      "04:13  Starting score evaluation\n",
      "04:13  Starting evaluation for estimator 13 / 20 in ensemble\n",
      "04:13  Loading evaluation data\n",
      "04:13  Starting score evaluation\n",
      "04:14  Starting evaluation for estimator 14 / 20 in ensemble\n",
      "04:14  Loading evaluation data\n",
      "04:14  Starting score evaluation\n",
      "04:14  Starting evaluation for estimator 15 / 20 in ensemble\n",
      "04:14  Loading evaluation data\n",
      "04:14  Starting score evaluation\n",
      "04:14  Starting evaluation for estimator 16 / 20 in ensemble\n",
      "04:14  Loading evaluation data\n",
      "04:14  Starting score evaluation\n",
      "04:14  Starting evaluation for estimator 17 / 20 in ensemble\n",
      "04:14  Loading evaluation data\n",
      "04:14  Starting score evaluation\n",
      "04:14  Starting evaluation for estimator 18 / 20 in ensemble\n",
      "04:14  Loading evaluation data\n",
      "04:14  Starting score evaluation\n",
      "04:14  Starting evaluation for estimator 19 / 20 in ensemble\n",
      "04:14  Loading evaluation data\n",
      "04:14  Starting score evaluation\n",
      "04:14  Starting evaluation for estimator 20 / 20 in ensemble\n",
      "04:14  Loading evaluation data\n",
      "04:14  Starting score evaluation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03217312,  0.04683511],\n",
       "       [ 0.00696393,  0.03543703],\n",
       "       [ 0.02575764,  0.01514627],\n",
       "       [-0.00298495,  0.04633479],\n",
       "       [ 0.07877649, -0.10351486],\n",
       "       [ 0.03489412, -0.04659313],\n",
       "       [-0.0472172 , -0.01366843],\n",
       "       [ 0.02494288, -0.11711625],\n",
       "       [ 0.02242893,  0.02772917],\n",
       "       [-0.0252324 , -0.03115297],\n",
       "       [-0.01470638, -0.0187749 ],\n",
       "       [ 0.0214741 , -0.02624243],\n",
       "       [ 0.01414475, -0.04351106],\n",
       "       [-0.04331799, -0.05038221],\n",
       "       [ 0.00418201, -0.02652313],\n",
       "       [ 0.06079252,  0.00274209],\n",
       "       [-0.01953946, -0.02757395],\n",
       "       [-0.07849615,  0.00724561],\n",
       "       [ 0.0044371 , -0.00227884],\n",
       "       [ 0.04675819,  0.01130327]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_met.calculate_expectation(\n",
    "    x_filename=sample_dir + 'validation/x_validation.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:15  Saving ensemble setup to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/ensemble.json\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_0_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_0_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_1_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_1_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_2_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_2_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_3_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_3_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_4_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_4_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_5_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_5_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_6_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_6_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_7_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_7_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_8_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_8_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_9_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_9_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_10_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_10_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_11_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_11_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_12_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_12_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_13_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_13_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_14_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_14_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_15_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_15_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_16_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_16_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_17_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_17_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_18_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_18_state_dict.pt\n",
      "04:15  Saving settings to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_19_settings.json\n",
      "04:15  Saving state dictionary to /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_met/estimator_19_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "ensemble_met.save(model_dir + 'sally_ensemble_met')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
