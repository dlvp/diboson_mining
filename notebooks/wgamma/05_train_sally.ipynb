{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=10, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=True)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.calculate_expectation(\n",
    "        x_filename=sample_dir + 'validation{}/x_validation.npy'.format(cut_label)\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:33  \n",
      "15:33  ------------------------------------------------------------\n",
      "15:33  |                                                          |\n",
      "15:33  |  MadMiner v2018.11.02                                    |\n",
      "15:33  |                                                          |\n",
      "15:33  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "15:33  |                                                          |\n",
      "15:33  ------------------------------------------------------------\n",
      "15:33  \n",
      "15:33  Training 10 estimators in ensemble\n",
      "15:33  Training estimator 1 / 10 in ensemble\n",
      "15:33  Starting training\n",
      "15:33    Method:                 sally\n",
      "15:33    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "15:33                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "15:33    Features:               all\n",
      "15:33    Method:                 sally\n",
      "15:33    Hidden layers:          (100, 100)\n",
      "15:33    Activation function:    tanh\n",
      "15:33    Batch size:             128\n",
      "15:33    Trainer:                amsgrad\n",
      "15:33    Epochs:                 50\n",
      "15:33    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:33    Validation split:       0.25\n",
      "15:33    Early stopping:         True\n",
      "15:33    Scale inputs:           True\n",
      "15:33    Regularization:         0.0 * |grad_x f(x)|^2\n",
      "15:33  Loading training data\n",
      "15:33  Found 1000000 samples with 2 parameters and 26 observables\n",
      "15:33  Rescaling inputs\n",
      "15:33  Observable ranges:\n",
      "15:33    x_1: mean -1.1787903986260062e-16, std 1.0000000000000113, range -1.2673566320840908 ... 54.865785821377585\n",
      "15:33    x_2: mean -2.0229151687090053e-17, std 1.000000000000013, range -1.7209861046510997 ... 1.7379548253063157\n",
      "15:33    x_3: mean -2.3775022839345183e-14, std 0.99999999999998, range -0.6607179631140454 ... 23.642024385525144\n",
      "15:33    x_4: mean -5.975664407742442e-18, std 0.9999999999999887, range -5.236215967365085 ... 5.778553575255393\n",
      "15:33    x_5: mean 3.3580249692022336e-17, std 1.0000000000000056, range -0.8171567002220321 ... 60.57111999567393\n",
      "15:33    x_6: mean -1.7061803703199984e-13, std 1.0000000000000409, range -0.7658709684655869 ... 31.58579096663225\n",
      "15:33    x_7: mean -1.837641150359559e-17, std 0.9999999999999845, range -2.0331702740701507 ... 2.0312482746036133\n",
      "15:33    x_8: mean -2.9121594025127704e-17, std 0.9999999999999715, range -1.7374598206681022 ... 1.7298168474095483\n",
      "15:33    x_9: mean 5.74438274725253e-17, std 1.000000000000024, range -1.2008108736681227 ... 70.27551614530168\n",
      "15:33    x_10: mean 5.772449185315053e-17, std 1.0000000000000258, range -0.9265561475658836 ... 44.99519749770933\n",
      "15:33    x_11: mean -7.711165039836488e-18, std 0.9999999999999462, range -1.9543595893791266 ... 1.9613330083731058\n",
      "15:33    x_12: mean -6.622258297284134e-18, std 0.9999999999999913, range -1.734757913447277 ... 1.731279911297759\n",
      "15:33    x_13: mean 4.880718051936128e-17, std 0.9999999999908175, range -0.432492350613682 ... 28.2023394143323\n",
      "15:33    x_14: mean -7.25515292288037e-15, std 0.9999999999929876, range -0.2648142032057478 ... 31.978017590069463\n",
      "15:33    x_15: mean 6.465583624049032e-17, std 1.0000000000024003, range -6.042747012048575 ... 6.153189194530111\n",
      "15:33    x_16: mean -2.844302571247681e-17, std 1.0000000000074225, range -3.9006680158369753 ... 3.907084244636895\n",
      "15:33    x_17: mean 1.2505552149377763e-18, std 0.9999999999998884, range -2.439224454527632 ... 2.4234474771697756\n",
      "15:33    x_18: mean 4.444444812179427e-17, std 1.0000000000000198, range -2.0659151747278504 ... 2.053230187888959\n",
      "15:33    x_19: mean -1.5373048256606126e-13, std 0.9999999999999891, range -1.574134084699959 ... 47.64770615292593\n",
      "15:33    x_20: mean 8.228420256273239e-14, std 1.0000000000000442, range -1.238201155678915 ... 55.42465953095185\n",
      "15:33    x_21: mean -3.7096775429290577e-14, std 0.9999999999999996, range -0.9627604520622827 ... 41.90796022623812\n",
      "15:33    x_22: mean -1.1622702800195839e-17, std 0.9999999999999969, range -3.771354321338035 ... 3.890052351256793\n",
      "15:33    x_23: mean 8.952838470577262e-19, std 1.0000000000000075, range -2.3523208340180104 ... 2.341460180099849\n",
      "15:33    x_24: mean -1.6594981389062014e-13, std 0.9999999999999591, range -1.3704650202703654 ... 42.39449703186085\n",
      "15:33    x_25: mean -7.186681472148848e-14, std 0.9999999999999769, range -0.9782892615300571 ... 49.631385581516696\n",
      "15:33    x_26: mean -1.8337598106654696e-16, std 0.9999999999981152, range -1.9366801048308915 ... 1.9463183087350397\n",
      "15:33  Creating model for method sally\n",
      "15:33  Training model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0a0cd15f914c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tight_cuts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_x_regularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-593b8f064c8a>\u001b[0m in \u001b[0;36mtrain_ensemble\u001b[0;34m(filename, use_tight_cuts, n_estimators, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mx_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train_local{}/x_train_{}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mt_xz0_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train_local{}/t_xz_train_{}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/projects/madminer/madminer/madminer/ml.py\u001b[0m in \u001b[0;36mtrain_all\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training estimator %s / %s in ensemble\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs_this_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta0_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/projects/madminer/madminer/madminer/ml.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, method, x_filename, y_filename, theta0_filename, theta1_filename, r_xz_filename, t_xz0_filename, t_xz1_filename, features, nde_type, n_hidden, activation, maf_n_mades, maf_batch_norm, maf_batch_norm_alpha, maf_mog_n_components, alpha, trainer, n_epochs, batch_size, initial_lr, final_lr, nesterov_momentum, validation_split, early_stopping, scale_inputs, grad_x_regularization)\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0mnesterov_momentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnesterov_momentum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"all\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"some\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                 \u001b[0mgrad_x_regularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_x_regularization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"nde\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scandal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/projects/madminer/madminer/madminer/utils/ml/score_trainer.py\u001b[0m in \u001b[0;36mtrain_local_score_model\u001b[0;34m(model, loss_functions, xs, t_xzs, loss_weights, loss_labels, batch_size, trainer, initial_learning_rate, final_learning_rate, nesterov_momentum, n_epochs, clip_gradient, run_on_gpu, double_precision, validation_split, early_stopping, early_stopping_patience, grad_x_regularization, learning_curve_folder, learning_curve_filename, verbose)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Convert to Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mt_xzs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt_xzs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/projects/madminer/madminer/madminer/utils/ml/score_trainer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Convert to Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mt_xzs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt_xzs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_ensemble('all', use_tight_cuts=False, grad_x_regularization=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble('all_tight', use_tight_cuts=True, grad_x_regularization=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ensemble('all_reg', use_tight_cuts=False, grad_x_regularization=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7d171470-6c7e-4a74-8ca8-5589004e32d6"
    }
   },
   "outputs": [],
   "source": [
    "train_ensemble('all_tight_reg', use_tight_cuts=True, grad_x_regularization=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal observable basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:35  Training 10 estimators in ensemble\n",
      "15:35  Training estimator 1 / 10 in ensemble\n",
      "15:35  Starting training\n",
      "15:35    Method:                 sally\n",
      "15:35    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "15:35                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "15:35    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "15:35    Method:                 sally\n",
      "15:35    Hidden layers:          (100, 100)\n",
      "15:35    Activation function:    tanh\n",
      "15:35    Batch size:             128\n",
      "15:35    Trainer:                amsgrad\n",
      "15:35    Epochs:                 50\n",
      "15:35    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:35    Validation split:       0.25\n",
      "15:35    Early stopping:         True\n",
      "15:35    Scale inputs:           True\n",
      "15:35    Regularization:         0.0 * |grad_x f(x)|^2\n",
      "15:35  Loading training data\n",
      "15:35  Found 1000000 samples with 2 parameters and 26 observables\n",
      "15:35  Rescaling inputs\n",
      "15:35  Observable ranges:\n",
      "15:35    x_1: mean -1.1787903986260062e-16, std 1.0000000000000113, range -1.2673566320840908 ... 54.865785821377585\n",
      "15:35    x_2: mean -2.0229151687090053e-17, std 1.000000000000013, range -1.7209861046510997 ... 1.7379548253063157\n",
      "15:35    x_3: mean -2.3775022839345183e-14, std 0.99999999999998, range -0.6607179631140454 ... 23.642024385525144\n",
      "15:35    x_4: mean -5.975664407742442e-18, std 0.9999999999999887, range -5.236215967365085 ... 5.778553575255393\n",
      "15:35    x_5: mean 3.3580249692022336e-17, std 1.0000000000000056, range -0.8171567002220321 ... 60.57111999567393\n",
      "15:35    x_6: mean -1.7061803703199984e-13, std 1.0000000000000409, range -0.7658709684655869 ... 31.58579096663225\n",
      "15:35    x_7: mean -1.837641150359559e-17, std 0.9999999999999845, range -2.0331702740701507 ... 2.0312482746036133\n",
      "15:35    x_8: mean -2.9121594025127704e-17, std 0.9999999999999715, range -1.7374598206681022 ... 1.7298168474095483\n",
      "15:35    x_9: mean 5.74438274725253e-17, std 1.000000000000024, range -1.2008108736681227 ... 70.27551614530168\n",
      "15:35    x_10: mean 5.772449185315053e-17, std 1.0000000000000258, range -0.9265561475658836 ... 44.99519749770933\n",
      "15:35    x_11: mean -7.711165039836488e-18, std 0.9999999999999462, range -1.9543595893791266 ... 1.9613330083731058\n",
      "15:35    x_12: mean -6.622258297284134e-18, std 0.9999999999999913, range -1.734757913447277 ... 1.731279911297759\n",
      "15:35    x_13: mean 4.880718051936128e-17, std 0.9999999999908175, range -0.432492350613682 ... 28.2023394143323\n",
      "15:35    x_14: mean -7.25515292288037e-15, std 0.9999999999929876, range -0.2648142032057478 ... 31.978017590069463\n",
      "15:35    x_15: mean 6.465583624049032e-17, std 1.0000000000024003, range -6.042747012048575 ... 6.153189194530111\n",
      "15:35    x_16: mean -2.844302571247681e-17, std 1.0000000000074225, range -3.9006680158369753 ... 3.907084244636895\n",
      "15:35    x_17: mean 1.2505552149377763e-18, std 0.9999999999998884, range -2.439224454527632 ... 2.4234474771697756\n",
      "15:35    x_18: mean 4.444444812179427e-17, std 1.0000000000000198, range -2.0659151747278504 ... 2.053230187888959\n",
      "15:35    x_19: mean -1.5373048256606126e-13, std 0.9999999999999891, range -1.574134084699959 ... 47.64770615292593\n",
      "15:35    x_20: mean 8.228420256273239e-14, std 1.0000000000000442, range -1.238201155678915 ... 55.42465953095185\n",
      "15:35    x_21: mean -3.7096775429290577e-14, std 0.9999999999999996, range -0.9627604520622827 ... 41.90796022623812\n",
      "15:35    x_22: mean -1.1622702800195839e-17, std 0.9999999999999969, range -3.771354321338035 ... 3.890052351256793\n",
      "15:35    x_23: mean 8.952838470577262e-19, std 1.0000000000000075, range -2.3523208340180104 ... 2.341460180099849\n",
      "15:35    x_24: mean -1.6594981389062014e-13, std 0.9999999999999591, range -1.3704650202703654 ... 42.39449703186085\n",
      "15:35    x_25: mean -7.186681472148848e-14, std 0.9999999999999769, range -0.9782892615300571 ... 49.631385581516696\n",
      "15:35    x_26: mean -1.8337598106654696e-16, std 0.9999999999981152, range -1.9366801048308915 ... 1.9463183087350397\n",
      "15:35  Only using 7 of 26 observables\n",
      "15:35  Creating model for method sally\n",
      "15:35  Training model\n",
      "15:35    Epoch 1: train loss 22.9247 (mse_score: 22.9247, l2_grad_x: 0.1196)\n",
      "            val. loss  36.8703 (mse_score: 36.8703, l2_grad_x: 0.0000) (*)\n",
      "15:36    Epoch 2: train loss 22.9027 (mse_score: 22.9027, l2_grad_x: 0.3462)\n",
      "            val. loss  36.8698 (mse_score: 36.8698, l2_grad_x: 0.0000) (*)\n",
      "15:36    Epoch 3: train loss 22.8902 (mse_score: 22.8902, l2_grad_x: 0.4072)\n",
      "            val. loss  36.8883 (mse_score: 36.8883, l2_grad_x: 0.0000)\n",
      "15:37    Epoch 4: train loss 22.8824 (mse_score: 22.8824, l2_grad_x: 0.6401)\n",
      "            val. loss  36.8933 (mse_score: 36.8933, l2_grad_x: 0.0000)\n",
      "15:37    Epoch 5: train loss 22.8644 (mse_score: 22.8644, l2_grad_x: 0.7257)\n",
      "            val. loss  36.8801 (mse_score: 36.8801, l2_grad_x: 0.0000)\n",
      "15:38    Epoch 6: train loss 22.8530 (mse_score: 22.8530, l2_grad_x: 0.7393)\n",
      "            val. loss  36.8886 (mse_score: 36.8886, l2_grad_x: 0.0000)\n",
      "15:38    Epoch 7: train loss 22.8331 (mse_score: 22.8331, l2_grad_x: 0.8680)\n",
      "            val. loss  36.9122 (mse_score: 36.9122, l2_grad_x: 0.0000)\n",
      "15:39    Epoch 8: train loss 22.8075 (mse_score: 22.8075, l2_grad_x: 1.0416)\n",
      "            val. loss  36.8901 (mse_score: 36.8901, l2_grad_x: 0.0000)\n",
      "15:39    Epoch 9: train loss 22.7957 (mse_score: 22.7957, l2_grad_x: 1.1912)\n",
      "            val. loss  36.9009 (mse_score: 36.9009, l2_grad_x: 0.0000)\n",
      "15:40    Epoch 10: train loss 22.7650 (mse_score: 22.7650, l2_grad_x: 1.2907)\n",
      "            val. loss  36.9399 (mse_score: 36.9399, l2_grad_x: 0.0000)\n",
      "15:40    Epoch 11: train loss 22.7411 (mse_score: 22.7411, l2_grad_x: 1.4291)\n",
      "            val. loss  36.9779 (mse_score: 36.9779, l2_grad_x: 0.0000)\n",
      "15:41    Epoch 12: train loss 22.7151 (mse_score: 22.7151, l2_grad_x: 1.8616)\n",
      "            val. loss  36.9477 (mse_score: 36.9477, l2_grad_x: 0.0000)\n",
      "15:42    Epoch 13: train loss 22.6980 (mse_score: 22.6980, l2_grad_x: 1.9348)\n",
      "            val. loss  36.9863 (mse_score: 36.9863, l2_grad_x: 0.0000)\n",
      "15:42    Epoch 14: train loss 22.6555 (mse_score: 22.6555, l2_grad_x: 2.1707)\n",
      "            val. loss  36.9451 (mse_score: 36.9451, l2_grad_x: 0.0000)\n",
      "15:43    Epoch 15: train loss 22.6314 (mse_score: 22.6314, l2_grad_x: 2.2643)\n",
      "            val. loss  36.9706 (mse_score: 36.9706, l2_grad_x: 0.0000)\n",
      "15:43    Epoch 16: train loss 22.6008 (mse_score: 22.6008, l2_grad_x: 2.2766)\n",
      "            val. loss  37.0603 (mse_score: 37.0603, l2_grad_x: 0.0000)\n",
      "15:44    Epoch 17: train loss 22.5770 (mse_score: 22.5770, l2_grad_x: 2.7002)\n",
      "            val. loss  37.0019 (mse_score: 37.0019, l2_grad_x: 0.0000)\n",
      "15:44    Epoch 18: train loss 22.5535 (mse_score: 22.5535, l2_grad_x: 2.8863)\n",
      "            val. loss  36.9901 (mse_score: 36.9901, l2_grad_x: 0.0000)\n",
      "15:45    Epoch 19: train loss 22.5155 (mse_score: 22.5155, l2_grad_x: 3.0116)\n",
      "            val. loss  37.0272 (mse_score: 37.0272, l2_grad_x: 0.0000)\n",
      "15:45    Epoch 20: train loss 22.5035 (mse_score: 22.5035, l2_grad_x: 3.2350)\n",
      "            val. loss  37.0347 (mse_score: 37.0347, l2_grad_x: 0.0000)\n",
      "15:46    Epoch 21: train loss 22.4562 (mse_score: 22.4562, l2_grad_x: 3.3806)\n",
      "            val. loss  37.0472 (mse_score: 37.0472, l2_grad_x: 0.0000)\n",
      "15:46    Epoch 22: train loss 22.4230 (mse_score: 22.4230, l2_grad_x: 3.4590)\n",
      "            val. loss  37.0399 (mse_score: 37.0399, l2_grad_x: 0.0000)\n",
      "15:46  No improvement for 20 epochs, stopping training\n",
      "15:46  Early stopping after epoch 2, with loss 36.87 compared to final loss 37.04\n",
      "15:46  Finished training\n",
      "15:46  Training estimator 2 / 10 in ensemble\n",
      "15:46  Starting training\n",
      "15:46    Method:                 sally\n",
      "15:46    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "15:46                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "15:46    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "15:46    Method:                 sally\n",
      "15:46    Hidden layers:          (100, 100)\n",
      "15:46    Activation function:    tanh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:46    Batch size:             128\n",
      "15:46    Trainer:                amsgrad\n",
      "15:46    Epochs:                 50\n",
      "15:46    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:46    Validation split:       0.25\n",
      "15:46    Early stopping:         True\n",
      "15:46    Scale inputs:           True\n",
      "15:46    Regularization:         0.0 * |grad_x f(x)|^2\n",
      "15:46  Loading training data\n",
      "15:46  Found 1000000 samples with 2 parameters and 26 observables\n",
      "15:46  Rescaling inputs\n",
      "15:46  Observable ranges:\n",
      "15:46    x_1: mean 1.2072121080564103e-16, std 1.0000000000000426, range -1.2696488812250217 ... 47.764449103988355\n",
      "15:46    x_2: mean 3.385025593161117e-17, std 1.0000000000000195, range -1.724847206027408 ... 1.7370336522745242\n",
      "15:46    x_3: mean -1.0077410905751095e-14, std 0.9999999999999807, range -0.6623534385666117 ... 20.19121695021946\n",
      "15:46    x_4: mean 1.0373923942097463e-18, std 1.0000000000000493, range -5.237725345301356 ... 5.777473918519132\n",
      "15:46    x_5: mean -1.963940121640917e-17, std 1.000000000000034, range -0.824848910405002 ... 43.604735086732894\n",
      "15:46    x_6: mean -1.0545602435740875e-13, std 0.9999999999999932, range -0.770332419991783 ... 31.7216314738656\n",
      "15:46    x_7: mean 5.400124791776762e-19, std 0.9999999999999728, range -2.0342017431164443 ... 2.0307701157173987\n",
      "15:46    x_8: mean -4.170708223227848e-17, std 0.999999999999945, range -1.7357271766753677 ... 1.7302997254053059\n",
      "15:46    x_9: mean -1.282849382278073e-16, std 1.0000000000000202, range -1.2077898684924921 ... 46.72705102841355\n",
      "15:46    x_10: mean -4.0927261579781773e-17, std 1.0000000000000069, range -0.9294871654893843 ... 38.46884750188855\n",
      "15:46    x_11: mean 1.7834622667578515e-17, std 1.0000000000000369, range -1.9542510380999039 ... 1.9600497645260977\n",
      "15:46    x_12: mean 2.212630079156952e-17, std 0.9999999999999656, range -1.7309689653829374 ... 1.7305224053857557\n",
      "15:46    x_13: mean 3.9804604057280814e-17, std 1.000000000002776, range -0.43343195465760387 ... 31.379537769571332\n",
      "15:46    x_14: mean -9.759030028533289e-15, std 0.9999999999980386, range -0.26661403008199885 ... 32.038039214851665\n",
      "15:46    x_15: mean 2.7931434942729535e-17, std 1.000000000001272, range -6.028530336404456 ... 6.024220710806009\n",
      "15:46    x_16: mean -9.130474154517287e-18, std 1.0000000000021376, range -3.893346256463077 ... 3.900043866980764\n",
      "15:46    x_17: mean 2.0407675549449778e-17, std 0.9999999999999724, range -2.4398507717628855 ... 2.416061562238581\n",
      "15:46    x_18: mean -3.739053511253587e-17, std 1.0000000000000202, range -2.063426214319083 ... 2.0553536022050682\n",
      "15:46    x_19: mean -2.540003407602853e-13, std 1.0000000000000169, range -1.5746321022684138 ... 43.71003777344256\n",
      "15:46    x_20: mean 1.3994475267509188e-13, std 1.0000000000000102, range -1.2463778150974487 ... 36.66430185270105\n",
      "15:46    x_21: mean -6.386242823452904e-14, std 0.9999999999999188, range -0.9695971174157503 ... 35.03809878179895\n",
      "15:46    x_22: mean -1.735767085619955e-17, std 1.0000000000000115, range -3.7644449425975854 ... 3.8826331582454388\n",
      "15:46    x_23: mean -1.2089884648958105e-17, std 1.0000000000000109, range -2.3480251436991826 ... 2.336658270477298\n",
      "15:46    x_24: mean -1.9600701861577362e-13, std 0.9999999999999856, range -1.3799867618264159 ... 34.59144869565067\n",
      "15:46    x_25: mean -8.640170179319284e-14, std 0.9999999999999509, range -0.9799189060397523 ... 49.887574583972\n",
      "15:46    x_26: mean 5.160281091320939e-16, std 0.9999999999993568, range -1.9350890019679785 ... 1.9471514126974876\n",
      "15:46  Only using 7 of 26 observables\n",
      "15:46  Creating model for method sally\n",
      "15:46  Training model\n",
      "15:47    Epoch 1: train loss 78.6140 (mse_score: 78.6140, l2_grad_x: 0.0468)\n",
      "            val. loss  25.0258 (mse_score: 25.0258, l2_grad_x: 0.0000) (*)\n",
      "15:48    Epoch 2: train loss 78.5949 (mse_score: 78.5949, l2_grad_x: 0.0787)\n",
      "            val. loss  25.0026 (mse_score: 25.0026, l2_grad_x: 0.0000) (*)\n",
      "15:48    Epoch 3: train loss 78.5507 (mse_score: 78.5507, l2_grad_x: 0.1974)\n",
      "            val. loss  24.9966 (mse_score: 24.9966, l2_grad_x: 0.0000) (*)\n",
      "15:49    Epoch 4: train loss 78.5125 (mse_score: 78.5125, l2_grad_x: 0.2701)\n",
      "            val. loss  24.9861 (mse_score: 24.9861, l2_grad_x: 0.0000) (*)\n",
      "15:49    Epoch 5: train loss 78.4738 (mse_score: 78.4738, l2_grad_x: 0.4013)\n",
      "            val. loss  25.0113 (mse_score: 25.0113, l2_grad_x: 0.0000)\n",
      "15:50    Epoch 6: train loss 78.4351 (mse_score: 78.4351, l2_grad_x: 0.4604)\n",
      "            val. loss  25.0562 (mse_score: 25.0562, l2_grad_x: 0.0000)\n",
      "15:50    Epoch 7: train loss 78.4444 (mse_score: 78.4444, l2_grad_x: 0.5313)\n",
      "            val. loss  24.9347 (mse_score: 24.9347, l2_grad_x: 0.0000) (*)\n",
      "15:51    Epoch 8: train loss 78.3824 (mse_score: 78.3824, l2_grad_x: 0.6340)\n",
      "            val. loss  24.9063 (mse_score: 24.9063, l2_grad_x: 0.0000) (*)\n",
      "15:51    Epoch 9: train loss 78.2859 (mse_score: 78.2859, l2_grad_x: 0.6508)\n",
      "            val. loss  24.9189 (mse_score: 24.9189, l2_grad_x: 0.0000)\n",
      "15:52    Epoch 10: train loss 78.2288 (mse_score: 78.2288, l2_grad_x: 0.7604)\n",
      "            val. loss  24.8618 (mse_score: 24.8618, l2_grad_x: 0.0000) (*)\n",
      "15:53    Epoch 11: train loss 78.1960 (mse_score: 78.1960, l2_grad_x: 0.8501)\n",
      "            val. loss  24.8895 (mse_score: 24.8895, l2_grad_x: 0.0000)\n",
      "15:53    Epoch 12: train loss 78.1281 (mse_score: 78.1281, l2_grad_x: 0.8701)\n",
      "            val. loss  24.9346 (mse_score: 24.9346, l2_grad_x: 0.0000)\n",
      "15:54    Epoch 13: train loss 78.0834 (mse_score: 78.0834, l2_grad_x: 0.9060)\n",
      "            val. loss  24.8442 (mse_score: 24.8442, l2_grad_x: 0.0000) (*)\n",
      "15:54    Epoch 14: train loss 78.0483 (mse_score: 78.0483, l2_grad_x: 1.0859)\n",
      "            val. loss  24.8639 (mse_score: 24.8639, l2_grad_x: 0.0000)\n",
      "15:55    Epoch 15: train loss 77.9925 (mse_score: 77.9925, l2_grad_x: 1.0933)\n",
      "            val. loss  24.8124 (mse_score: 24.8124, l2_grad_x: 0.0000) (*)\n",
      "15:55    Epoch 16: train loss 77.9381 (mse_score: 77.9381, l2_grad_x: 1.1530)\n",
      "            val. loss  24.8263 (mse_score: 24.8263, l2_grad_x: 0.0000)\n",
      "15:56    Epoch 17: train loss 77.8939 (mse_score: 77.8939, l2_grad_x: 1.2800)\n",
      "            val. loss  24.7939 (mse_score: 24.7939, l2_grad_x: 0.0000) (*)\n",
      "15:56    Epoch 18: train loss 77.9145 (mse_score: 77.9145, l2_grad_x: 1.3071)\n",
      "            val. loss  24.7904 (mse_score: 24.7904, l2_grad_x: 0.0000) (*)\n",
      "15:57    Epoch 19: train loss 77.7790 (mse_score: 77.7790, l2_grad_x: 1.3259)\n",
      "            val. loss  24.7728 (mse_score: 24.7728, l2_grad_x: 0.0000) (*)\n",
      "15:57    Epoch 20: train loss 77.7373 (mse_score: 77.7373, l2_grad_x: 1.4215)\n",
      "            val. loss  24.7545 (mse_score: 24.7545, l2_grad_x: 0.0000) (*)\n",
      "15:58    Epoch 21: train loss 77.6744 (mse_score: 77.6744, l2_grad_x: 1.5160)\n",
      "            val. loss  24.8041 (mse_score: 24.8041, l2_grad_x: 0.0000)\n",
      "15:59    Epoch 22: train loss 77.6495 (mse_score: 77.6495, l2_grad_x: 1.5707)\n",
      "            val. loss  24.7478 (mse_score: 24.7478, l2_grad_x: 0.0000) (*)\n",
      "15:59    Epoch 23: train loss 77.6071 (mse_score: 77.6071, l2_grad_x: 1.6349)\n",
      "            val. loss  24.7331 (mse_score: 24.7331, l2_grad_x: 0.0000) (*)\n",
      "16:00    Epoch 24: train loss 77.5638 (mse_score: 77.5638, l2_grad_x: 1.7670)\n",
      "            val. loss  24.7313 (mse_score: 24.7313, l2_grad_x: 0.0000) (*)\n",
      "16:00    Epoch 25: train loss 77.5245 (mse_score: 77.5245, l2_grad_x: 1.7942)\n",
      "            val. loss  24.7192 (mse_score: 24.7192, l2_grad_x: 0.0000) (*)\n",
      "16:01    Epoch 26: train loss 77.4781 (mse_score: 77.4781, l2_grad_x: 1.8248)\n",
      "            val. loss  24.9627 (mse_score: 24.9627, l2_grad_x: 0.0000)\n",
      "16:02    Epoch 27: train loss 77.4415 (mse_score: 77.4415, l2_grad_x: 1.9456)\n",
      "            val. loss  24.7084 (mse_score: 24.7084, l2_grad_x: 0.0000) (*)\n",
      "16:02    Epoch 28: train loss 77.4121 (mse_score: 77.4121, l2_grad_x: 1.9536)\n",
      "            val. loss  24.7095 (mse_score: 24.7095, l2_grad_x: 0.0000)\n",
      "16:03    Epoch 29: train loss 77.3757 (mse_score: 77.3757, l2_grad_x: 2.0777)\n",
      "            val. loss  24.6880 (mse_score: 24.6880, l2_grad_x: 0.0000) (*)\n",
      "16:03    Epoch 30: train loss 77.3507 (mse_score: 77.3507, l2_grad_x: 2.0640)\n",
      "            val. loss  24.7038 (mse_score: 24.7038, l2_grad_x: 0.0000)\n",
      "16:04    Epoch 31: train loss 77.3175 (mse_score: 77.3175, l2_grad_x: 2.1978)\n",
      "            val. loss  24.6916 (mse_score: 24.6916, l2_grad_x: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:04    Epoch 32: train loss 77.2839 (mse_score: 77.2839, l2_grad_x: 2.1981)\n",
      "            val. loss  24.6806 (mse_score: 24.6806, l2_grad_x: 0.0000) (*)\n",
      "16:05    Epoch 33: train loss 77.2587 (mse_score: 77.2587, l2_grad_x: 2.2737)\n",
      "            val. loss  24.6693 (mse_score: 24.6693, l2_grad_x: 0.0000) (*)\n",
      "16:05    Epoch 34: train loss 77.2279 (mse_score: 77.2279, l2_grad_x: 2.3399)\n",
      "            val. loss  24.6704 (mse_score: 24.6704, l2_grad_x: 0.0000)\n",
      "16:06    Epoch 35: train loss 77.1943 (mse_score: 77.1943, l2_grad_x: 2.3411)\n",
      "            val. loss  24.6687 (mse_score: 24.6687, l2_grad_x: 0.0000) (*)\n",
      "16:07    Epoch 36: train loss 77.1764 (mse_score: 77.1764, l2_grad_x: 2.4068)\n",
      "            val. loss  24.6706 (mse_score: 24.6706, l2_grad_x: 0.0000)\n",
      "16:07    Epoch 37: train loss 77.1481 (mse_score: 77.1481, l2_grad_x: 2.4525)\n",
      "            val. loss  24.6546 (mse_score: 24.6546, l2_grad_x: 0.0000) (*)\n",
      "16:08    Epoch 38: train loss 77.1198 (mse_score: 77.1198, l2_grad_x: 2.4683)\n",
      "            val. loss  24.6623 (mse_score: 24.6623, l2_grad_x: 0.0000)\n",
      "16:08    Epoch 39: train loss 77.0988 (mse_score: 77.0988, l2_grad_x: 2.5352)\n",
      "            val. loss  24.6532 (mse_score: 24.6532, l2_grad_x: 0.0000) (*)\n",
      "16:09    Epoch 40: train loss 77.0650 (mse_score: 77.0650, l2_grad_x: 2.5576)\n",
      "            val. loss  24.6507 (mse_score: 24.6507, l2_grad_x: 0.0000) (*)\n",
      "16:10    Epoch 41: train loss 77.0531 (mse_score: 77.0531, l2_grad_x: 2.6234)\n",
      "            val. loss  24.6610 (mse_score: 24.6610, l2_grad_x: 0.0000)\n",
      "16:10    Epoch 42: train loss 77.0284 (mse_score: 77.0284, l2_grad_x: 2.6276)\n",
      "            val. loss  24.6501 (mse_score: 24.6501, l2_grad_x: 0.0000) (*)\n",
      "16:11    Epoch 43: train loss 77.0088 (mse_score: 77.0088, l2_grad_x: 2.6910)\n",
      "            val. loss  24.6415 (mse_score: 24.6415, l2_grad_x: 0.0000) (*)\n",
      "16:11    Epoch 44: train loss 76.9906 (mse_score: 76.9906, l2_grad_x: 2.7269)\n",
      "            val. loss  24.6489 (mse_score: 24.6489, l2_grad_x: 0.0000)\n",
      "16:12    Epoch 45: train loss 76.9704 (mse_score: 76.9704, l2_grad_x: 2.7411)\n",
      "            val. loss  24.6396 (mse_score: 24.6396, l2_grad_x: 0.0000) (*)\n",
      "16:12    Epoch 46: train loss 76.9514 (mse_score: 76.9514, l2_grad_x: 2.8060)\n",
      "            val. loss  24.6616 (mse_score: 24.6616, l2_grad_x: 0.0000)\n",
      "16:13    Epoch 47: train loss 76.9339 (mse_score: 76.9339, l2_grad_x: 2.7909)\n",
      "            val. loss  24.6404 (mse_score: 24.6404, l2_grad_x: 0.0000)\n",
      "16:13    Epoch 48: train loss 76.9328 (mse_score: 76.9328, l2_grad_x: 2.8456)\n",
      "            val. loss  24.6433 (mse_score: 24.6433, l2_grad_x: 0.0000)\n",
      "16:14    Epoch 49: train loss 76.9040 (mse_score: 76.9040, l2_grad_x: 2.8824)\n",
      "            val. loss  24.6434 (mse_score: 24.6434, l2_grad_x: 0.0000)\n",
      "16:14    Epoch 50: train loss 76.8929 (mse_score: 76.8929, l2_grad_x: 2.9456)\n",
      "            val. loss  24.6272 (mse_score: 24.6272, l2_grad_x: 0.0000) (*)\n",
      "16:14  Early stopping did not improve performance\n",
      "16:14  Finished training\n",
      "16:14  Training estimator 3 / 10 in ensemble\n",
      "16:14  Starting training\n",
      "16:14    Method:                 sally\n",
      "16:14    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "16:14                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "16:14    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "16:14    Method:                 sally\n",
      "16:14    Hidden layers:          (100, 100)\n",
      "16:14    Activation function:    tanh\n",
      "16:14    Batch size:             128\n",
      "16:14    Trainer:                amsgrad\n",
      "16:14    Epochs:                 50\n",
      "16:14    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:14    Validation split:       0.25\n",
      "16:14    Early stopping:         True\n",
      "16:14    Scale inputs:           True\n",
      "16:14    Regularization:         0.0 * |grad_x f(x)|^2\n",
      "16:14  Loading training data\n",
      "16:14  Found 1000000 samples with 2 parameters and 26 observables\n",
      "16:14  Rescaling inputs\n",
      "16:14  Observable ranges:\n",
      "16:14    x_1: mean -8.745359991735313e-17, std 0.999999999999928, range -1.2645241374845888 ... 62.15923716147685\n",
      "16:14    x_2: mean -2.211208993685432e-17, std 0.9999999999999636, range -1.7235060704255298 ... 1.7375978640382093\n",
      "16:14    x_3: mean -1.8484296759879727e-14, std 1.0000000000000184, range -0.6655850024272274 ... 28.497147308065337\n",
      "16:14    x_4: mean -8.461320533115213e-17, std 1.0000000000000782, range -5.2391095950705004 ... 5.774859705744465\n",
      "16:14    x_5: mean -8.44266878630151e-17, std 1.000000000000065, range -0.8261498022672548 ... 44.838489356767376\n",
      "16:14    x_6: mean -1.1359989748882526e-13, std 0.999999999999993, range -0.7690980277219608 ... 46.082820223582694\n",
      "16:14    x_7: mean 1.3997691894473973e-17, std 0.9999999999999779, range -2.0362255763639254 ... 2.0273751566157996\n",
      "16:14    x_8: mean 3.4765967882322e-17, std 1.000000000000008, range -1.734394746263361 ... 1.7301204640479633\n",
      "16:14    x_9: mean 1.0595968547022494e-17, std 0.9999999999999591, range -1.2066633661548378 ... 53.46819411980262\n",
      "16:14    x_10: mean -6.047073952686333e-17, std 1.0000000000000213, range -0.926389945741766 ... 36.892248257868914\n",
      "16:14    x_11: mean -1.9264589923295717e-17, std 0.9999999999999849, range -1.9591353103484914 ... 1.9606719845751694\n",
      "16:14    x_12: mean -1.2686740546996589e-17, std 1.0000000000000353, range -1.7334714990144964 ... 1.7319257596292126\n",
      "16:14    x_13: mean 3.4281910643585434e-17, std 0.9999999999983711, range -0.43225072254310964 ... 35.39533565337532\n",
      "16:14    x_14: mean -8.228436598756161e-15, std 1.0000000000028468, range -0.2673030413653257 ... 32.47320011976762\n",
      "16:14    x_15: mean 5.0974335863429586e-17, std 0.9999999999998841, range -6.052608798050151 ... 6.0486031702911145\n",
      "16:14    x_16: mean 5.892175636290631e-18, std 0.9999999999956939, range -3.8998953471878677 ... 3.901361631368039\n",
      "16:14    x_17: mean -2.8901325777042075e-17, std 0.999999999999982, range -2.4375500717049383 ... 2.4154544366975874\n",
      "16:14    x_18: mean 3.309885698854487e-17, std 0.9999999999999442, range -2.066423148219281 ... 2.0566000623527887\n",
      "16:14    x_19: mean -2.4333058590286784e-13, std 1.00000000000003, range -1.5720696114180406 ... 49.99309095300419\n",
      "16:14    x_20: mean 1.475837070330499e-13, std 1.0000000000000115, range -1.2463782488967077 ... 60.84215294538339\n",
      "16:14    x_21: mean -8.734057921344629e-15, std 1.0000000000000102, range -0.9686239294639671 ... 35.61328822493701\n",
      "16:14    x_22: mean 2.0170531911389845e-17, std 1.0000000000000135, range -3.7657615522633314 ... 3.882996403980894\n",
      "16:14    x_23: mean 2.0172308268229244e-17, std 0.9999999999999724, range -2.3491856485401055 ... 2.3393612686933927\n",
      "16:14    x_24: mean -2.2228259766166047e-13, std 1.0000000000000453, range -1.5289373280031247 ... 44.832579471679836\n",
      "16:14    x_25: mean -5.3694545698590446e-14, std 0.9999999999999996, range -0.9812129599740883 ... 42.732988862639544\n",
      "16:14    x_26: mean -5.3695714541390774e-17, std 1.0000000000012998, range -1.9370292406476144 ... 1.9454525868224892\n",
      "16:14  Only using 7 of 26 observables\n",
      "16:14  Creating model for method sally\n",
      "16:14  Training model\n",
      "16:15    Epoch 1: train loss 24.9647 (mse_score: 24.9647, l2_grad_x: 0.0873)\n",
      "            val. loss  15.4879 (mse_score: 15.4879, l2_grad_x: 0.0000) (*)\n",
      "16:16    Epoch 2: train loss 24.9477 (mse_score: 24.9477, l2_grad_x: 0.2048)\n",
      "            val. loss  15.4856 (mse_score: 15.4856, l2_grad_x: 0.0000) (*)\n",
      "16:16    Epoch 3: train loss 24.9388 (mse_score: 24.9388, l2_grad_x: 0.3225)\n",
      "            val. loss  15.4758 (mse_score: 15.4758, l2_grad_x: 0.0000) (*)\n",
      "16:17    Epoch 4: train loss 24.9284 (mse_score: 24.9284, l2_grad_x: 0.5414)\n",
      "            val. loss  17.2005 (mse_score: 17.2005, l2_grad_x: 0.0000)\n",
      "16:17    Epoch 5: train loss 24.9254 (mse_score: 24.9254, l2_grad_x: 0.6500)\n",
      "            val. loss  15.4953 (mse_score: 15.4953, l2_grad_x: 0.0000)\n",
      "16:18    Epoch 6: train loss 24.9162 (mse_score: 24.9162, l2_grad_x: 0.5961)\n",
      "            val. loss  15.5105 (mse_score: 15.5105, l2_grad_x: 0.0000)\n",
      "16:19    Epoch 7: train loss 24.9046 (mse_score: 24.9046, l2_grad_x: 0.7391)\n",
      "            val. loss  15.5066 (mse_score: 15.5066, l2_grad_x: 0.0000)\n",
      "16:19    Epoch 8: train loss 24.8974 (mse_score: 24.8974, l2_grad_x: 0.9052)\n",
      "            val. loss  15.5060 (mse_score: 15.5060, l2_grad_x: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:20    Epoch 9: train loss 24.8872 (mse_score: 24.8872, l2_grad_x: 1.0081)\n",
      "            val. loss  15.5160 (mse_score: 15.5160, l2_grad_x: 0.0000)\n",
      "16:20    Epoch 10: train loss 24.8689 (mse_score: 24.8689, l2_grad_x: 1.0575)\n",
      "            val. loss  15.5059 (mse_score: 15.5059, l2_grad_x: 0.0000)\n",
      "16:21    Epoch 11: train loss 24.8585 (mse_score: 24.8585, l2_grad_x: 1.2029)\n",
      "            val. loss  15.5216 (mse_score: 15.5216, l2_grad_x: 0.0000)\n",
      "16:22    Epoch 12: train loss 24.8385 (mse_score: 24.8385, l2_grad_x: 1.2526)\n",
      "            val. loss  15.5416 (mse_score: 15.5416, l2_grad_x: 0.0000)\n",
      "16:22    Epoch 13: train loss 24.8163 (mse_score: 24.8163, l2_grad_x: 1.4874)\n",
      "            val. loss  15.5479 (mse_score: 15.5479, l2_grad_x: 0.0000)\n",
      "16:23    Epoch 14: train loss 24.7992 (mse_score: 24.7992, l2_grad_x: 1.6468)\n",
      "            val. loss  15.5636 (mse_score: 15.5636, l2_grad_x: 0.0000)\n",
      "16:23    Epoch 15: train loss 24.7775 (mse_score: 24.7775, l2_grad_x: 1.6966)\n",
      "            val. loss  15.5607 (mse_score: 15.5607, l2_grad_x: 0.0000)\n",
      "16:24    Epoch 16: train loss 24.7611 (mse_score: 24.7611, l2_grad_x: 1.8753)\n",
      "            val. loss  15.5709 (mse_score: 15.5709, l2_grad_x: 0.0000)\n",
      "16:24    Epoch 17: train loss 24.7301 (mse_score: 24.7301, l2_grad_x: 2.1002)\n",
      "            val. loss  15.5337 (mse_score: 15.5337, l2_grad_x: 0.0000)\n",
      "16:25    Epoch 18: train loss 24.7150 (mse_score: 24.7150, l2_grad_x: 2.0927)\n",
      "            val. loss  15.6111 (mse_score: 15.6111, l2_grad_x: 0.0000)\n",
      "16:25    Epoch 19: train loss 24.6851 (mse_score: 24.6851, l2_grad_x: 2.4212)\n",
      "            val. loss  15.6216 (mse_score: 15.6216, l2_grad_x: 0.0000)\n",
      "16:26    Epoch 20: train loss 24.6610 (mse_score: 24.6610, l2_grad_x: 2.5822)\n",
      "            val. loss  15.5957 (mse_score: 15.5957, l2_grad_x: 0.0000)\n",
      "16:27    Epoch 21: train loss 24.6391 (mse_score: 24.6391, l2_grad_x: 2.7501)\n",
      "            val. loss  15.6137 (mse_score: 15.6137, l2_grad_x: 0.0000)\n",
      "16:28    Epoch 22: train loss 24.6187 (mse_score: 24.6187, l2_grad_x: 2.9854)\n",
      "            val. loss  15.6110 (mse_score: 15.6110, l2_grad_x: 0.0000)\n",
      "16:29    Epoch 23: train loss 24.5976 (mse_score: 24.5976, l2_grad_x: 2.9874)\n",
      "            val. loss  15.6353 (mse_score: 15.6353, l2_grad_x: 0.0000)\n",
      "16:29  No improvement for 20 epochs, stopping training\n",
      "16:29  Early stopping after epoch 3, with loss 15.48 compared to final loss 15.64\n",
      "16:29  Finished training\n",
      "16:29  Training estimator 4 / 10 in ensemble\n",
      "16:29  Starting training\n",
      "16:29    Method:                 sally\n",
      "16:29    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "16:29                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "16:29    Features:               [0, 5, 6, 7, 9, 10, 11]\n",
      "16:29    Method:                 sally\n",
      "16:29    Hidden layers:          (100, 100)\n",
      "16:29    Activation function:    tanh\n",
      "16:29    Batch size:             128\n",
      "16:29    Trainer:                amsgrad\n",
      "16:29    Epochs:                 50\n",
      "16:29    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:29    Validation split:       0.25\n",
      "16:29    Early stopping:         True\n",
      "16:29    Scale inputs:           True\n",
      "16:29    Regularization:         0.0 * |grad_x f(x)|^2\n",
      "16:29  Loading training data\n",
      "16:29  Found 1000000 samples with 2 parameters and 26 observables\n",
      "16:29  Rescaling inputs\n",
      "16:29  Observable ranges:\n",
      "16:29    x_1: mean 1.0707346120852889e-16, std 0.999999999999957, range -1.2613389913197341 ... 47.462325183011096\n",
      "16:29    x_2: mean 1.949018724189955e-17, std 1.000000000000006, range -1.7232965205436817 ... 1.7388180101194501\n",
      "16:29    x_3: mean -1.06343946981724e-14, std 0.9999999999999722, range -0.6625179066429997 ... 26.525840916434852\n",
      "16:29    x_4: mean -3.894484734701109e-17, std 0.9999999999999852, range -5.235308479534169 ... 5.7737712903320375\n",
      "16:29    x_5: mean 8.65174598629892e-17, std 1.0000000000000384, range -0.8214329435637701 ... 33.53180834165783\n",
      "16:29    x_6: mean -8.520693484115326e-14, std 0.9999999999999764, range -0.7653267311473838 ... 39.13261744107349\n",
      "16:29    x_7: mean 9.805489753489382e-19, std 1.0000000000000482, range -2.0346275036263215 ... 2.0294573281481356\n",
      "16:29    x_8: mean -2.853184355444682e-17, std 1.000000000000013, range -1.7361913213077425 ... 1.731395872880918\n",
      "16:29    x_9: mean -1.1685585832310607e-16, std 0.9999999999999798, range -1.203841298474923 ... 42.24116067243962\n",
      "16:29    x_10: mean 6.122036211309023e-17, std 0.9999999999999885, range -0.9314320934699807 ... 37.2854569111454\n",
      "16:29    x_11: mean -1.7152501641248818e-17, std 0.9999999999999963, range -1.9560589945151146 ... 1.9617129566356057\n",
      "16:29    x_12: mean -2.3808510718481557e-17, std 1.0000000000000187, range -1.7332134910690944 ... 1.729823921265003\n",
      "16:29    x_13: mean 2.5927704427886055e-17, std 0.9999999999915896, range -0.43230536502173555 ... 23.81903281808663\n",
      "16:29    x_14: mean -3.5164404721399476e-15, std 1.0000000000033458, range -0.2676749844571356 ... 32.26649601316251\n",
      "16:29    x_15: mean 3.375077994860476e-19, std 1.0000000000021005, range -6.0292328410002645 ... 6.028738091193727\n",
      "16:29    x_16: mean 4.408207132655662e-17, std 1.0000000000036713, range -3.899134994098279 ... 3.9019391617622374\n",
      "16:29    x_17: mean -2.3341328869719292e-17, std 1.0000000000000226, range -2.4391261753724165 ... 2.4264449516788984\n",
      "16:29    x_18: mean 5.88542548030091e-17, std 1.000000000000032, range -2.066567003539685 ... 2.054687544955519\n",
      "16:29    x_19: mean -2.2067784044565997e-13, std 1.0000000000000329, range -1.5695808016423842 ... 43.59262372806746\n",
      "16:29    x_20: mean 7.279704838936141e-14, std 0.9999999999999548, range -1.2396166338300998 ... 41.20449004640651\n",
      "16:29    x_21: mean -6.246115447083866e-14, std 0.9999999999999817, range -0.9633748811956536 ... 47.18259767809366\n",
      "16:29    x_22: mean -3.549160965121701e-18, std 0.999999999999993, range -3.800251491095108 ... 3.889215081536306\n",
      "16:29    x_23: mean 1.8641088672666228e-17, std 0.9999999999999888, range -2.3491938219906627 ... 2.34020954843773\n",
      "16:29    x_24: mean -2.064112436528376e-13, std 1.0000000000000178, range -1.3151449499058927 ... 40.69585579666305\n",
      "16:29    x_25: mean -5.991349638634347e-16, std 1.0000000000000324, range -0.9794349829418331 ... 32.221107724666076\n",
      "16:29    x_26: mean -1.964597373671495e-16, std 1.0000000000013964, range -1.9355212045749028 ... 1.9457526388727249\n",
      "16:29  Only using 7 of 26 observables\n",
      "16:29  Creating model for method sally\n",
      "16:29  Training model\n",
      "16:30    Epoch 1: train loss 35.1683 (mse_score: 35.1683, l2_grad_x: 0.1252)\n",
      "            val. loss  18.2905 (mse_score: 18.2905, l2_grad_x: 0.0000) (*)\n",
      "16:31    Epoch 2: train loss 35.1581 (mse_score: 35.1581, l2_grad_x: 0.1498)\n",
      "            val. loss  18.3554 (mse_score: 18.3554, l2_grad_x: 0.0000)\n",
      "16:31    Epoch 3: train loss 35.1521 (mse_score: 35.1521, l2_grad_x: 0.2264)\n",
      "            val. loss  18.2401 (mse_score: 18.2401, l2_grad_x: 0.0000) (*)\n",
      "16:32    Epoch 4: train loss 35.1467 (mse_score: 35.1467, l2_grad_x: 0.3640)\n",
      "            val. loss  18.2333 (mse_score: 18.2333, l2_grad_x: 0.0000) (*)\n",
      "16:32    Epoch 5: train loss 35.1360 (mse_score: 35.1360, l2_grad_x: 0.4314)\n",
      "            val. loss  18.2768 (mse_score: 18.2768, l2_grad_x: 0.0000)\n",
      "16:33    Epoch 6: train loss 35.1344 (mse_score: 35.1344, l2_grad_x: 0.5341)\n",
      "            val. loss  18.2378 (mse_score: 18.2378, l2_grad_x: 0.0000)\n",
      "16:34    Epoch 7: train loss 35.1264 (mse_score: 35.1264, l2_grad_x: 0.6821)\n",
      "            val. loss  18.2403 (mse_score: 18.2403, l2_grad_x: 0.0000)\n",
      "16:34    Epoch 8: train loss 35.1319 (mse_score: 35.1319, l2_grad_x: 0.7056)\n",
      "            val. loss  18.2546 (mse_score: 18.2546, l2_grad_x: 0.0000)\n",
      "16:35    Epoch 9: train loss 35.1075 (mse_score: 35.1075, l2_grad_x: 0.7496)\n",
      "            val. loss  18.2500 (mse_score: 18.2500, l2_grad_x: 0.0000)\n",
      "16:35    Epoch 10: train loss 35.0906 (mse_score: 35.0906, l2_grad_x: 0.8559)\n",
      "            val. loss  18.2520 (mse_score: 18.2520, l2_grad_x: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'minimal',\n",
    "    use_tight_cuts=False,\n",
    "    grad_x_regularization=0.0,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight',\n",
    "    use_tight_cuts=True,\n",
    "    grad_x_regularization=0.0,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_reg',\n",
    "    use_tight_cuts=False,\n",
    "    grad_x_regularization=0.1,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight_reg',\n",
    "    use_tight_cuts=True,\n",
    "    grad_x_regularization=0.1,\n",
    "    features=[[0,5,6,7,9,10,11] for _ in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
