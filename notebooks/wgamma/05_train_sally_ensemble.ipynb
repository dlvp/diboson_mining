{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s  %(message)s', datefmt='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma/'\n",
    "log_dir = base_dir + 'logs/wgamma/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:14  \n",
      "18:14  ------------------------------------------------------------\n",
      "18:14  |                                                          |\n",
      "18:14  |  MadMiner v2018.10.22                                    |\n",
      "18:14  |                                                          |\n",
      "18:14  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "18:14  |                                                          |\n",
      "18:14  ------------------------------------------------------------\n",
      "18:14  \n"
     ]
    }
   ],
   "source": [
    "n_estimators = 10\n",
    "\n",
    "ensemble = EnsembleForge(\n",
    "    [MLForge(debug=False) for _ in range(n_estimators)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SALLY on all observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:14  Training 10 estimators in ensemble\n",
      "18:14  Training estimator 1 / 10 in ensemble\n",
      "18:14  Starting training\n",
      "18:14    Method:                 sally\n",
      "18:14    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_0.npy\n",
      "18:14                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_0.npy\n",
      "18:14    Features:               all\n",
      "18:14    Method:                 sally\n",
      "18:14    Hidden layers:          (50,)\n",
      "18:14    Activation function:    tanh\n",
      "18:14    Batch size:             256\n",
      "18:14    Epochs:                 10\n",
      "18:14    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "18:14    Validation split:       None\n",
      "18:14    Early stopping:         True\n",
      "18:14  Loading training data\n",
      "18:14  Found 1000000 samples with 2 parameters and 27 observables\n",
      "18:14  Creating model for method sally\n",
      "18:14  Training model\n",
      "18:14    Epoch 1: train loss 29.17 ([29.16833857])\n",
      "18:14    Epoch 2: train loss 29.16 ([29.16334708])\n",
      "18:14    Epoch 3: train loss 29.15 ([29.15434711])\n",
      "18:15    Epoch 4: train loss 29.17 ([29.17172994])\n",
      "18:15    Epoch 5: train loss 29.14 ([29.14184183])\n",
      "18:15    Epoch 6: train loss 29.14 ([29.13998818])\n",
      "18:15    Epoch 7: train loss 29.13 ([29.1343588])\n",
      "18:16    Epoch 8: train loss 29.13 ([29.12855283])\n",
      "18:16    Epoch 9: train loss 29.12 ([29.12492153])\n",
      "18:16    Epoch 10: train loss 29.20 ([29.2028826])\n",
      "18:16  Finished training\n",
      "18:16  Training estimator 2 / 10 in ensemble\n",
      "18:16  Starting training\n",
      "18:16    Method:                 sally\n",
      "18:16    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_1.npy\n",
      "18:16                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_1.npy\n",
      "18:16    Features:               all\n",
      "18:16    Method:                 sally\n",
      "18:16    Hidden layers:          (100,)\n",
      "18:16    Activation function:    tanh\n",
      "18:16    Batch size:             256\n",
      "18:16    Epochs:                 10\n",
      "18:16    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "18:16    Validation split:       None\n",
      "18:16    Early stopping:         True\n",
      "18:16  Loading training data\n",
      "18:16  Found 1000000 samples with 2 parameters and 27 observables\n",
      "18:16  Creating model for method sally\n",
      "18:16  Training model\n",
      "18:17    Epoch 1: train loss 22.15 ([22.14870782])\n",
      "18:17    Epoch 2: train loss 22.14 ([22.13968172])\n",
      "18:17    Epoch 3: train loss 22.13 ([22.12755587])\n",
      "18:17    Epoch 4: train loss 22.12 ([22.12003771])\n",
      "18:18    Epoch 5: train loss 22.11 ([22.10752013])\n",
      "18:18    Epoch 6: train loss 22.10 ([22.10133845])\n",
      "18:18    Epoch 7: train loss 22.09 ([22.09460642])\n",
      "18:18    Epoch 8: train loss 22.09 ([22.0907131])\n",
      "18:19    Epoch 9: train loss 22.09 ([22.09192933])\n",
      "18:19    Epoch 10: train loss 22.08 ([22.08276723])\n",
      "18:19  Finished training\n",
      "18:19  Training estimator 3 / 10 in ensemble\n",
      "18:19  Starting training\n",
      "18:19    Method:                 sally\n",
      "18:19    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_2.npy\n",
      "18:19                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_2.npy\n",
      "18:19    Features:               all\n",
      "18:19    Method:                 sally\n",
      "18:19    Hidden layers:          (50, 50)\n",
      "18:19    Activation function:    tanh\n",
      "18:19    Batch size:             256\n",
      "18:19    Epochs:                 10\n",
      "18:19    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "18:19    Validation split:       None\n",
      "18:19    Early stopping:         True\n",
      "18:19  Loading training data\n",
      "18:19  Found 1000000 samples with 2 parameters and 27 observables\n",
      "18:19  Creating model for method sally\n",
      "18:19  Training model\n",
      "18:19    Epoch 1: train loss 26.42 ([26.42322073])\n",
      "18:20    Epoch 2: train loss 26.41 ([26.41423213])\n",
      "18:20    Epoch 3: train loss 26.41 ([26.40689063])\n",
      "18:20    Epoch 4: train loss 26.40 ([26.40239112])\n",
      "18:20    Epoch 5: train loss 26.40 ([26.39606686])\n",
      "18:21    Epoch 6: train loss 26.39 ([26.38662583])\n",
      "18:21    Epoch 7: train loss 26.38 ([26.38378066])\n",
      "18:21    Epoch 8: train loss 26.38 ([26.38156833])\n",
      "18:21    Epoch 9: train loss 26.38 ([26.37550073])\n",
      "18:22    Epoch 10: train loss 26.37 ([26.36793081])\n",
      "18:22  Finished training\n",
      "18:22  Training estimator 4 / 10 in ensemble\n",
      "18:22  Starting training\n",
      "18:22    Method:                 sally\n",
      "18:22    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_3.npy\n",
      "18:22                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_3.npy\n",
      "18:22    Features:               all\n",
      "18:22    Method:                 sally\n",
      "18:22    Hidden layers:          (50, 20)\n",
      "18:22    Activation function:    tanh\n",
      "18:22    Batch size:             256\n",
      "18:22    Epochs:                 10\n",
      "18:22    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "18:22    Validation split:       None\n",
      "18:22    Early stopping:         True\n",
      "18:22  Loading training data\n",
      "18:22  Found 1000000 samples with 2 parameters and 27 observables\n",
      "18:22  Creating model for method sally\n",
      "18:22  Training model\n",
      "18:22    Epoch 1: train loss 43.46 ([43.46143193])\n",
      "18:22    Epoch 2: train loss 43.46 ([43.45550634])\n",
      "18:23    Epoch 3: train loss 43.45 ([43.45195256])\n",
      "18:23    Epoch 4: train loss 43.45 ([43.44863836])\n",
      "18:23    Epoch 5: train loss 43.44 ([43.44071359])\n",
      "18:23    Epoch 6: train loss 43.43 ([43.43476666])\n",
      "18:23    Epoch 7: train loss 43.43 ([43.43233801])\n",
      "18:24    Epoch 8: train loss 43.43 ([43.42808301])\n",
      "18:24    Epoch 9: train loss 43.42 ([43.42450672])\n",
      "18:24    Epoch 10: train loss 43.42 ([43.42319085])\n",
      "18:24  Finished training\n",
      "18:24  Training estimator 5 / 10 in ensemble\n",
      "18:24  Starting training\n",
      "18:24    Method:                 sally\n",
      "18:24    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_4.npy\n",
      "18:24                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_4.npy\n",
      "18:24    Features:               all\n",
      "18:24    Method:                 sally\n",
      "18:24    Hidden layers:          (100, 100)\n",
      "18:24    Activation function:    tanh\n",
      "18:24    Batch size:             256\n",
      "18:24    Epochs:                 10\n",
      "18:24    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "18:24    Validation split:       None\n",
      "18:24    Early stopping:         True\n",
      "18:24  Loading training data\n",
      "18:24  Found 1000000 samples with 2 parameters and 27 observables\n",
      "18:24  Creating model for method sally\n",
      "18:24  Training model\n",
      "18:25    Epoch 1: train loss 21.21 ([21.212088])\n",
      "18:25    Epoch 2: train loss 21.21 ([21.20741823])\n",
      "18:25    Epoch 3: train loss 21.20 ([21.19889286])\n",
      "18:26    Epoch 4: train loss 21.19 ([21.18526032])\n",
      "18:26    Epoch 5: train loss 21.18 ([21.17626908])\n",
      "18:26    Epoch 6: train loss 21.17 ([21.16724181])\n",
      "18:27    Epoch 7: train loss 21.16 ([21.15862373])\n",
      "18:27    Epoch 8: train loss 21.15 ([21.14837773])\n",
      "18:27    Epoch 9: train loss 21.14 ([21.14189528])\n",
      "18:28    Epoch 10: train loss 21.13 ([21.13412631])\n",
      "18:28  Finished training\n",
      "18:28  Training estimator 6 / 10 in ensemble\n",
      "18:28  Starting training\n",
      "18:28    Method:                 sally\n",
      "18:28    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_5.npy\n",
      "18:28                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_5.npy\n",
      "18:28    Features:               all\n",
      "18:28    Method:                 sally\n",
      "18:28    Hidden layers:          (100, 20)\n",
      "18:28    Activation function:    tanh\n",
      "18:28    Batch size:             256\n",
      "18:28    Epochs:                 10\n",
      "18:28    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "18:28    Validation split:       None\n",
      "18:28    Early stopping:         True\n",
      "18:28  Loading training data\n",
      "18:28  Found 1000000 samples with 2 parameters and 27 observables\n",
      "18:28  Creating model for method sally\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:28  Training model\n",
      "18:28    Epoch 1: train loss 77.17 ([77.1709123])\n",
      "18:28    Epoch 2: train loss 77.16 ([77.16493485])\n",
      "18:29    Epoch 3: train loss 77.16 ([77.15595141])\n",
      "18:29    Epoch 4: train loss 77.14 ([77.14477613])\n",
      "18:29    Epoch 5: train loss 77.14 ([77.14116496])\n",
      "18:29    Epoch 6: train loss 77.14 ([77.13569414])\n",
      "18:30    Epoch 7: train loss 77.12 ([77.12474735])\n",
      "18:30    Epoch 8: train loss 77.12 ([77.11678682])\n",
      "18:30    Epoch 9: train loss 77.11 ([77.11386909])\n",
      "18:31    Epoch 10: train loss 77.12 ([77.11762383])\n",
      "18:31  Finished training\n",
      "18:31  Training estimator 7 / 10 in ensemble\n",
      "18:31  Starting training\n",
      "18:31    Method:                 sally\n",
      "18:31    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_6.npy\n",
      "18:31                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_6.npy\n",
      "18:31    Features:               all\n",
      "18:31    Method:                 sally\n",
      "18:31    Hidden layers:          (50, 50, 50)\n",
      "18:31    Activation function:    tanh\n",
      "18:31    Batch size:             256\n",
      "18:31    Epochs:                 10\n",
      "18:31    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "18:31    Validation split:       None\n",
      "18:31    Early stopping:         True\n",
      "18:31  Loading training data\n",
      "18:31  Found 1000000 samples with 2 parameters and 27 observables\n",
      "18:31  Creating model for method sally\n",
      "18:31  Training model\n",
      "18:31    Epoch 1: train loss 56.97 ([56.97324881])\n",
      "18:31    Epoch 2: train loss 56.97 ([56.97017734])\n",
      "18:32    Epoch 3: train loss 56.96 ([56.9619666])\n",
      "18:32    Epoch 4: train loss 56.95 ([56.95490531])\n",
      "18:32    Epoch 5: train loss 56.95 ([56.94981894])\n",
      "18:32    Epoch 6: train loss 56.94 ([56.94096591])\n",
      "18:33    Epoch 7: train loss 56.93 ([56.93167])\n",
      "18:33    Epoch 8: train loss 56.92 ([56.92140969])\n",
      "18:33    Epoch 9: train loss 56.92 ([56.91954159])\n",
      "18:33    Epoch 10: train loss 56.92 ([56.91513455])\n",
      "18:33  Finished training\n",
      "18:33  Training estimator 8 / 10 in ensemble\n",
      "18:33  Starting training\n",
      "18:33    Method:                 sally\n",
      "18:33    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_7.npy\n",
      "18:33                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_7.npy\n",
      "18:33    Features:               all\n",
      "18:33    Method:                 sally\n",
      "18:33    Hidden layers:          (50, 20, 10)\n",
      "18:33    Activation function:    tanh\n",
      "18:33    Batch size:             256\n",
      "18:33    Epochs:                 10\n",
      "18:33    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "18:33    Validation split:       None\n",
      "18:33    Early stopping:         True\n",
      "18:33  Loading training data\n",
      "18:33  Found 1000000 samples with 2 parameters and 27 observables\n",
      "18:33  Creating model for method sally\n",
      "18:33  Training model\n",
      "18:34    Epoch 1: train loss 19.03 ([19.0264071])\n",
      "18:34    Epoch 2: train loss 19.02 ([19.02309916])\n",
      "18:34    Epoch 3: train loss 19.02 ([19.01700334])\n",
      "18:34    Epoch 4: train loss 19.02 ([19.01587739])\n",
      "18:35    Epoch 5: train loss 19.01 ([19.01046153])\n",
      "18:35    Epoch 6: train loss 19.01 ([19.0061022])\n",
      "18:35    Epoch 7: train loss 19.00 ([18.99961431])\n",
      "18:35    Epoch 8: train loss 18.99 ([18.98931233])\n",
      "18:35    Epoch 9: train loss 18.98 ([18.98324488])\n",
      "18:36    Epoch 10: train loss 18.98 ([18.98045183])\n",
      "18:36  Finished training\n",
      "18:36  Training estimator 9 / 10 in ensemble\n",
      "18:36  Starting training\n",
      "18:36    Method:                 sally\n",
      "18:36    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_8.npy\n",
      "18:36                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_8.npy\n",
      "18:36    Features:               all\n",
      "18:36    Method:                 sally\n",
      "18:36    Hidden layers:          (100, 100, 100)\n",
      "18:36    Activation function:    tanh\n",
      "18:36    Batch size:             256\n",
      "18:36    Epochs:                 10\n",
      "18:36    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "18:36    Validation split:       None\n",
      "18:36    Early stopping:         True\n",
      "18:36  Loading training data\n",
      "18:36  Found 1000000 samples with 2 parameters and 27 observables\n",
      "18:36  Creating model for method sally\n",
      "18:36  Training model\n",
      "18:36    Epoch 1: train loss 21.11 ([21.10696126])\n",
      "18:37    Epoch 2: train loss 21.10 ([21.10368874])\n",
      "18:37    Epoch 3: train loss 21.10 ([21.09589558])\n",
      "18:37    Epoch 4: train loss 21.08 ([21.08286128])\n",
      "18:37    Epoch 5: train loss 21.07 ([21.07282529])\n",
      "18:38    Epoch 6: train loss 21.06 ([21.06300205])\n",
      "18:38    Epoch 7: train loss 21.05 ([21.05092308])\n",
      "18:38    Epoch 8: train loss 21.04 ([21.04252678])\n",
      "18:39    Epoch 9: train loss 21.03 ([21.02944213])\n",
      "18:39    Epoch 10: train loss 21.02 ([21.02033499])\n",
      "18:39  Finished training\n",
      "18:39  Training estimator 10 / 10 in ensemble\n",
      "18:39  Starting training\n",
      "18:39    Method:                 sally\n",
      "18:39    Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/x_train_9.npy\n",
      "18:39                   t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma/train_local/t_xz_train_9.npy\n",
      "18:39    Features:               all\n",
      "18:39    Method:                 sally\n",
      "18:39    Hidden layers:          (100, 50, 20)\n",
      "18:39    Activation function:    tanh\n",
      "18:39    Batch size:             256\n",
      "18:39    Epochs:                 10\n",
      "18:39    Learning rate:          0.002 initially, decaying to 0.0001\n",
      "18:39    Validation split:       None\n",
      "18:39    Early stopping:         True\n",
      "18:39  Loading training data\n",
      "18:39  Found 1000000 samples with 2 parameters and 27 observables\n",
      "18:39  Creating model for method sally\n",
      "18:39  Training model\n",
      "18:40    Epoch 1: train loss 34.11 ([34.11009272])\n",
      "18:40    Epoch 2: train loss 34.10 ([34.10317536])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-123b403fa1b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/projects/madminer/madminer/madminer/ml.py\u001b[0m in \u001b[0;36mtrain_all\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training estimator %s / %s in ensemble'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs_this_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     def calculate_expectation(self,\n",
      "\u001b[0;32m~/work/projects/madminer/madminer/madminer/ml.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, method, x_filename, y_filename, theta0_filename, theta1_filename, r_xz_filename, t_xz0_filename, t_xz1_filename, features, nde_type, n_hidden, activation, maf_n_mades, maf_batch_norm, maf_batch_norm_alpha, maf_mog_n_components, alpha, n_epochs, batch_size, initial_lr, final_lr, validation_split, early_stopping)\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0mfinal_learning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_lr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m                 \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m                 \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             )\n\u001b[1;32m    424\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'nde'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scandal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/projects/madminer/madminer/madminer/utils/ml/score_trainer.py\u001b[0m in \u001b[0;36mtrain_local_score_model\u001b[0;34m(model, loss_functions, xs, t_xzs, loss_weights, loss_labels, batch_size, initial_learning_rate, final_learning_rate, n_epochs, clip_gradient, run_on_gpu, double_precision, validation_split, early_stopping, early_stopping_patience, learning_curve_folder, learning_curve_filename, verbose)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# Loop over batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_xz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mt_xz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_xz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ensemble.train_all(\n",
    "    method='sally',\n",
    "    x_filename=[sample_dir + 'train_local/x_train_{}.npy'.format(i) for i in range(10)],\n",
    "    t_xz0_filename=[sample_dir + 'train_local/t_xz_train_{}.npy'.format(i) for i in range(10)],\n",
    "    n_epochs=10,\n",
    "    batch_size=256,\n",
    "    validation_split=None,\n",
    "    n_hidden=[(50,), (100,), (50,50), (50,20), (100,100), (100, 20), (50,50,50), (50,20,10), (100,100,100), (100, 50, 20)]\n",
    ")\n",
    "\n",
    "ensemble.save(model_dir + 'sally_ensemble_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:40  Calculating expectation for 10 estimators in ensemble\n",
      "17:40  Starting evaluation for estimator 1 / 10 in ensemble\n",
      "17:40  Loading evaluation data\n",
      "17:40  Starting score evaluation\n",
      "17:40  Starting evaluation for estimator 2 / 10 in ensemble\n",
      "17:40  Loading evaluation data\n",
      "17:40  Starting score evaluation\n",
      "17:40  Starting evaluation for estimator 3 / 10 in ensemble\n",
      "17:40  Loading evaluation data\n",
      "17:40  Starting score evaluation\n",
      "17:40  Starting evaluation for estimator 4 / 10 in ensemble\n",
      "17:40  Loading evaluation data\n",
      "17:40  Starting score evaluation\n",
      "17:41  Starting evaluation for estimator 5 / 10 in ensemble\n",
      "17:41  Loading evaluation data\n",
      "17:41  Starting score evaluation\n",
      "17:41  Starting evaluation for estimator 6 / 10 in ensemble\n",
      "17:41  Loading evaluation data\n",
      "17:41  Starting score evaluation\n",
      "17:41  Starting evaluation for estimator 7 / 10 in ensemble\n",
      "17:41  Loading evaluation data\n",
      "17:41  Starting score evaluation\n",
      "17:41  Starting evaluation for estimator 8 / 10 in ensemble\n",
      "17:41  Loading evaluation data\n",
      "17:41  Starting score evaluation\n",
      "17:41  Starting evaluation for estimator 9 / 10 in ensemble\n",
      "17:41  Loading evaluation data\n",
      "17:41  Starting score evaluation\n",
      "17:42  Starting evaluation for estimator 10 / 10 in ensemble\n",
      "17:42  Loading evaluation data\n",
      "17:42  Starting score evaluation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00634092,  0.06508537],\n",
       "       [ 0.02007575,  0.02556378],\n",
       "       [ 0.03975902, -0.01588391],\n",
       "       [-0.02189546,  0.02988753],\n",
       "       [-0.01648937, -0.00555408],\n",
       "       [-0.00935855,  0.00249242],\n",
       "       [ 0.04448106, -0.01657623],\n",
       "       [ 0.0275156 , -0.01794838],\n",
       "       [-0.05467386,  0.020074  ],\n",
       "       [-0.00354157, -0.00803116]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.calculate_expectation(\n",
    "    x_filename=sample_dir + 'validation/x_validation.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/logging/__init__.py\", line 992, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/logging/__init__.py\", line 838, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/logging/__init__.py\", line 575, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/logging/__init__.py\", line 338, in getMessage\n",
      "    msg = msg % self.args\n",
      "ValueError: unsupported format character '/' (0x2f) at index 26\n",
      "Call stack:\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-f175459d6b46>\", line 1, in <module>\n",
      "    ensemble.save(model_dir + 'sally_ensemble_all')\n",
      "  File \"/Users/johannbrehmer/work/projects/madminer/madminer/madminer/ml.py\", line 1186, in save\n",
      "    logging.info('Saving ensemble setup to %s/ensemble.json', folder)\n",
      "Message: 'Saving ensemble setup to %/ensemble.json'\n",
      "Arguments: ('/Users/johannbrehmer/work/projects/madminer/diboson_mining/data/models/wgamma/sally_ensemble_all',)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type 'ndarray' is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f175459d6b46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'sally_ensemble_all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/projects/madminer/madminer/madminer/ml.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, folder)\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0mexpectations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'None'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m             \u001b[0mexpectations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpectations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         settings = {'n_estimators': self.n_estimators,\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n\u001b[0;32m--> 180\u001b[0;31m                         o.__class__.__name__)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type 'ndarray' is not JSON serializable"
     ]
    }
   ],
   "source": [
    "ensemble.save(model_dir + 'sally_ensemble_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d toy study (delta phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.train_all(\n",
    "    method='sally',\n",
    "    x_filename=[sample_dir + 'train_local/x_train_{}.npy'.format(i) for i in range(10)],\n",
    "    t_xz0_filename=[sample_dir + 'train_local/t_xz_train_{}.npy'.format(i) for i in range(10)],\n",
    "    features=[25],\n",
    "    n_epochs=10,\n",
    "    batch_size=256,\n",
    "    validation_split=None,\n",
    "    n_hidden=[(50,), (100,), (50,50), (50,20), (100,100), (100, 20), (50,50,50), (50,20,10), (100,100,100), (100, 50, 20)]\n",
    ")\n",
    "\n",
    "ensemble.save(model_dir + 'sally_ensemble_deltaphi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
