{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Marco Farina, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)-5.5s %(name)-20.20s %(levelname)-7.7s %(message)s',\n",
    "    datefmt='%H:%M',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"madminer\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma_sys/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma_sys/'\n",
    "log_dir = base_dir + 'logs/wgamma_sys/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma_sys/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=n_estimators, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=True)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:07 madminer.ml          INFO    Training 10 estimators in ensemble\n",
      "23:07 madminer.ml          INFO    Training estimator 1 / 10 in ensemble\n",
      "23:07 madminer.ml          INFO    Starting training\n",
      "23:07 madminer.ml          INFO      Method:                 sally\n",
      "23:07 madminer.ml          INFO      Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_0.npy\n",
      "23:07 madminer.ml          INFO                     t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_0.npy\n",
      "23:07 madminer.ml          INFO      Features:               all\n",
      "23:07 madminer.ml          INFO      Method:                 sally\n",
      "23:07 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "23:07 madminer.ml          INFO      Activation function:    tanh\n",
      "23:07 madminer.ml          INFO      Batch size:             128\n",
      "23:07 madminer.ml          INFO      Trainer:                amsgrad\n",
      "23:07 madminer.ml          INFO      Epochs:                 50\n",
      "23:07 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "23:07 madminer.ml          INFO      Validation split:       0.5\n",
      "23:07 madminer.ml          INFO      Early stopping:         True\n",
      "23:07 madminer.ml          INFO      Scale inputs:           True\n",
      "23:07 madminer.ml          INFO      Shuffle labels          False\n",
      "23:07 madminer.ml          INFO      Regularization:         None\n",
      "23:07 madminer.ml          INFO    Loading training data\n",
      "23:07 madminer.ml          INFO    Found 1000000 samples with 2 parameters and 33 observables\n",
      "23:07 madminer.ml          INFO    Rescaling inputs\n",
      "23:07 madminer.ml          INFO    Creating model for method sally\n",
      "23:07 madminer.ml          INFO    Training model\n",
      "23:08 madminer.utils.ml.sc INFO      Epoch 1: train loss 2.4024 (mse_score: 2.4024)\n",
      "23:08 madminer.utils.ml.sc INFO                val. loss  3.7886 (mse_score: 3.7886) (*)\n",
      "23:08 madminer.utils.ml.sc INFO      Epoch 2: train loss 2.2788 (mse_score: 2.2788)\n",
      "23:08 madminer.utils.ml.sc INFO                val. loss  3.6365 (mse_score: 3.6365) (*)\n",
      "23:09 madminer.utils.ml.sc INFO      Epoch 3: train loss 2.2133 (mse_score: 2.2133)\n",
      "23:09 madminer.utils.ml.sc INFO                val. loss  3.6211 (mse_score: 3.6211) (*)\n",
      "23:09 madminer.utils.ml.sc INFO      Epoch 4: train loss 2.1691 (mse_score: 2.1691)\n",
      "23:09 madminer.utils.ml.sc INFO                val. loss  3.6243 (mse_score: 3.6243)\n",
      "23:09 madminer.utils.ml.sc INFO      Epoch 5: train loss 2.1229 (mse_score: 2.1229)\n",
      "23:09 madminer.utils.ml.sc INFO                val. loss  3.5037 (mse_score: 3.5037) (*)\n",
      "23:10 madminer.utils.ml.sc INFO      Epoch 6: train loss 2.0659 (mse_score: 2.0659)\n",
      "23:10 madminer.utils.ml.sc INFO                val. loss  3.5033 (mse_score: 3.5033) (*)\n",
      "23:10 madminer.utils.ml.sc INFO      Epoch 7: train loss 2.0544 (mse_score: 2.0544)\n",
      "23:10 madminer.utils.ml.sc INFO                val. loss  3.4640 (mse_score: 3.4640) (*)\n",
      "23:11 madminer.utils.ml.sc INFO      Epoch 8: train loss 2.0181 (mse_score: 2.0181)\n",
      "23:11 madminer.utils.ml.sc INFO                val. loss  3.4738 (mse_score: 3.4738)\n",
      "23:11 madminer.utils.ml.sc INFO      Epoch 9: train loss 1.9862 (mse_score: 1.9862)\n",
      "23:11 madminer.utils.ml.sc INFO                val. loss  3.4443 (mse_score: 3.4443) (*)\n",
      "23:11 madminer.utils.ml.sc INFO      Epoch 10: train loss 1.9549 (mse_score: 1.9549)\n",
      "23:11 madminer.utils.ml.sc INFO                val. loss  3.4369 (mse_score: 3.4369) (*)\n",
      "23:12 madminer.utils.ml.sc INFO      Epoch 11: train loss 1.9246 (mse_score: 1.9246)\n",
      "23:12 madminer.utils.ml.sc INFO                val. loss  3.4299 (mse_score: 3.4299) (*)\n",
      "23:12 madminer.utils.ml.sc INFO      Epoch 12: train loss 1.8963 (mse_score: 1.8963)\n",
      "23:12 madminer.utils.ml.sc INFO                val. loss  3.5318 (mse_score: 3.5318)\n",
      "23:12 madminer.utils.ml.sc INFO      Epoch 13: train loss 1.8917 (mse_score: 1.8917)\n",
      "23:12 madminer.utils.ml.sc INFO                val. loss  3.4044 (mse_score: 3.4044) (*)\n",
      "23:13 madminer.utils.ml.sc INFO      Epoch 14: train loss 1.8579 (mse_score: 1.8579)\n",
      "23:13 madminer.utils.ml.sc INFO                val. loss  3.3924 (mse_score: 3.3924) (*)\n",
      "23:13 madminer.utils.ml.sc INFO      Epoch 15: train loss 1.8367 (mse_score: 1.8367)\n",
      "23:13 madminer.utils.ml.sc INFO                val. loss  3.3852 (mse_score: 3.3852) (*)\n",
      "23:13 madminer.utils.ml.sc INFO      Epoch 16: train loss 1.8066 (mse_score: 1.8066)\n",
      "23:13 madminer.utils.ml.sc INFO                val. loss  3.3834 (mse_score: 3.3834) (*)\n",
      "23:14 madminer.utils.ml.sc INFO      Epoch 17: train loss 1.7838 (mse_score: 1.7838)\n",
      "23:14 madminer.utils.ml.sc INFO                val. loss  3.3775 (mse_score: 3.3775) (*)\n",
      "23:14 madminer.utils.ml.sc INFO      Epoch 18: train loss 1.7634 (mse_score: 1.7634)\n",
      "23:14 madminer.utils.ml.sc INFO                val. loss  3.3735 (mse_score: 3.3735) (*)\n",
      "23:14 madminer.utils.ml.sc INFO      Epoch 19: train loss 1.7474 (mse_score: 1.7474)\n",
      "23:14 madminer.utils.ml.sc INFO                val. loss  3.3613 (mse_score: 3.3613) (*)\n",
      "23:15 madminer.utils.ml.sc INFO      Epoch 20: train loss 1.7279 (mse_score: 1.7279)\n",
      "23:15 madminer.utils.ml.sc INFO                val. loss  3.3805 (mse_score: 3.3805)\n",
      "23:15 madminer.utils.ml.sc INFO      Epoch 21: train loss 1.7030 (mse_score: 1.7030)\n",
      "23:15 madminer.utils.ml.sc INFO                val. loss  3.3783 (mse_score: 3.3783)\n",
      "23:15 madminer.utils.ml.sc INFO      Epoch 22: train loss 1.6862 (mse_score: 1.6862)\n",
      "23:15 madminer.utils.ml.sc INFO                val. loss  3.3374 (mse_score: 3.3374) (*)\n",
      "23:16 madminer.utils.ml.sc INFO      Epoch 23: train loss 1.6694 (mse_score: 1.6694)\n",
      "23:16 madminer.utils.ml.sc INFO                val. loss  3.3621 (mse_score: 3.3621)\n",
      "23:16 madminer.utils.ml.sc INFO      Epoch 24: train loss 1.6521 (mse_score: 1.6521)\n",
      "23:16 madminer.utils.ml.sc INFO                val. loss  3.3870 (mse_score: 3.3870)\n",
      "23:16 madminer.utils.ml.sc INFO      Epoch 25: train loss 1.6298 (mse_score: 1.6298)\n",
      "23:16 madminer.utils.ml.sc INFO                val. loss  3.3668 (mse_score: 3.3668)\n",
      "23:17 madminer.utils.ml.sc INFO      Epoch 26: train loss 1.6201 (mse_score: 1.6201)\n",
      "23:17 madminer.utils.ml.sc INFO                val. loss  3.3582 (mse_score: 3.3582)\n",
      "23:17 madminer.utils.ml.sc INFO      Epoch 27: train loss 1.6029 (mse_score: 1.6029)\n",
      "23:17 madminer.utils.ml.sc INFO                val. loss  3.3648 (mse_score: 3.3648)\n",
      "23:17 madminer.utils.ml.sc INFO      Epoch 28: train loss 1.5831 (mse_score: 1.5831)\n",
      "23:17 madminer.utils.ml.sc INFO                val. loss  3.4111 (mse_score: 3.4111)\n",
      "23:18 madminer.utils.ml.sc INFO      Epoch 29: train loss 1.5757 (mse_score: 1.5757)\n",
      "23:18 madminer.utils.ml.sc INFO                val. loss  3.3805 (mse_score: 3.3805)\n",
      "23:18 madminer.utils.ml.sc INFO      Epoch 30: train loss 1.5559 (mse_score: 1.5559)\n",
      "23:18 madminer.utils.ml.sc INFO                val. loss  3.3652 (mse_score: 3.3652)\n",
      "23:19 madminer.utils.ml.sc INFO      Epoch 31: train loss 1.5453 (mse_score: 1.5453)\n",
      "23:19 madminer.utils.ml.sc INFO                val. loss  3.3568 (mse_score: 3.3568)\n",
      "23:19 madminer.utils.ml.sc INFO      Epoch 32: train loss 1.5328 (mse_score: 1.5328)\n",
      "23:19 madminer.utils.ml.sc INFO                val. loss  3.3756 (mse_score: 3.3756)\n",
      "23:19 madminer.utils.ml.sc INFO      Epoch 33: train loss 1.5221 (mse_score: 1.5221)\n",
      "23:19 madminer.utils.ml.sc INFO                val. loss  3.3576 (mse_score: 3.3576)\n",
      "23:20 madminer.utils.ml.sc INFO      Epoch 34: train loss 1.5108 (mse_score: 1.5108)\n",
      "23:20 madminer.utils.ml.sc INFO                val. loss  3.3624 (mse_score: 3.3624)\n",
      "23:20 madminer.utils.ml.sc INFO      Epoch 35: train loss 1.4966 (mse_score: 1.4966)\n",
      "23:20 madminer.utils.ml.sc INFO                val. loss  3.3619 (mse_score: 3.3619)\n",
      "23:20 madminer.utils.ml.sc INFO      Epoch 36: train loss 1.4890 (mse_score: 1.4890)\n",
      "23:20 madminer.utils.ml.sc INFO                val. loss  3.3738 (mse_score: 3.3738)\n",
      "23:21 madminer.utils.ml.sc INFO      Epoch 37: train loss 1.4796 (mse_score: 1.4796)\n",
      "23:21 madminer.utils.ml.sc INFO                val. loss  3.3532 (mse_score: 3.3532)\n",
      "23:21 madminer.utils.ml.sc INFO      Epoch 38: train loss 1.4697 (mse_score: 1.4697)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:21 madminer.utils.ml.sc INFO                val. loss  3.3558 (mse_score: 3.3558)\n",
      "23:22 madminer.utils.ml.sc INFO      Epoch 39: train loss 1.4614 (mse_score: 1.4614)\n",
      "23:22 madminer.utils.ml.sc INFO                val. loss  3.3410 (mse_score: 3.3410)\n",
      "23:22 madminer.utils.ml.sc INFO      Epoch 40: train loss 1.4538 (mse_score: 1.4538)\n",
      "23:22 madminer.utils.ml.sc INFO                val. loss  3.3542 (mse_score: 3.3542)\n",
      "23:22 madminer.utils.ml.sc INFO      Epoch 41: train loss 1.4431 (mse_score: 1.4431)\n",
      "23:22 madminer.utils.ml.sc INFO                val. loss  3.3496 (mse_score: 3.3496)\n",
      "23:23 madminer.utils.ml.sc INFO      Epoch 42: train loss 1.4374 (mse_score: 1.4374)\n",
      "23:23 madminer.utils.ml.sc INFO                val. loss  3.3542 (mse_score: 3.3542)\n",
      "23:23 madminer.utils.ml.sc INFO      Epoch 43: train loss 1.4276 (mse_score: 1.4276)\n",
      "23:23 madminer.utils.ml.sc INFO                val. loss  3.3429 (mse_score: 3.3429)\n",
      "23:23 madminer.utils.ml.sc INFO      Epoch 44: train loss 1.4236 (mse_score: 1.4236)\n",
      "23:23 madminer.utils.ml.sc INFO                val. loss  3.3473 (mse_score: 3.3473)\n",
      "23:24 madminer.utils.ml.sc INFO      Epoch 45: train loss 1.4150 (mse_score: 1.4150)\n",
      "23:24 madminer.utils.ml.sc INFO                val. loss  3.3451 (mse_score: 3.3451)\n",
      "23:24 madminer.utils.ml.sc INFO      Epoch 46: train loss 1.4081 (mse_score: 1.4081)\n",
      "23:24 madminer.utils.ml.sc INFO                val. loss  3.3503 (mse_score: 3.3503)\n",
      "23:25 madminer.utils.ml.sc INFO      Epoch 47: train loss 1.4039 (mse_score: 1.4039)\n",
      "23:25 madminer.utils.ml.sc INFO                val. loss  3.3585 (mse_score: 3.3585)\n",
      "23:25 madminer.utils.ml.sc INFO      Epoch 48: train loss 1.3973 (mse_score: 1.3973)\n",
      "23:25 madminer.utils.ml.sc INFO                val. loss  3.3543 (mse_score: 3.3543)\n",
      "23:25 madminer.utils.ml.sc INFO      Epoch 49: train loss 1.3914 (mse_score: 1.3914)\n",
      "23:25 madminer.utils.ml.sc INFO                val. loss  3.3458 (mse_score: 3.3458)\n",
      "23:26 madminer.utils.ml.sc INFO      Epoch 50: train loss 1.3859 (mse_score: 1.3859)\n",
      "23:26 madminer.utils.ml.sc INFO                val. loss  3.3446 (mse_score: 3.3446)\n",
      "23:26 madminer.utils.ml.sc INFO    Early stopping after epoch 22, with loss 3.34 compared to final loss 3.34\n",
      "23:26 madminer.utils.ml.sc INFO    Finished training\n",
      "23:26 madminer.ml          INFO    Training estimator 2 / 10 in ensemble\n",
      "23:26 madminer.ml          INFO    Starting training\n",
      "23:26 madminer.ml          INFO      Method:                 sally\n",
      "23:26 madminer.ml          INFO      Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_1.npy\n",
      "23:26 madminer.ml          INFO                     t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_1.npy\n",
      "23:26 madminer.ml          INFO      Features:               all\n",
      "23:26 madminer.ml          INFO      Method:                 sally\n",
      "23:26 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "23:26 madminer.ml          INFO      Activation function:    tanh\n",
      "23:26 madminer.ml          INFO      Batch size:             128\n",
      "23:26 madminer.ml          INFO      Trainer:                amsgrad\n",
      "23:26 madminer.ml          INFO      Epochs:                 50\n",
      "23:26 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "23:26 madminer.ml          INFO      Validation split:       0.5\n",
      "23:26 madminer.ml          INFO      Early stopping:         True\n",
      "23:26 madminer.ml          INFO      Scale inputs:           True\n",
      "23:26 madminer.ml          INFO      Shuffle labels          False\n",
      "23:26 madminer.ml          INFO      Regularization:         None\n",
      "23:26 madminer.ml          INFO    Loading training data\n",
      "23:26 madminer.ml          INFO    Found 1000000 samples with 2 parameters and 33 observables\n",
      "23:26 madminer.ml          INFO    Rescaling inputs\n",
      "23:26 madminer.ml          INFO    Creating model for method sally\n",
      "23:26 madminer.ml          INFO    Training model\n",
      "23:27 madminer.utils.ml.sc INFO      Epoch 1: train loss 2.6109 (mse_score: 2.6109)\n",
      "23:27 madminer.utils.ml.sc INFO                val. loss  2.4418 (mse_score: 2.4418) (*)\n",
      "23:27 madminer.utils.ml.sc INFO      Epoch 2: train loss 2.5032 (mse_score: 2.5032)\n",
      "23:27 madminer.utils.ml.sc INFO                val. loss  2.3302 (mse_score: 2.3302) (*)\n",
      "23:27 madminer.utils.ml.sc INFO      Epoch 3: train loss 2.4306 (mse_score: 2.4306)\n",
      "23:27 madminer.utils.ml.sc INFO                val. loss  2.2999 (mse_score: 2.2999) (*)\n",
      "23:28 madminer.utils.ml.sc INFO      Epoch 4: train loss 2.3795 (mse_score: 2.3795)\n",
      "23:28 madminer.utils.ml.sc INFO                val. loss  2.3378 (mse_score: 2.3378)\n",
      "23:28 madminer.utils.ml.sc INFO      Epoch 5: train loss 2.3416 (mse_score: 2.3416)\n",
      "23:28 madminer.utils.ml.sc INFO                val. loss  2.2238 (mse_score: 2.2238) (*)\n",
      "23:29 madminer.utils.ml.sc INFO      Epoch 6: train loss 2.3055 (mse_score: 2.3055)\n",
      "23:29 madminer.utils.ml.sc INFO                val. loss  2.2183 (mse_score: 2.2183) (*)\n",
      "23:29 madminer.utils.ml.sc INFO      Epoch 7: train loss 2.2714 (mse_score: 2.2714)\n",
      "23:29 madminer.utils.ml.sc INFO                val. loss  2.2911 (mse_score: 2.2911)\n",
      "23:30 madminer.utils.ml.sc INFO      Epoch 8: train loss 2.2375 (mse_score: 2.2375)\n",
      "23:30 madminer.utils.ml.sc INFO                val. loss  2.1929 (mse_score: 2.1929) (*)\n",
      "23:30 madminer.utils.ml.sc INFO      Epoch 9: train loss 2.2153 (mse_score: 2.2153)\n",
      "23:30 madminer.utils.ml.sc INFO                val. loss  2.1972 (mse_score: 2.1972)\n",
      "23:31 madminer.utils.ml.sc INFO      Epoch 10: train loss 2.1845 (mse_score: 2.1845)\n",
      "23:31 madminer.utils.ml.sc INFO                val. loss  2.1879 (mse_score: 2.1879) (*)\n",
      "23:31 madminer.utils.ml.sc INFO      Epoch 11: train loss 2.1495 (mse_score: 2.1495)\n",
      "23:31 madminer.utils.ml.sc INFO                val. loss  2.1830 (mse_score: 2.1830) (*)\n",
      "23:31 madminer.utils.ml.sc INFO      Epoch 12: train loss 2.1224 (mse_score: 2.1224)\n",
      "23:31 madminer.utils.ml.sc INFO                val. loss  2.2105 (mse_score: 2.2105)\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'all',\n",
    "    use_tight_cuts=False,\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'all_tight',\n",
    "    use_tight_cuts=True,\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal observable basis (no jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_obs = [0,1] + list(range(4,12)) + list(range(16,33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal',\n",
    "    use_tight_cuts=False,\n",
    "    features=[min_obs for _ in range(n_estimators)],\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[min_obs for _ in range(n_estimators)],\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just resurrection phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'phi_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[32] for _ in range(n_estimators)],\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (higgs_inference)",
   "language": "python",
   "name": "higgs_inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
