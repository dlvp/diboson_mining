{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Marco Farina, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)-5.5s %(name)-20.20s %(levelname)-7.7s %(message)s',\n",
    "    datefmt='%H:%M',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"madminer\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma_sys/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma_sys/'\n",
    "log_dir = base_dir + 'logs/wgamma_sys/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma_sys/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=n_estimators, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=True)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:13 madminer.ml          INFO    Training 10 estimators in ensemble\n",
      "13:13 madminer.ml          INFO    Training estimator 1 / 10 in ensemble\n",
      "13:13 madminer.ml          INFO    Starting training\n",
      "13:13 madminer.ml          INFO      Method:                 sally\n",
      "13:13 madminer.ml          INFO      Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_0.npy\n",
      "13:13 madminer.ml          INFO                     t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_0.npy\n",
      "13:13 madminer.ml          INFO      Features:               all\n",
      "13:13 madminer.ml          INFO      Method:                 sally\n",
      "13:13 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "13:13 madminer.ml          INFO      Activation function:    tanh\n",
      "13:13 madminer.ml          INFO      Batch size:             128\n",
      "13:13 madminer.ml          INFO      Trainer:                amsgrad\n",
      "13:13 madminer.ml          INFO      Epochs:                 50\n",
      "13:13 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:13 madminer.ml          INFO      Validation split:       0.5\n",
      "13:13 madminer.ml          INFO      Early stopping:         True\n",
      "13:13 madminer.ml          INFO      Scale inputs:           True\n",
      "13:13 madminer.ml          INFO      Shuffle labels          False\n",
      "13:13 madminer.ml          INFO      Regularization:         None\n",
      "13:13 madminer.ml          INFO    Loading training data\n",
      "13:13 madminer.ml          INFO    Found 1000000 samples with 57 parameters and 33 observables\n",
      "13:13 madminer.ml          INFO    Rescaling inputs\n",
      "13:13 madminer.ml          INFO    Creating model for method sally\n",
      "13:13 madminer.ml          INFO    Training model\n",
      "13:14 madminer.utils.ml.sc INFO      Epoch 01: train loss 0.1396 (mse_score: 0.1396)\n",
      "13:14 madminer.utils.ml.sc INFO                val. loss  0.0791 (mse_score: 0.0791) (*)\n",
      "13:14 madminer.utils.ml.sc INFO      Epoch 02: train loss 0.1359 (mse_score: 0.1359)\n",
      "13:14 madminer.utils.ml.sc INFO                val. loss  0.0768 (mse_score: 0.0768) (*)\n",
      "13:15 madminer.utils.ml.sc INFO      Epoch 03: train loss 0.1330 (mse_score: 0.1330)\n",
      "13:15 madminer.utils.ml.sc INFO                val. loss  0.0803 (mse_score: 0.0803)\n",
      "13:15 madminer.utils.ml.sc INFO      Epoch 04: train loss 0.1321 (mse_score: 0.1321)\n",
      "13:15 madminer.utils.ml.sc INFO                val. loss  0.0743 (mse_score: 0.0743) (*)\n",
      "13:16 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1295 (mse_score: 0.1295)\n",
      "13:16 madminer.utils.ml.sc INFO                val. loss  0.0730 (mse_score: 0.0730) (*)\n",
      "13:16 madminer.utils.ml.sc INFO      Epoch 06: train loss 0.1280 (mse_score: 0.1280)\n",
      "13:16 madminer.utils.ml.sc INFO                val. loss  0.0735 (mse_score: 0.0735)\n",
      "13:16 madminer.utils.ml.sc INFO      Epoch 07: train loss 0.1261 (mse_score: 0.1261)\n",
      "13:16 madminer.utils.ml.sc INFO                val. loss  0.0729 (mse_score: 0.0729) (*)\n",
      "13:17 madminer.utils.ml.sc INFO      Epoch 08: train loss 0.1239 (mse_score: 0.1239)\n",
      "13:17 madminer.utils.ml.sc INFO                val. loss  0.0739 (mse_score: 0.0739)\n",
      "13:17 madminer.utils.ml.sc INFO      Epoch 09: train loss 0.1220 (mse_score: 0.1220)\n",
      "13:17 madminer.utils.ml.sc INFO                val. loss  0.0713 (mse_score: 0.0713) (*)\n",
      "13:17 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.1207 (mse_score: 0.1207)\n",
      "13:17 madminer.utils.ml.sc INFO                val. loss  0.0709 (mse_score: 0.0709) (*)\n",
      "13:18 madminer.utils.ml.sc INFO      Epoch 11: train loss 0.1193 (mse_score: 0.1193)\n",
      "13:18 madminer.utils.ml.sc INFO                val. loss  0.0713 (mse_score: 0.0713)\n",
      "13:18 madminer.utils.ml.sc INFO      Epoch 12: train loss 0.1175 (mse_score: 0.1175)\n",
      "13:18 madminer.utils.ml.sc INFO                val. loss  0.0712 (mse_score: 0.0712)\n",
      "13:18 madminer.utils.ml.sc INFO      Epoch 13: train loss 0.1164 (mse_score: 0.1164)\n",
      "13:18 madminer.utils.ml.sc INFO                val. loss  0.0711 (mse_score: 0.0711)\n",
      "13:19 madminer.utils.ml.sc INFO      Epoch 14: train loss 0.1149 (mse_score: 0.1149)\n",
      "13:19 madminer.utils.ml.sc INFO                val. loss  0.0699 (mse_score: 0.0699) (*)\n",
      "13:19 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.1138 (mse_score: 0.1138)\n",
      "13:19 madminer.utils.ml.sc INFO                val. loss  0.0714 (mse_score: 0.0714)\n",
      "13:19 madminer.utils.ml.sc INFO      Epoch 16: train loss 0.1116 (mse_score: 0.1116)\n",
      "13:19 madminer.utils.ml.sc INFO                val. loss  0.0703 (mse_score: 0.0703)\n",
      "13:20 madminer.utils.ml.sc INFO      Epoch 17: train loss 0.1111 (mse_score: 0.1111)\n",
      "13:20 madminer.utils.ml.sc INFO                val. loss  0.0692 (mse_score: 0.0692) (*)\n",
      "13:20 madminer.utils.ml.sc INFO      Epoch 18: train loss 0.1096 (mse_score: 0.1096)\n",
      "13:20 madminer.utils.ml.sc INFO                val. loss  0.0691 (mse_score: 0.0691) (*)\n",
      "13:20 madminer.utils.ml.sc INFO      Epoch 19: train loss 0.1083 (mse_score: 0.1083)\n",
      "13:20 madminer.utils.ml.sc INFO                val. loss  0.0691 (mse_score: 0.0691)\n",
      "13:21 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.1072 (mse_score: 0.1072)\n",
      "13:21 madminer.utils.ml.sc INFO                val. loss  0.0690 (mse_score: 0.0690) (*)\n",
      "13:21 madminer.utils.ml.sc INFO      Epoch 21: train loss 0.1057 (mse_score: 0.1057)\n",
      "13:21 madminer.utils.ml.sc INFO                val. loss  0.0691 (mse_score: 0.0691)\n",
      "13:21 madminer.utils.ml.sc INFO      Epoch 22: train loss 0.1048 (mse_score: 0.1048)\n",
      "13:21 madminer.utils.ml.sc INFO                val. loss  0.0709 (mse_score: 0.0709)\n",
      "13:22 madminer.utils.ml.sc INFO      Epoch 23: train loss 0.1039 (mse_score: 0.1039)\n",
      "13:22 madminer.utils.ml.sc INFO                val. loss  0.0697 (mse_score: 0.0697)\n",
      "13:22 madminer.utils.ml.sc INFO      Epoch 24: train loss 0.1030 (mse_score: 0.1030)\n",
      "13:22 madminer.utils.ml.sc INFO                val. loss  0.0701 (mse_score: 0.0701)\n",
      "13:23 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.1020 (mse_score: 0.1020)\n",
      "13:23 madminer.utils.ml.sc INFO                val. loss  0.0705 (mse_score: 0.0705)\n",
      "13:23 madminer.utils.ml.sc INFO      Epoch 26: train loss 0.1009 (mse_score: 0.1009)\n",
      "13:23 madminer.utils.ml.sc INFO                val. loss  0.0692 (mse_score: 0.0692)\n",
      "13:23 madminer.utils.ml.sc INFO      Epoch 27: train loss 0.1003 (mse_score: 0.1003)\n",
      "13:23 madminer.utils.ml.sc INFO                val. loss  0.0699 (mse_score: 0.0699)\n",
      "13:24 madminer.utils.ml.sc INFO      Epoch 28: train loss 0.0992 (mse_score: 0.0992)\n",
      "13:24 madminer.utils.ml.sc INFO                val. loss  0.0708 (mse_score: 0.0708)\n",
      "13:24 madminer.utils.ml.sc INFO      Epoch 29: train loss 0.0984 (mse_score: 0.0984)\n",
      "13:24 madminer.utils.ml.sc INFO                val. loss  0.0710 (mse_score: 0.0710)\n",
      "13:25 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0978 (mse_score: 0.0978)\n",
      "13:25 madminer.utils.ml.sc INFO                val. loss  0.0699 (mse_score: 0.0699)\n",
      "13:25 madminer.utils.ml.sc INFO      Epoch 31: train loss 0.0969 (mse_score: 0.0969)\n",
      "13:25 madminer.utils.ml.sc INFO                val. loss  0.0698 (mse_score: 0.0698)\n",
      "13:25 madminer.utils.ml.sc INFO      Epoch 32: train loss 0.0962 (mse_score: 0.0962)\n",
      "13:25 madminer.utils.ml.sc INFO                val. loss  0.0704 (mse_score: 0.0704)\n",
      "13:26 madminer.utils.ml.sc INFO      Epoch 33: train loss 0.0956 (mse_score: 0.0956)\n",
      "13:26 madminer.utils.ml.sc INFO                val. loss  0.0720 (mse_score: 0.0720)\n",
      "13:26 madminer.utils.ml.sc INFO      Epoch 34: train loss 0.0949 (mse_score: 0.0949)\n",
      "13:26 madminer.utils.ml.sc INFO                val. loss  0.0697 (mse_score: 0.0697)\n",
      "13:27 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0942 (mse_score: 0.0942)\n",
      "13:27 madminer.utils.ml.sc INFO                val. loss  0.0701 (mse_score: 0.0701)\n",
      "13:27 madminer.utils.ml.sc INFO      Epoch 36: train loss 0.0936 (mse_score: 0.0936)\n",
      "13:27 madminer.utils.ml.sc INFO                val. loss  0.0704 (mse_score: 0.0704)\n",
      "13:27 madminer.utils.ml.sc INFO      Epoch 37: train loss 0.0929 (mse_score: 0.0929)\n",
      "13:27 madminer.utils.ml.sc INFO                val. loss  0.0711 (mse_score: 0.0711)\n",
      "13:28 madminer.utils.ml.sc INFO      Epoch 38: train loss 0.0924 (mse_score: 0.0924)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:28 madminer.utils.ml.sc INFO                val. loss  0.0709 (mse_score: 0.0709)\n",
      "13:28 madminer.utils.ml.sc INFO      Epoch 39: train loss 0.0919 (mse_score: 0.0919)\n",
      "13:28 madminer.utils.ml.sc INFO                val. loss  0.0708 (mse_score: 0.0708)\n",
      "13:28 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0913 (mse_score: 0.0913)\n",
      "13:28 madminer.utils.ml.sc INFO                val. loss  0.0715 (mse_score: 0.0715)\n",
      "13:29 madminer.utils.ml.sc INFO      Epoch 41: train loss 0.0909 (mse_score: 0.0909)\n",
      "13:29 madminer.utils.ml.sc INFO                val. loss  0.0712 (mse_score: 0.0712)\n",
      "13:29 madminer.utils.ml.sc INFO      Epoch 42: train loss 0.0904 (mse_score: 0.0904)\n",
      "13:29 madminer.utils.ml.sc INFO                val. loss  0.0706 (mse_score: 0.0706)\n",
      "13:30 madminer.utils.ml.sc INFO      Epoch 43: train loss 0.0900 (mse_score: 0.0900)\n",
      "13:30 madminer.utils.ml.sc INFO                val. loss  0.0708 (mse_score: 0.0708)\n",
      "13:30 madminer.utils.ml.sc INFO      Epoch 44: train loss 0.0895 (mse_score: 0.0895)\n",
      "13:30 madminer.utils.ml.sc INFO                val. loss  0.0710 (mse_score: 0.0710)\n",
      "13:31 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0892 (mse_score: 0.0892)\n",
      "13:31 madminer.utils.ml.sc INFO                val. loss  0.0713 (mse_score: 0.0713)\n",
      "13:31 madminer.utils.ml.sc INFO      Epoch 46: train loss 0.0888 (mse_score: 0.0888)\n",
      "13:31 madminer.utils.ml.sc INFO                val. loss  0.0709 (mse_score: 0.0709)\n",
      "13:32 madminer.utils.ml.sc INFO      Epoch 47: train loss 0.0884 (mse_score: 0.0884)\n",
      "13:32 madminer.utils.ml.sc INFO                val. loss  0.0706 (mse_score: 0.0706)\n",
      "13:32 madminer.utils.ml.sc INFO      Epoch 48: train loss 0.0880 (mse_score: 0.0880)\n",
      "13:32 madminer.utils.ml.sc INFO                val. loss  0.0723 (mse_score: 0.0723)\n",
      "13:33 madminer.utils.ml.sc INFO      Epoch 49: train loss 0.0877 (mse_score: 0.0877)\n",
      "13:33 madminer.utils.ml.sc INFO                val. loss  0.0712 (mse_score: 0.0712)\n",
      "13:33 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0874 (mse_score: 0.0874)\n",
      "13:33 madminer.utils.ml.sc INFO                val. loss  0.0715 (mse_score: 0.0715)\n",
      "13:33 madminer.utils.ml.sc INFO    Early stopping after epoch 20, with loss 0.07 compared to final loss 0.07\n",
      "13:33 madminer.utils.ml.sc INFO    Finished training\n",
      "13:33 madminer.ml          INFO    Training estimator 2 / 10 in ensemble\n",
      "13:33 madminer.ml          INFO    Starting training\n",
      "13:33 madminer.ml          INFO      Method:                 sally\n",
      "13:33 madminer.ml          INFO      Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_1.npy\n",
      "13:33 madminer.ml          INFO                     t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_1.npy\n",
      "13:33 madminer.ml          INFO      Features:               all\n",
      "13:33 madminer.ml          INFO      Method:                 sally\n",
      "13:33 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "13:33 madminer.ml          INFO      Activation function:    tanh\n",
      "13:33 madminer.ml          INFO      Batch size:             128\n",
      "13:33 madminer.ml          INFO      Trainer:                amsgrad\n",
      "13:33 madminer.ml          INFO      Epochs:                 50\n",
      "13:33 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:33 madminer.ml          INFO      Validation split:       0.5\n",
      "13:33 madminer.ml          INFO      Early stopping:         True\n",
      "13:33 madminer.ml          INFO      Scale inputs:           True\n",
      "13:33 madminer.ml          INFO      Shuffle labels          False\n",
      "13:33 madminer.ml          INFO      Regularization:         None\n",
      "13:33 madminer.ml          INFO    Loading training data\n",
      "13:33 madminer.ml          INFO    Found 1000000 samples with 57 parameters and 33 observables\n",
      "13:33 madminer.ml          INFO    Rescaling inputs\n",
      "13:33 madminer.ml          INFO    Creating model for method sally\n",
      "13:33 madminer.ml          INFO    Training model\n",
      "13:34 madminer.utils.ml.sc INFO      Epoch 01: train loss 0.1181 (mse_score: 0.1181)\n",
      "13:34 madminer.utils.ml.sc INFO                val. loss  0.1555 (mse_score: 0.1555) (*)\n",
      "13:34 madminer.utils.ml.sc INFO      Epoch 02: train loss 0.1148 (mse_score: 0.1148)\n",
      "13:34 madminer.utils.ml.sc INFO                val. loss  0.1531 (mse_score: 0.1531) (*)\n",
      "13:35 madminer.utils.ml.sc INFO      Epoch 03: train loss 0.1131 (mse_score: 0.1131)\n",
      "13:35 madminer.utils.ml.sc INFO                val. loss  0.1533 (mse_score: 0.1533)\n",
      "13:35 madminer.utils.ml.sc INFO      Epoch 04: train loss 0.1119 (mse_score: 0.1119)\n",
      "13:35 madminer.utils.ml.sc INFO                val. loss  0.1504 (mse_score: 0.1504) (*)\n",
      "13:36 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1105 (mse_score: 0.1105)\n",
      "13:36 madminer.utils.ml.sc INFO                val. loss  0.1490 (mse_score: 0.1490) (*)\n",
      "13:36 madminer.utils.ml.sc INFO      Epoch 06: train loss 0.1091 (mse_score: 0.1091)\n",
      "13:36 madminer.utils.ml.sc INFO                val. loss  0.1488 (mse_score: 0.1488) (*)\n",
      "13:37 madminer.utils.ml.sc INFO      Epoch 07: train loss 0.1086 (mse_score: 0.1086)\n",
      "13:37 madminer.utils.ml.sc INFO                val. loss  0.1468 (mse_score: 0.1468) (*)\n",
      "13:37 madminer.utils.ml.sc INFO      Epoch 08: train loss 0.1076 (mse_score: 0.1076)\n",
      "13:37 madminer.utils.ml.sc INFO                val. loss  0.1481 (mse_score: 0.1481)\n",
      "13:37 madminer.utils.ml.sc INFO      Epoch 09: train loss 0.1064 (mse_score: 0.1064)\n",
      "13:37 madminer.utils.ml.sc INFO                val. loss  0.1463 (mse_score: 0.1463) (*)\n",
      "13:38 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.1052 (mse_score: 0.1052)\n",
      "13:38 madminer.utils.ml.sc INFO                val. loss  0.1461 (mse_score: 0.1461) (*)\n",
      "13:38 madminer.utils.ml.sc INFO      Epoch 11: train loss 0.1048 (mse_score: 0.1048)\n",
      "13:38 madminer.utils.ml.sc INFO                val. loss  0.1479 (mse_score: 0.1479)\n",
      "13:39 madminer.utils.ml.sc INFO      Epoch 12: train loss 0.1037 (mse_score: 0.1037)\n",
      "13:39 madminer.utils.ml.sc INFO                val. loss  0.1457 (mse_score: 0.1457) (*)\n",
      "13:39 madminer.utils.ml.sc INFO      Epoch 13: train loss 0.1029 (mse_score: 0.1029)\n",
      "13:39 madminer.utils.ml.sc INFO                val. loss  0.1469 (mse_score: 0.1469)\n",
      "13:40 madminer.utils.ml.sc INFO      Epoch 14: train loss 0.1022 (mse_score: 0.1022)\n",
      "13:40 madminer.utils.ml.sc INFO                val. loss  0.1444 (mse_score: 0.1444) (*)\n",
      "13:40 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.1016 (mse_score: 0.1016)\n",
      "13:40 madminer.utils.ml.sc INFO                val. loss  0.1445 (mse_score: 0.1445)\n",
      "13:41 madminer.utils.ml.sc INFO      Epoch 16: train loss 0.1005 (mse_score: 0.1005)\n",
      "13:41 madminer.utils.ml.sc INFO                val. loss  0.1455 (mse_score: 0.1455)\n",
      "13:41 madminer.utils.ml.sc INFO      Epoch 17: train loss 0.0996 (mse_score: 0.0996)\n",
      "13:41 madminer.utils.ml.sc INFO                val. loss  0.1451 (mse_score: 0.1451)\n",
      "13:42 madminer.utils.ml.sc INFO      Epoch 18: train loss 0.0986 (mse_score: 0.0986)\n",
      "13:42 madminer.utils.ml.sc INFO                val. loss  0.1448 (mse_score: 0.1448)\n",
      "13:42 madminer.utils.ml.sc INFO      Epoch 19: train loss 0.0979 (mse_score: 0.0979)\n",
      "13:42 madminer.utils.ml.sc INFO                val. loss  0.1443 (mse_score: 0.1443) (*)\n",
      "13:43 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0974 (mse_score: 0.0974)\n",
      "13:43 madminer.utils.ml.sc INFO                val. loss  0.1433 (mse_score: 0.1433) (*)\n",
      "13:43 madminer.utils.ml.sc INFO      Epoch 21: train loss 0.0967 (mse_score: 0.0967)\n",
      "13:43 madminer.utils.ml.sc INFO                val. loss  0.1449 (mse_score: 0.1449)\n",
      "13:43 madminer.utils.ml.sc INFO      Epoch 22: train loss 0.0957 (mse_score: 0.0957)\n",
      "13:43 madminer.utils.ml.sc INFO                val. loss  0.1433 (mse_score: 0.1433)\n",
      "13:44 madminer.utils.ml.sc INFO      Epoch 23: train loss 0.0950 (mse_score: 0.0950)\n",
      "13:44 madminer.utils.ml.sc INFO                val. loss  0.1436 (mse_score: 0.1436)\n",
      "13:44 madminer.utils.ml.sc INFO      Epoch 24: train loss 0.0941 (mse_score: 0.0941)\n",
      "13:44 madminer.utils.ml.sc INFO                val. loss  0.1437 (mse_score: 0.1437)\n",
      "13:45 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0937 (mse_score: 0.0937)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:45 madminer.utils.ml.sc INFO                val. loss  0.1440 (mse_score: 0.1440)\n",
      "13:45 madminer.utils.ml.sc INFO      Epoch 26: train loss 0.0928 (mse_score: 0.0928)\n",
      "13:45 madminer.utils.ml.sc INFO                val. loss  0.1447 (mse_score: 0.1447)\n",
      "13:45 madminer.utils.ml.sc INFO      Epoch 27: train loss 0.0921 (mse_score: 0.0921)\n",
      "13:45 madminer.utils.ml.sc INFO                val. loss  0.1429 (mse_score: 0.1429) (*)\n",
      "13:46 madminer.utils.ml.sc INFO      Epoch 28: train loss 0.0918 (mse_score: 0.0918)\n",
      "13:46 madminer.utils.ml.sc INFO                val. loss  0.1449 (mse_score: 0.1449)\n",
      "13:46 madminer.utils.ml.sc INFO      Epoch 29: train loss 0.0910 (mse_score: 0.0910)\n",
      "13:46 madminer.utils.ml.sc INFO                val. loss  0.1425 (mse_score: 0.1425) (*)\n",
      "13:47 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0906 (mse_score: 0.0906)\n",
      "13:47 madminer.utils.ml.sc INFO                val. loss  0.1440 (mse_score: 0.1440)\n",
      "13:47 madminer.utils.ml.sc INFO      Epoch 31: train loss 0.0902 (mse_score: 0.0902)\n",
      "13:47 madminer.utils.ml.sc INFO                val. loss  0.1430 (mse_score: 0.1430)\n",
      "13:47 madminer.utils.ml.sc INFO      Epoch 32: train loss 0.0895 (mse_score: 0.0895)\n",
      "13:47 madminer.utils.ml.sc INFO                val. loss  0.1433 (mse_score: 0.1433)\n",
      "13:48 madminer.utils.ml.sc INFO      Epoch 33: train loss 0.0891 (mse_score: 0.0891)\n",
      "13:48 madminer.utils.ml.sc INFO                val. loss  0.1426 (mse_score: 0.1426)\n",
      "13:49 madminer.utils.ml.sc INFO      Epoch 34: train loss 0.0886 (mse_score: 0.0886)\n",
      "13:49 madminer.utils.ml.sc INFO                val. loss  0.1428 (mse_score: 0.1428)\n",
      "13:49 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0883 (mse_score: 0.0883)\n",
      "13:49 madminer.utils.ml.sc INFO                val. loss  0.1422 (mse_score: 0.1422) (*)\n",
      "13:50 madminer.utils.ml.sc INFO      Epoch 36: train loss 0.0876 (mse_score: 0.0876)\n",
      "13:50 madminer.utils.ml.sc INFO                val. loss  0.1427 (mse_score: 0.1427)\n",
      "13:50 madminer.utils.ml.sc INFO      Epoch 37: train loss 0.0874 (mse_score: 0.0874)\n",
      "13:50 madminer.utils.ml.sc INFO                val. loss  0.1423 (mse_score: 0.1423)\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'all',\n",
    "    use_tight_cuts=False,\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'all_tight',\n",
    "    use_tight_cuts=True,\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal observable basis (no jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_obs = [0,1] + list(range(4,12)) + list(range(16,33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal',\n",
    "    use_tight_cuts=False,\n",
    "    features=[min_obs for _ in range(n_estimators)],\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[min_obs for _ in range(n_estimators)],\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just resurrection phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'phi_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[32] for _ in range(n_estimators)],\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (higgs_inference)",
   "language": "python",
   "name": "higgs_inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
