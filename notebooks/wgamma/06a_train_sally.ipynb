{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Marco Farina, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)-5.5s %(name)-20.20s %(levelname)-7.7s %(message)s',\n",
    "    datefmt='%H:%M',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"madminer\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma_sys/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma_sys/'\n",
    "log_dir = base_dir + 'logs/wgamma_sys/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma_sys/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, n_estimators=n_estimators, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators, debug=True)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.calculate_expectation(\n",
    "        x_filename=sample_dir + 'validation{}/x_validation.npy'.format(cut_label)\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'all',\n",
    "    use_tight_cuts=False,\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:40 madminer.ml          INFO    Training 3 estimators in ensemble\n",
      "16:40 madminer.ml          INFO    Training estimator 1 / 3 in ensemble\n",
      "16:40 madminer.ml          INFO    Starting training\n",
      "16:40 madminer.ml          INFO      Method:                 sally\n",
      "16:40 madminer.ml          INFO      Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_tight/x_train_0.npy\n",
      "16:40 madminer.ml          INFO                     t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_tight/t_xz_train_0.npy\n",
      "16:40 madminer.ml          INFO      Features:               all\n",
      "16:40 madminer.ml          INFO      Method:                 sally\n",
      "16:40 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "16:40 madminer.ml          INFO      Activation function:    tanh\n",
      "16:40 madminer.ml          INFO      Batch size:             128\n",
      "16:40 madminer.ml          INFO      Trainer:                amsgrad\n",
      "16:40 madminer.ml          INFO      Epochs:                 50\n",
      "16:40 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:40 madminer.ml          INFO      Validation split:       0.5\n",
      "16:40 madminer.ml          INFO      Early stopping:         True\n",
      "16:40 madminer.ml          INFO      Scale inputs:           True\n",
      "16:40 madminer.ml          INFO      Shuffle labels          False\n",
      "16:40 madminer.ml          INFO      Regularization:         None\n",
      "16:40 madminer.ml          INFO    Loading training data\n",
      "16:40 madminer.ml          INFO    Found 1000000 samples with 57 parameters and 33 observables\n",
      "16:40 madminer.ml          INFO    Rescaling inputs\n",
      "16:40 madminer.ml          INFO    Creating model for method sally\n",
      "16:40 madminer.ml          INFO    Training model\n",
      "16:41 madminer.utils.ml.sc INFO      Epoch 1: train loss 173.1293 (mse_score: 173.1293)\n",
      "16:41 madminer.utils.ml.sc INFO                val. loss  161.6374 (mse_score: 161.6374) (*)\n",
      "16:42 madminer.utils.ml.sc INFO      Epoch 2: train loss 163.5011 (mse_score: 163.5011)\n",
      "16:42 madminer.utils.ml.sc INFO                val. loss  157.0922 (mse_score: 157.0922) (*)\n",
      "16:43 madminer.utils.ml.sc INFO      Epoch 3: train loss 160.3444 (mse_score: 160.3444)\n",
      "16:43 madminer.utils.ml.sc INFO                val. loss  155.3801 (mse_score: 155.3801) (*)\n",
      "16:43 madminer.utils.ml.sc INFO      Epoch 4: train loss 158.5215 (mse_score: 158.5215)\n",
      "16:43 madminer.utils.ml.sc INFO                val. loss  154.0949 (mse_score: 154.0949) (*)\n",
      "16:44 madminer.utils.ml.sc INFO      Epoch 5: train loss 157.0420 (mse_score: 157.0420)\n",
      "16:44 madminer.utils.ml.sc INFO                val. loss  152.6917 (mse_score: 152.6917) (*)\n",
      "16:44 madminer.utils.ml.sc INFO      Epoch 6: train loss 155.7157 (mse_score: 155.7157)\n",
      "16:44 madminer.utils.ml.sc INFO                val. loss  151.7935 (mse_score: 151.7935) (*)\n",
      "16:44 madminer.utils.ml.sc INFO      Epoch 7: train loss 154.6361 (mse_score: 154.6361)\n",
      "16:44 madminer.utils.ml.sc INFO                val. loss  151.1749 (mse_score: 151.1749) (*)\n",
      "16:45 madminer.utils.ml.sc INFO      Epoch 8: train loss 153.6396 (mse_score: 153.6396)\n",
      "16:45 madminer.utils.ml.sc INFO                val. loss  150.3923 (mse_score: 150.3923) (*)\n",
      "16:45 madminer.utils.ml.sc INFO      Epoch 9: train loss 152.7717 (mse_score: 152.7717)\n",
      "16:45 madminer.utils.ml.sc INFO                val. loss  150.3205 (mse_score: 150.3205) (*)\n",
      "16:45 madminer.utils.ml.sc INFO      Epoch 10: train loss 152.0376 (mse_score: 152.0376)\n",
      "16:45 madminer.utils.ml.sc INFO                val. loss  148.8455 (mse_score: 148.8455) (*)\n",
      "16:46 madminer.utils.ml.sc INFO      Epoch 11: train loss 151.1637 (mse_score: 151.1637)\n",
      "16:46 madminer.utils.ml.sc INFO                val. loss  148.5244 (mse_score: 148.5244) (*)\n",
      "16:46 madminer.utils.ml.sc INFO      Epoch 12: train loss 150.5297 (mse_score: 150.5297)\n",
      "16:46 madminer.utils.ml.sc INFO                val. loss  148.1189 (mse_score: 148.1189) (*)\n",
      "16:47 madminer.utils.ml.sc INFO      Epoch 13: train loss 149.8857 (mse_score: 149.8857)\n",
      "16:47 madminer.utils.ml.sc INFO                val. loss  147.4369 (mse_score: 147.4369) (*)\n",
      "16:47 madminer.utils.ml.sc INFO      Epoch 14: train loss 149.1502 (mse_score: 149.1502)\n",
      "16:47 madminer.utils.ml.sc INFO                val. loss  147.0086 (mse_score: 147.0086) (*)\n",
      "16:47 madminer.utils.ml.sc INFO      Epoch 15: train loss 148.6234 (mse_score: 148.6234)\n",
      "16:47 madminer.utils.ml.sc INFO                val. loss  146.8694 (mse_score: 146.8694) (*)\n",
      "16:48 madminer.utils.ml.sc INFO      Epoch 16: train loss 148.0406 (mse_score: 148.0406)\n",
      "16:48 madminer.utils.ml.sc INFO                val. loss  146.1785 (mse_score: 146.1785) (*)\n",
      "16:48 madminer.utils.ml.sc INFO      Epoch 17: train loss 147.4702 (mse_score: 147.4702)\n",
      "16:48 madminer.utils.ml.sc INFO                val. loss  145.7860 (mse_score: 145.7860) (*)\n",
      "16:49 madminer.utils.ml.sc INFO      Epoch 18: train loss 146.9153 (mse_score: 146.9153)\n",
      "16:49 madminer.utils.ml.sc INFO                val. loss  145.4073 (mse_score: 145.4073) (*)\n",
      "16:49 madminer.utils.ml.sc INFO      Epoch 19: train loss 146.4663 (mse_score: 146.4663)\n",
      "16:49 madminer.utils.ml.sc INFO                val. loss  145.1449 (mse_score: 145.1449) (*)\n",
      "16:50 madminer.utils.ml.sc INFO      Epoch 20: train loss 145.9741 (mse_score: 145.9741)\n",
      "16:50 madminer.utils.ml.sc INFO                val. loss  144.9860 (mse_score: 144.9860) (*)\n",
      "16:50 madminer.utils.ml.sc INFO      Epoch 21: train loss 145.6268 (mse_score: 145.6268)\n",
      "16:50 madminer.utils.ml.sc INFO                val. loss  144.9267 (mse_score: 144.9267) (*)\n",
      "16:50 madminer.utils.ml.sc INFO      Epoch 22: train loss 145.1349 (mse_score: 145.1349)\n",
      "16:50 madminer.utils.ml.sc INFO                val. loss  144.3405 (mse_score: 144.3405) (*)\n",
      "16:51 madminer.utils.ml.sc INFO      Epoch 23: train loss 144.8707 (mse_score: 144.8707)\n",
      "16:51 madminer.utils.ml.sc INFO                val. loss  144.2433 (mse_score: 144.2433) (*)\n",
      "16:51 madminer.utils.ml.sc INFO      Epoch 24: train loss 144.5396 (mse_score: 144.5396)\n",
      "16:51 madminer.utils.ml.sc INFO                val. loss  143.8115 (mse_score: 143.8115) (*)\n",
      "16:52 madminer.utils.ml.sc INFO      Epoch 25: train loss 144.1174 (mse_score: 144.1174)\n",
      "16:52 madminer.utils.ml.sc INFO                val. loss  143.7202 (mse_score: 143.7202) (*)\n",
      "16:52 madminer.utils.ml.sc INFO      Epoch 26: train loss 143.8060 (mse_score: 143.8060)\n",
      "16:52 madminer.utils.ml.sc INFO                val. loss  143.5945 (mse_score: 143.5945) (*)\n",
      "16:52 madminer.utils.ml.sc INFO      Epoch 27: train loss 143.5258 (mse_score: 143.5258)\n",
      "16:52 madminer.utils.ml.sc INFO                val. loss  143.2812 (mse_score: 143.2812) (*)\n",
      "16:53 madminer.utils.ml.sc INFO      Epoch 28: train loss 143.2584 (mse_score: 143.2584)\n",
      "16:53 madminer.utils.ml.sc INFO                val. loss  143.1123 (mse_score: 143.1123) (*)\n",
      "16:53 madminer.utils.ml.sc INFO      Epoch 29: train loss 142.9171 (mse_score: 142.9171)\n",
      "16:53 madminer.utils.ml.sc INFO                val. loss  142.9784 (mse_score: 142.9784) (*)\n",
      "16:53 madminer.utils.ml.sc INFO      Epoch 30: train loss 142.7277 (mse_score: 142.7277)\n",
      "16:53 madminer.utils.ml.sc INFO                val. loss  142.8082 (mse_score: 142.8082) (*)\n",
      "16:54 madminer.utils.ml.sc INFO      Epoch 31: train loss 142.4632 (mse_score: 142.4632)\n",
      "16:54 madminer.utils.ml.sc INFO                val. loss  142.5956 (mse_score: 142.5956) (*)\n",
      "16:54 madminer.utils.ml.sc INFO      Epoch 32: train loss 142.2114 (mse_score: 142.2114)\n",
      "16:54 madminer.utils.ml.sc INFO                val. loss  142.6102 (mse_score: 142.6102)\n",
      "16:54 madminer.utils.ml.sc INFO      Epoch 33: train loss 142.0900 (mse_score: 142.0900)\n",
      "16:54 madminer.utils.ml.sc INFO                val. loss  142.4235 (mse_score: 142.4235) (*)\n",
      "16:55 madminer.utils.ml.sc INFO      Epoch 34: train loss 141.7509 (mse_score: 141.7509)\n",
      "16:55 madminer.utils.ml.sc INFO                val. loss  142.1698 (mse_score: 142.1698) (*)\n",
      "16:55 madminer.utils.ml.sc INFO      Epoch 35: train loss 141.5839 (mse_score: 141.5839)\n",
      "16:55 madminer.utils.ml.sc INFO                val. loss  142.1596 (mse_score: 142.1596) (*)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:55 madminer.utils.ml.sc INFO      Epoch 36: train loss 141.4577 (mse_score: 141.4577)\n",
      "16:55 madminer.utils.ml.sc INFO                val. loss  141.9442 (mse_score: 141.9442) (*)\n",
      "16:56 madminer.utils.ml.sc INFO      Epoch 37: train loss 141.2071 (mse_score: 141.2071)\n",
      "16:56 madminer.utils.ml.sc INFO                val. loss  141.9816 (mse_score: 141.9816)\n",
      "16:56 madminer.utils.ml.sc INFO      Epoch 38: train loss 141.0516 (mse_score: 141.0516)\n",
      "16:56 madminer.utils.ml.sc INFO                val. loss  141.8954 (mse_score: 141.8954) (*)\n",
      "16:56 madminer.utils.ml.sc INFO      Epoch 39: train loss 140.8780 (mse_score: 140.8780)\n",
      "16:56 madminer.utils.ml.sc INFO                val. loss  141.7160 (mse_score: 141.7160) (*)\n",
      "16:57 madminer.utils.ml.sc INFO      Epoch 40: train loss 140.7259 (mse_score: 140.7259)\n",
      "16:57 madminer.utils.ml.sc INFO                val. loss  142.4466 (mse_score: 142.4466)\n",
      "16:57 madminer.utils.ml.sc INFO      Epoch 41: train loss 140.5936 (mse_score: 140.5936)\n",
      "16:57 madminer.utils.ml.sc INFO                val. loss  141.6328 (mse_score: 141.6328) (*)\n",
      "16:58 madminer.utils.ml.sc INFO      Epoch 42: train loss 140.4391 (mse_score: 140.4391)\n",
      "16:58 madminer.utils.ml.sc INFO                val. loss  141.5491 (mse_score: 141.5491) (*)\n",
      "16:58 madminer.utils.ml.sc INFO      Epoch 43: train loss 140.3128 (mse_score: 140.3128)\n",
      "16:58 madminer.utils.ml.sc INFO                val. loss  141.4674 (mse_score: 141.4674) (*)\n",
      "16:58 madminer.utils.ml.sc INFO      Epoch 44: train loss 140.1936 (mse_score: 140.1936)\n",
      "16:58 madminer.utils.ml.sc INFO                val. loss  141.4127 (mse_score: 141.4127) (*)\n",
      "16:59 madminer.utils.ml.sc INFO      Epoch 45: train loss 140.0491 (mse_score: 140.0491)\n",
      "16:59 madminer.utils.ml.sc INFO                val. loss  141.2801 (mse_score: 141.2801) (*)\n",
      "16:59 madminer.utils.ml.sc INFO      Epoch 46: train loss 139.9447 (mse_score: 139.9447)\n",
      "16:59 madminer.utils.ml.sc INFO                val. loss  141.2957 (mse_score: 141.2957)\n",
      "17:00 madminer.utils.ml.sc INFO      Epoch 47: train loss 139.8416 (mse_score: 139.8416)\n",
      "17:00 madminer.utils.ml.sc INFO                val. loss  141.3165 (mse_score: 141.3165)\n",
      "17:00 madminer.utils.ml.sc INFO      Epoch 48: train loss 139.7546 (mse_score: 139.7546)\n",
      "17:00 madminer.utils.ml.sc INFO                val. loss  141.1695 (mse_score: 141.1695) (*)\n",
      "17:00 madminer.utils.ml.sc INFO      Epoch 49: train loss 139.6259 (mse_score: 139.6259)\n",
      "17:00 madminer.utils.ml.sc INFO                val. loss  141.0997 (mse_score: 141.0997) (*)\n",
      "17:01 madminer.utils.ml.sc INFO      Epoch 50: train loss 139.5266 (mse_score: 139.5266)\n",
      "17:01 madminer.utils.ml.sc INFO                val. loss  141.0587 (mse_score: 141.0587) (*)\n",
      "17:01 madminer.utils.ml.sc INFO    Early stopping did not improve performance\n",
      "17:01 madminer.utils.ml.sc INFO    Finished training\n",
      "17:01 madminer.ml          INFO    Training estimator 2 / 3 in ensemble\n",
      "17:01 madminer.ml          INFO    Starting training\n",
      "17:01 madminer.ml          INFO      Method:                 sally\n",
      "17:01 madminer.ml          INFO      Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_tight/x_train_1.npy\n",
      "17:01 madminer.ml          INFO                     t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_tight/t_xz_train_1.npy\n",
      "17:01 madminer.ml          INFO      Features:               all\n",
      "17:01 madminer.ml          INFO      Method:                 sally\n",
      "17:01 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "17:01 madminer.ml          INFO      Activation function:    tanh\n",
      "17:01 madminer.ml          INFO      Batch size:             128\n",
      "17:01 madminer.ml          INFO      Trainer:                amsgrad\n",
      "17:01 madminer.ml          INFO      Epochs:                 50\n",
      "17:01 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "17:01 madminer.ml          INFO      Validation split:       0.5\n",
      "17:01 madminer.ml          INFO      Early stopping:         True\n",
      "17:01 madminer.ml          INFO      Scale inputs:           True\n",
      "17:01 madminer.ml          INFO      Shuffle labels          False\n",
      "17:01 madminer.ml          INFO      Regularization:         None\n",
      "17:01 madminer.ml          INFO    Loading training data\n",
      "17:01 madminer.ml          INFO    Found 1000000 samples with 57 parameters and 33 observables\n",
      "17:01 madminer.ml          INFO    Rescaling inputs\n",
      "17:01 madminer.ml          INFO    Creating model for method sally\n",
      "17:01 madminer.ml          INFO    Training model\n",
      "17:02 madminer.utils.ml.sc INFO      Epoch 1: train loss 174.4399 (mse_score: 174.4399)\n",
      "17:02 madminer.utils.ml.sc INFO                val. loss  165.7449 (mse_score: 165.7449) (*)\n",
      "17:02 madminer.utils.ml.sc INFO      Epoch 2: train loss 165.2730 (mse_score: 165.2730)\n",
      "17:02 madminer.utils.ml.sc INFO                val. loss  161.9360 (mse_score: 161.9360) (*)\n",
      "17:02 madminer.utils.ml.sc INFO      Epoch 3: train loss 162.1069 (mse_score: 162.1069)\n",
      "17:02 madminer.utils.ml.sc INFO                val. loss  159.0038 (mse_score: 159.0038) (*)\n",
      "17:03 madminer.utils.ml.sc INFO      Epoch 4: train loss 159.8646 (mse_score: 159.8646)\n",
      "17:03 madminer.utils.ml.sc INFO                val. loss  157.9636 (mse_score: 157.9636) (*)\n",
      "17:03 madminer.utils.ml.sc INFO      Epoch 5: train loss 158.1762 (mse_score: 158.1762)\n",
      "17:03 madminer.utils.ml.sc INFO                val. loss  156.1982 (mse_score: 156.1982) (*)\n",
      "17:03 madminer.utils.ml.sc INFO      Epoch 6: train loss 157.0567 (mse_score: 157.0567)\n",
      "17:03 madminer.utils.ml.sc INFO                val. loss  155.0632 (mse_score: 155.0632) (*)\n",
      "17:04 madminer.utils.ml.sc INFO      Epoch 7: train loss 156.1964 (mse_score: 156.1964)\n",
      "17:04 madminer.utils.ml.sc INFO                val. loss  154.5154 (mse_score: 154.5154) (*)\n",
      "17:04 madminer.utils.ml.sc INFO      Epoch 8: train loss 155.0288 (mse_score: 155.0288)\n",
      "17:04 madminer.utils.ml.sc INFO                val. loss  153.8278 (mse_score: 153.8278) (*)\n",
      "17:04 madminer.utils.ml.sc INFO      Epoch 9: train loss 154.2155 (mse_score: 154.2155)\n",
      "17:04 madminer.utils.ml.sc INFO                val. loss  152.8512 (mse_score: 152.8512) (*)\n",
      "17:05 madminer.utils.ml.sc INFO      Epoch 10: train loss 153.4690 (mse_score: 153.4690)\n",
      "17:05 madminer.utils.ml.sc INFO                val. loss  152.7346 (mse_score: 152.7346) (*)\n",
      "17:05 madminer.utils.ml.sc INFO      Epoch 11: train loss 152.7712 (mse_score: 152.7712)\n",
      "17:05 madminer.utils.ml.sc INFO                val. loss  151.9842 (mse_score: 151.9842) (*)\n",
      "17:05 madminer.utils.ml.sc INFO      Epoch 12: train loss 152.1083 (mse_score: 152.1083)\n",
      "17:05 madminer.utils.ml.sc INFO                val. loss  151.6662 (mse_score: 151.6662) (*)\n",
      "17:06 madminer.utils.ml.sc INFO      Epoch 13: train loss 151.4478 (mse_score: 151.4478)\n",
      "17:06 madminer.utils.ml.sc INFO                val. loss  150.8160 (mse_score: 150.8160) (*)\n",
      "17:06 madminer.utils.ml.sc INFO      Epoch 14: train loss 150.7813 (mse_score: 150.7813)\n",
      "17:06 madminer.utils.ml.sc INFO                val. loss  150.7112 (mse_score: 150.7112) (*)\n",
      "17:06 madminer.utils.ml.sc INFO      Epoch 15: train loss 150.2592 (mse_score: 150.2592)\n",
      "17:06 madminer.utils.ml.sc INFO                val. loss  150.0022 (mse_score: 150.0022) (*)\n",
      "17:07 madminer.utils.ml.sc INFO      Epoch 16: train loss 149.6610 (mse_score: 149.6610)\n",
      "17:07 madminer.utils.ml.sc INFO                val. loss  149.8401 (mse_score: 149.8401) (*)\n",
      "17:07 madminer.utils.ml.sc INFO      Epoch 17: train loss 149.1155 (mse_score: 149.1155)\n",
      "17:07 madminer.utils.ml.sc INFO                val. loss  149.6362 (mse_score: 149.6362) (*)\n",
      "17:07 madminer.utils.ml.sc INFO      Epoch 18: train loss 148.6752 (mse_score: 148.6752)\n",
      "17:07 madminer.utils.ml.sc INFO                val. loss  149.0497 (mse_score: 149.0497) (*)\n",
      "17:08 madminer.utils.ml.sc INFO      Epoch 19: train loss 148.2376 (mse_score: 148.2376)\n",
      "17:08 madminer.utils.ml.sc INFO                val. loss  148.8095 (mse_score: 148.8095) (*)\n",
      "17:08 madminer.utils.ml.sc INFO      Epoch 20: train loss 147.7592 (mse_score: 147.7592)\n",
      "17:08 madminer.utils.ml.sc INFO                val. loss  148.4904 (mse_score: 148.4904) (*)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:09 madminer.utils.ml.sc INFO      Epoch 21: train loss 147.3945 (mse_score: 147.3945)\n",
      "17:09 madminer.utils.ml.sc INFO                val. loss  148.6041 (mse_score: 148.6041)\n",
      "17:09 madminer.utils.ml.sc INFO      Epoch 22: train loss 146.9756 (mse_score: 146.9756)\n",
      "17:09 madminer.utils.ml.sc INFO                val. loss  148.1880 (mse_score: 148.1880) (*)\n",
      "17:09 madminer.utils.ml.sc INFO      Epoch 23: train loss 146.6536 (mse_score: 146.6536)\n",
      "17:09 madminer.utils.ml.sc INFO                val. loss  148.0531 (mse_score: 148.0531) (*)\n",
      "17:10 madminer.utils.ml.sc INFO      Epoch 24: train loss 146.2626 (mse_score: 146.2626)\n",
      "17:10 madminer.utils.ml.sc INFO                val. loss  147.7134 (mse_score: 147.7134) (*)\n",
      "17:10 madminer.utils.ml.sc INFO      Epoch 25: train loss 145.9794 (mse_score: 145.9794)\n",
      "17:10 madminer.utils.ml.sc INFO                val. loss  147.4437 (mse_score: 147.4437) (*)\n",
      "17:11 madminer.utils.ml.sc INFO      Epoch 26: train loss 145.6842 (mse_score: 145.6842)\n",
      "17:11 madminer.utils.ml.sc INFO                val. loss  147.2234 (mse_score: 147.2234) (*)\n",
      "17:11 madminer.utils.ml.sc INFO      Epoch 27: train loss 145.4032 (mse_score: 145.4032)\n",
      "17:11 madminer.utils.ml.sc INFO                val. loss  147.5568 (mse_score: 147.5568)\n",
      "17:12 madminer.utils.ml.sc INFO      Epoch 28: train loss 145.1104 (mse_score: 145.1104)\n",
      "17:12 madminer.utils.ml.sc INFO                val. loss  147.1655 (mse_score: 147.1655) (*)\n",
      "17:12 madminer.utils.ml.sc INFO      Epoch 29: train loss 144.8515 (mse_score: 144.8515)\n",
      "17:12 madminer.utils.ml.sc INFO                val. loss  146.7877 (mse_score: 146.7877) (*)\n",
      "17:13 madminer.utils.ml.sc INFO      Epoch 30: train loss 144.6305 (mse_score: 144.6305)\n",
      "17:13 madminer.utils.ml.sc INFO                val. loss  146.6902 (mse_score: 146.6902) (*)\n",
      "17:13 madminer.utils.ml.sc INFO      Epoch 31: train loss 144.3861 (mse_score: 144.3861)\n",
      "17:13 madminer.utils.ml.sc INFO                val. loss  146.5655 (mse_score: 146.5655) (*)\n",
      "17:13 madminer.utils.ml.sc INFO      Epoch 32: train loss 144.1993 (mse_score: 144.1993)\n",
      "17:13 madminer.utils.ml.sc INFO                val. loss  146.5089 (mse_score: 146.5089) (*)\n",
      "17:14 madminer.utils.ml.sc INFO      Epoch 33: train loss 143.9972 (mse_score: 143.9972)\n",
      "17:14 madminer.utils.ml.sc INFO                val. loss  146.3500 (mse_score: 146.3500) (*)\n",
      "17:15 madminer.utils.ml.sc INFO      Epoch 34: train loss 143.7833 (mse_score: 143.7833)\n",
      "17:15 madminer.utils.ml.sc INFO                val. loss  146.5630 (mse_score: 146.5630)\n",
      "17:15 madminer.utils.ml.sc INFO      Epoch 35: train loss 143.6265 (mse_score: 143.6265)\n",
      "17:15 madminer.utils.ml.sc INFO                val. loss  146.1738 (mse_score: 146.1738) (*)\n",
      "17:16 madminer.utils.ml.sc INFO      Epoch 36: train loss 143.4179 (mse_score: 143.4179)\n",
      "17:16 madminer.utils.ml.sc INFO                val. loss  146.0926 (mse_score: 146.0926) (*)\n",
      "17:17 madminer.utils.ml.sc INFO      Epoch 37: train loss 143.2622 (mse_score: 143.2622)\n",
      "17:17 madminer.utils.ml.sc INFO                val. loss  145.9830 (mse_score: 145.9830) (*)\n",
      "17:17 madminer.utils.ml.sc INFO      Epoch 38: train loss 143.1265 (mse_score: 143.1265)\n",
      "17:17 madminer.utils.ml.sc INFO                val. loss  145.9792 (mse_score: 145.9792) (*)\n",
      "17:18 madminer.utils.ml.sc INFO      Epoch 39: train loss 142.9759 (mse_score: 142.9759)\n",
      "17:18 madminer.utils.ml.sc INFO                val. loss  145.7006 (mse_score: 145.7006) (*)\n",
      "17:18 madminer.utils.ml.sc INFO      Epoch 40: train loss 142.8259 (mse_score: 142.8259)\n",
      "17:18 madminer.utils.ml.sc INFO                val. loss  145.9814 (mse_score: 145.9814)\n",
      "17:19 madminer.utils.ml.sc INFO      Epoch 41: train loss 142.7030 (mse_score: 142.7030)\n",
      "17:19 madminer.utils.ml.sc INFO                val. loss  145.5064 (mse_score: 145.5064) (*)\n",
      "17:20 madminer.utils.ml.sc INFO      Epoch 42: train loss 142.5822 (mse_score: 142.5822)\n",
      "17:20 madminer.utils.ml.sc INFO                val. loss  145.5863 (mse_score: 145.5863)\n",
      "17:21 madminer.utils.ml.sc INFO      Epoch 43: train loss 142.4441 (mse_score: 142.4441)\n",
      "17:21 madminer.utils.ml.sc INFO                val. loss  145.4736 (mse_score: 145.4736) (*)\n",
      "17:21 madminer.utils.ml.sc INFO      Epoch 44: train loss 142.3419 (mse_score: 142.3419)\n",
      "17:21 madminer.utils.ml.sc INFO                val. loss  145.4089 (mse_score: 145.4089) (*)\n",
      "17:22 madminer.utils.ml.sc INFO      Epoch 45: train loss 142.2215 (mse_score: 142.2215)\n",
      "17:22 madminer.utils.ml.sc INFO                val. loss  145.3152 (mse_score: 145.3152) (*)\n",
      "17:22 madminer.utils.ml.sc INFO      Epoch 46: train loss 142.1096 (mse_score: 142.1096)\n",
      "17:22 madminer.utils.ml.sc INFO                val. loss  145.2965 (mse_score: 145.2965) (*)\n",
      "17:23 madminer.utils.ml.sc INFO      Epoch 47: train loss 142.0174 (mse_score: 142.0174)\n",
      "17:23 madminer.utils.ml.sc INFO                val. loss  145.1993 (mse_score: 145.1993) (*)\n",
      "17:23 madminer.utils.ml.sc INFO      Epoch 48: train loss 141.9181 (mse_score: 141.9181)\n",
      "17:23 madminer.utils.ml.sc INFO                val. loss  145.1312 (mse_score: 145.1312) (*)\n",
      "17:24 madminer.utils.ml.sc INFO      Epoch 49: train loss 142.3216 (mse_score: 142.3216)\n",
      "17:24 madminer.utils.ml.sc INFO                val. loss  145.1146 (mse_score: 145.1146) (*)\n",
      "17:25 madminer.utils.ml.sc INFO      Epoch 50: train loss 141.7366 (mse_score: 141.7366)\n",
      "17:25 madminer.utils.ml.sc INFO                val. loss  145.0386 (mse_score: 145.0386) (*)\n",
      "17:25 madminer.utils.ml.sc INFO    Early stopping did not improve performance\n",
      "17:25 madminer.utils.ml.sc INFO    Finished training\n",
      "17:25 madminer.ml          INFO    Training estimator 3 / 3 in ensemble\n",
      "17:25 madminer.ml          INFO    Starting training\n",
      "17:25 madminer.ml          INFO      Method:                 sally\n",
      "17:25 madminer.ml          INFO      Training data: x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_tight/x_train_2.npy\n",
      "17:25 madminer.ml          INFO                     t_xz (theta0) at  /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_tight/t_xz_train_2.npy\n",
      "17:25 madminer.ml          INFO      Features:               all\n",
      "17:25 madminer.ml          INFO      Method:                 sally\n",
      "17:25 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "17:25 madminer.ml          INFO      Activation function:    tanh\n",
      "17:25 madminer.ml          INFO      Batch size:             128\n",
      "17:25 madminer.ml          INFO      Trainer:                amsgrad\n",
      "17:25 madminer.ml          INFO      Epochs:                 50\n",
      "17:25 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "17:25 madminer.ml          INFO      Validation split:       0.5\n",
      "17:25 madminer.ml          INFO      Early stopping:         True\n",
      "17:25 madminer.ml          INFO      Scale inputs:           True\n",
      "17:25 madminer.ml          INFO      Shuffle labels          False\n",
      "17:25 madminer.ml          INFO      Regularization:         None\n",
      "17:25 madminer.ml          INFO    Loading training data\n",
      "17:25 madminer.ml          INFO    Found 1000000 samples with 57 parameters and 33 observables\n",
      "17:25 madminer.ml          INFO    Rescaling inputs\n",
      "17:25 madminer.ml          INFO    Creating model for method sally\n",
      "17:25 madminer.ml          INFO    Training model\n",
      "17:26 madminer.utils.ml.sc INFO      Epoch 1: train loss 174.1640 (mse_score: 174.1640)\n",
      "17:26 madminer.utils.ml.sc INFO                val. loss  168.5468 (mse_score: 168.5468) (*)\n",
      "17:26 madminer.utils.ml.sc INFO      Epoch 2: train loss 165.4072 (mse_score: 165.4072)\n",
      "17:26 madminer.utils.ml.sc INFO                val. loss  164.8471 (mse_score: 164.8471) (*)\n",
      "17:27 madminer.utils.ml.sc INFO      Epoch 3: train loss 162.3038 (mse_score: 162.3038)\n",
      "17:27 madminer.utils.ml.sc INFO                val. loss  163.3096 (mse_score: 163.3096) (*)\n",
      "17:27 madminer.utils.ml.sc INFO      Epoch 4: train loss 160.0070 (mse_score: 160.0070)\n",
      "17:27 madminer.utils.ml.sc INFO                val. loss  160.8881 (mse_score: 160.8881) (*)\n",
      "17:27 madminer.utils.ml.sc INFO      Epoch 5: train loss 158.0622 (mse_score: 158.0622)\n",
      "17:27 madminer.utils.ml.sc INFO                val. loss  159.3147 (mse_score: 159.3147) (*)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:28 madminer.utils.ml.sc INFO      Epoch 6: train loss 156.6738 (mse_score: 156.6738)\n",
      "17:28 madminer.utils.ml.sc INFO                val. loss  158.1348 (mse_score: 158.1348) (*)\n",
      "17:28 madminer.utils.ml.sc INFO      Epoch 7: train loss 155.5461 (mse_score: 155.5461)\n",
      "17:28 madminer.utils.ml.sc INFO                val. loss  157.1368 (mse_score: 157.1368) (*)\n",
      "17:29 madminer.utils.ml.sc INFO      Epoch 8: train loss 154.6864 (mse_score: 154.6864)\n",
      "17:29 madminer.utils.ml.sc INFO                val. loss  156.5643 (mse_score: 156.5643) (*)\n",
      "17:29 madminer.utils.ml.sc INFO      Epoch 9: train loss 153.7223 (mse_score: 153.7223)\n",
      "17:29 madminer.utils.ml.sc INFO                val. loss  156.1281 (mse_score: 156.1281) (*)\n",
      "17:30 madminer.utils.ml.sc INFO      Epoch 10: train loss 153.1002 (mse_score: 153.1002)\n",
      "17:30 madminer.utils.ml.sc INFO                val. loss  155.5065 (mse_score: 155.5065) (*)\n",
      "17:30 madminer.utils.ml.sc INFO      Epoch 11: train loss 152.3693 (mse_score: 152.3693)\n",
      "17:30 madminer.utils.ml.sc INFO                val. loss  155.0987 (mse_score: 155.0987) (*)\n",
      "17:31 madminer.utils.ml.sc INFO      Epoch 12: train loss 151.6368 (mse_score: 151.6368)\n",
      "17:31 madminer.utils.ml.sc INFO                val. loss  155.3456 (mse_score: 155.3456)\n",
      "17:31 madminer.utils.ml.sc INFO      Epoch 13: train loss 151.0292 (mse_score: 151.0292)\n",
      "17:31 madminer.utils.ml.sc INFO                val. loss  154.0803 (mse_score: 154.0803) (*)\n",
      "17:32 madminer.utils.ml.sc INFO      Epoch 14: train loss 150.3134 (mse_score: 150.3134)\n",
      "17:32 madminer.utils.ml.sc INFO                val. loss  153.5811 (mse_score: 153.5811) (*)\n",
      "17:32 madminer.utils.ml.sc INFO      Epoch 15: train loss 149.7247 (mse_score: 149.7247)\n",
      "17:32 madminer.utils.ml.sc INFO                val. loss  153.0233 (mse_score: 153.0233) (*)\n",
      "17:33 madminer.utils.ml.sc INFO      Epoch 16: train loss 149.1267 (mse_score: 149.1267)\n",
      "17:33 madminer.utils.ml.sc INFO                val. loss  152.4913 (mse_score: 152.4913) (*)\n",
      "17:33 madminer.utils.ml.sc INFO      Epoch 17: train loss 148.6269 (mse_score: 148.6269)\n",
      "17:33 madminer.utils.ml.sc INFO                val. loss  152.1287 (mse_score: 152.1287) (*)\n",
      "17:33 madminer.utils.ml.sc INFO      Epoch 18: train loss 148.0947 (mse_score: 148.0947)\n",
      "17:33 madminer.utils.ml.sc INFO                val. loss  151.8801 (mse_score: 151.8801) (*)\n",
      "17:34 madminer.utils.ml.sc INFO      Epoch 19: train loss 147.6326 (mse_score: 147.6326)\n",
      "17:34 madminer.utils.ml.sc INFO                val. loss  151.7312 (mse_score: 151.7312) (*)\n",
      "17:34 madminer.utils.ml.sc INFO      Epoch 20: train loss 147.1791 (mse_score: 147.1791)\n",
      "17:34 madminer.utils.ml.sc INFO                val. loss  151.4375 (mse_score: 151.4375) (*)\n",
      "17:35 madminer.utils.ml.sc INFO      Epoch 21: train loss 147.0050 (mse_score: 147.0050)\n",
      "17:35 madminer.utils.ml.sc INFO                val. loss  151.1048 (mse_score: 151.1048) (*)\n",
      "17:35 madminer.utils.ml.sc INFO      Epoch 22: train loss 146.3857 (mse_score: 146.3857)\n",
      "17:35 madminer.utils.ml.sc INFO                val. loss  150.7699 (mse_score: 150.7699) (*)\n",
      "17:35 madminer.utils.ml.sc INFO      Epoch 23: train loss 146.0201 (mse_score: 146.0201)\n",
      "17:35 madminer.utils.ml.sc INFO                val. loss  150.6769 (mse_score: 150.6769) (*)\n",
      "17:36 madminer.utils.ml.sc INFO      Epoch 24: train loss 145.6937 (mse_score: 145.6937)\n",
      "17:36 madminer.utils.ml.sc INFO                val. loss  150.3512 (mse_score: 150.3512) (*)\n",
      "17:36 madminer.utils.ml.sc INFO      Epoch 25: train loss 145.3274 (mse_score: 145.3274)\n",
      "17:36 madminer.utils.ml.sc INFO                val. loss  150.1726 (mse_score: 150.1726) (*)\n",
      "17:37 madminer.utils.ml.sc INFO      Epoch 26: train loss 145.0209 (mse_score: 145.0209)\n",
      "17:37 madminer.utils.ml.sc INFO                val. loss  150.1543 (mse_score: 150.1543) (*)\n",
      "17:37 madminer.utils.ml.sc INFO      Epoch 27: train loss 144.7565 (mse_score: 144.7565)\n",
      "17:37 madminer.utils.ml.sc INFO                val. loss  149.6840 (mse_score: 149.6840) (*)\n",
      "17:38 madminer.utils.ml.sc INFO      Epoch 28: train loss 144.4886 (mse_score: 144.4886)\n",
      "17:38 madminer.utils.ml.sc INFO                val. loss  149.5795 (mse_score: 149.5795) (*)\n",
      "17:38 madminer.utils.ml.sc INFO      Epoch 29: train loss 144.2341 (mse_score: 144.2341)\n",
      "17:38 madminer.utils.ml.sc INFO                val. loss  149.3522 (mse_score: 149.3522) (*)\n",
      "17:38 madminer.utils.ml.sc INFO      Epoch 30: train loss 143.9536 (mse_score: 143.9536)\n",
      "17:38 madminer.utils.ml.sc INFO                val. loss  149.3748 (mse_score: 149.3748)\n",
      "17:39 madminer.utils.ml.sc INFO      Epoch 31: train loss 143.7453 (mse_score: 143.7453)\n",
      "17:39 madminer.utils.ml.sc INFO                val. loss  148.9965 (mse_score: 148.9965) (*)\n",
      "17:39 madminer.utils.ml.sc INFO      Epoch 32: train loss 143.5061 (mse_score: 143.5061)\n",
      "17:39 madminer.utils.ml.sc INFO                val. loss  148.8500 (mse_score: 148.8500) (*)\n",
      "17:40 madminer.utils.ml.sc INFO      Epoch 33: train loss 143.2797 (mse_score: 143.2797)\n",
      "17:40 madminer.utils.ml.sc INFO                val. loss  148.8241 (mse_score: 148.8241) (*)\n",
      "17:40 madminer.utils.ml.sc INFO      Epoch 34: train loss 143.1118 (mse_score: 143.1118)\n",
      "17:40 madminer.utils.ml.sc INFO                val. loss  148.8728 (mse_score: 148.8728)\n",
      "17:41 madminer.utils.ml.sc INFO      Epoch 35: train loss 142.9048 (mse_score: 142.9048)\n",
      "17:41 madminer.utils.ml.sc INFO                val. loss  148.5409 (mse_score: 148.5409) (*)\n",
      "17:41 madminer.utils.ml.sc INFO      Epoch 36: train loss 142.7390 (mse_score: 142.7390)\n",
      "17:41 madminer.utils.ml.sc INFO                val. loss  148.4846 (mse_score: 148.4846) (*)\n",
      "17:42 madminer.utils.ml.sc INFO      Epoch 37: train loss 142.5626 (mse_score: 142.5626)\n",
      "17:42 madminer.utils.ml.sc INFO                val. loss  148.3459 (mse_score: 148.3459) (*)\n",
      "17:42 madminer.utils.ml.sc INFO      Epoch 38: train loss 142.4193 (mse_score: 142.4193)\n",
      "17:42 madminer.utils.ml.sc INFO                val. loss  148.3459 (mse_score: 148.3459) (*)\n",
      "17:42 madminer.utils.ml.sc INFO      Epoch 39: train loss 142.2524 (mse_score: 142.2524)\n",
      "17:42 madminer.utils.ml.sc INFO                val. loss  148.1354 (mse_score: 148.1354) (*)\n",
      "17:43 madminer.utils.ml.sc INFO      Epoch 40: train loss 142.0823 (mse_score: 142.0823)\n",
      "17:43 madminer.utils.ml.sc INFO                val. loss  148.2546 (mse_score: 148.2546)\n",
      "17:43 madminer.utils.ml.sc INFO      Epoch 41: train loss 141.9631 (mse_score: 141.9631)\n",
      "17:43 madminer.utils.ml.sc INFO                val. loss  148.0455 (mse_score: 148.0455) (*)\n",
      "17:43 madminer.utils.ml.sc INFO      Epoch 42: train loss 141.9579 (mse_score: 141.9579)\n",
      "17:43 madminer.utils.ml.sc INFO                val. loss  147.9808 (mse_score: 147.9808) (*)\n",
      "17:44 madminer.utils.ml.sc INFO      Epoch 43: train loss 141.6843 (mse_score: 141.6843)\n",
      "17:44 madminer.utils.ml.sc INFO                val. loss  147.8579 (mse_score: 147.8579) (*)\n",
      "17:44 madminer.utils.ml.sc INFO      Epoch 44: train loss 141.5710 (mse_score: 141.5710)\n",
      "17:44 madminer.utils.ml.sc INFO                val. loss  147.8235 (mse_score: 147.8235) (*)\n",
      "17:44 madminer.utils.ml.sc INFO      Epoch 45: train loss 141.4521 (mse_score: 141.4521)\n",
      "17:44 madminer.utils.ml.sc INFO                val. loss  147.6736 (mse_score: 147.6736) (*)\n",
      "17:45 madminer.utils.ml.sc INFO      Epoch 46: train loss 141.3587 (mse_score: 141.3587)\n",
      "17:45 madminer.utils.ml.sc INFO                val. loss  147.6403 (mse_score: 147.6403) (*)\n",
      "17:45 madminer.utils.ml.sc INFO      Epoch 47: train loss 141.2857 (mse_score: 141.2857)\n",
      "17:45 madminer.utils.ml.sc INFO                val. loss  147.5737 (mse_score: 147.5737) (*)\n",
      "17:45 madminer.utils.ml.sc INFO      Epoch 48: train loss 141.1512 (mse_score: 141.1512)\n",
      "17:45 madminer.utils.ml.sc INFO                val. loss  147.5714 (mse_score: 147.5714) (*)\n",
      "17:46 madminer.utils.ml.sc INFO      Epoch 49: train loss 141.0621 (mse_score: 141.0621)\n",
      "17:46 madminer.utils.ml.sc INFO                val. loss  147.5109 (mse_score: 147.5109) (*)\n",
      "17:46 madminer.utils.ml.sc INFO      Epoch 50: train loss 140.9819 (mse_score: 140.9819)\n",
      "17:46 madminer.utils.ml.sc INFO                val. loss  147.4634 (mse_score: 147.4634) (*)\n",
      "17:46 madminer.utils.ml.sc INFO    Early stopping did not improve performance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:46 madminer.utils.ml.sc INFO    Finished training\n",
      "17:46 madminer.ml          INFO    Calculating expectation for 3 estimators in ensemble\n",
      "17:46 madminer.ml          INFO    Starting evaluation for estimator 1 / 3 in ensemble\n",
      "17:46 madminer.ml          INFO    Starting evaluation for estimator 2 / 3 in ensemble\n",
      "17:46 madminer.ml          INFO    Starting evaluation for estimator 3 / 3 in ensemble\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'all_tight',\n",
    "    use_tight_cuts=True,\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffled label check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'all_shuffled',\n",
    "    use_tight_cuts=False,\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True,\n",
    "    shuffle_labels=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal observable basis (no jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_obs = [0,1] + list(range(4,12)) + list(range(16,33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal',\n",
    "    use_tight_cuts=False,\n",
    "    features=[min_obs for _ in range(n_estimators)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[min_obs for _ in range(n_estimators)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just resurrection phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'phi_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[32] for _ in range(n_estimators)],\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (higgs_inference)",
   "language": "python",
   "name": "higgs_inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
