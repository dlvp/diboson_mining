{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9deb73c-b62f-4cff-8d83-724074098c92"
    }
   },
   "source": [
    "# Train SALLY ensemble\n",
    "\n",
    "Johann Brehmer, Kyle Cranmer, Marco Farina, Felix Kling, Duccio Pappadopulo, Josh Ruderman 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fe57a76c-4838-44c4-b0cc-5ee166785e4a"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)-5.5s %(name)-20.20s %(levelname)-7.7s %(message)s',\n",
    "    datefmt='%H:%M',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"madminer\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "f3463c40-6421-42a1-8681-527c3ec42541"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '/Users/johannbrehmer/work/projects/madminer/diboson_mining/'\n",
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "b2c73eca-c625-4f7a-9cee-4ccb2dcbb3e9"
    }
   },
   "outputs": [],
   "source": [
    "sample_dir = base_dir + 'data/samples/wgamma_sys/'\n",
    "card_dir = base_dir + 'cards/wgamma/'\n",
    "ufo_model_dir = card_dir + 'SMWgamma_UFO'\n",
    "run_card_dir = card_dir + 'run_cards/'\n",
    "mg_process_dir = base_dir + 'data/mg_processes/wgamma_sys/'\n",
    "log_dir = base_dir + 'logs/wgamma_sys/'\n",
    "temp_dir = base_dir + 'data/temp'\n",
    "delphes_dir = mg_dir + 'Delphes'\n",
    "model_dir = base_dir + 'data/models/wgamma_sys/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(filename, use_tight_cuts=True, use_antitight_cuts=False, n_estimators=n_estimators, **kwargs):\n",
    "    cut_label = '_tight' if use_tight_cuts else '_antitight' if use_antitight_cuts else ''\n",
    "    \n",
    "    ensemble = EnsembleForge(n_estimators)\n",
    "\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[sample_dir + 'train_local{}/x_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[sample_dir + 'train_local{}/t_xz_train_{}.npy'.format(cut_label, i) for i in range(n_estimators)],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    ensemble.save(model_dir + 'sally_ensemble_' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b45e7f73-8f4c-4261-a381-4b7ad6af120f"
    }
   },
   "source": [
    "## All observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:14 madminer.ml          INFO    Training 10 estimators in ensemble\n",
      "14:14 madminer.ml          INFO    Training estimator 1 / 10 in ensemble\n",
      "14:14 madminer.ml          INFO    Starting training\n",
      "14:14 madminer.ml          INFO      Method:                 sally\n",
      "14:14 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_0.npy\n",
      "14:14 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_0.npy\n",
      "14:14 madminer.ml          INFO      Features:               all\n",
      "14:14 madminer.ml          INFO      Method:                 sally\n",
      "14:14 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "14:14 madminer.ml          INFO      Activation function:    tanh\n",
      "14:14 madminer.ml          INFO      Batch size:             128\n",
      "14:14 madminer.ml          INFO      Trainer:                amsgrad\n",
      "14:14 madminer.ml          INFO      Epochs:                 50\n",
      "14:14 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:14 madminer.ml          INFO      Validation split:       0.5\n",
      "14:14 madminer.ml          INFO      Early stopping:         True\n",
      "14:14 madminer.ml          INFO      Scale inputs:           True\n",
      "14:14 madminer.ml          INFO      Shuffle labels          False\n",
      "14:14 madminer.ml          INFO      Regularization:         None\n",
      "14:14 madminer.ml          INFO      Samples:                all\n",
      "14:14 madminer.ml          INFO    Loading training data\n",
      "14:14 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "14:14 madminer.ml          INFO    Rescaling inputs\n",
      "14:14 madminer.ml          INFO    Creating model for method sally\n",
      "14:14 madminer.ml          INFO    Training model\n",
      "14:17 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1027 (mse_score: 0.1027)\n",
      "14:17 madminer.utils.ml.sc INFO                val. loss  0.1194 (mse_score: 0.1194) (*)\n",
      "14:21 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0816 (mse_score: 0.0816)\n",
      "14:21 madminer.utils.ml.sc INFO                val. loss  0.1191 (mse_score: 0.1191)\n",
      "14:24 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0705 (mse_score: 0.0705)\n",
      "14:24 madminer.utils.ml.sc INFO                val. loss  0.1096 (mse_score: 0.1096) (*)\n",
      "14:27 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0626 (mse_score: 0.0626)\n",
      "14:27 madminer.utils.ml.sc INFO                val. loss  0.1103 (mse_score: 0.1103)\n",
      "14:31 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0574 (mse_score: 0.0574)\n",
      "14:31 madminer.utils.ml.sc INFO                val. loss  0.1105 (mse_score: 0.1105)\n",
      "14:34 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0530 (mse_score: 0.0530)\n",
      "14:34 madminer.utils.ml.sc INFO                val. loss  0.1106 (mse_score: 0.1106)\n",
      "14:38 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0500 (mse_score: 0.0500)\n",
      "14:38 madminer.utils.ml.sc INFO                val. loss  0.1119 (mse_score: 0.1119)\n",
      "14:42 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0476 (mse_score: 0.0476)\n",
      "14:42 madminer.utils.ml.sc INFO                val. loss  0.1115 (mse_score: 0.1115)\n",
      "14:46 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0458 (mse_score: 0.0458)\n",
      "14:46 madminer.utils.ml.sc INFO                val. loss  0.1117 (mse_score: 0.1117)\n",
      "14:51 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0445 (mse_score: 0.0445)\n",
      "14:51 madminer.utils.ml.sc INFO                val. loss  0.1116 (mse_score: 0.1116)\n",
      "14:51 madminer.utils.ml.sc INFO    Early stopping after epoch 15, with loss 0.11 compared to final loss 0.11\n",
      "14:51 madminer.utils.ml.sc INFO    Finished training\n",
      "14:51 madminer.ml          INFO    Training estimator 2 / 10 in ensemble\n",
      "14:51 madminer.ml          INFO    Starting training\n",
      "14:51 madminer.ml          INFO      Method:                 sally\n",
      "14:51 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_1.npy\n",
      "14:51 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_1.npy\n",
      "14:51 madminer.ml          INFO      Features:               all\n",
      "14:51 madminer.ml          INFO      Method:                 sally\n",
      "14:51 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "14:51 madminer.ml          INFO      Activation function:    tanh\n",
      "14:51 madminer.ml          INFO      Batch size:             128\n",
      "14:51 madminer.ml          INFO      Trainer:                amsgrad\n",
      "14:51 madminer.ml          INFO      Epochs:                 50\n",
      "14:51 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:51 madminer.ml          INFO      Validation split:       0.5\n",
      "14:51 madminer.ml          INFO      Early stopping:         True\n",
      "14:51 madminer.ml          INFO      Scale inputs:           True\n",
      "14:51 madminer.ml          INFO      Shuffle labels          False\n",
      "14:51 madminer.ml          INFO      Regularization:         None\n",
      "14:51 madminer.ml          INFO      Samples:                all\n",
      "14:51 madminer.ml          INFO    Loading training data\n",
      "14:51 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "14:51 madminer.ml          INFO    Rescaling inputs\n",
      "14:51 madminer.ml          INFO    Creating model for method sally\n",
      "14:51 madminer.ml          INFO    Training model\n",
      "14:54 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1163 (mse_score: 0.1163)\n",
      "14:54 madminer.utils.ml.sc INFO                val. loss  0.1160 (mse_score: 0.1160) (*)\n",
      "14:58 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0916 (mse_score: 0.0916)\n",
      "14:58 madminer.utils.ml.sc INFO                val. loss  0.1040 (mse_score: 0.1040) (*)\n",
      "15:01 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0804 (mse_score: 0.0804)\n",
      "15:01 madminer.utils.ml.sc INFO                val. loss  0.1008 (mse_score: 0.1008) (*)\n",
      "15:04 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0712 (mse_score: 0.0712)\n",
      "15:04 madminer.utils.ml.sc INFO                val. loss  0.0988 (mse_score: 0.0988) (*)\n",
      "15:07 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0648 (mse_score: 0.0648)\n",
      "15:07 madminer.utils.ml.sc INFO                val. loss  0.0996 (mse_score: 0.0996)\n",
      "15:10 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0602 (mse_score: 0.0602)\n",
      "15:10 madminer.utils.ml.sc INFO                val. loss  0.1005 (mse_score: 0.1005)\n",
      "15:13 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0569 (mse_score: 0.0569)\n",
      "15:13 madminer.utils.ml.sc INFO                val. loss  0.0982 (mse_score: 0.0982)\n",
      "16:01 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0544 (mse_score: 0.0544)\n",
      "16:01 madminer.utils.ml.sc INFO                val. loss  0.0978 (mse_score: 0.0978)\n",
      "16:03 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0524 (mse_score: 0.0524)\n",
      "16:03 madminer.utils.ml.sc INFO                val. loss  0.0977 (mse_score: 0.0977)\n",
      "16:06 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0510 (mse_score: 0.0510)\n",
      "16:06 madminer.utils.ml.sc INFO                val. loss  0.0976 (mse_score: 0.0976)\n",
      "16:06 madminer.utils.ml.sc INFO    Early stopping after epoch 36, with loss 0.10 compared to final loss 0.10\n",
      "16:06 madminer.utils.ml.sc INFO    Finished training\n",
      "16:06 madminer.ml          INFO    Training estimator 3 / 10 in ensemble\n",
      "16:06 madminer.ml          INFO    Starting training\n",
      "16:06 madminer.ml          INFO      Method:                 sally\n",
      "16:06 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_2.npy\n",
      "16:06 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_2.npy\n",
      "16:06 madminer.ml          INFO      Features:               all\n",
      "16:06 madminer.ml          INFO      Method:                 sally\n",
      "16:06 madminer.ml          INFO      Hidden layers:          (100, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:06 madminer.ml          INFO      Activation function:    tanh\n",
      "16:06 madminer.ml          INFO      Batch size:             128\n",
      "16:06 madminer.ml          INFO      Trainer:                amsgrad\n",
      "16:06 madminer.ml          INFO      Epochs:                 50\n",
      "16:06 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:06 madminer.ml          INFO      Validation split:       0.5\n",
      "16:06 madminer.ml          INFO      Early stopping:         True\n",
      "16:06 madminer.ml          INFO      Scale inputs:           True\n",
      "16:06 madminer.ml          INFO      Shuffle labels          False\n",
      "16:06 madminer.ml          INFO      Regularization:         None\n",
      "16:06 madminer.ml          INFO      Samples:                all\n",
      "16:06 madminer.ml          INFO    Loading training data\n",
      "16:06 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "16:06 madminer.ml          INFO    Rescaling inputs\n",
      "16:06 madminer.ml          INFO    Creating model for method sally\n",
      "16:06 madminer.ml          INFO    Training model\n",
      "16:08 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1143 (mse_score: 0.1143)\n",
      "16:08 madminer.utils.ml.sc INFO                val. loss  0.1615 (mse_score: 0.1615) (*)\n",
      "16:10 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0951 (mse_score: 0.0951)\n",
      "16:10 madminer.utils.ml.sc INFO                val. loss  0.1494 (mse_score: 0.1494) (*)\n",
      "16:12 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0838 (mse_score: 0.0838)\n",
      "16:12 madminer.utils.ml.sc INFO                val. loss  0.1449 (mse_score: 0.1449)\n",
      "16:15 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0769 (mse_score: 0.0769)\n",
      "16:15 madminer.utils.ml.sc INFO                val. loss  0.1427 (mse_score: 0.1427) (*)\n",
      "16:18 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0724 (mse_score: 0.0724)\n",
      "16:18 madminer.utils.ml.sc INFO                val. loss  0.1419 (mse_score: 0.1419)\n",
      "16:20 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0686 (mse_score: 0.0686)\n",
      "16:20 madminer.utils.ml.sc INFO                val. loss  0.1408 (mse_score: 0.1408) (*)\n",
      "16:23 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0659 (mse_score: 0.0659)\n",
      "16:23 madminer.utils.ml.sc INFO                val. loss  0.1401 (mse_score: 0.1401)\n",
      "16:25 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0639 (mse_score: 0.0639)\n",
      "16:25 madminer.utils.ml.sc INFO                val. loss  0.1403 (mse_score: 0.1403)\n",
      "16:28 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0623 (mse_score: 0.0623)\n",
      "16:28 madminer.utils.ml.sc INFO                val. loss  0.1394 (mse_score: 0.1394) (*)\n",
      "16:31 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0611 (mse_score: 0.0611)\n",
      "16:31 madminer.utils.ml.sc INFO                val. loss  0.1397 (mse_score: 0.1397)\n",
      "16:31 madminer.utils.ml.sc INFO    Early stopping after epoch 45, with loss 0.14 compared to final loss 0.14\n",
      "16:31 madminer.utils.ml.sc INFO    Finished training\n",
      "16:31 madminer.ml          INFO    Training estimator 4 / 10 in ensemble\n",
      "16:31 madminer.ml          INFO    Starting training\n",
      "16:31 madminer.ml          INFO      Method:                 sally\n",
      "16:31 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_3.npy\n",
      "16:31 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_3.npy\n",
      "16:31 madminer.ml          INFO      Features:               all\n",
      "16:31 madminer.ml          INFO      Method:                 sally\n",
      "16:31 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "16:31 madminer.ml          INFO      Activation function:    tanh\n",
      "16:31 madminer.ml          INFO      Batch size:             128\n",
      "16:31 madminer.ml          INFO      Trainer:                amsgrad\n",
      "16:31 madminer.ml          INFO      Epochs:                 50\n",
      "16:31 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:31 madminer.ml          INFO      Validation split:       0.5\n",
      "16:31 madminer.ml          INFO      Early stopping:         True\n",
      "16:31 madminer.ml          INFO      Scale inputs:           True\n",
      "16:31 madminer.ml          INFO      Shuffle labels          False\n",
      "16:31 madminer.ml          INFO      Regularization:         None\n",
      "16:31 madminer.ml          INFO      Samples:                all\n",
      "16:31 madminer.ml          INFO    Loading training data\n",
      "16:31 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "16:31 madminer.ml          INFO    Rescaling inputs\n",
      "16:31 madminer.ml          INFO    Creating model for method sally\n",
      "16:31 madminer.ml          INFO    Training model\n",
      "16:33 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1062 (mse_score: 0.1062)\n",
      "16:33 madminer.utils.ml.sc INFO                val. loss  0.1499 (mse_score: 0.1499) (*)\n",
      "16:35 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0838 (mse_score: 0.0838)\n",
      "16:35 madminer.utils.ml.sc INFO                val. loss  0.1425 (mse_score: 0.1425)\n",
      "16:37 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0729 (mse_score: 0.0729)\n",
      "16:37 madminer.utils.ml.sc INFO                val. loss  0.1383 (mse_score: 0.1383)\n",
      "16:39 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0660 (mse_score: 0.0660)\n",
      "16:39 madminer.utils.ml.sc INFO                val. loss  0.1338 (mse_score: 0.1338) (*)\n",
      "16:41 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0605 (mse_score: 0.0605)\n",
      "16:41 madminer.utils.ml.sc INFO                val. loss  0.1347 (mse_score: 0.1347)\n",
      "16:43 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0569 (mse_score: 0.0569)\n",
      "16:43 madminer.utils.ml.sc INFO                val. loss  0.1331 (mse_score: 0.1331)\n",
      "16:45 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0540 (mse_score: 0.0540)\n",
      "16:45 madminer.utils.ml.sc INFO                val. loss  0.1336 (mse_score: 0.1336)\n",
      "16:47 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0517 (mse_score: 0.0517)\n",
      "16:47 madminer.utils.ml.sc INFO                val. loss  0.1328 (mse_score: 0.1328)\n",
      "16:50 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0500 (mse_score: 0.0500)\n",
      "16:50 madminer.utils.ml.sc INFO                val. loss  0.1323 (mse_score: 0.1323)\n",
      "16:52 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0488 (mse_score: 0.0488)\n",
      "16:52 madminer.utils.ml.sc INFO                val. loss  0.1322 (mse_score: 0.1322)\n",
      "16:52 madminer.utils.ml.sc INFO    Early stopping after epoch 49, with loss 0.13 compared to final loss 0.13\n",
      "16:52 madminer.utils.ml.sc INFO    Finished training\n",
      "16:52 madminer.ml          INFO    Training estimator 5 / 10 in ensemble\n",
      "16:52 madminer.ml          INFO    Starting training\n",
      "16:52 madminer.ml          INFO      Method:                 sally\n",
      "16:52 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_4.npy\n",
      "16:52 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_4.npy\n",
      "16:52 madminer.ml          INFO      Features:               all\n",
      "16:52 madminer.ml          INFO      Method:                 sally\n",
      "16:52 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "16:52 madminer.ml          INFO      Activation function:    tanh\n",
      "16:52 madminer.ml          INFO      Batch size:             128\n",
      "16:52 madminer.ml          INFO      Trainer:                amsgrad\n",
      "16:52 madminer.ml          INFO      Epochs:                 50\n",
      "16:52 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:52 madminer.ml          INFO      Validation split:       0.5\n",
      "16:52 madminer.ml          INFO      Early stopping:         True\n",
      "16:52 madminer.ml          INFO      Scale inputs:           True\n",
      "16:52 madminer.ml          INFO      Shuffle labels          False\n",
      "16:52 madminer.ml          INFO      Regularization:         None\n",
      "16:52 madminer.ml          INFO      Samples:                all\n",
      "16:52 madminer.ml          INFO    Loading training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:52 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "16:52 madminer.ml          INFO    Rescaling inputs\n",
      "16:52 madminer.ml          INFO    Creating model for method sally\n",
      "16:52 madminer.ml          INFO    Training model\n",
      "16:54 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1060 (mse_score: 0.1060)\n",
      "16:54 madminer.utils.ml.sc INFO                val. loss  0.1430 (mse_score: 0.1430) (*)\n",
      "16:57 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0858 (mse_score: 0.0858)\n",
      "16:57 madminer.utils.ml.sc INFO                val. loss  0.1323 (mse_score: 0.1323) (*)\n",
      "16:59 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0763 (mse_score: 0.0763)\n",
      "16:59 madminer.utils.ml.sc INFO                val. loss  0.1258 (mse_score: 0.1258) (*)\n",
      "17:01 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0699 (mse_score: 0.0699)\n",
      "17:01 madminer.utils.ml.sc INFO                val. loss  0.1260 (mse_score: 0.1260)\n",
      "17:03 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0659 (mse_score: 0.0659)\n",
      "17:03 madminer.utils.ml.sc INFO                val. loss  0.1244 (mse_score: 0.1244)\n",
      "17:06 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0630 (mse_score: 0.0630)\n",
      "17:06 madminer.utils.ml.sc INFO                val. loss  0.1234 (mse_score: 0.1234) (*)\n",
      "17:08 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0607 (mse_score: 0.0607)\n",
      "17:08 madminer.utils.ml.sc INFO                val. loss  0.1223 (mse_score: 0.1223) (*)\n",
      "17:11 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0589 (mse_score: 0.0589)\n",
      "17:11 madminer.utils.ml.sc INFO                val. loss  0.1228 (mse_score: 0.1228)\n",
      "17:14 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0577 (mse_score: 0.0577)\n",
      "17:14 madminer.utils.ml.sc INFO                val. loss  0.1220 (mse_score: 0.1220) (*)\n",
      "17:16 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0567 (mse_score: 0.0567)\n",
      "17:16 madminer.utils.ml.sc INFO                val. loss  0.1221 (mse_score: 0.1221)\n",
      "17:16 madminer.utils.ml.sc INFO    Early stopping after epoch 48, with loss 0.12 compared to final loss 0.12\n",
      "17:16 madminer.utils.ml.sc INFO    Finished training\n",
      "17:16 madminer.ml          INFO    Training estimator 6 / 10 in ensemble\n",
      "17:16 madminer.ml          INFO    Starting training\n",
      "17:16 madminer.ml          INFO      Method:                 sally\n",
      "17:16 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_5.npy\n",
      "17:16 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_5.npy\n",
      "17:16 madminer.ml          INFO      Features:               all\n",
      "17:16 madminer.ml          INFO      Method:                 sally\n",
      "17:16 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "17:16 madminer.ml          INFO      Activation function:    tanh\n",
      "17:16 madminer.ml          INFO      Batch size:             128\n",
      "17:16 madminer.ml          INFO      Trainer:                amsgrad\n",
      "17:16 madminer.ml          INFO      Epochs:                 50\n",
      "17:16 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "17:16 madminer.ml          INFO      Validation split:       0.5\n",
      "17:16 madminer.ml          INFO      Early stopping:         True\n",
      "17:16 madminer.ml          INFO      Scale inputs:           True\n",
      "17:16 madminer.ml          INFO      Shuffle labels          False\n",
      "17:16 madminer.ml          INFO      Regularization:         None\n",
      "17:16 madminer.ml          INFO      Samples:                all\n",
      "17:16 madminer.ml          INFO    Loading training data\n",
      "17:16 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "17:16 madminer.ml          INFO    Rescaling inputs\n",
      "17:16 madminer.ml          INFO    Creating model for method sally\n",
      "17:16 madminer.ml          INFO    Training model\n",
      "17:18 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1258 (mse_score: 0.1258)\n",
      "17:18 madminer.utils.ml.sc INFO                val. loss  0.1373 (mse_score: 0.1373) (*)\n",
      "17:21 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.1052 (mse_score: 0.1052)\n",
      "17:21 madminer.utils.ml.sc INFO                val. loss  0.1229 (mse_score: 0.1229) (*)\n",
      "17:23 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0937 (mse_score: 0.0937)\n",
      "17:23 madminer.utils.ml.sc INFO                val. loss  0.1167 (mse_score: 0.1167) (*)\n",
      "17:26 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0873 (mse_score: 0.0873)\n",
      "17:26 madminer.utils.ml.sc INFO                val. loss  0.1152 (mse_score: 0.1152) (*)\n",
      "17:29 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0828 (mse_score: 0.0828)\n",
      "17:29 madminer.utils.ml.sc INFO                val. loss  0.1139 (mse_score: 0.1139)\n",
      "17:32 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0794 (mse_score: 0.0794)\n",
      "17:32 madminer.utils.ml.sc INFO                val. loss  0.1122 (mse_score: 0.1122) (*)\n",
      "17:35 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0766 (mse_score: 0.0766)\n",
      "17:35 madminer.utils.ml.sc INFO                val. loss  0.1118 (mse_score: 0.1118) (*)\n",
      "17:38 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0748 (mse_score: 0.0748)\n",
      "17:38 madminer.utils.ml.sc INFO                val. loss  0.1116 (mse_score: 0.1116) (*)\n",
      "17:41 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0732 (mse_score: 0.0732)\n",
      "17:41 madminer.utils.ml.sc INFO                val. loss  0.1116 (mse_score: 0.1116)\n",
      "17:45 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0720 (mse_score: 0.0720)\n",
      "17:45 madminer.utils.ml.sc INFO                val. loss  0.1113 (mse_score: 0.1113)\n",
      "17:45 madminer.utils.ml.sc INFO    Early stopping after epoch 43, with loss 0.11 compared to final loss 0.11\n",
      "17:45 madminer.utils.ml.sc INFO    Finished training\n",
      "17:45 madminer.ml          INFO    Training estimator 7 / 10 in ensemble\n",
      "17:45 madminer.ml          INFO    Starting training\n",
      "17:45 madminer.ml          INFO      Method:                 sally\n",
      "17:45 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_6.npy\n",
      "17:45 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_6.npy\n",
      "17:45 madminer.ml          INFO      Features:               all\n",
      "17:45 madminer.ml          INFO      Method:                 sally\n",
      "17:45 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "17:45 madminer.ml          INFO      Activation function:    tanh\n",
      "17:45 madminer.ml          INFO      Batch size:             128\n",
      "17:45 madminer.ml          INFO      Trainer:                amsgrad\n",
      "17:45 madminer.ml          INFO      Epochs:                 50\n",
      "17:45 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "17:45 madminer.ml          INFO      Validation split:       0.5\n",
      "17:45 madminer.ml          INFO      Early stopping:         True\n",
      "17:45 madminer.ml          INFO      Scale inputs:           True\n",
      "17:45 madminer.ml          INFO      Shuffle labels          False\n",
      "17:45 madminer.ml          INFO      Regularization:         None\n",
      "17:45 madminer.ml          INFO      Samples:                all\n",
      "17:45 madminer.ml          INFO    Loading training data\n",
      "17:45 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "17:45 madminer.ml          INFO    Rescaling inputs\n",
      "17:45 madminer.ml          INFO    Creating model for method sally\n",
      "17:45 madminer.ml          INFO    Training model\n",
      "17:47 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.0927 (mse_score: 0.0927)\n",
      "17:47 madminer.utils.ml.sc INFO                val. loss  0.2004 (mse_score: 0.2004) (*)\n",
      "17:50 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0721 (mse_score: 0.0721)\n",
      "17:50 madminer.utils.ml.sc INFO                val. loss  0.1896 (mse_score: 0.1896) (*)\n",
      "17:53 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0614 (mse_score: 0.0614)\n",
      "17:53 madminer.utils.ml.sc INFO                val. loss  0.1858 (mse_score: 0.1858) (*)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:57 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0546 (mse_score: 0.0546)\n",
      "17:57 madminer.utils.ml.sc INFO                val. loss  0.1867 (mse_score: 0.1867)\n",
      "18:00 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0498 (mse_score: 0.0498)\n",
      "18:00 madminer.utils.ml.sc INFO                val. loss  0.1848 (mse_score: 0.1848) (*)\n",
      "18:02 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0464 (mse_score: 0.0464)\n",
      "18:02 madminer.utils.ml.sc INFO                val. loss  0.1837 (mse_score: 0.1837) (*)\n",
      "18:05 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0437 (mse_score: 0.0437)\n",
      "18:05 madminer.utils.ml.sc INFO                val. loss  0.1879 (mse_score: 0.1879)\n",
      "18:08 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0418 (mse_score: 0.0418)\n",
      "18:08 madminer.utils.ml.sc INFO                val. loss  0.1846 (mse_score: 0.1846)\n",
      "18:10 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0404 (mse_score: 0.0404)\n",
      "18:10 madminer.utils.ml.sc INFO                val. loss  0.1847 (mse_score: 0.1847)\n",
      "18:13 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0392 (mse_score: 0.0392)\n",
      "18:13 madminer.utils.ml.sc INFO                val. loss  0.1848 (mse_score: 0.1848)\n",
      "18:13 madminer.utils.ml.sc INFO    Early stopping after epoch 46, with loss 0.18 compared to final loss 0.18\n",
      "18:13 madminer.utils.ml.sc INFO    Finished training\n",
      "18:13 madminer.ml          INFO    Training estimator 8 / 10 in ensemble\n",
      "18:13 madminer.ml          INFO    Starting training\n",
      "18:13 madminer.ml          INFO      Method:                 sally\n",
      "18:13 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_7.npy\n",
      "18:13 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_7.npy\n",
      "18:13 madminer.ml          INFO      Features:               all\n",
      "18:13 madminer.ml          INFO      Method:                 sally\n",
      "18:13 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "18:13 madminer.ml          INFO      Activation function:    tanh\n",
      "18:13 madminer.ml          INFO      Batch size:             128\n",
      "18:13 madminer.ml          INFO      Trainer:                amsgrad\n",
      "18:13 madminer.ml          INFO      Epochs:                 50\n",
      "18:13 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "18:13 madminer.ml          INFO      Validation split:       0.5\n",
      "18:13 madminer.ml          INFO      Early stopping:         True\n",
      "18:13 madminer.ml          INFO      Scale inputs:           True\n",
      "18:13 madminer.ml          INFO      Shuffle labels          False\n",
      "18:13 madminer.ml          INFO      Regularization:         None\n",
      "18:13 madminer.ml          INFO      Samples:                all\n",
      "18:13 madminer.ml          INFO    Loading training data\n",
      "18:13 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "18:13 madminer.ml          INFO    Rescaling inputs\n",
      "18:13 madminer.ml          INFO    Creating model for method sally\n",
      "18:13 madminer.ml          INFO    Training model\n",
      "18:15 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1523 (mse_score: 0.1523)\n",
      "18:15 madminer.utils.ml.sc INFO                val. loss  0.1965 (mse_score: 0.1965) (*)\n",
      "18:18 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.1285 (mse_score: 0.1285)\n",
      "18:18 madminer.utils.ml.sc INFO                val. loss  0.1813 (mse_score: 0.1813) (*)\n",
      "18:21 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.1170 (mse_score: 0.1170)\n",
      "18:21 madminer.utils.ml.sc INFO                val. loss  0.1798 (mse_score: 0.1798) (*)\n",
      "18:23 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.1083 (mse_score: 0.1083)\n",
      "18:23 madminer.utils.ml.sc INFO                val. loss  0.1769 (mse_score: 0.1769)\n",
      "18:25 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.1017 (mse_score: 0.1017)\n",
      "18:25 madminer.utils.ml.sc INFO                val. loss  0.1760 (mse_score: 0.1760)\n",
      "18:27 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0975 (mse_score: 0.0975)\n",
      "18:27 madminer.utils.ml.sc INFO                val. loss  0.1752 (mse_score: 0.1752)\n",
      "18:30 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0940 (mse_score: 0.0940)\n",
      "18:30 madminer.utils.ml.sc INFO                val. loss  0.1748 (mse_score: 0.1748)\n",
      "18:32 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0917 (mse_score: 0.0917)\n",
      "18:32 madminer.utils.ml.sc INFO                val. loss  0.1748 (mse_score: 0.1748)\n",
      "18:34 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0894 (mse_score: 0.0894)\n",
      "18:34 madminer.utils.ml.sc INFO                val. loss  0.1747 (mse_score: 0.1747)\n",
      "18:37 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0879 (mse_score: 0.0879)\n",
      "18:37 madminer.utils.ml.sc INFO                val. loss  0.1748 (mse_score: 0.1748)\n",
      "18:37 madminer.utils.ml.sc INFO    Early stopping after epoch 42, with loss 0.17 compared to final loss 0.17\n",
      "18:37 madminer.utils.ml.sc INFO    Finished training\n",
      "18:37 madminer.ml          INFO    Training estimator 9 / 10 in ensemble\n",
      "18:37 madminer.ml          INFO    Starting training\n",
      "18:37 madminer.ml          INFO      Method:                 sally\n",
      "18:37 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_8.npy\n",
      "18:37 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_8.npy\n",
      "18:37 madminer.ml          INFO      Features:               all\n",
      "18:37 madminer.ml          INFO      Method:                 sally\n",
      "18:37 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "18:37 madminer.ml          INFO      Activation function:    tanh\n",
      "18:37 madminer.ml          INFO      Batch size:             128\n",
      "18:37 madminer.ml          INFO      Trainer:                amsgrad\n",
      "18:37 madminer.ml          INFO      Epochs:                 50\n",
      "18:37 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "18:37 madminer.ml          INFO      Validation split:       0.5\n",
      "18:37 madminer.ml          INFO      Early stopping:         True\n",
      "18:37 madminer.ml          INFO      Scale inputs:           True\n",
      "18:37 madminer.ml          INFO      Shuffle labels          False\n",
      "18:37 madminer.ml          INFO      Regularization:         None\n",
      "18:37 madminer.ml          INFO      Samples:                all\n",
      "18:37 madminer.ml          INFO    Loading training data\n",
      "18:37 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "18:37 madminer.ml          INFO    Rescaling inputs\n",
      "18:37 madminer.ml          INFO    Creating model for method sally\n",
      "18:37 madminer.ml          INFO    Training model\n",
      "18:39 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1403 (mse_score: 0.1403)\n",
      "18:39 madminer.utils.ml.sc INFO                val. loss  0.2073 (mse_score: 0.2073)\n",
      "18:42 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.1257 (mse_score: 0.1257)\n",
      "18:42 madminer.utils.ml.sc INFO                val. loss  0.1963 (mse_score: 0.1963) (*)\n",
      "18:45 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.1069 (mse_score: 0.1069)\n",
      "18:45 madminer.utils.ml.sc INFO                val. loss  0.1934 (mse_score: 0.1934)\n",
      "18:48 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0991 (mse_score: 0.0991)\n",
      "18:48 madminer.utils.ml.sc INFO                val. loss  0.1953 (mse_score: 0.1953)\n",
      "18:50 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0938 (mse_score: 0.0938)\n",
      "18:50 madminer.utils.ml.sc INFO                val. loss  0.1917 (mse_score: 0.1917)\n",
      "18:54 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0899 (mse_score: 0.0899)\n",
      "18:54 madminer.utils.ml.sc INFO                val. loss  0.1921 (mse_score: 0.1921)\n",
      "18:56 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0868 (mse_score: 0.0868)\n",
      "18:56 madminer.utils.ml.sc INFO                val. loss  0.1918 (mse_score: 0.1918)\n",
      "18:59 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0845 (mse_score: 0.0845)\n",
      "18:59 madminer.utils.ml.sc INFO                val. loss  0.1914 (mse_score: 0.1914)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:02 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0828 (mse_score: 0.0828)\n",
      "19:02 madminer.utils.ml.sc INFO                val. loss  0.1900 (mse_score: 0.1900)\n",
      "19:06 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0815 (mse_score: 0.0815)\n",
      "19:06 madminer.utils.ml.sc INFO                val. loss  0.1905 (mse_score: 0.1905)\n",
      "19:06 madminer.utils.ml.sc INFO    Early stopping after epoch 42, with loss 0.19 compared to final loss 0.19\n",
      "19:06 madminer.utils.ml.sc INFO    Finished training\n",
      "19:06 madminer.ml          INFO    Training estimator 10 / 10 in ensemble\n",
      "19:06 madminer.ml          INFO    Starting training\n",
      "19:06 madminer.ml          INFO      Method:                 sally\n",
      "19:06 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_9.npy\n",
      "19:06 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_9.npy\n",
      "19:06 madminer.ml          INFO      Features:               all\n",
      "19:06 madminer.ml          INFO      Method:                 sally\n",
      "19:06 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "19:06 madminer.ml          INFO      Activation function:    tanh\n",
      "19:06 madminer.ml          INFO      Batch size:             128\n",
      "19:06 madminer.ml          INFO      Trainer:                amsgrad\n",
      "19:06 madminer.ml          INFO      Epochs:                 50\n",
      "19:06 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "19:06 madminer.ml          INFO      Validation split:       0.5\n",
      "19:06 madminer.ml          INFO      Early stopping:         True\n",
      "19:06 madminer.ml          INFO      Scale inputs:           True\n",
      "19:06 madminer.ml          INFO      Shuffle labels          False\n",
      "19:06 madminer.ml          INFO      Regularization:         None\n",
      "19:06 madminer.ml          INFO      Samples:                all\n",
      "19:06 madminer.ml          INFO    Loading training data\n",
      "19:06 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "19:06 madminer.ml          INFO    Rescaling inputs\n",
      "19:06 madminer.ml          INFO    Creating model for method sally\n",
      "19:06 madminer.ml          INFO    Training model\n",
      "19:09 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.0873 (mse_score: 0.0873)\n",
      "19:09 madminer.utils.ml.sc INFO                val. loss  0.1606 (mse_score: 0.1606)\n",
      "19:12 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0676 (mse_score: 0.0676)\n",
      "19:12 madminer.utils.ml.sc INFO                val. loss  0.1423 (mse_score: 0.1423) (*)\n",
      "19:14 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0580 (mse_score: 0.0580)\n",
      "19:14 madminer.utils.ml.sc INFO                val. loss  0.1403 (mse_score: 0.1403)\n",
      "19:17 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0525 (mse_score: 0.0525)\n",
      "19:17 madminer.utils.ml.sc INFO                val. loss  0.1385 (mse_score: 0.1385)\n",
      "19:19 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0479 (mse_score: 0.0479)\n",
      "19:19 madminer.utils.ml.sc INFO                val. loss  0.1363 (mse_score: 0.1363)\n",
      "19:21 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0448 (mse_score: 0.0448)\n",
      "19:21 madminer.utils.ml.sc INFO                val. loss  0.1357 (mse_score: 0.1357)\n",
      "19:24 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0424 (mse_score: 0.0424)\n",
      "19:24 madminer.utils.ml.sc INFO                val. loss  0.1357 (mse_score: 0.1357)\n",
      "19:26 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0406 (mse_score: 0.0406)\n",
      "19:26 madminer.utils.ml.sc INFO                val. loss  0.1360 (mse_score: 0.1360)\n",
      "19:29 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0391 (mse_score: 0.0391)\n",
      "19:29 madminer.utils.ml.sc INFO                val. loss  0.1362 (mse_score: 0.1362)\n",
      "19:31 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0380 (mse_score: 0.0380)\n",
      "19:31 madminer.utils.ml.sc INFO                val. loss  0.1361 (mse_score: 0.1361)\n",
      "19:31 madminer.utils.ml.sc INFO    Early stopping after epoch 22, with loss 0.14 compared to final loss 0.14\n",
      "19:31 madminer.utils.ml.sc INFO    Finished training\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'all',\n",
    "    use_tight_cuts=False,\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'all_tight',\n",
    "    use_tight_cuts=True,\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:58 madminer.ml          INFO    Training 10 estimators in ensemble\n",
      "17:58 madminer.ml          INFO    Training estimator 1 / 10 in ensemble\n",
      "17:58 madminer.ml          INFO    Starting training\n",
      "17:58 madminer.ml          INFO      Method:                 sally\n",
      "17:58 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/x_train_0.npy\n",
      "17:58 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/t_xz_train_0.npy\n",
      "17:58 madminer.ml          INFO      Features:               all\n",
      "17:58 madminer.ml          INFO      Method:                 sally\n",
      "17:58 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "17:58 madminer.ml          INFO      Activation function:    tanh\n",
      "17:58 madminer.ml          INFO      Batch size:             128\n",
      "17:58 madminer.ml          INFO      Trainer:                amsgrad\n",
      "17:58 madminer.ml          INFO      Epochs:                 50\n",
      "17:58 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "17:58 madminer.ml          INFO      Validation split:       0.5\n",
      "17:58 madminer.ml          INFO      Early stopping:         True\n",
      "17:58 madminer.ml          INFO      Scale inputs:           True\n",
      "17:58 madminer.ml          INFO      Shuffle labels          False\n",
      "17:58 madminer.ml          INFO      Regularization:         None\n",
      "17:58 madminer.ml          INFO      Samples:                all\n",
      "17:58 madminer.ml          INFO    Loading training data\n",
      "17:58 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "17:58 madminer.ml          INFO    Rescaling inputs\n",
      "17:58 madminer.ml          INFO    Creating model for method sally\n",
      "17:58 madminer.ml          INFO    Training model\n",
      "17:58 madminer.utils.ml.sc INFO      Epoch 01: train loss 0.0991 (mse_score: 0.0991)\n",
      "17:58 madminer.utils.ml.sc INFO                val. loss  0.0864 (mse_score: 0.0864) (*)\n",
      "17:59 madminer.utils.ml.sc INFO      Epoch 02: train loss 0.0769 (mse_score: 0.0769)\n",
      "17:59 madminer.utils.ml.sc INFO                val. loss  0.0554 (mse_score: 0.0554) (*)\n",
      "17:59 madminer.utils.ml.sc INFO      Epoch 03: train loss 0.0658 (mse_score: 0.0658)\n",
      "17:59 madminer.utils.ml.sc INFO                val. loss  0.0467 (mse_score: 0.0467) (*)\n",
      "17:59 madminer.utils.ml.sc INFO      Epoch 04: train loss 0.0601 (mse_score: 0.0601)\n",
      "17:59 madminer.utils.ml.sc INFO                val. loss  0.0441 (mse_score: 0.0441) (*)\n",
      "18:00 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.0563 (mse_score: 0.0563)\n",
      "18:00 madminer.utils.ml.sc INFO                val. loss  0.0420 (mse_score: 0.0420) (*)\n",
      "18:00 madminer.utils.ml.sc INFO      Epoch 06: train loss 0.0535 (mse_score: 0.0535)\n",
      "18:00 madminer.utils.ml.sc INFO                val. loss  0.0403 (mse_score: 0.0403) (*)\n",
      "18:01 madminer.utils.ml.sc INFO      Epoch 07: train loss 0.0517 (mse_score: 0.0517)\n",
      "18:01 madminer.utils.ml.sc INFO                val. loss  0.0394 (mse_score: 0.0394) (*)\n",
      "18:01 madminer.utils.ml.sc INFO      Epoch 08: train loss 0.0507 (mse_score: 0.0507)\n",
      "18:01 madminer.utils.ml.sc INFO                val. loss  0.0385 (mse_score: 0.0385) (*)\n",
      "18:01 madminer.utils.ml.sc INFO      Epoch 09: train loss 0.0488 (mse_score: 0.0488)\n",
      "18:01 madminer.utils.ml.sc INFO                val. loss  0.0375 (mse_score: 0.0375) (*)\n",
      "18:02 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0476 (mse_score: 0.0476)\n",
      "18:02 madminer.utils.ml.sc INFO                val. loss  0.0379 (mse_score: 0.0379)\n",
      "18:02 madminer.utils.ml.sc INFO      Epoch 11: train loss 0.0468 (mse_score: 0.0468)\n",
      "18:02 madminer.utils.ml.sc INFO                val. loss  0.0366 (mse_score: 0.0366) (*)\n",
      "18:02 madminer.utils.ml.sc INFO      Epoch 12: train loss 0.0457 (mse_score: 0.0457)\n",
      "18:02 madminer.utils.ml.sc INFO                val. loss  0.0369 (mse_score: 0.0369)\n",
      "18:03 madminer.utils.ml.sc INFO      Epoch 13: train loss 0.0451 (mse_score: 0.0451)\n",
      "18:03 madminer.utils.ml.sc INFO                val. loss  0.0363 (mse_score: 0.0363) (*)\n",
      "18:03 madminer.utils.ml.sc INFO      Epoch 14: train loss 0.0446 (mse_score: 0.0446)\n",
      "18:03 madminer.utils.ml.sc INFO                val. loss  0.0356 (mse_score: 0.0356) (*)\n",
      "18:03 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0439 (mse_score: 0.0439)\n",
      "18:03 madminer.utils.ml.sc INFO                val. loss  0.0349 (mse_score: 0.0349) (*)\n",
      "18:04 madminer.utils.ml.sc INFO      Epoch 16: train loss 0.0435 (mse_score: 0.0435)\n",
      "18:04 madminer.utils.ml.sc INFO                val. loss  0.0347 (mse_score: 0.0347) (*)\n",
      "18:04 madminer.utils.ml.sc INFO      Epoch 17: train loss 0.0429 (mse_score: 0.0429)\n",
      "18:04 madminer.utils.ml.sc INFO                val. loss  0.0360 (mse_score: 0.0360)\n",
      "18:05 madminer.utils.ml.sc INFO      Epoch 18: train loss 0.0423 (mse_score: 0.0423)\n",
      "18:05 madminer.utils.ml.sc INFO                val. loss  0.0341 (mse_score: 0.0341) (*)\n",
      "18:05 madminer.utils.ml.sc INFO      Epoch 19: train loss 0.0419 (mse_score: 0.0419)\n",
      "18:05 madminer.utils.ml.sc INFO                val. loss  0.0342 (mse_score: 0.0342)\n",
      "18:05 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0415 (mse_score: 0.0415)\n",
      "18:05 madminer.utils.ml.sc INFO                val. loss  0.0339 (mse_score: 0.0339) (*)\n",
      "18:06 madminer.utils.ml.sc INFO      Epoch 21: train loss 0.0411 (mse_score: 0.0411)\n",
      "18:06 madminer.utils.ml.sc INFO                val. loss  0.0339 (mse_score: 0.0339) (*)\n",
      "18:06 madminer.utils.ml.sc INFO      Epoch 22: train loss 0.0406 (mse_score: 0.0406)\n",
      "18:06 madminer.utils.ml.sc INFO                val. loss  0.0337 (mse_score: 0.0337) (*)\n",
      "18:06 madminer.utils.ml.sc INFO      Epoch 23: train loss 0.0404 (mse_score: 0.0404)\n",
      "18:06 madminer.utils.ml.sc INFO                val. loss  0.0334 (mse_score: 0.0334) (*)\n",
      "18:07 madminer.utils.ml.sc INFO      Epoch 24: train loss 0.0401 (mse_score: 0.0401)\n",
      "18:07 madminer.utils.ml.sc INFO                val. loss  0.0334 (mse_score: 0.0334) (*)\n",
      "18:07 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0398 (mse_score: 0.0398)\n",
      "18:07 madminer.utils.ml.sc INFO                val. loss  0.0338 (mse_score: 0.0338)\n",
      "18:07 madminer.utils.ml.sc INFO      Epoch 26: train loss 0.0395 (mse_score: 0.0395)\n",
      "18:07 madminer.utils.ml.sc INFO                val. loss  0.0331 (mse_score: 0.0331) (*)\n",
      "18:08 madminer.utils.ml.sc INFO      Epoch 27: train loss 0.0392 (mse_score: 0.0392)\n",
      "18:08 madminer.utils.ml.sc INFO                val. loss  0.0332 (mse_score: 0.0332)\n",
      "18:08 madminer.utils.ml.sc INFO      Epoch 28: train loss 0.0389 (mse_score: 0.0389)\n",
      "18:08 madminer.utils.ml.sc INFO                val. loss  0.0333 (mse_score: 0.0333)\n",
      "18:09 madminer.utils.ml.sc INFO      Epoch 29: train loss 0.0387 (mse_score: 0.0387)\n",
      "18:09 madminer.utils.ml.sc INFO                val. loss  0.0330 (mse_score: 0.0330) (*)\n",
      "18:09 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0385 (mse_score: 0.0385)\n",
      "18:09 madminer.utils.ml.sc INFO                val. loss  0.0330 (mse_score: 0.0330)\n",
      "18:09 madminer.utils.ml.sc INFO      Epoch 31: train loss 0.0383 (mse_score: 0.0383)\n",
      "18:09 madminer.utils.ml.sc INFO                val. loss  0.0330 (mse_score: 0.0330)\n",
      "18:10 madminer.utils.ml.sc INFO      Epoch 32: train loss 0.0381 (mse_score: 0.0381)\n",
      "18:10 madminer.utils.ml.sc INFO                val. loss  0.0330 (mse_score: 0.0330)\n",
      "18:10 madminer.utils.ml.sc INFO      Epoch 33: train loss 0.0379 (mse_score: 0.0379)\n",
      "18:10 madminer.utils.ml.sc INFO                val. loss  0.0325 (mse_score: 0.0325) (*)\n",
      "18:10 madminer.utils.ml.sc INFO      Epoch 34: train loss 0.0377 (mse_score: 0.0377)\n",
      "18:10 madminer.utils.ml.sc INFO                val. loss  0.0360 (mse_score: 0.0360)\n",
      "18:11 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0376 (mse_score: 0.0376)\n",
      "18:11 madminer.utils.ml.sc INFO                val. loss  0.0325 (mse_score: 0.0325) (*)\n",
      "18:11 madminer.utils.ml.sc INFO      Epoch 36: train loss 0.0374 (mse_score: 0.0374)\n",
      "18:11 madminer.utils.ml.sc INFO                val. loss  0.0329 (mse_score: 0.0329)\n",
      "18:12 madminer.utils.ml.sc INFO      Epoch 37: train loss 0.0373 (mse_score: 0.0373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:12 madminer.utils.ml.sc INFO                val. loss  0.0327 (mse_score: 0.0327)\n",
      "18:12 madminer.utils.ml.sc INFO      Epoch 38: train loss 0.0371 (mse_score: 0.0371)\n",
      "18:12 madminer.utils.ml.sc INFO                val. loss  0.0324 (mse_score: 0.0324) (*)\n",
      "18:12 madminer.utils.ml.sc INFO      Epoch 39: train loss 0.0370 (mse_score: 0.0370)\n",
      "18:12 madminer.utils.ml.sc INFO                val. loss  0.0324 (mse_score: 0.0324)\n",
      "18:13 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0368 (mse_score: 0.0368)\n",
      "18:13 madminer.utils.ml.sc INFO                val. loss  0.0325 (mse_score: 0.0325)\n",
      "18:13 madminer.utils.ml.sc INFO      Epoch 41: train loss 0.0367 (mse_score: 0.0367)\n",
      "18:13 madminer.utils.ml.sc INFO                val. loss  0.0322 (mse_score: 0.0322) (*)\n",
      "18:14 madminer.utils.ml.sc INFO      Epoch 42: train loss 0.0366 (mse_score: 0.0366)\n",
      "18:14 madminer.utils.ml.sc INFO                val. loss  0.0324 (mse_score: 0.0324)\n",
      "18:14 madminer.utils.ml.sc INFO      Epoch 43: train loss 0.0365 (mse_score: 0.0365)\n",
      "18:14 madminer.utils.ml.sc INFO                val. loss  0.0325 (mse_score: 0.0325)\n",
      "18:14 madminer.utils.ml.sc INFO      Epoch 44: train loss 0.0364 (mse_score: 0.0364)\n",
      "18:14 madminer.utils.ml.sc INFO                val. loss  0.0324 (mse_score: 0.0324)\n",
      "18:15 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0362 (mse_score: 0.0362)\n",
      "18:15 madminer.utils.ml.sc INFO                val. loss  0.0321 (mse_score: 0.0321) (*)\n",
      "18:15 madminer.utils.ml.sc INFO      Epoch 46: train loss 0.0361 (mse_score: 0.0361)\n",
      "18:15 madminer.utils.ml.sc INFO                val. loss  0.0324 (mse_score: 0.0324)\n",
      "18:15 madminer.utils.ml.sc INFO      Epoch 47: train loss 0.0361 (mse_score: 0.0361)\n",
      "18:15 madminer.utils.ml.sc INFO                val. loss  0.0321 (mse_score: 0.0321) (*)\n",
      "18:16 madminer.utils.ml.sc INFO      Epoch 48: train loss 0.0360 (mse_score: 0.0360)\n",
      "18:16 madminer.utils.ml.sc INFO                val. loss  0.0320 (mse_score: 0.0320) (*)\n",
      "18:16 madminer.utils.ml.sc INFO      Epoch 49: train loss 0.0359 (mse_score: 0.0359)\n",
      "18:16 madminer.utils.ml.sc INFO                val. loss  0.0321 (mse_score: 0.0321)\n",
      "18:16 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0358 (mse_score: 0.0358)\n",
      "18:16 madminer.utils.ml.sc INFO                val. loss  0.0320 (mse_score: 0.0320) (*)\n",
      "18:16 madminer.utils.ml.sc INFO    Early stopping did not improve performance\n",
      "18:16 madminer.utils.ml.sc INFO    Finished training\n",
      "18:16 madminer.ml          INFO    Training estimator 2 / 10 in ensemble\n",
      "18:16 madminer.ml          INFO    Starting training\n",
      "18:16 madminer.ml          INFO      Method:                 sally\n",
      "18:16 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/x_train_1.npy\n",
      "18:16 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/t_xz_train_1.npy\n",
      "18:16 madminer.ml          INFO      Features:               all\n",
      "18:16 madminer.ml          INFO      Method:                 sally\n",
      "18:16 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "18:16 madminer.ml          INFO      Activation function:    tanh\n",
      "18:16 madminer.ml          INFO      Batch size:             128\n",
      "18:16 madminer.ml          INFO      Trainer:                amsgrad\n",
      "18:16 madminer.ml          INFO      Epochs:                 50\n",
      "18:16 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "18:16 madminer.ml          INFO      Validation split:       0.5\n",
      "18:16 madminer.ml          INFO      Early stopping:         True\n",
      "18:16 madminer.ml          INFO      Scale inputs:           True\n",
      "18:16 madminer.ml          INFO      Shuffle labels          False\n",
      "18:16 madminer.ml          INFO      Regularization:         None\n",
      "18:16 madminer.ml          INFO      Samples:                all\n",
      "18:16 madminer.ml          INFO    Loading training data\n",
      "18:16 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "18:16 madminer.ml          INFO    Rescaling inputs\n",
      "18:17 madminer.ml          INFO    Creating model for method sally\n",
      "18:17 madminer.ml          INFO    Training model\n",
      "18:17 madminer.utils.ml.sc INFO      Epoch 01: train loss 0.0804 (mse_score: 0.0804)\n",
      "18:17 madminer.utils.ml.sc INFO                val. loss  0.0572 (mse_score: 0.0572) (*)\n",
      "18:17 madminer.utils.ml.sc INFO      Epoch 02: train loss 0.0589 (mse_score: 0.0589)\n",
      "18:17 madminer.utils.ml.sc INFO                val. loss  0.0499 (mse_score: 0.0499) (*)\n",
      "18:18 madminer.utils.ml.sc INFO      Epoch 03: train loss 0.0472 (mse_score: 0.0472)\n",
      "18:18 madminer.utils.ml.sc INFO                val. loss  0.0411 (mse_score: 0.0411) (*)\n",
      "18:18 madminer.utils.ml.sc INFO      Epoch 04: train loss 0.0417 (mse_score: 0.0417)\n",
      "18:18 madminer.utils.ml.sc INFO                val. loss  0.0334 (mse_score: 0.0334) (*)\n",
      "18:18 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.0385 (mse_score: 0.0385)\n",
      "18:18 madminer.utils.ml.sc INFO                val. loss  0.0311 (mse_score: 0.0311) (*)\n",
      "18:19 madminer.utils.ml.sc INFO      Epoch 06: train loss 0.0358 (mse_score: 0.0358)\n",
      "18:19 madminer.utils.ml.sc INFO                val. loss  0.0300 (mse_score: 0.0300) (*)\n",
      "18:19 madminer.utils.ml.sc INFO      Epoch 07: train loss 0.0342 (mse_score: 0.0342)\n",
      "18:19 madminer.utils.ml.sc INFO                val. loss  0.0294 (mse_score: 0.0294) (*)\n",
      "18:20 madminer.utils.ml.sc INFO      Epoch 08: train loss 0.0326 (mse_score: 0.0326)\n",
      "18:20 madminer.utils.ml.sc INFO                val. loss  0.0293 (mse_score: 0.0293) (*)\n",
      "18:20 madminer.utils.ml.sc INFO      Epoch 09: train loss 0.0313 (mse_score: 0.0313)\n",
      "18:20 madminer.utils.ml.sc INFO                val. loss  0.0330 (mse_score: 0.0330)\n",
      "18:20 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0306 (mse_score: 0.0306)\n",
      "18:20 madminer.utils.ml.sc INFO                val. loss  0.0276 (mse_score: 0.0276) (*)\n",
      "18:21 madminer.utils.ml.sc INFO      Epoch 11: train loss 0.0295 (mse_score: 0.0295)\n",
      "18:21 madminer.utils.ml.sc INFO                val. loss  0.0271 (mse_score: 0.0271) (*)\n",
      "18:21 madminer.utils.ml.sc INFO      Epoch 12: train loss 0.0289 (mse_score: 0.0289)\n",
      "18:21 madminer.utils.ml.sc INFO                val. loss  0.0280 (mse_score: 0.0280)\n",
      "18:22 madminer.utils.ml.sc INFO      Epoch 13: train loss 0.0282 (mse_score: 0.0282)\n",
      "18:22 madminer.utils.ml.sc INFO                val. loss  0.0254 (mse_score: 0.0254) (*)\n",
      "18:22 madminer.utils.ml.sc INFO      Epoch 14: train loss 0.0275 (mse_score: 0.0275)\n",
      "18:22 madminer.utils.ml.sc INFO                val. loss  0.0251 (mse_score: 0.0251) (*)\n",
      "18:22 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0270 (mse_score: 0.0270)\n",
      "18:22 madminer.utils.ml.sc INFO                val. loss  0.0257 (mse_score: 0.0257)\n",
      "18:23 madminer.utils.ml.sc INFO      Epoch 16: train loss 0.0266 (mse_score: 0.0266)\n",
      "18:23 madminer.utils.ml.sc INFO                val. loss  0.0260 (mse_score: 0.0260)\n",
      "18:23 madminer.utils.ml.sc INFO      Epoch 17: train loss 0.0260 (mse_score: 0.0260)\n",
      "18:23 madminer.utils.ml.sc INFO                val. loss  0.0243 (mse_score: 0.0243) (*)\n",
      "18:24 madminer.utils.ml.sc INFO      Epoch 18: train loss 0.0256 (mse_score: 0.0256)\n",
      "18:24 madminer.utils.ml.sc INFO                val. loss  0.0243 (mse_score: 0.0243) (*)\n",
      "18:24 madminer.utils.ml.sc INFO      Epoch 19: train loss 0.0252 (mse_score: 0.0252)\n",
      "18:24 madminer.utils.ml.sc INFO                val. loss  0.0239 (mse_score: 0.0239) (*)\n",
      "18:24 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0249 (mse_score: 0.0249)\n",
      "18:24 madminer.utils.ml.sc INFO                val. loss  0.0247 (mse_score: 0.0247)\n",
      "18:25 madminer.utils.ml.sc INFO      Epoch 21: train loss 0.0246 (mse_score: 0.0246)\n",
      "18:25 madminer.utils.ml.sc INFO                val. loss  0.0247 (mse_score: 0.0247)\n",
      "18:25 madminer.utils.ml.sc INFO      Epoch 22: train loss 0.0242 (mse_score: 0.0242)\n",
      "18:25 madminer.utils.ml.sc INFO                val. loss  0.0239 (mse_score: 0.0239) (*)\n",
      "18:26 madminer.utils.ml.sc INFO      Epoch 23: train loss 0.0239 (mse_score: 0.0239)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:26 madminer.utils.ml.sc INFO                val. loss  0.0248 (mse_score: 0.0248)\n",
      "18:26 madminer.utils.ml.sc INFO      Epoch 24: train loss 0.0236 (mse_score: 0.0236)\n",
      "18:26 madminer.utils.ml.sc INFO                val. loss  0.0236 (mse_score: 0.0236) (*)\n",
      "18:26 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0233 (mse_score: 0.0233)\n",
      "18:26 madminer.utils.ml.sc INFO                val. loss  0.0231 (mse_score: 0.0231) (*)\n",
      "18:27 madminer.utils.ml.sc INFO      Epoch 26: train loss 0.0231 (mse_score: 0.0231)\n",
      "18:27 madminer.utils.ml.sc INFO                val. loss  0.0238 (mse_score: 0.0238)\n",
      "18:27 madminer.utils.ml.sc INFO      Epoch 27: train loss 0.0229 (mse_score: 0.0229)\n",
      "18:27 madminer.utils.ml.sc INFO                val. loss  0.0230 (mse_score: 0.0230) (*)\n",
      "18:28 madminer.utils.ml.sc INFO      Epoch 28: train loss 0.0227 (mse_score: 0.0227)\n",
      "18:28 madminer.utils.ml.sc INFO                val. loss  0.0230 (mse_score: 0.0230) (*)\n",
      "18:28 madminer.utils.ml.sc INFO      Epoch 29: train loss 0.0224 (mse_score: 0.0224)\n",
      "18:28 madminer.utils.ml.sc INFO                val. loss  0.0239 (mse_score: 0.0239)\n",
      "18:29 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0223 (mse_score: 0.0223)\n",
      "18:29 madminer.utils.ml.sc INFO                val. loss  0.0229 (mse_score: 0.0229) (*)\n",
      "18:29 madminer.utils.ml.sc INFO      Epoch 31: train loss 0.0222 (mse_score: 0.0222)\n",
      "18:29 madminer.utils.ml.sc INFO                val. loss  0.0228 (mse_score: 0.0228) (*)\n",
      "18:29 madminer.utils.ml.sc INFO      Epoch 32: train loss 0.0220 (mse_score: 0.0220)\n",
      "18:29 madminer.utils.ml.sc INFO                val. loss  0.0228 (mse_score: 0.0228)\n",
      "18:30 madminer.utils.ml.sc INFO      Epoch 33: train loss 0.0218 (mse_score: 0.0218)\n",
      "18:30 madminer.utils.ml.sc INFO                val. loss  0.0226 (mse_score: 0.0226) (*)\n",
      "18:30 madminer.utils.ml.sc INFO      Epoch 34: train loss 0.0217 (mse_score: 0.0217)\n",
      "18:30 madminer.utils.ml.sc INFO                val. loss  0.0227 (mse_score: 0.0227)\n",
      "18:31 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0216 (mse_score: 0.0216)\n",
      "18:31 madminer.utils.ml.sc INFO                val. loss  0.0225 (mse_score: 0.0225) (*)\n",
      "18:31 madminer.utils.ml.sc INFO      Epoch 36: train loss 0.0214 (mse_score: 0.0214)\n",
      "18:31 madminer.utils.ml.sc INFO                val. loss  0.0226 (mse_score: 0.0226)\n",
      "18:32 madminer.utils.ml.sc INFO      Epoch 37: train loss 0.0213 (mse_score: 0.0213)\n",
      "18:32 madminer.utils.ml.sc INFO                val. loss  0.0226 (mse_score: 0.0226)\n",
      "18:32 madminer.utils.ml.sc INFO      Epoch 38: train loss 0.0212 (mse_score: 0.0212)\n",
      "18:32 madminer.utils.ml.sc INFO                val. loss  0.0227 (mse_score: 0.0227)\n",
      "18:33 madminer.utils.ml.sc INFO      Epoch 39: train loss 0.0211 (mse_score: 0.0211)\n",
      "18:33 madminer.utils.ml.sc INFO                val. loss  0.0226 (mse_score: 0.0226)\n",
      "18:33 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0210 (mse_score: 0.0210)\n",
      "18:33 madminer.utils.ml.sc INFO                val. loss  0.0237 (mse_score: 0.0237)\n",
      "18:34 madminer.utils.ml.sc INFO      Epoch 41: train loss 0.0209 (mse_score: 0.0209)\n",
      "18:34 madminer.utils.ml.sc INFO                val. loss  0.0224 (mse_score: 0.0224) (*)\n",
      "18:34 madminer.utils.ml.sc INFO      Epoch 42: train loss 0.0208 (mse_score: 0.0208)\n",
      "18:34 madminer.utils.ml.sc INFO                val. loss  0.0227 (mse_score: 0.0227)\n",
      "18:35 madminer.utils.ml.sc INFO      Epoch 43: train loss 0.0207 (mse_score: 0.0207)\n",
      "18:35 madminer.utils.ml.sc INFO                val. loss  0.0223 (mse_score: 0.0223) (*)\n",
      "18:35 madminer.utils.ml.sc INFO      Epoch 44: train loss 0.0207 (mse_score: 0.0207)\n",
      "18:35 madminer.utils.ml.sc INFO                val. loss  0.0224 (mse_score: 0.0224)\n",
      "18:35 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0206 (mse_score: 0.0206)\n",
      "18:35 madminer.utils.ml.sc INFO                val. loss  0.0223 (mse_score: 0.0223) (*)\n",
      "18:36 madminer.utils.ml.sc INFO      Epoch 46: train loss 0.0205 (mse_score: 0.0205)\n",
      "18:36 madminer.utils.ml.sc INFO                val. loss  0.0222 (mse_score: 0.0222) (*)\n",
      "18:36 madminer.utils.ml.sc INFO      Epoch 47: train loss 0.0204 (mse_score: 0.0204)\n",
      "18:36 madminer.utils.ml.sc INFO                val. loss  0.0223 (mse_score: 0.0223)\n",
      "18:37 madminer.utils.ml.sc INFO      Epoch 48: train loss 0.0204 (mse_score: 0.0204)\n",
      "18:37 madminer.utils.ml.sc INFO                val. loss  0.0221 (mse_score: 0.0221) (*)\n",
      "18:37 madminer.utils.ml.sc INFO      Epoch 49: train loss 0.0203 (mse_score: 0.0203)\n",
      "18:37 madminer.utils.ml.sc INFO                val. loss  0.0222 (mse_score: 0.0222)\n",
      "18:38 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0202 (mse_score: 0.0202)\n",
      "18:38 madminer.utils.ml.sc INFO                val. loss  0.0221 (mse_score: 0.0221)\n",
      "18:38 madminer.utils.ml.sc INFO    Early stopping after epoch 48, with loss 0.02 compared to final loss 0.02\n",
      "18:38 madminer.utils.ml.sc INFO    Finished training\n",
      "18:38 madminer.ml          INFO    Training estimator 3 / 10 in ensemble\n",
      "18:38 madminer.ml          INFO    Starting training\n",
      "18:38 madminer.ml          INFO      Method:                 sally\n",
      "18:38 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/x_train_2.npy\n",
      "18:38 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/t_xz_train_2.npy\n",
      "18:38 madminer.ml          INFO      Features:               all\n",
      "18:38 madminer.ml          INFO      Method:                 sally\n",
      "18:38 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "18:38 madminer.ml          INFO      Activation function:    tanh\n",
      "18:38 madminer.ml          INFO      Batch size:             128\n",
      "18:38 madminer.ml          INFO      Trainer:                amsgrad\n",
      "18:38 madminer.ml          INFO      Epochs:                 50\n",
      "18:38 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "18:38 madminer.ml          INFO      Validation split:       0.5\n",
      "18:38 madminer.ml          INFO      Early stopping:         True\n",
      "18:38 madminer.ml          INFO      Scale inputs:           True\n",
      "18:38 madminer.ml          INFO      Shuffle labels          False\n",
      "18:38 madminer.ml          INFO      Regularization:         None\n",
      "18:38 madminer.ml          INFO      Samples:                all\n",
      "18:38 madminer.ml          INFO    Loading training data\n",
      "18:38 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "18:38 madminer.ml          INFO    Rescaling inputs\n",
      "18:38 madminer.ml          INFO    Creating model for method sally\n",
      "18:38 madminer.ml          INFO    Training model\n",
      "18:38 madminer.utils.ml.sc INFO      Epoch 01: train loss 0.0888 (mse_score: 0.0888)\n",
      "18:38 madminer.utils.ml.sc INFO                val. loss  0.0910 (mse_score: 0.0910) (*)\n",
      "18:39 madminer.utils.ml.sc INFO      Epoch 02: train loss 0.0690 (mse_score: 0.0690)\n",
      "18:39 madminer.utils.ml.sc INFO                val. loss  0.0760 (mse_score: 0.0760) (*)\n",
      "18:39 madminer.utils.ml.sc INFO      Epoch 03: train loss 0.0586 (mse_score: 0.0586)\n",
      "18:39 madminer.utils.ml.sc INFO                val. loss  0.0739 (mse_score: 0.0739) (*)\n",
      "18:40 madminer.utils.ml.sc INFO      Epoch 04: train loss 0.0520 (mse_score: 0.0520)\n",
      "18:40 madminer.utils.ml.sc INFO                val. loss  0.0655 (mse_score: 0.0655) (*)\n",
      "18:40 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.0489 (mse_score: 0.0489)\n",
      "18:40 madminer.utils.ml.sc INFO                val. loss  0.0666 (mse_score: 0.0666)\n",
      "18:40 madminer.utils.ml.sc INFO      Epoch 06: train loss 0.0463 (mse_score: 0.0463)\n",
      "18:40 madminer.utils.ml.sc INFO                val. loss  0.0621 (mse_score: 0.0621) (*)\n",
      "18:41 madminer.utils.ml.sc INFO      Epoch 07: train loss 0.0437 (mse_score: 0.0437)\n",
      "18:41 madminer.utils.ml.sc INFO                val. loss  0.0611 (mse_score: 0.0611) (*)\n",
      "18:41 madminer.utils.ml.sc INFO      Epoch 08: train loss 0.0419 (mse_score: 0.0419)\n",
      "18:41 madminer.utils.ml.sc INFO                val. loss  0.0604 (mse_score: 0.0604) (*)\n",
      "18:42 madminer.utils.ml.sc INFO      Epoch 09: train loss 0.0405 (mse_score: 0.0405)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:42 madminer.utils.ml.sc INFO                val. loss  0.0592 (mse_score: 0.0592) (*)\n",
      "18:42 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0394 (mse_score: 0.0394)\n",
      "18:42 madminer.utils.ml.sc INFO                val. loss  0.0598 (mse_score: 0.0598)\n",
      "18:43 madminer.utils.ml.sc INFO      Epoch 11: train loss 0.0384 (mse_score: 0.0384)\n",
      "18:43 madminer.utils.ml.sc INFO                val. loss  0.0584 (mse_score: 0.0584) (*)\n",
      "18:43 madminer.utils.ml.sc INFO      Epoch 12: train loss 0.0374 (mse_score: 0.0374)\n",
      "18:43 madminer.utils.ml.sc INFO                val. loss  0.0585 (mse_score: 0.0585)\n",
      "18:44 madminer.utils.ml.sc INFO      Epoch 13: train loss 0.0370 (mse_score: 0.0370)\n",
      "18:44 madminer.utils.ml.sc INFO                val. loss  0.0575 (mse_score: 0.0575) (*)\n",
      "18:44 madminer.utils.ml.sc INFO      Epoch 14: train loss 0.0362 (mse_score: 0.0362)\n",
      "18:44 madminer.utils.ml.sc INFO                val. loss  0.0572 (mse_score: 0.0572) (*)\n",
      "18:45 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0355 (mse_score: 0.0355)\n",
      "18:45 madminer.utils.ml.sc INFO                val. loss  0.0566 (mse_score: 0.0566) (*)\n",
      "18:45 madminer.utils.ml.sc INFO      Epoch 16: train loss 0.0350 (mse_score: 0.0350)\n",
      "18:45 madminer.utils.ml.sc INFO                val. loss  0.0568 (mse_score: 0.0568)\n",
      "18:45 madminer.utils.ml.sc INFO      Epoch 17: train loss 0.0345 (mse_score: 0.0345)\n",
      "18:45 madminer.utils.ml.sc INFO                val. loss  0.0565 (mse_score: 0.0565) (*)\n",
      "18:46 madminer.utils.ml.sc INFO      Epoch 18: train loss 0.0340 (mse_score: 0.0340)\n",
      "18:46 madminer.utils.ml.sc INFO                val. loss  0.0560 (mse_score: 0.0560) (*)\n",
      "18:46 madminer.utils.ml.sc INFO      Epoch 19: train loss 0.0336 (mse_score: 0.0336)\n",
      "18:46 madminer.utils.ml.sc INFO                val. loss  0.0557 (mse_score: 0.0557) (*)\n",
      "18:47 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0332 (mse_score: 0.0332)\n",
      "18:47 madminer.utils.ml.sc INFO                val. loss  0.0565 (mse_score: 0.0565)\n",
      "18:48 madminer.utils.ml.sc INFO      Epoch 21: train loss 0.0328 (mse_score: 0.0328)\n",
      "18:48 madminer.utils.ml.sc INFO                val. loss  0.0563 (mse_score: 0.0563)\n",
      "18:48 madminer.utils.ml.sc INFO      Epoch 22: train loss 0.0325 (mse_score: 0.0325)\n",
      "18:48 madminer.utils.ml.sc INFO                val. loss  0.0552 (mse_score: 0.0552) (*)\n",
      "18:49 madminer.utils.ml.sc INFO      Epoch 23: train loss 0.0322 (mse_score: 0.0322)\n",
      "18:49 madminer.utils.ml.sc INFO                val. loss  0.0553 (mse_score: 0.0553)\n",
      "18:49 madminer.utils.ml.sc INFO      Epoch 24: train loss 0.0319 (mse_score: 0.0319)\n",
      "18:49 madminer.utils.ml.sc INFO                val. loss  0.0554 (mse_score: 0.0554)\n",
      "18:50 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0317 (mse_score: 0.0317)\n",
      "18:50 madminer.utils.ml.sc INFO                val. loss  0.0553 (mse_score: 0.0553)\n",
      "18:50 madminer.utils.ml.sc INFO      Epoch 26: train loss 0.0314 (mse_score: 0.0314)\n",
      "18:50 madminer.utils.ml.sc INFO                val. loss  0.0547 (mse_score: 0.0547) (*)\n",
      "18:51 madminer.utils.ml.sc INFO      Epoch 27: train loss 0.0312 (mse_score: 0.0312)\n",
      "18:51 madminer.utils.ml.sc INFO                val. loss  0.0551 (mse_score: 0.0551)\n",
      "18:51 madminer.utils.ml.sc INFO      Epoch 28: train loss 0.0310 (mse_score: 0.0310)\n",
      "18:51 madminer.utils.ml.sc INFO                val. loss  0.0549 (mse_score: 0.0549)\n",
      "18:52 madminer.utils.ml.sc INFO      Epoch 29: train loss 0.0307 (mse_score: 0.0307)\n",
      "18:52 madminer.utils.ml.sc INFO                val. loss  0.0547 (mse_score: 0.0547) (*)\n",
      "18:52 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0305 (mse_score: 0.0305)\n",
      "18:52 madminer.utils.ml.sc INFO                val. loss  0.0546 (mse_score: 0.0546) (*)\n",
      "18:53 madminer.utils.ml.sc INFO      Epoch 31: train loss 0.0304 (mse_score: 0.0304)\n",
      "18:53 madminer.utils.ml.sc INFO                val. loss  0.0546 (mse_score: 0.0546)\n",
      "18:53 madminer.utils.ml.sc INFO      Epoch 32: train loss 0.0302 (mse_score: 0.0302)\n",
      "18:53 madminer.utils.ml.sc INFO                val. loss  0.0546 (mse_score: 0.0546) (*)\n",
      "18:54 madminer.utils.ml.sc INFO      Epoch 33: train loss 0.0300 (mse_score: 0.0300)\n",
      "18:54 madminer.utils.ml.sc INFO                val. loss  0.0545 (mse_score: 0.0545) (*)\n",
      "18:54 madminer.utils.ml.sc INFO      Epoch 34: train loss 0.0299 (mse_score: 0.0299)\n",
      "18:54 madminer.utils.ml.sc INFO                val. loss  0.0543 (mse_score: 0.0543) (*)\n",
      "18:55 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0297 (mse_score: 0.0297)\n",
      "18:55 madminer.utils.ml.sc INFO                val. loss  0.0542 (mse_score: 0.0542) (*)\n",
      "18:55 madminer.utils.ml.sc INFO      Epoch 36: train loss 0.0296 (mse_score: 0.0296)\n",
      "18:55 madminer.utils.ml.sc INFO                val. loss  0.0543 (mse_score: 0.0543)\n",
      "18:56 madminer.utils.ml.sc INFO      Epoch 37: train loss 0.0295 (mse_score: 0.0295)\n",
      "18:56 madminer.utils.ml.sc INFO                val. loss  0.0542 (mse_score: 0.0542)\n",
      "18:56 madminer.utils.ml.sc INFO      Epoch 38: train loss 0.0294 (mse_score: 0.0294)\n",
      "18:56 madminer.utils.ml.sc INFO                val. loss  0.0541 (mse_score: 0.0541) (*)\n",
      "18:56 madminer.utils.ml.sc INFO      Epoch 39: train loss 0.0292 (mse_score: 0.0292)\n",
      "18:56 madminer.utils.ml.sc INFO                val. loss  0.0540 (mse_score: 0.0540) (*)\n",
      "18:57 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0291 (mse_score: 0.0291)\n",
      "18:57 madminer.utils.ml.sc INFO                val. loss  0.0540 (mse_score: 0.0540) (*)\n",
      "18:57 madminer.utils.ml.sc INFO      Epoch 41: train loss 0.0290 (mse_score: 0.0290)\n",
      "18:57 madminer.utils.ml.sc INFO                val. loss  0.0540 (mse_score: 0.0540)\n",
      "18:58 madminer.utils.ml.sc INFO      Epoch 42: train loss 0.0289 (mse_score: 0.0289)\n",
      "18:58 madminer.utils.ml.sc INFO                val. loss  0.0539 (mse_score: 0.0539) (*)\n",
      "18:58 madminer.utils.ml.sc INFO      Epoch 43: train loss 0.0288 (mse_score: 0.0288)\n",
      "18:58 madminer.utils.ml.sc INFO                val. loss  0.0540 (mse_score: 0.0540)\n",
      "18:59 madminer.utils.ml.sc INFO      Epoch 44: train loss 0.0287 (mse_score: 0.0287)\n",
      "18:59 madminer.utils.ml.sc INFO                val. loss  0.0540 (mse_score: 0.0540)\n",
      "18:59 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0286 (mse_score: 0.0286)\n",
      "18:59 madminer.utils.ml.sc INFO                val. loss  0.0539 (mse_score: 0.0539)\n",
      "18:59 madminer.utils.ml.sc INFO      Epoch 46: train loss 0.0285 (mse_score: 0.0285)\n",
      "18:59 madminer.utils.ml.sc INFO                val. loss  0.0539 (mse_score: 0.0539)\n",
      "19:00 madminer.utils.ml.sc INFO      Epoch 47: train loss 0.0284 (mse_score: 0.0284)\n",
      "19:00 madminer.utils.ml.sc INFO                val. loss  0.0538 (mse_score: 0.0538) (*)\n",
      "19:00 madminer.utils.ml.sc INFO      Epoch 48: train loss 0.0284 (mse_score: 0.0284)\n",
      "19:00 madminer.utils.ml.sc INFO                val. loss  0.0538 (mse_score: 0.0538) (*)\n",
      "19:01 madminer.utils.ml.sc INFO      Epoch 49: train loss 0.0283 (mse_score: 0.0283)\n",
      "19:01 madminer.utils.ml.sc INFO                val. loss  0.0538 (mse_score: 0.0538)\n",
      "19:01 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0282 (mse_score: 0.0282)\n",
      "19:01 madminer.utils.ml.sc INFO                val. loss  0.0537 (mse_score: 0.0537) (*)\n",
      "19:01 madminer.utils.ml.sc INFO    Early stopping did not improve performance\n",
      "19:01 madminer.utils.ml.sc INFO    Finished training\n",
      "19:01 madminer.ml          INFO    Training estimator 4 / 10 in ensemble\n",
      "19:01 madminer.ml          INFO    Starting training\n",
      "19:01 madminer.ml          INFO      Method:                 sally\n",
      "19:01 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/x_train_3.npy\n",
      "19:01 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/t_xz_train_3.npy\n",
      "19:01 madminer.ml          INFO      Features:               all\n",
      "19:01 madminer.ml          INFO      Method:                 sally\n",
      "19:01 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "19:01 madminer.ml          INFO      Activation function:    tanh\n",
      "19:01 madminer.ml          INFO      Batch size:             128\n",
      "19:01 madminer.ml          INFO      Trainer:                amsgrad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:01 madminer.ml          INFO      Epochs:                 50\n",
      "19:01 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "19:01 madminer.ml          INFO      Validation split:       0.5\n",
      "19:01 madminer.ml          INFO      Early stopping:         True\n",
      "19:01 madminer.ml          INFO      Scale inputs:           True\n",
      "19:01 madminer.ml          INFO      Shuffle labels          False\n",
      "19:01 madminer.ml          INFO      Regularization:         None\n",
      "19:01 madminer.ml          INFO      Samples:                all\n",
      "19:01 madminer.ml          INFO    Loading training data\n",
      "19:01 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "19:01 madminer.ml          INFO    Rescaling inputs\n",
      "19:01 madminer.ml          INFO    Creating model for method sally\n",
      "19:01 madminer.ml          INFO    Training model\n",
      "19:02 madminer.utils.ml.sc INFO      Epoch 01: train loss 0.0977 (mse_score: 0.0977)\n",
      "19:02 madminer.utils.ml.sc INFO                val. loss  0.0926 (mse_score: 0.0926) (*)\n",
      "19:02 madminer.utils.ml.sc INFO      Epoch 02: train loss 0.0794 (mse_score: 0.0794)\n",
      "19:02 madminer.utils.ml.sc INFO                val. loss  0.0756 (mse_score: 0.0756) (*)\n",
      "19:03 madminer.utils.ml.sc INFO      Epoch 03: train loss 0.0688 (mse_score: 0.0688)\n",
      "19:03 madminer.utils.ml.sc INFO                val. loss  0.0688 (mse_score: 0.0688) (*)\n",
      "19:03 madminer.utils.ml.sc INFO      Epoch 04: train loss 0.0635 (mse_score: 0.0635)\n",
      "19:03 madminer.utils.ml.sc INFO                val. loss  0.0651 (mse_score: 0.0651) (*)\n",
      "19:03 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.0593 (mse_score: 0.0593)\n",
      "19:03 madminer.utils.ml.sc INFO                val. loss  0.0643 (mse_score: 0.0643) (*)\n",
      "19:04 madminer.utils.ml.sc INFO      Epoch 06: train loss 0.0563 (mse_score: 0.0563)\n",
      "19:04 madminer.utils.ml.sc INFO                val. loss  0.0634 (mse_score: 0.0634) (*)\n",
      "19:04 madminer.utils.ml.sc INFO      Epoch 07: train loss 0.0541 (mse_score: 0.0541)\n",
      "19:04 madminer.utils.ml.sc INFO                val. loss  0.0611 (mse_score: 0.0611) (*)\n",
      "19:05 madminer.utils.ml.sc INFO      Epoch 08: train loss 0.0524 (mse_score: 0.0524)\n",
      "19:05 madminer.utils.ml.sc INFO                val. loss  0.0587 (mse_score: 0.0587) (*)\n",
      "19:05 madminer.utils.ml.sc INFO      Epoch 09: train loss 0.0513 (mse_score: 0.0513)\n",
      "19:05 madminer.utils.ml.sc INFO                val. loss  0.0579 (mse_score: 0.0579) (*)\n",
      "19:06 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0498 (mse_score: 0.0498)\n",
      "19:06 madminer.utils.ml.sc INFO                val. loss  0.0595 (mse_score: 0.0595)\n",
      "19:06 madminer.utils.ml.sc INFO      Epoch 11: train loss 0.0490 (mse_score: 0.0490)\n",
      "19:06 madminer.utils.ml.sc INFO                val. loss  0.0567 (mse_score: 0.0567) (*)\n",
      "19:06 madminer.utils.ml.sc INFO      Epoch 12: train loss 0.0477 (mse_score: 0.0477)\n",
      "19:06 madminer.utils.ml.sc INFO                val. loss  0.0569 (mse_score: 0.0569)\n",
      "19:07 madminer.utils.ml.sc INFO      Epoch 13: train loss 0.0473 (mse_score: 0.0473)\n",
      "19:07 madminer.utils.ml.sc INFO                val. loss  0.0571 (mse_score: 0.0571)\n",
      "19:07 madminer.utils.ml.sc INFO      Epoch 14: train loss 0.0463 (mse_score: 0.0463)\n",
      "19:07 madminer.utils.ml.sc INFO                val. loss  0.0575 (mse_score: 0.0575)\n",
      "19:08 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0457 (mse_score: 0.0457)\n",
      "19:08 madminer.utils.ml.sc INFO                val. loss  0.0553 (mse_score: 0.0553) (*)\n",
      "19:08 madminer.utils.ml.sc INFO      Epoch 16: train loss 0.0449 (mse_score: 0.0449)\n",
      "19:08 madminer.utils.ml.sc INFO                val. loss  0.0552 (mse_score: 0.0552) (*)\n",
      "19:09 madminer.utils.ml.sc INFO      Epoch 17: train loss 0.0444 (mse_score: 0.0444)\n",
      "19:09 madminer.utils.ml.sc INFO                val. loss  0.0541 (mse_score: 0.0541) (*)\n",
      "19:09 madminer.utils.ml.sc INFO      Epoch 18: train loss 0.0439 (mse_score: 0.0439)\n",
      "19:09 madminer.utils.ml.sc INFO                val. loss  0.0542 (mse_score: 0.0542)\n",
      "19:10 madminer.utils.ml.sc INFO      Epoch 19: train loss 0.0433 (mse_score: 0.0433)\n",
      "19:10 madminer.utils.ml.sc INFO                val. loss  0.0538 (mse_score: 0.0538) (*)\n",
      "19:10 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0429 (mse_score: 0.0429)\n",
      "19:10 madminer.utils.ml.sc INFO                val. loss  0.0535 (mse_score: 0.0535) (*)\n",
      "19:10 madminer.utils.ml.sc INFO      Epoch 21: train loss 0.0424 (mse_score: 0.0424)\n",
      "19:10 madminer.utils.ml.sc INFO                val. loss  0.0537 (mse_score: 0.0537)\n",
      "19:11 madminer.utils.ml.sc INFO      Epoch 22: train loss 0.0421 (mse_score: 0.0421)\n",
      "19:11 madminer.utils.ml.sc INFO                val. loss  0.0536 (mse_score: 0.0536)\n",
      "19:11 madminer.utils.ml.sc INFO      Epoch 23: train loss 0.0417 (mse_score: 0.0417)\n",
      "19:11 madminer.utils.ml.sc INFO                val. loss  0.0533 (mse_score: 0.0533) (*)\n",
      "19:12 madminer.utils.ml.sc INFO      Epoch 24: train loss 0.0414 (mse_score: 0.0414)\n",
      "19:12 madminer.utils.ml.sc INFO                val. loss  0.0533 (mse_score: 0.0533) (*)\n",
      "19:12 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0411 (mse_score: 0.0411)\n",
      "19:12 madminer.utils.ml.sc INFO                val. loss  0.0532 (mse_score: 0.0532) (*)\n",
      "19:13 madminer.utils.ml.sc INFO      Epoch 26: train loss 0.0408 (mse_score: 0.0408)\n",
      "19:13 madminer.utils.ml.sc INFO                val. loss  0.0529 (mse_score: 0.0529) (*)\n",
      "19:13 madminer.utils.ml.sc INFO      Epoch 27: train loss 0.0405 (mse_score: 0.0405)\n",
      "19:13 madminer.utils.ml.sc INFO                val. loss  0.0527 (mse_score: 0.0527) (*)\n",
      "19:14 madminer.utils.ml.sc INFO      Epoch 28: train loss 0.0402 (mse_score: 0.0402)\n",
      "19:14 madminer.utils.ml.sc INFO                val. loss  0.0525 (mse_score: 0.0525) (*)\n",
      "19:14 madminer.utils.ml.sc INFO      Epoch 29: train loss 0.0400 (mse_score: 0.0400)\n",
      "19:14 madminer.utils.ml.sc INFO                val. loss  0.0528 (mse_score: 0.0528)\n",
      "19:14 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0398 (mse_score: 0.0398)\n",
      "19:14 madminer.utils.ml.sc INFO                val. loss  0.0526 (mse_score: 0.0526)\n",
      "19:15 madminer.utils.ml.sc INFO      Epoch 31: train loss 0.0396 (mse_score: 0.0396)\n",
      "19:15 madminer.utils.ml.sc INFO                val. loss  0.0524 (mse_score: 0.0524) (*)\n",
      "19:15 madminer.utils.ml.sc INFO      Epoch 32: train loss 0.0393 (mse_score: 0.0393)\n",
      "19:15 madminer.utils.ml.sc INFO                val. loss  0.0525 (mse_score: 0.0525)\n",
      "19:16 madminer.utils.ml.sc INFO      Epoch 33: train loss 0.0392 (mse_score: 0.0392)\n",
      "19:16 madminer.utils.ml.sc INFO                val. loss  0.0521 (mse_score: 0.0521) (*)\n",
      "19:16 madminer.utils.ml.sc INFO      Epoch 34: train loss 0.0390 (mse_score: 0.0390)\n",
      "19:16 madminer.utils.ml.sc INFO                val. loss  0.0520 (mse_score: 0.0520) (*)\n",
      "19:17 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0388 (mse_score: 0.0388)\n",
      "19:17 madminer.utils.ml.sc INFO                val. loss  0.0519 (mse_score: 0.0519) (*)\n",
      "19:17 madminer.utils.ml.sc INFO      Epoch 36: train loss 0.0386 (mse_score: 0.0386)\n",
      "19:17 madminer.utils.ml.sc INFO                val. loss  0.0521 (mse_score: 0.0521)\n",
      "19:17 madminer.utils.ml.sc INFO      Epoch 37: train loss 0.0385 (mse_score: 0.0385)\n",
      "19:17 madminer.utils.ml.sc INFO                val. loss  0.0521 (mse_score: 0.0521)\n",
      "19:18 madminer.utils.ml.sc INFO      Epoch 38: train loss 0.0383 (mse_score: 0.0383)\n",
      "19:18 madminer.utils.ml.sc INFO                val. loss  0.0520 (mse_score: 0.0520)\n",
      "19:18 madminer.utils.ml.sc INFO      Epoch 39: train loss 0.0382 (mse_score: 0.0382)\n",
      "19:18 madminer.utils.ml.sc INFO                val. loss  0.0518 (mse_score: 0.0518) (*)\n",
      "19:19 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0381 (mse_score: 0.0381)\n",
      "19:19 madminer.utils.ml.sc INFO                val. loss  0.0519 (mse_score: 0.0519)\n",
      "19:19 madminer.utils.ml.sc INFO      Epoch 41: train loss 0.0379 (mse_score: 0.0379)\n",
      "19:19 madminer.utils.ml.sc INFO                val. loss  0.0518 (mse_score: 0.0518) (*)\n",
      "19:20 madminer.utils.ml.sc INFO      Epoch 42: train loss 0.0378 (mse_score: 0.0378)\n",
      "19:20 madminer.utils.ml.sc INFO                val. loss  0.0517 (mse_score: 0.0517) (*)\n",
      "19:20 madminer.utils.ml.sc INFO      Epoch 43: train loss 0.0377 (mse_score: 0.0377)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:20 madminer.utils.ml.sc INFO                val. loss  0.0519 (mse_score: 0.0519)\n",
      "19:20 madminer.utils.ml.sc INFO      Epoch 44: train loss 0.0376 (mse_score: 0.0376)\n",
      "19:20 madminer.utils.ml.sc INFO                val. loss  0.0517 (mse_score: 0.0517) (*)\n",
      "19:21 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0375 (mse_score: 0.0375)\n",
      "19:21 madminer.utils.ml.sc INFO                val. loss  0.0517 (mse_score: 0.0517)\n",
      "19:21 madminer.utils.ml.sc INFO      Epoch 46: train loss 0.0374 (mse_score: 0.0374)\n",
      "19:21 madminer.utils.ml.sc INFO                val. loss  0.0518 (mse_score: 0.0518)\n",
      "19:22 madminer.utils.ml.sc INFO      Epoch 47: train loss 0.0373 (mse_score: 0.0373)\n",
      "19:22 madminer.utils.ml.sc INFO                val. loss  0.0515 (mse_score: 0.0515) (*)\n",
      "19:22 madminer.utils.ml.sc INFO      Epoch 48: train loss 0.0373 (mse_score: 0.0373)\n",
      "19:22 madminer.utils.ml.sc INFO                val. loss  0.0515 (mse_score: 0.0515)\n",
      "19:23 madminer.utils.ml.sc INFO      Epoch 49: train loss 0.0371 (mse_score: 0.0371)\n",
      "19:23 madminer.utils.ml.sc INFO                val. loss  0.0515 (mse_score: 0.0515)\n",
      "19:23 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0371 (mse_score: 0.0371)\n",
      "19:23 madminer.utils.ml.sc INFO                val. loss  0.0515 (mse_score: 0.0515)\n",
      "19:23 madminer.utils.ml.sc INFO    Early stopping after epoch 47, with loss 0.05 compared to final loss 0.05\n",
      "19:23 madminer.utils.ml.sc INFO    Finished training\n",
      "19:23 madminer.ml          INFO    Training estimator 5 / 10 in ensemble\n",
      "19:23 madminer.ml          INFO    Starting training\n",
      "19:23 madminer.ml          INFO      Method:                 sally\n",
      "19:23 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/x_train_4.npy\n",
      "19:23 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/t_xz_train_4.npy\n",
      "19:23 madminer.ml          INFO      Features:               all\n",
      "19:23 madminer.ml          INFO      Method:                 sally\n",
      "19:23 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "19:23 madminer.ml          INFO      Activation function:    tanh\n",
      "19:23 madminer.ml          INFO      Batch size:             128\n",
      "19:23 madminer.ml          INFO      Trainer:                amsgrad\n",
      "19:23 madminer.ml          INFO      Epochs:                 50\n",
      "19:23 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "19:23 madminer.ml          INFO      Validation split:       0.5\n",
      "19:23 madminer.ml          INFO      Early stopping:         True\n",
      "19:23 madminer.ml          INFO      Scale inputs:           True\n",
      "19:23 madminer.ml          INFO      Shuffle labels          False\n",
      "19:23 madminer.ml          INFO      Regularization:         None\n",
      "19:23 madminer.ml          INFO      Samples:                all\n",
      "19:23 madminer.ml          INFO    Loading training data\n",
      "19:23 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "19:23 madminer.ml          INFO    Rescaling inputs\n",
      "19:23 madminer.ml          INFO    Creating model for method sally\n",
      "19:23 madminer.ml          INFO    Training model\n",
      "19:23 madminer.utils.ml.sc INFO      Epoch 01: train loss 0.0721 (mse_score: 0.0721)\n",
      "19:23 madminer.utils.ml.sc INFO                val. loss  0.0887 (mse_score: 0.0887) (*)\n",
      "19:24 madminer.utils.ml.sc INFO      Epoch 02: train loss 0.0510 (mse_score: 0.0510)\n",
      "19:24 madminer.utils.ml.sc INFO                val. loss  0.0747 (mse_score: 0.0747) (*)\n",
      "19:24 madminer.utils.ml.sc INFO      Epoch 03: train loss 0.0415 (mse_score: 0.0415)\n",
      "19:24 madminer.utils.ml.sc INFO                val. loss  0.0690 (mse_score: 0.0690) (*)\n",
      "19:24 madminer.utils.ml.sc INFO      Epoch 04: train loss 0.0366 (mse_score: 0.0366)\n",
      "19:24 madminer.utils.ml.sc INFO                val. loss  0.0666 (mse_score: 0.0666) (*)\n",
      "19:25 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.0335 (mse_score: 0.0335)\n",
      "19:25 madminer.utils.ml.sc INFO                val. loss  0.0639 (mse_score: 0.0639) (*)\n",
      "19:25 madminer.utils.ml.sc INFO      Epoch 06: train loss 0.0313 (mse_score: 0.0313)\n",
      "19:25 madminer.utils.ml.sc INFO                val. loss  0.0616 (mse_score: 0.0616) (*)\n",
      "19:26 madminer.utils.ml.sc INFO      Epoch 07: train loss 0.0294 (mse_score: 0.0294)\n",
      "19:26 madminer.utils.ml.sc INFO                val. loss  0.0623 (mse_score: 0.0623)\n",
      "19:26 madminer.utils.ml.sc INFO      Epoch 08: train loss 0.0280 (mse_score: 0.0280)\n",
      "19:26 madminer.utils.ml.sc INFO                val. loss  0.0617 (mse_score: 0.0617)\n",
      "19:26 madminer.utils.ml.sc INFO      Epoch 09: train loss 0.0268 (mse_score: 0.0268)\n",
      "19:26 madminer.utils.ml.sc INFO                val. loss  0.0593 (mse_score: 0.0593) (*)\n",
      "19:27 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0259 (mse_score: 0.0259)\n",
      "19:27 madminer.utils.ml.sc INFO                val. loss  0.0585 (mse_score: 0.0585) (*)\n",
      "19:27 madminer.utils.ml.sc INFO      Epoch 11: train loss 0.0248 (mse_score: 0.0248)\n",
      "19:27 madminer.utils.ml.sc INFO                val. loss  0.0582 (mse_score: 0.0582) (*)\n",
      "19:27 madminer.utils.ml.sc INFO      Epoch 12: train loss 0.0241 (mse_score: 0.0241)\n",
      "19:27 madminer.utils.ml.sc INFO                val. loss  0.0573 (mse_score: 0.0573) (*)\n",
      "19:28 madminer.utils.ml.sc INFO      Epoch 13: train loss 0.0235 (mse_score: 0.0235)\n",
      "19:28 madminer.utils.ml.sc INFO                val. loss  0.0578 (mse_score: 0.0578)\n",
      "19:28 madminer.utils.ml.sc INFO      Epoch 14: train loss 0.0229 (mse_score: 0.0229)\n",
      "19:28 madminer.utils.ml.sc INFO                val. loss  0.0569 (mse_score: 0.0569) (*)\n",
      "19:29 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0224 (mse_score: 0.0224)\n",
      "19:29 madminer.utils.ml.sc INFO                val. loss  0.0566 (mse_score: 0.0566) (*)\n",
      "19:29 madminer.utils.ml.sc INFO      Epoch 16: train loss 0.0217 (mse_score: 0.0217)\n",
      "19:29 madminer.utils.ml.sc INFO                val. loss  0.0567 (mse_score: 0.0567)\n",
      "19:29 madminer.utils.ml.sc INFO      Epoch 17: train loss 0.0213 (mse_score: 0.0213)\n",
      "19:29 madminer.utils.ml.sc INFO                val. loss  0.0562 (mse_score: 0.0562) (*)\n",
      "19:30 madminer.utils.ml.sc INFO      Epoch 18: train loss 0.0210 (mse_score: 0.0210)\n",
      "19:30 madminer.utils.ml.sc INFO                val. loss  0.0561 (mse_score: 0.0561) (*)\n",
      "19:30 madminer.utils.ml.sc INFO      Epoch 19: train loss 0.0206 (mse_score: 0.0206)\n",
      "19:30 madminer.utils.ml.sc INFO                val. loss  0.0558 (mse_score: 0.0558) (*)\n",
      "19:31 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0202 (mse_score: 0.0202)\n",
      "19:31 madminer.utils.ml.sc INFO                val. loss  0.0557 (mse_score: 0.0557) (*)\n",
      "19:31 madminer.utils.ml.sc INFO      Epoch 21: train loss 0.0199 (mse_score: 0.0199)\n",
      "19:31 madminer.utils.ml.sc INFO                val. loss  0.0554 (mse_score: 0.0554) (*)\n",
      "19:32 madminer.utils.ml.sc INFO      Epoch 22: train loss 0.0196 (mse_score: 0.0196)\n",
      "19:32 madminer.utils.ml.sc INFO                val. loss  0.0564 (mse_score: 0.0564)\n",
      "19:32 madminer.utils.ml.sc INFO      Epoch 23: train loss 0.0194 (mse_score: 0.0194)\n",
      "19:32 madminer.utils.ml.sc INFO                val. loss  0.0552 (mse_score: 0.0552) (*)\n",
      "19:32 madminer.utils.ml.sc INFO      Epoch 24: train loss 0.0192 (mse_score: 0.0192)\n",
      "19:32 madminer.utils.ml.sc INFO                val. loss  0.0551 (mse_score: 0.0551) (*)\n",
      "19:33 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0190 (mse_score: 0.0190)\n",
      "19:33 madminer.utils.ml.sc INFO                val. loss  0.0554 (mse_score: 0.0554)\n",
      "19:33 madminer.utils.ml.sc INFO      Epoch 26: train loss 0.0187 (mse_score: 0.0187)\n",
      "19:33 madminer.utils.ml.sc INFO                val. loss  0.0548 (mse_score: 0.0548) (*)\n",
      "19:34 madminer.utils.ml.sc INFO      Epoch 27: train loss 0.0185 (mse_score: 0.0185)\n",
      "19:34 madminer.utils.ml.sc INFO                val. loss  0.0545 (mse_score: 0.0545) (*)\n",
      "19:34 madminer.utils.ml.sc INFO      Epoch 28: train loss 0.0184 (mse_score: 0.0184)\n",
      "19:34 madminer.utils.ml.sc INFO                val. loss  0.0546 (mse_score: 0.0546)\n",
      "19:34 madminer.utils.ml.sc INFO      Epoch 29: train loss 0.0182 (mse_score: 0.0182)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:34 madminer.utils.ml.sc INFO                val. loss  0.0545 (mse_score: 0.0545) (*)\n",
      "19:35 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0181 (mse_score: 0.0181)\n",
      "19:35 madminer.utils.ml.sc INFO                val. loss  0.0547 (mse_score: 0.0547)\n",
      "19:35 madminer.utils.ml.sc INFO      Epoch 31: train loss 0.0179 (mse_score: 0.0179)\n",
      "19:35 madminer.utils.ml.sc INFO                val. loss  0.0546 (mse_score: 0.0546)\n",
      "19:36 madminer.utils.ml.sc INFO      Epoch 32: train loss 0.0178 (mse_score: 0.0178)\n",
      "19:36 madminer.utils.ml.sc INFO                val. loss  0.0545 (mse_score: 0.0545) (*)\n",
      "19:36 madminer.utils.ml.sc INFO      Epoch 33: train loss 0.0177 (mse_score: 0.0177)\n",
      "19:36 madminer.utils.ml.sc INFO                val. loss  0.0542 (mse_score: 0.0542) (*)\n",
      "19:37 madminer.utils.ml.sc INFO      Epoch 34: train loss 0.0176 (mse_score: 0.0176)\n",
      "19:37 madminer.utils.ml.sc INFO                val. loss  0.0543 (mse_score: 0.0543)\n",
      "19:37 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0174 (mse_score: 0.0174)\n",
      "19:37 madminer.utils.ml.sc INFO                val. loss  0.0540 (mse_score: 0.0540) (*)\n",
      "19:37 madminer.utils.ml.sc INFO      Epoch 36: train loss 0.0173 (mse_score: 0.0173)\n",
      "19:37 madminer.utils.ml.sc INFO                val. loss  0.0542 (mse_score: 0.0542)\n",
      "19:38 madminer.utils.ml.sc INFO      Epoch 37: train loss 0.0172 (mse_score: 0.0172)\n",
      "19:38 madminer.utils.ml.sc INFO                val. loss  0.0541 (mse_score: 0.0541)\n",
      "19:38 madminer.utils.ml.sc INFO      Epoch 38: train loss 0.0171 (mse_score: 0.0171)\n",
      "19:38 madminer.utils.ml.sc INFO                val. loss  0.0541 (mse_score: 0.0541)\n",
      "19:39 madminer.utils.ml.sc INFO      Epoch 39: train loss 0.0170 (mse_score: 0.0170)\n",
      "19:39 madminer.utils.ml.sc INFO                val. loss  0.0540 (mse_score: 0.0540)\n",
      "19:39 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0169 (mse_score: 0.0169)\n",
      "19:39 madminer.utils.ml.sc INFO                val. loss  0.0539 (mse_score: 0.0539) (*)\n",
      "19:39 madminer.utils.ml.sc INFO      Epoch 41: train loss 0.0169 (mse_score: 0.0169)\n",
      "19:39 madminer.utils.ml.sc INFO                val. loss  0.0540 (mse_score: 0.0540)\n",
      "19:40 madminer.utils.ml.sc INFO      Epoch 42: train loss 0.0168 (mse_score: 0.0168)\n",
      "19:40 madminer.utils.ml.sc INFO                val. loss  0.0540 (mse_score: 0.0540)\n",
      "19:40 madminer.utils.ml.sc INFO      Epoch 43: train loss 0.0167 (mse_score: 0.0167)\n",
      "19:40 madminer.utils.ml.sc INFO                val. loss  0.0539 (mse_score: 0.0539) (*)\n",
      "19:41 madminer.utils.ml.sc INFO      Epoch 44: train loss 0.0166 (mse_score: 0.0166)\n",
      "19:41 madminer.utils.ml.sc INFO                val. loss  0.0538 (mse_score: 0.0538) (*)\n",
      "19:41 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0166 (mse_score: 0.0166)\n",
      "19:41 madminer.utils.ml.sc INFO                val. loss  0.0538 (mse_score: 0.0538) (*)\n",
      "19:42 madminer.utils.ml.sc INFO      Epoch 46: train loss 0.0165 (mse_score: 0.0165)\n",
      "19:42 madminer.utils.ml.sc INFO                val. loss  0.0539 (mse_score: 0.0539)\n",
      "19:42 madminer.utils.ml.sc INFO      Epoch 47: train loss 0.0165 (mse_score: 0.0165)\n",
      "19:42 madminer.utils.ml.sc INFO                val. loss  0.0538 (mse_score: 0.0538) (*)\n",
      "19:42 madminer.utils.ml.sc INFO      Epoch 48: train loss 0.0164 (mse_score: 0.0164)\n",
      "19:42 madminer.utils.ml.sc INFO                val. loss  0.0538 (mse_score: 0.0538)\n",
      "19:43 madminer.utils.ml.sc INFO      Epoch 49: train loss 0.0163 (mse_score: 0.0163)\n",
      "19:43 madminer.utils.ml.sc INFO                val. loss  0.0537 (mse_score: 0.0537) (*)\n",
      "19:43 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0163 (mse_score: 0.0163)\n",
      "19:43 madminer.utils.ml.sc INFO                val. loss  0.0537 (mse_score: 0.0537)\n",
      "19:43 madminer.utils.ml.sc INFO    Early stopping after epoch 49, with loss 0.05 compared to final loss 0.05\n",
      "19:43 madminer.utils.ml.sc INFO    Finished training\n",
      "19:43 madminer.ml          INFO    Training estimator 6 / 10 in ensemble\n",
      "19:43 madminer.ml          INFO    Starting training\n",
      "19:43 madminer.ml          INFO      Method:                 sally\n",
      "19:43 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/x_train_5.npy\n",
      "19:43 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/t_xz_train_5.npy\n",
      "19:43 madminer.ml          INFO      Features:               all\n",
      "19:43 madminer.ml          INFO      Method:                 sally\n",
      "19:43 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "19:43 madminer.ml          INFO      Activation function:    tanh\n",
      "19:43 madminer.ml          INFO      Batch size:             128\n",
      "19:43 madminer.ml          INFO      Trainer:                amsgrad\n",
      "19:43 madminer.ml          INFO      Epochs:                 50\n",
      "19:43 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "19:43 madminer.ml          INFO      Validation split:       0.5\n",
      "19:43 madminer.ml          INFO      Early stopping:         True\n",
      "19:43 madminer.ml          INFO      Scale inputs:           True\n",
      "19:43 madminer.ml          INFO      Shuffle labels          False\n",
      "19:43 madminer.ml          INFO      Regularization:         None\n",
      "19:43 madminer.ml          INFO      Samples:                all\n",
      "19:43 madminer.ml          INFO    Loading training data\n",
      "19:43 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "19:43 madminer.ml          INFO    Rescaling inputs\n",
      "19:43 madminer.ml          INFO    Creating model for method sally\n",
      "19:43 madminer.ml          INFO    Training model\n",
      "19:44 madminer.utils.ml.sc INFO      Epoch 01: train loss 0.2040 (mse_score: 0.2040)\n",
      "19:44 madminer.utils.ml.sc INFO                val. loss  0.0653 (mse_score: 0.0653) (*)\n",
      "19:44 madminer.utils.ml.sc INFO      Epoch 02: train loss 0.1887 (mse_score: 0.1887)\n",
      "19:44 madminer.utils.ml.sc INFO                val. loss  0.0537 (mse_score: 0.0537) (*)\n",
      "19:44 madminer.utils.ml.sc INFO      Epoch 03: train loss 0.1800 (mse_score: 0.1800)\n",
      "19:44 madminer.utils.ml.sc INFO                val. loss  0.0474 (mse_score: 0.0474) (*)\n",
      "19:45 madminer.utils.ml.sc INFO      Epoch 04: train loss 0.1743 (mse_score: 0.1743)\n",
      "19:45 madminer.utils.ml.sc INFO                val. loss  0.0433 (mse_score: 0.0433) (*)\n",
      "19:45 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1684 (mse_score: 0.1684)\n",
      "19:45 madminer.utils.ml.sc INFO                val. loss  0.0388 (mse_score: 0.0388) (*)\n",
      "19:45 madminer.utils.ml.sc INFO      Epoch 06: train loss 0.1638 (mse_score: 0.1638)\n",
      "19:45 madminer.utils.ml.sc INFO                val. loss  0.0372 (mse_score: 0.0372) (*)\n",
      "19:46 madminer.utils.ml.sc INFO      Epoch 07: train loss 0.1606 (mse_score: 0.1606)\n",
      "19:46 madminer.utils.ml.sc INFO                val. loss  0.0363 (mse_score: 0.0363) (*)\n",
      "19:46 madminer.utils.ml.sc INFO      Epoch 08: train loss 0.1583 (mse_score: 0.1583)\n",
      "19:46 madminer.utils.ml.sc INFO                val. loss  0.0360 (mse_score: 0.0360) (*)\n",
      "19:47 madminer.utils.ml.sc INFO      Epoch 09: train loss 0.1563 (mse_score: 0.1563)\n",
      "19:47 madminer.utils.ml.sc INFO                val. loss  0.0330 (mse_score: 0.0330) (*)\n",
      "19:47 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.1543 (mse_score: 0.1543)\n",
      "19:47 madminer.utils.ml.sc INFO                val. loss  0.0320 (mse_score: 0.0320) (*)\n",
      "19:47 madminer.utils.ml.sc INFO      Epoch 11: train loss 0.1527 (mse_score: 0.1527)\n",
      "19:47 madminer.utils.ml.sc INFO                val. loss  0.0311 (mse_score: 0.0311) (*)\n",
      "19:48 madminer.utils.ml.sc INFO      Epoch 12: train loss 0.1513 (mse_score: 0.1513)\n",
      "19:48 madminer.utils.ml.sc INFO                val. loss  0.0309 (mse_score: 0.0309) (*)\n",
      "19:48 madminer.utils.ml.sc INFO      Epoch 13: train loss 0.1500 (mse_score: 0.1500)\n",
      "19:48 madminer.utils.ml.sc INFO                val. loss  0.0299 (mse_score: 0.0299) (*)\n",
      "19:48 madminer.utils.ml.sc INFO      Epoch 14: train loss 0.1490 (mse_score: 0.1490)\n",
      "19:48 madminer.utils.ml.sc INFO                val. loss  0.0310 (mse_score: 0.0310)\n",
      "19:49 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.1479 (mse_score: 0.1479)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:49 madminer.utils.ml.sc INFO                val. loss  0.0293 (mse_score: 0.0293) (*)\n",
      "19:49 madminer.utils.ml.sc INFO      Epoch 16: train loss 0.1471 (mse_score: 0.1471)\n",
      "19:49 madminer.utils.ml.sc INFO                val. loss  0.0285 (mse_score: 0.0285) (*)\n",
      "19:50 madminer.utils.ml.sc INFO      Epoch 17: train loss 0.1461 (mse_score: 0.1461)\n",
      "19:50 madminer.utils.ml.sc INFO                val. loss  0.0287 (mse_score: 0.0287)\n",
      "19:50 madminer.utils.ml.sc INFO      Epoch 18: train loss 0.1454 (mse_score: 0.1454)\n",
      "19:50 madminer.utils.ml.sc INFO                val. loss  0.0278 (mse_score: 0.0278) (*)\n",
      "19:50 madminer.utils.ml.sc INFO      Epoch 19: train loss 0.1447 (mse_score: 0.1447)\n",
      "19:50 madminer.utils.ml.sc INFO                val. loss  0.0276 (mse_score: 0.0276) (*)\n",
      "19:51 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.1440 (mse_score: 0.1440)\n",
      "19:51 madminer.utils.ml.sc INFO                val. loss  0.0276 (mse_score: 0.0276) (*)\n",
      "19:51 madminer.utils.ml.sc INFO      Epoch 21: train loss 0.1434 (mse_score: 0.1434)\n",
      "19:51 madminer.utils.ml.sc INFO                val. loss  0.0273 (mse_score: 0.0273) (*)\n",
      "19:52 madminer.utils.ml.sc INFO      Epoch 22: train loss 0.1426 (mse_score: 0.1426)\n",
      "19:52 madminer.utils.ml.sc INFO                val. loss  0.0270 (mse_score: 0.0270) (*)\n",
      "19:52 madminer.utils.ml.sc INFO      Epoch 23: train loss 0.1422 (mse_score: 0.1422)\n",
      "19:52 madminer.utils.ml.sc INFO                val. loss  0.0269 (mse_score: 0.0269) (*)\n",
      "19:52 madminer.utils.ml.sc INFO      Epoch 24: train loss 0.1416 (mse_score: 0.1416)\n",
      "19:52 madminer.utils.ml.sc INFO                val. loss  0.0275 (mse_score: 0.0275)\n",
      "19:53 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.1412 (mse_score: 0.1412)\n",
      "19:53 madminer.utils.ml.sc INFO                val. loss  0.0268 (mse_score: 0.0268) (*)\n",
      "19:53 madminer.utils.ml.sc INFO      Epoch 26: train loss 0.1408 (mse_score: 0.1408)\n",
      "19:53 madminer.utils.ml.sc INFO                val. loss  0.0268 (mse_score: 0.0268) (*)\n",
      "19:54 madminer.utils.ml.sc INFO      Epoch 27: train loss 0.1403 (mse_score: 0.1403)\n",
      "19:54 madminer.utils.ml.sc INFO                val. loss  0.0267 (mse_score: 0.0267) (*)\n",
      "19:54 madminer.utils.ml.sc INFO      Epoch 28: train loss 0.1399 (mse_score: 0.1399)\n",
      "19:54 madminer.utils.ml.sc INFO                val. loss  0.0262 (mse_score: 0.0262) (*)\n",
      "19:54 madminer.utils.ml.sc INFO      Epoch 29: train loss 0.1395 (mse_score: 0.1395)\n",
      "19:54 madminer.utils.ml.sc INFO                val. loss  0.0261 (mse_score: 0.0261) (*)\n",
      "19:55 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.1392 (mse_score: 0.1392)\n",
      "19:55 madminer.utils.ml.sc INFO                val. loss  0.0267 (mse_score: 0.0267)\n",
      "19:55 madminer.utils.ml.sc INFO      Epoch 31: train loss 0.1388 (mse_score: 0.1388)\n",
      "19:55 madminer.utils.ml.sc INFO                val. loss  0.0260 (mse_score: 0.0260) (*)\n",
      "19:56 madminer.utils.ml.sc INFO      Epoch 32: train loss 0.1385 (mse_score: 0.1385)\n",
      "19:56 madminer.utils.ml.sc INFO                val. loss  0.0259 (mse_score: 0.0259) (*)\n",
      "19:56 madminer.utils.ml.sc INFO      Epoch 33: train loss 0.1382 (mse_score: 0.1382)\n",
      "19:56 madminer.utils.ml.sc INFO                val. loss  0.0259 (mse_score: 0.0259)\n",
      "19:57 madminer.utils.ml.sc INFO      Epoch 34: train loss 0.1380 (mse_score: 0.1380)\n",
      "19:57 madminer.utils.ml.sc INFO                val. loss  0.0258 (mse_score: 0.0258) (*)\n",
      "19:57 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.1377 (mse_score: 0.1377)\n",
      "19:57 madminer.utils.ml.sc INFO                val. loss  0.0259 (mse_score: 0.0259)\n",
      "19:57 madminer.utils.ml.sc INFO      Epoch 36: train loss 0.1374 (mse_score: 0.1374)\n",
      "19:57 madminer.utils.ml.sc INFO                val. loss  0.0257 (mse_score: 0.0257) (*)\n",
      "19:58 madminer.utils.ml.sc INFO      Epoch 37: train loss 0.1372 (mse_score: 0.1372)\n",
      "19:58 madminer.utils.ml.sc INFO                val. loss  0.0256 (mse_score: 0.0256) (*)\n",
      "19:58 madminer.utils.ml.sc INFO      Epoch 38: train loss 0.1369 (mse_score: 0.1369)\n",
      "19:58 madminer.utils.ml.sc INFO                val. loss  0.0255 (mse_score: 0.0255) (*)\n",
      "19:59 madminer.utils.ml.sc INFO      Epoch 39: train loss 0.1367 (mse_score: 0.1367)\n",
      "19:59 madminer.utils.ml.sc INFO                val. loss  0.0255 (mse_score: 0.0255) (*)\n",
      "19:59 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.1365 (mse_score: 0.1365)\n",
      "19:59 madminer.utils.ml.sc INFO                val. loss  0.0254 (mse_score: 0.0254) (*)\n",
      "20:00 madminer.utils.ml.sc INFO      Epoch 41: train loss 0.1363 (mse_score: 0.1363)\n",
      "20:00 madminer.utils.ml.sc INFO                val. loss  0.0259 (mse_score: 0.0259)\n",
      "20:00 madminer.utils.ml.sc INFO      Epoch 42: train loss 0.1362 (mse_score: 0.1362)\n",
      "20:00 madminer.utils.ml.sc INFO                val. loss  0.0256 (mse_score: 0.0256)\n",
      "20:00 madminer.utils.ml.sc INFO      Epoch 43: train loss 0.1360 (mse_score: 0.1360)\n",
      "20:00 madminer.utils.ml.sc INFO                val. loss  0.0253 (mse_score: 0.0253) (*)\n",
      "20:01 madminer.utils.ml.sc INFO      Epoch 44: train loss 0.1358 (mse_score: 0.1358)\n",
      "20:01 madminer.utils.ml.sc INFO                val. loss  0.0255 (mse_score: 0.0255)\n",
      "20:01 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.1357 (mse_score: 0.1357)\n",
      "20:01 madminer.utils.ml.sc INFO                val. loss  0.0253 (mse_score: 0.0253) (*)\n",
      "20:02 madminer.utils.ml.sc INFO      Epoch 46: train loss 0.1355 (mse_score: 0.1355)\n",
      "20:02 madminer.utils.ml.sc INFO                val. loss  0.0253 (mse_score: 0.0253) (*)\n",
      "20:02 madminer.utils.ml.sc INFO      Epoch 47: train loss 0.1354 (mse_score: 0.1354)\n",
      "20:02 madminer.utils.ml.sc INFO                val. loss  0.0253 (mse_score: 0.0253) (*)\n",
      "20:02 madminer.utils.ml.sc INFO      Epoch 48: train loss 0.1352 (mse_score: 0.1352)\n",
      "20:02 madminer.utils.ml.sc INFO                val. loss  0.0253 (mse_score: 0.0253)\n",
      "20:03 madminer.utils.ml.sc INFO      Epoch 49: train loss 0.1351 (mse_score: 0.1351)\n",
      "20:03 madminer.utils.ml.sc INFO                val. loss  0.0251 (mse_score: 0.0251) (*)\n",
      "20:03 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.1350 (mse_score: 0.1350)\n",
      "20:03 madminer.utils.ml.sc INFO                val. loss  0.0252 (mse_score: 0.0252)\n",
      "20:03 madminer.utils.ml.sc INFO    Early stopping after epoch 49, with loss 0.03 compared to final loss 0.03\n",
      "20:03 madminer.utils.ml.sc INFO    Finished training\n",
      "20:03 madminer.ml          INFO    Training estimator 7 / 10 in ensemble\n",
      "20:03 madminer.ml          INFO    Starting training\n",
      "20:03 madminer.ml          INFO      Method:                 sally\n",
      "20:03 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/x_train_6.npy\n",
      "20:03 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/t_xz_train_6.npy\n",
      "20:03 madminer.ml          INFO      Features:               all\n",
      "20:03 madminer.ml          INFO      Method:                 sally\n",
      "20:03 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "20:03 madminer.ml          INFO      Activation function:    tanh\n",
      "20:03 madminer.ml          INFO      Batch size:             128\n",
      "20:03 madminer.ml          INFO      Trainer:                amsgrad\n",
      "20:03 madminer.ml          INFO      Epochs:                 50\n",
      "20:03 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:03 madminer.ml          INFO      Validation split:       0.5\n",
      "20:03 madminer.ml          INFO      Early stopping:         True\n",
      "20:03 madminer.ml          INFO      Scale inputs:           True\n",
      "20:03 madminer.ml          INFO      Shuffle labels          False\n",
      "20:03 madminer.ml          INFO      Regularization:         None\n",
      "20:03 madminer.ml          INFO      Samples:                all\n",
      "20:03 madminer.ml          INFO    Loading training data\n",
      "20:03 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "20:03 madminer.ml          INFO    Rescaling inputs\n",
      "20:03 madminer.ml          INFO    Creating model for method sally\n",
      "20:03 madminer.ml          INFO    Training model\n",
      "20:04 madminer.utils.ml.sc INFO      Epoch 01: train loss 0.0779 (mse_score: 0.0779)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:04 madminer.utils.ml.sc INFO                val. loss  0.0603 (mse_score: 0.0603) (*)\n",
      "20:04 madminer.utils.ml.sc INFO      Epoch 02: train loss 0.0561 (mse_score: 0.0561)\n",
      "20:04 madminer.utils.ml.sc INFO                val. loss  0.0477 (mse_score: 0.0477) (*)\n",
      "20:05 madminer.utils.ml.sc INFO      Epoch 03: train loss 0.0448 (mse_score: 0.0448)\n",
      "20:05 madminer.utils.ml.sc INFO                val. loss  0.0405 (mse_score: 0.0405) (*)\n",
      "20:05 madminer.utils.ml.sc INFO      Epoch 04: train loss 0.0393 (mse_score: 0.0393)\n",
      "20:05 madminer.utils.ml.sc INFO                val. loss  0.0364 (mse_score: 0.0364) (*)\n",
      "20:05 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.0355 (mse_score: 0.0355)\n",
      "20:05 madminer.utils.ml.sc INFO                val. loss  0.0344 (mse_score: 0.0344) (*)\n",
      "20:06 madminer.utils.ml.sc INFO      Epoch 06: train loss 0.0330 (mse_score: 0.0330)\n",
      "20:06 madminer.utils.ml.sc INFO                val. loss  0.0357 (mse_score: 0.0357)\n",
      "20:06 madminer.utils.ml.sc INFO      Epoch 07: train loss 0.0310 (mse_score: 0.0310)\n",
      "20:06 madminer.utils.ml.sc INFO                val. loss  0.0320 (mse_score: 0.0320) (*)\n",
      "20:07 madminer.utils.ml.sc INFO      Epoch 08: train loss 0.0294 (mse_score: 0.0294)\n",
      "20:07 madminer.utils.ml.sc INFO                val. loss  0.0311 (mse_score: 0.0311) (*)\n",
      "20:07 madminer.utils.ml.sc INFO      Epoch 09: train loss 0.0282 (mse_score: 0.0282)\n",
      "20:07 madminer.utils.ml.sc INFO                val. loss  0.0305 (mse_score: 0.0305) (*)\n",
      "20:07 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0271 (mse_score: 0.0271)\n",
      "20:07 madminer.utils.ml.sc INFO                val. loss  0.0294 (mse_score: 0.0294) (*)\n",
      "20:08 madminer.utils.ml.sc INFO      Epoch 11: train loss 0.0259 (mse_score: 0.0259)\n",
      "20:08 madminer.utils.ml.sc INFO                val. loss  0.0291 (mse_score: 0.0291) (*)\n",
      "20:08 madminer.utils.ml.sc INFO      Epoch 12: train loss 0.0253 (mse_score: 0.0253)\n",
      "20:08 madminer.utils.ml.sc INFO                val. loss  0.0305 (mse_score: 0.0305)\n",
      "20:09 madminer.utils.ml.sc INFO      Epoch 13: train loss 0.0244 (mse_score: 0.0244)\n",
      "20:09 madminer.utils.ml.sc INFO                val. loss  0.0290 (mse_score: 0.0290) (*)\n",
      "20:09 madminer.utils.ml.sc INFO      Epoch 14: train loss 0.0238 (mse_score: 0.0238)\n",
      "20:09 madminer.utils.ml.sc INFO                val. loss  0.0278 (mse_score: 0.0278) (*)\n",
      "20:10 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0232 (mse_score: 0.0232)\n",
      "20:10 madminer.utils.ml.sc INFO                val. loss  0.0283 (mse_score: 0.0283)\n",
      "20:10 madminer.utils.ml.sc INFO      Epoch 16: train loss 0.0225 (mse_score: 0.0225)\n",
      "20:10 madminer.utils.ml.sc INFO                val. loss  0.0273 (mse_score: 0.0273) (*)\n",
      "20:10 madminer.utils.ml.sc INFO      Epoch 17: train loss 0.0221 (mse_score: 0.0221)\n",
      "20:10 madminer.utils.ml.sc INFO                val. loss  0.0284 (mse_score: 0.0284)\n",
      "20:11 madminer.utils.ml.sc INFO      Epoch 18: train loss 0.0216 (mse_score: 0.0216)\n",
      "20:11 madminer.utils.ml.sc INFO                val. loss  0.0268 (mse_score: 0.0268) (*)\n",
      "20:11 madminer.utils.ml.sc INFO      Epoch 19: train loss 0.0212 (mse_score: 0.0212)\n",
      "20:11 madminer.utils.ml.sc INFO                val. loss  0.0267 (mse_score: 0.0267) (*)\n",
      "20:12 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0208 (mse_score: 0.0208)\n",
      "20:12 madminer.utils.ml.sc INFO                val. loss  0.0270 (mse_score: 0.0270)\n",
      "20:12 madminer.utils.ml.sc INFO      Epoch 21: train loss 0.0204 (mse_score: 0.0204)\n",
      "20:12 madminer.utils.ml.sc INFO                val. loss  0.0269 (mse_score: 0.0269)\n",
      "20:12 madminer.utils.ml.sc INFO      Epoch 22: train loss 0.0202 (mse_score: 0.0202)\n",
      "20:12 madminer.utils.ml.sc INFO                val. loss  0.0264 (mse_score: 0.0264) (*)\n",
      "20:13 madminer.utils.ml.sc INFO      Epoch 23: train loss 0.0198 (mse_score: 0.0198)\n",
      "20:13 madminer.utils.ml.sc INFO                val. loss  0.0264 (mse_score: 0.0264) (*)\n",
      "20:13 madminer.utils.ml.sc INFO      Epoch 24: train loss 0.0195 (mse_score: 0.0195)\n",
      "20:13 madminer.utils.ml.sc INFO                val. loss  0.0262 (mse_score: 0.0262) (*)\n",
      "20:14 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0192 (mse_score: 0.0192)\n",
      "20:14 madminer.utils.ml.sc INFO                val. loss  0.0258 (mse_score: 0.0258) (*)\n",
      "20:14 madminer.utils.ml.sc INFO      Epoch 26: train loss 0.0190 (mse_score: 0.0190)\n",
      "20:14 madminer.utils.ml.sc INFO                val. loss  0.0258 (mse_score: 0.0258) (*)\n",
      "20:15 madminer.utils.ml.sc INFO      Epoch 27: train loss 0.0188 (mse_score: 0.0188)\n",
      "20:15 madminer.utils.ml.sc INFO                val. loss  0.0256 (mse_score: 0.0256) (*)\n",
      "20:15 madminer.utils.ml.sc INFO      Epoch 28: train loss 0.0185 (mse_score: 0.0185)\n",
      "20:15 madminer.utils.ml.sc INFO                val. loss  0.0254 (mse_score: 0.0254) (*)\n",
      "20:15 madminer.utils.ml.sc INFO      Epoch 29: train loss 0.0184 (mse_score: 0.0184)\n",
      "20:15 madminer.utils.ml.sc INFO                val. loss  0.0256 (mse_score: 0.0256)\n",
      "20:16 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0182 (mse_score: 0.0182)\n",
      "20:16 madminer.utils.ml.sc INFO                val. loss  0.0253 (mse_score: 0.0253) (*)\n",
      "20:16 madminer.utils.ml.sc INFO      Epoch 31: train loss 0.0180 (mse_score: 0.0180)\n",
      "20:16 madminer.utils.ml.sc INFO                val. loss  0.0254 (mse_score: 0.0254)\n",
      "20:17 madminer.utils.ml.sc INFO      Epoch 32: train loss 0.0178 (mse_score: 0.0178)\n",
      "20:17 madminer.utils.ml.sc INFO                val. loss  0.0253 (mse_score: 0.0253) (*)\n",
      "20:17 madminer.utils.ml.sc INFO      Epoch 33: train loss 0.0176 (mse_score: 0.0176)\n",
      "20:17 madminer.utils.ml.sc INFO                val. loss  0.0253 (mse_score: 0.0253)\n",
      "20:17 madminer.utils.ml.sc INFO      Epoch 34: train loss 0.0176 (mse_score: 0.0176)\n",
      "20:17 madminer.utils.ml.sc INFO                val. loss  0.0250 (mse_score: 0.0250) (*)\n",
      "20:18 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0174 (mse_score: 0.0174)\n",
      "20:18 madminer.utils.ml.sc INFO                val. loss  0.0250 (mse_score: 0.0250) (*)\n",
      "20:18 madminer.utils.ml.sc INFO      Epoch 36: train loss 0.0173 (mse_score: 0.0173)\n",
      "20:18 madminer.utils.ml.sc INFO                val. loss  0.0249 (mse_score: 0.0249) (*)\n",
      "20:19 madminer.utils.ml.sc INFO      Epoch 37: train loss 0.0172 (mse_score: 0.0172)\n",
      "20:19 madminer.utils.ml.sc INFO                val. loss  0.0250 (mse_score: 0.0250)\n",
      "20:19 madminer.utils.ml.sc INFO      Epoch 38: train loss 0.0170 (mse_score: 0.0170)\n",
      "20:19 madminer.utils.ml.sc INFO                val. loss  0.0251 (mse_score: 0.0251)\n",
      "20:20 madminer.utils.ml.sc INFO      Epoch 39: train loss 0.0169 (mse_score: 0.0169)\n",
      "20:20 madminer.utils.ml.sc INFO                val. loss  0.0248 (mse_score: 0.0248) (*)\n",
      "20:20 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0168 (mse_score: 0.0168)\n",
      "20:20 madminer.utils.ml.sc INFO                val. loss  0.0249 (mse_score: 0.0249)\n",
      "20:20 madminer.utils.ml.sc INFO      Epoch 41: train loss 0.0167 (mse_score: 0.0167)\n",
      "20:20 madminer.utils.ml.sc INFO                val. loss  0.0248 (mse_score: 0.0248) (*)\n",
      "20:21 madminer.utils.ml.sc INFO      Epoch 42: train loss 0.0166 (mse_score: 0.0166)\n",
      "20:21 madminer.utils.ml.sc INFO                val. loss  0.0249 (mse_score: 0.0249)\n",
      "20:21 madminer.utils.ml.sc INFO      Epoch 43: train loss 0.0165 (mse_score: 0.0165)\n",
      "20:21 madminer.utils.ml.sc INFO                val. loss  0.0248 (mse_score: 0.0248)\n",
      "20:22 madminer.utils.ml.sc INFO      Epoch 44: train loss 0.0164 (mse_score: 0.0164)\n",
      "20:22 madminer.utils.ml.sc INFO                val. loss  0.0246 (mse_score: 0.0246) (*)\n",
      "20:22 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0163 (mse_score: 0.0163)\n",
      "20:22 madminer.utils.ml.sc INFO                val. loss  0.0248 (mse_score: 0.0248)\n",
      "20:22 madminer.utils.ml.sc INFO      Epoch 46: train loss 0.0163 (mse_score: 0.0163)\n",
      "20:22 madminer.utils.ml.sc INFO                val. loss  0.0246 (mse_score: 0.0246) (*)\n",
      "20:23 madminer.utils.ml.sc INFO      Epoch 47: train loss 0.0162 (mse_score: 0.0162)\n",
      "20:23 madminer.utils.ml.sc INFO                val. loss  0.0247 (mse_score: 0.0247)\n",
      "20:23 madminer.utils.ml.sc INFO      Epoch 48: train loss 0.0161 (mse_score: 0.0161)\n",
      "20:23 madminer.utils.ml.sc INFO                val. loss  0.0248 (mse_score: 0.0248)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:24 madminer.utils.ml.sc INFO      Epoch 49: train loss 0.0160 (mse_score: 0.0160)\n",
      "20:24 madminer.utils.ml.sc INFO                val. loss  0.0245 (mse_score: 0.0245) (*)\n",
      "20:24 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0160 (mse_score: 0.0160)\n",
      "20:24 madminer.utils.ml.sc INFO                val. loss  0.0246 (mse_score: 0.0246)\n",
      "20:24 madminer.utils.ml.sc INFO    Early stopping after epoch 49, with loss 0.02 compared to final loss 0.02\n",
      "20:24 madminer.utils.ml.sc INFO    Finished training\n",
      "20:24 madminer.ml          INFO    Training estimator 8 / 10 in ensemble\n",
      "20:24 madminer.ml          INFO    Starting training\n",
      "20:24 madminer.ml          INFO      Method:                 sally\n",
      "20:24 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/x_train_7.npy\n",
      "20:24 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/t_xz_train_7.npy\n",
      "20:24 madminer.ml          INFO      Features:               all\n",
      "20:24 madminer.ml          INFO      Method:                 sally\n",
      "20:24 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "20:24 madminer.ml          INFO      Activation function:    tanh\n",
      "20:24 madminer.ml          INFO      Batch size:             128\n",
      "20:24 madminer.ml          INFO      Trainer:                amsgrad\n",
      "20:24 madminer.ml          INFO      Epochs:                 50\n",
      "20:24 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:24 madminer.ml          INFO      Validation split:       0.5\n",
      "20:24 madminer.ml          INFO      Early stopping:         True\n",
      "20:24 madminer.ml          INFO      Scale inputs:           True\n",
      "20:24 madminer.ml          INFO      Shuffle labels          False\n",
      "20:24 madminer.ml          INFO      Regularization:         None\n",
      "20:24 madminer.ml          INFO      Samples:                all\n",
      "20:24 madminer.ml          INFO    Loading training data\n",
      "20:24 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "20:24 madminer.ml          INFO    Rescaling inputs\n",
      "20:24 madminer.ml          INFO    Creating model for method sally\n",
      "20:24 madminer.ml          INFO    Training model\n",
      "20:25 madminer.utils.ml.sc INFO      Epoch 01: train loss 0.0941 (mse_score: 0.0941)\n",
      "20:25 madminer.utils.ml.sc INFO                val. loss  0.0588 (mse_score: 0.0588) (*)\n",
      "20:25 madminer.utils.ml.sc INFO      Epoch 02: train loss 0.0731 (mse_score: 0.0731)\n",
      "20:25 madminer.utils.ml.sc INFO                val. loss  0.0452 (mse_score: 0.0452) (*)\n",
      "20:25 madminer.utils.ml.sc INFO      Epoch 03: train loss 0.0619 (mse_score: 0.0619)\n",
      "20:25 madminer.utils.ml.sc INFO                val. loss  0.0374 (mse_score: 0.0374) (*)\n",
      "20:26 madminer.utils.ml.sc INFO      Epoch 04: train loss 0.0558 (mse_score: 0.0558)\n",
      "20:26 madminer.utils.ml.sc INFO                val. loss  0.0336 (mse_score: 0.0336) (*)\n",
      "20:26 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.0520 (mse_score: 0.0520)\n",
      "20:26 madminer.utils.ml.sc INFO                val. loss  0.0310 (mse_score: 0.0310) (*)\n",
      "20:26 madminer.utils.ml.sc INFO      Epoch 06: train loss 0.0487 (mse_score: 0.0487)\n",
      "20:26 madminer.utils.ml.sc INFO                val. loss  0.0306 (mse_score: 0.0306) (*)\n",
      "20:27 madminer.utils.ml.sc INFO      Epoch 07: train loss 0.0468 (mse_score: 0.0468)\n",
      "20:27 madminer.utils.ml.sc INFO                val. loss  0.0506 (mse_score: 0.0506)\n",
      "20:27 madminer.utils.ml.sc INFO      Epoch 08: train loss 0.0446 (mse_score: 0.0446)\n",
      "20:27 madminer.utils.ml.sc INFO                val. loss  0.0282 (mse_score: 0.0282) (*)\n",
      "20:28 madminer.utils.ml.sc INFO      Epoch 09: train loss 0.0435 (mse_score: 0.0435)\n",
      "20:28 madminer.utils.ml.sc INFO                val. loss  0.0269 (mse_score: 0.0269) (*)\n",
      "20:28 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0418 (mse_score: 0.0418)\n",
      "20:28 madminer.utils.ml.sc INFO                val. loss  0.0275 (mse_score: 0.0275)\n",
      "20:47 madminer.utils.ml.sc INFO      Epoch 11: train loss 0.0408 (mse_score: 0.0408)\n",
      "20:47 madminer.utils.ml.sc INFO                val. loss  0.0267 (mse_score: 0.0267) (*)\n",
      "20:47 madminer.utils.ml.sc INFO      Epoch 12: train loss 0.0398 (mse_score: 0.0398)\n",
      "20:47 madminer.utils.ml.sc INFO                val. loss  0.0260 (mse_score: 0.0260) (*)\n",
      "21:27 madminer.utils.ml.sc INFO      Epoch 13: train loss 0.0389 (mse_score: 0.0389)\n",
      "21:27 madminer.utils.ml.sc INFO                val. loss  0.0252 (mse_score: 0.0252) (*)\n",
      "21:28 madminer.utils.ml.sc INFO      Epoch 14: train loss 0.0381 (mse_score: 0.0381)\n",
      "21:28 madminer.utils.ml.sc INFO                val. loss  0.0271 (mse_score: 0.0271)\n",
      "22:05 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0371 (mse_score: 0.0371)\n",
      "22:05 madminer.utils.ml.sc INFO                val. loss  0.0251 (mse_score: 0.0251) (*)\n",
      "22:05 madminer.utils.ml.sc INFO      Epoch 16: train loss 0.0364 (mse_score: 0.0364)\n",
      "22:05 madminer.utils.ml.sc INFO                val. loss  0.0249 (mse_score: 0.0249) (*)\n",
      "22:06 madminer.utils.ml.sc INFO      Epoch 17: train loss 0.0357 (mse_score: 0.0357)\n",
      "22:06 madminer.utils.ml.sc INFO                val. loss  0.0242 (mse_score: 0.0242) (*)\n",
      "22:37 madminer.utils.ml.sc INFO      Epoch 18: train loss 0.0350 (mse_score: 0.0350)\n",
      "22:37 madminer.utils.ml.sc INFO                val. loss  0.0242 (mse_score: 0.0242) (*)\n",
      "22:37 madminer.utils.ml.sc INFO      Epoch 19: train loss 0.0343 (mse_score: 0.0343)\n",
      "22:37 madminer.utils.ml.sc INFO                val. loss  0.0245 (mse_score: 0.0245)\n",
      "23:15 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0341 (mse_score: 0.0341)\n",
      "23:15 madminer.utils.ml.sc INFO                val. loss  0.0238 (mse_score: 0.0238) (*)\n",
      "23:16 madminer.utils.ml.sc INFO      Epoch 21: train loss 0.0336 (mse_score: 0.0336)\n",
      "23:16 madminer.utils.ml.sc INFO                val. loss  0.0237 (mse_score: 0.0237) (*)\n",
      "23:16 madminer.utils.ml.sc INFO      Epoch 22: train loss 0.0331 (mse_score: 0.0331)\n",
      "23:16 madminer.utils.ml.sc INFO                val. loss  0.0241 (mse_score: 0.0241)\n",
      "23:51 madminer.utils.ml.sc INFO      Epoch 23: train loss 0.0327 (mse_score: 0.0327)\n",
      "23:51 madminer.utils.ml.sc INFO                val. loss  0.0233 (mse_score: 0.0233) (*)\n",
      "23:52 madminer.utils.ml.sc INFO      Epoch 24: train loss 0.0324 (mse_score: 0.0324)\n",
      "23:52 madminer.utils.ml.sc INFO                val. loss  0.0277 (mse_score: 0.0277)\n",
      "00:25 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0320 (mse_score: 0.0320)\n",
      "00:25 madminer.utils.ml.sc INFO                val. loss  0.0231 (mse_score: 0.0231) (*)\n",
      "00:26 madminer.utils.ml.sc INFO      Epoch 26: train loss 0.0316 (mse_score: 0.0316)\n",
      "00:26 madminer.utils.ml.sc INFO                val. loss  0.0234 (mse_score: 0.0234)\n",
      "01:03 madminer.utils.ml.sc INFO      Epoch 27: train loss 0.0313 (mse_score: 0.0313)\n",
      "01:03 madminer.utils.ml.sc INFO                val. loss  0.0232 (mse_score: 0.0232)\n",
      "01:03 madminer.utils.ml.sc INFO      Epoch 28: train loss 0.0311 (mse_score: 0.0311)\n",
      "01:03 madminer.utils.ml.sc INFO                val. loss  0.0233 (mse_score: 0.0233)\n",
      "01:04 madminer.utils.ml.sc INFO      Epoch 29: train loss 0.0308 (mse_score: 0.0308)\n",
      "01:04 madminer.utils.ml.sc INFO                val. loss  0.0227 (mse_score: 0.0227) (*)\n",
      "01:35 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0305 (mse_score: 0.0305)\n",
      "01:35 madminer.utils.ml.sc INFO                val. loss  0.0229 (mse_score: 0.0229)\n",
      "01:35 madminer.utils.ml.sc INFO      Epoch 31: train loss 0.0302 (mse_score: 0.0302)\n",
      "01:35 madminer.utils.ml.sc INFO                val. loss  0.0231 (mse_score: 0.0231)\n",
      "02:08 madminer.utils.ml.sc INFO      Epoch 32: train loss 0.0300 (mse_score: 0.0300)\n",
      "02:08 madminer.utils.ml.sc INFO                val. loss  0.0225 (mse_score: 0.0225) (*)\n",
      "02:08 madminer.utils.ml.sc INFO      Epoch 33: train loss 0.0298 (mse_score: 0.0298)\n",
      "02:08 madminer.utils.ml.sc INFO                val. loss  0.0226 (mse_score: 0.0226)\n",
      "02:09 madminer.utils.ml.sc INFO      Epoch 34: train loss 0.0296 (mse_score: 0.0296)\n",
      "02:09 madminer.utils.ml.sc INFO                val. loss  0.0227 (mse_score: 0.0227)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:09 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0294 (mse_score: 0.0294)\n",
      "02:09 madminer.utils.ml.sc INFO                val. loss  0.0223 (mse_score: 0.0223) (*)\n",
      "02:10 madminer.utils.ml.sc INFO      Epoch 36: train loss 0.0292 (mse_score: 0.0292)\n",
      "02:10 madminer.utils.ml.sc INFO                val. loss  0.0224 (mse_score: 0.0224)\n",
      "02:10 madminer.utils.ml.sc INFO      Epoch 37: train loss 0.0290 (mse_score: 0.0290)\n",
      "02:10 madminer.utils.ml.sc INFO                val. loss  0.0228 (mse_score: 0.0228)\n",
      "02:10 madminer.utils.ml.sc INFO      Epoch 38: train loss 0.0288 (mse_score: 0.0288)\n",
      "02:10 madminer.utils.ml.sc INFO                val. loss  0.0224 (mse_score: 0.0224)\n",
      "02:11 madminer.utils.ml.sc INFO      Epoch 39: train loss 0.0287 (mse_score: 0.0287)\n",
      "02:11 madminer.utils.ml.sc INFO                val. loss  0.0222 (mse_score: 0.0222) (*)\n",
      "02:11 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0285 (mse_score: 0.0285)\n",
      "02:11 madminer.utils.ml.sc INFO                val. loss  0.0223 (mse_score: 0.0223)\n",
      "02:11 madminer.utils.ml.sc INFO      Epoch 41: train loss 0.0283 (mse_score: 0.0283)\n",
      "02:11 madminer.utils.ml.sc INFO                val. loss  0.0224 (mse_score: 0.0224)\n",
      "02:12 madminer.utils.ml.sc INFO      Epoch 42: train loss 0.0282 (mse_score: 0.0282)\n",
      "02:12 madminer.utils.ml.sc INFO                val. loss  0.0223 (mse_score: 0.0223)\n",
      "02:12 madminer.utils.ml.sc INFO      Epoch 43: train loss 0.0281 (mse_score: 0.0281)\n",
      "02:12 madminer.utils.ml.sc INFO                val. loss  0.0221 (mse_score: 0.0221) (*)\n",
      "02:13 madminer.utils.ml.sc INFO      Epoch 44: train loss 0.0279 (mse_score: 0.0279)\n",
      "02:13 madminer.utils.ml.sc INFO                val. loss  0.0224 (mse_score: 0.0224)\n",
      "02:13 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0279 (mse_score: 0.0279)\n",
      "02:13 madminer.utils.ml.sc INFO                val. loss  0.0223 (mse_score: 0.0223)\n",
      "02:13 madminer.utils.ml.sc INFO      Epoch 46: train loss 0.0277 (mse_score: 0.0277)\n",
      "02:13 madminer.utils.ml.sc INFO                val. loss  0.0230 (mse_score: 0.0230)\n",
      "02:14 madminer.utils.ml.sc INFO      Epoch 47: train loss 0.0277 (mse_score: 0.0277)\n",
      "02:14 madminer.utils.ml.sc INFO                val. loss  0.0220 (mse_score: 0.0220) (*)\n",
      "02:14 madminer.utils.ml.sc INFO      Epoch 48: train loss 0.0275 (mse_score: 0.0275)\n",
      "02:14 madminer.utils.ml.sc INFO                val. loss  0.0221 (mse_score: 0.0221)\n",
      "02:14 madminer.utils.ml.sc INFO      Epoch 49: train loss 0.0274 (mse_score: 0.0274)\n",
      "02:14 madminer.utils.ml.sc INFO                val. loss  0.0221 (mse_score: 0.0221)\n",
      "02:15 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0273 (mse_score: 0.0273)\n",
      "02:15 madminer.utils.ml.sc INFO                val. loss  0.0221 (mse_score: 0.0221)\n",
      "02:15 madminer.utils.ml.sc INFO    Early stopping after epoch 47, with loss 0.02 compared to final loss 0.02\n",
      "02:15 madminer.utils.ml.sc INFO    Finished training\n",
      "02:15 madminer.ml          INFO    Training estimator 9 / 10 in ensemble\n",
      "02:15 madminer.ml          INFO    Starting training\n",
      "02:15 madminer.ml          INFO      Method:                 sally\n",
      "02:15 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/x_train_8.npy\n",
      "02:15 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/t_xz_train_8.npy\n",
      "02:15 madminer.ml          INFO      Features:               all\n",
      "02:15 madminer.ml          INFO      Method:                 sally\n",
      "02:15 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "02:15 madminer.ml          INFO      Activation function:    tanh\n",
      "02:15 madminer.ml          INFO      Batch size:             128\n",
      "02:15 madminer.ml          INFO      Trainer:                amsgrad\n",
      "02:15 madminer.ml          INFO      Epochs:                 50\n",
      "02:15 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "02:15 madminer.ml          INFO      Validation split:       0.5\n",
      "02:15 madminer.ml          INFO      Early stopping:         True\n",
      "02:15 madminer.ml          INFO      Scale inputs:           True\n",
      "02:15 madminer.ml          INFO      Shuffle labels          False\n",
      "02:15 madminer.ml          INFO      Regularization:         None\n",
      "02:15 madminer.ml          INFO      Samples:                all\n",
      "02:15 madminer.ml          INFO    Loading training data\n",
      "02:15 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "02:15 madminer.ml          INFO    Rescaling inputs\n",
      "02:15 madminer.ml          INFO    Creating model for method sally\n",
      "02:15 madminer.ml          INFO    Training model\n",
      "02:15 madminer.utils.ml.sc INFO      Epoch 01: train loss 0.0715 (mse_score: 0.0715)\n",
      "02:15 madminer.utils.ml.sc INFO                val. loss  0.0715 (mse_score: 0.0715) (*)\n",
      "02:16 madminer.utils.ml.sc INFO      Epoch 02: train loss 0.0536 (mse_score: 0.0536)\n",
      "02:16 madminer.utils.ml.sc INFO                val. loss  0.0565 (mse_score: 0.0565) (*)\n",
      "02:16 madminer.utils.ml.sc INFO      Epoch 03: train loss 0.0413 (mse_score: 0.0413)\n",
      "02:16 madminer.utils.ml.sc INFO                val. loss  0.0485 (mse_score: 0.0485) (*)\n",
      "02:16 madminer.utils.ml.sc INFO      Epoch 04: train loss 0.0352 (mse_score: 0.0352)\n",
      "02:16 madminer.utils.ml.sc INFO                val. loss  0.0454 (mse_score: 0.0454) (*)\n",
      "02:17 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.0328 (mse_score: 0.0328)\n",
      "02:17 madminer.utils.ml.sc INFO                val. loss  0.0409 (mse_score: 0.0409) (*)\n",
      "02:17 madminer.utils.ml.sc INFO      Epoch 06: train loss 0.0301 (mse_score: 0.0301)\n",
      "02:17 madminer.utils.ml.sc INFO                val. loss  0.0397 (mse_score: 0.0397) (*)\n",
      "02:17 madminer.utils.ml.sc INFO      Epoch 07: train loss 0.0286 (mse_score: 0.0286)\n",
      "02:17 madminer.utils.ml.sc INFO                val. loss  0.0387 (mse_score: 0.0387) (*)\n",
      "02:18 madminer.utils.ml.sc INFO      Epoch 08: train loss 0.0272 (mse_score: 0.0272)\n",
      "02:18 madminer.utils.ml.sc INFO                val. loss  0.0386 (mse_score: 0.0386) (*)\n",
      "02:18 madminer.utils.ml.sc INFO      Epoch 09: train loss 0.0260 (mse_score: 0.0260)\n",
      "02:18 madminer.utils.ml.sc INFO                val. loss  0.0398 (mse_score: 0.0398)\n",
      "02:18 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0251 (mse_score: 0.0251)\n",
      "02:18 madminer.utils.ml.sc INFO                val. loss  0.0367 (mse_score: 0.0367) (*)\n",
      "02:19 madminer.utils.ml.sc INFO      Epoch 11: train loss 0.0241 (mse_score: 0.0241)\n",
      "02:19 madminer.utils.ml.sc INFO                val. loss  0.0371 (mse_score: 0.0371)\n",
      "02:19 madminer.utils.ml.sc INFO      Epoch 12: train loss 0.0233 (mse_score: 0.0233)\n",
      "02:19 madminer.utils.ml.sc INFO                val. loss  0.0356 (mse_score: 0.0356) (*)\n",
      "02:19 madminer.utils.ml.sc INFO      Epoch 13: train loss 0.0227 (mse_score: 0.0227)\n",
      "02:19 madminer.utils.ml.sc INFO                val. loss  0.0350 (mse_score: 0.0350) (*)\n",
      "02:20 madminer.utils.ml.sc INFO      Epoch 14: train loss 0.0222 (mse_score: 0.0222)\n",
      "02:20 madminer.utils.ml.sc INFO                val. loss  0.0356 (mse_score: 0.0356)\n",
      "02:20 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0215 (mse_score: 0.0215)\n",
      "02:20 madminer.utils.ml.sc INFO                val. loss  0.0349 (mse_score: 0.0349) (*)\n",
      "02:21 madminer.utils.ml.sc INFO      Epoch 16: train loss 0.0211 (mse_score: 0.0211)\n",
      "02:21 madminer.utils.ml.sc INFO                val. loss  0.0344 (mse_score: 0.0344) (*)\n",
      "02:21 madminer.utils.ml.sc INFO      Epoch 17: train loss 0.0206 (mse_score: 0.0206)\n",
      "02:21 madminer.utils.ml.sc INFO                val. loss  0.0344 (mse_score: 0.0344) (*)\n",
      "02:21 madminer.utils.ml.sc INFO      Epoch 18: train loss 0.0203 (mse_score: 0.0203)\n",
      "02:21 madminer.utils.ml.sc INFO                val. loss  0.0339 (mse_score: 0.0339) (*)\n",
      "02:22 madminer.utils.ml.sc INFO      Epoch 19: train loss 0.0199 (mse_score: 0.0199)\n",
      "02:22 madminer.utils.ml.sc INFO                val. loss  0.0338 (mse_score: 0.0338) (*)\n",
      "02:22 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0196 (mse_score: 0.0196)\n",
      "02:22 madminer.utils.ml.sc INFO                val. loss  0.0334 (mse_score: 0.0334) (*)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:22 madminer.utils.ml.sc INFO      Epoch 21: train loss 0.0193 (mse_score: 0.0193)\n",
      "02:22 madminer.utils.ml.sc INFO                val. loss  0.0333 (mse_score: 0.0333) (*)\n",
      "02:23 madminer.utils.ml.sc INFO      Epoch 22: train loss 0.0190 (mse_score: 0.0190)\n",
      "02:23 madminer.utils.ml.sc INFO                val. loss  0.0330 (mse_score: 0.0330) (*)\n",
      "02:23 madminer.utils.ml.sc INFO      Epoch 23: train loss 0.0187 (mse_score: 0.0187)\n",
      "02:23 madminer.utils.ml.sc INFO                val. loss  0.0341 (mse_score: 0.0341)\n",
      "02:23 madminer.utils.ml.sc INFO      Epoch 24: train loss 0.0185 (mse_score: 0.0185)\n",
      "02:23 madminer.utils.ml.sc INFO                val. loss  0.0331 (mse_score: 0.0331)\n",
      "02:24 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0184 (mse_score: 0.0184)\n",
      "02:24 madminer.utils.ml.sc INFO                val. loss  0.0328 (mse_score: 0.0328) (*)\n",
      "02:24 madminer.utils.ml.sc INFO      Epoch 26: train loss 0.0181 (mse_score: 0.0181)\n",
      "02:24 madminer.utils.ml.sc INFO                val. loss  0.0326 (mse_score: 0.0326) (*)\n",
      "02:24 madminer.utils.ml.sc INFO      Epoch 27: train loss 0.0179 (mse_score: 0.0179)\n",
      "02:24 madminer.utils.ml.sc INFO                val. loss  0.0341 (mse_score: 0.0341)\n",
      "02:25 madminer.utils.ml.sc INFO      Epoch 28: train loss 0.0178 (mse_score: 0.0178)\n",
      "02:25 madminer.utils.ml.sc INFO                val. loss  0.0327 (mse_score: 0.0327)\n",
      "02:25 madminer.utils.ml.sc INFO      Epoch 29: train loss 0.0176 (mse_score: 0.0176)\n",
      "02:25 madminer.utils.ml.sc INFO                val. loss  0.0324 (mse_score: 0.0324) (*)\n",
      "02:26 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0174 (mse_score: 0.0174)\n",
      "02:26 madminer.utils.ml.sc INFO                val. loss  0.0325 (mse_score: 0.0325)\n",
      "02:26 madminer.utils.ml.sc INFO      Epoch 31: train loss 0.0173 (mse_score: 0.0173)\n",
      "02:26 madminer.utils.ml.sc INFO                val. loss  0.0322 (mse_score: 0.0322) (*)\n",
      "02:26 madminer.utils.ml.sc INFO      Epoch 32: train loss 0.0171 (mse_score: 0.0171)\n",
      "02:26 madminer.utils.ml.sc INFO                val. loss  0.0324 (mse_score: 0.0324)\n",
      "02:27 madminer.utils.ml.sc INFO      Epoch 33: train loss 0.0170 (mse_score: 0.0170)\n",
      "02:27 madminer.utils.ml.sc INFO                val. loss  0.0321 (mse_score: 0.0321) (*)\n",
      "02:27 madminer.utils.ml.sc INFO      Epoch 34: train loss 0.0169 (mse_score: 0.0169)\n",
      "02:27 madminer.utils.ml.sc INFO                val. loss  0.0321 (mse_score: 0.0321) (*)\n",
      "02:28 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0168 (mse_score: 0.0168)\n",
      "02:28 madminer.utils.ml.sc INFO                val. loss  0.0320 (mse_score: 0.0320) (*)\n",
      "02:28 madminer.utils.ml.sc INFO      Epoch 36: train loss 0.0167 (mse_score: 0.0167)\n",
      "02:28 madminer.utils.ml.sc INFO                val. loss  0.0321 (mse_score: 0.0321)\n",
      "02:28 madminer.utils.ml.sc INFO      Epoch 37: train loss 0.0166 (mse_score: 0.0166)\n",
      "02:28 madminer.utils.ml.sc INFO                val. loss  0.0320 (mse_score: 0.0320) (*)\n",
      "02:29 madminer.utils.ml.sc INFO      Epoch 38: train loss 0.0165 (mse_score: 0.0165)\n",
      "02:29 madminer.utils.ml.sc INFO                val. loss  0.0320 (mse_score: 0.0320) (*)\n",
      "02:29 madminer.utils.ml.sc INFO      Epoch 39: train loss 0.0164 (mse_score: 0.0164)\n",
      "02:29 madminer.utils.ml.sc INFO                val. loss  0.0319 (mse_score: 0.0319) (*)\n",
      "02:30 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0163 (mse_score: 0.0163)\n",
      "02:30 madminer.utils.ml.sc INFO                val. loss  0.0318 (mse_score: 0.0318) (*)\n",
      "02:30 madminer.utils.ml.sc INFO      Epoch 41: train loss 0.0162 (mse_score: 0.0162)\n",
      "02:30 madminer.utils.ml.sc INFO                val. loss  0.0318 (mse_score: 0.0318) (*)\n",
      "02:30 madminer.utils.ml.sc INFO      Epoch 42: train loss 0.0162 (mse_score: 0.0162)\n",
      "02:30 madminer.utils.ml.sc INFO                val. loss  0.0318 (mse_score: 0.0318) (*)\n",
      "02:31 madminer.utils.ml.sc INFO      Epoch 43: train loss 0.0161 (mse_score: 0.0161)\n",
      "02:31 madminer.utils.ml.sc INFO                val. loss  0.0319 (mse_score: 0.0319)\n",
      "02:31 madminer.utils.ml.sc INFO      Epoch 44: train loss 0.0160 (mse_score: 0.0160)\n",
      "02:31 madminer.utils.ml.sc INFO                val. loss  0.0318 (mse_score: 0.0318) (*)\n",
      "02:32 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0160 (mse_score: 0.0160)\n",
      "02:32 madminer.utils.ml.sc INFO                val. loss  0.0317 (mse_score: 0.0317) (*)\n",
      "02:32 madminer.utils.ml.sc INFO      Epoch 46: train loss 0.0159 (mse_score: 0.0159)\n",
      "02:32 madminer.utils.ml.sc INFO                val. loss  0.0318 (mse_score: 0.0318)\n",
      "02:32 madminer.utils.ml.sc INFO      Epoch 47: train loss 0.0158 (mse_score: 0.0158)\n",
      "02:32 madminer.utils.ml.sc INFO                val. loss  0.0317 (mse_score: 0.0317) (*)\n",
      "02:33 madminer.utils.ml.sc INFO      Epoch 48: train loss 0.0158 (mse_score: 0.0158)\n",
      "02:33 madminer.utils.ml.sc INFO                val. loss  0.0316 (mse_score: 0.0316) (*)\n",
      "02:33 madminer.utils.ml.sc INFO      Epoch 49: train loss 0.0157 (mse_score: 0.0157)\n",
      "02:33 madminer.utils.ml.sc INFO                val. loss  0.0317 (mse_score: 0.0317)\n",
      "02:34 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0157 (mse_score: 0.0157)\n",
      "02:34 madminer.utils.ml.sc INFO                val. loss  0.0316 (mse_score: 0.0316)\n",
      "02:34 madminer.utils.ml.sc INFO    Early stopping after epoch 48, with loss 0.03 compared to final loss 0.03\n",
      "02:34 madminer.utils.ml.sc INFO    Finished training\n",
      "02:34 madminer.ml          INFO    Training estimator 10 / 10 in ensemble\n",
      "02:34 madminer.ml          INFO    Starting training\n",
      "02:34 madminer.ml          INFO      Method:                 sally\n",
      "02:34 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/x_train_9.npy\n",
      "02:34 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local_antitight/t_xz_train_9.npy\n",
      "02:34 madminer.ml          INFO      Features:               all\n",
      "02:34 madminer.ml          INFO      Method:                 sally\n",
      "02:34 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "02:34 madminer.ml          INFO      Activation function:    tanh\n",
      "02:34 madminer.ml          INFO      Batch size:             128\n",
      "02:34 madminer.ml          INFO      Trainer:                amsgrad\n",
      "02:34 madminer.ml          INFO      Epochs:                 50\n",
      "02:34 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "02:34 madminer.ml          INFO      Validation split:       0.5\n",
      "02:34 madminer.ml          INFO      Early stopping:         True\n",
      "02:34 madminer.ml          INFO      Scale inputs:           True\n",
      "02:34 madminer.ml          INFO      Shuffle labels          False\n",
      "02:34 madminer.ml          INFO      Regularization:         None\n",
      "02:34 madminer.ml          INFO      Samples:                all\n",
      "02:34 madminer.ml          INFO    Loading training data\n",
      "02:34 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "02:34 madminer.ml          INFO    Rescaling inputs\n",
      "02:34 madminer.ml          INFO    Creating model for method sally\n",
      "02:34 madminer.ml          INFO    Training model\n",
      "02:34 madminer.utils.ml.sc INFO      Epoch 01: train loss 0.0900 (mse_score: 0.0900)\n",
      "02:34 madminer.utils.ml.sc INFO                val. loss  0.0820 (mse_score: 0.0820) (*)\n",
      "02:34 madminer.utils.ml.sc INFO      Epoch 02: train loss 0.0718 (mse_score: 0.0718)\n",
      "02:34 madminer.utils.ml.sc INFO                val. loss  0.0536 (mse_score: 0.0536) (*)\n",
      "02:35 madminer.utils.ml.sc INFO      Epoch 03: train loss 0.0622 (mse_score: 0.0622)\n",
      "02:35 madminer.utils.ml.sc INFO                val. loss  0.0460 (mse_score: 0.0460) (*)\n",
      "02:35 madminer.utils.ml.sc INFO      Epoch 04: train loss 0.0565 (mse_score: 0.0565)\n",
      "02:35 madminer.utils.ml.sc INFO                val. loss  0.0438 (mse_score: 0.0438) (*)\n",
      "02:35 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.0530 (mse_score: 0.0530)\n",
      "02:35 madminer.utils.ml.sc INFO                val. loss  0.0415 (mse_score: 0.0415) (*)\n",
      "02:36 madminer.utils.ml.sc INFO      Epoch 06: train loss 0.0507 (mse_score: 0.0507)\n",
      "02:36 madminer.utils.ml.sc INFO                val. loss  0.0387 (mse_score: 0.0387) (*)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:36 madminer.utils.ml.sc INFO      Epoch 07: train loss 0.0483 (mse_score: 0.0483)\n",
      "02:36 madminer.utils.ml.sc INFO                val. loss  0.0370 (mse_score: 0.0370) (*)\n",
      "02:37 madminer.utils.ml.sc INFO      Epoch 08: train loss 0.0464 (mse_score: 0.0464)\n",
      "02:37 madminer.utils.ml.sc INFO                val. loss  0.0364 (mse_score: 0.0364) (*)\n",
      "02:37 madminer.utils.ml.sc INFO      Epoch 09: train loss 0.0450 (mse_score: 0.0450)\n",
      "02:37 madminer.utils.ml.sc INFO                val. loss  0.0360 (mse_score: 0.0360) (*)\n",
      "02:37 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0439 (mse_score: 0.0439)\n",
      "02:37 madminer.utils.ml.sc INFO                val. loss  0.0350 (mse_score: 0.0350) (*)\n",
      "02:38 madminer.utils.ml.sc INFO      Epoch 11: train loss 0.0428 (mse_score: 0.0428)\n",
      "02:38 madminer.utils.ml.sc INFO                val. loss  0.0343 (mse_score: 0.0343) (*)\n",
      "02:38 madminer.utils.ml.sc INFO      Epoch 12: train loss 0.0420 (mse_score: 0.0420)\n",
      "02:38 madminer.utils.ml.sc INFO                val. loss  0.0344 (mse_score: 0.0344)\n",
      "02:39 madminer.utils.ml.sc INFO      Epoch 13: train loss 0.0411 (mse_score: 0.0411)\n",
      "02:39 madminer.utils.ml.sc INFO                val. loss  0.0338 (mse_score: 0.0338) (*)\n",
      "02:39 madminer.utils.ml.sc INFO      Epoch 14: train loss 0.0404 (mse_score: 0.0404)\n",
      "02:39 madminer.utils.ml.sc INFO                val. loss  0.0348 (mse_score: 0.0348)\n",
      "02:39 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0398 (mse_score: 0.0398)\n",
      "02:39 madminer.utils.ml.sc INFO                val. loss  0.0326 (mse_score: 0.0326) (*)\n",
      "02:40 madminer.utils.ml.sc INFO      Epoch 16: train loss 0.0391 (mse_score: 0.0391)\n",
      "02:40 madminer.utils.ml.sc INFO                val. loss  0.0327 (mse_score: 0.0327)\n",
      "02:40 madminer.utils.ml.sc INFO      Epoch 17: train loss 0.0385 (mse_score: 0.0385)\n",
      "02:40 madminer.utils.ml.sc INFO                val. loss  0.0320 (mse_score: 0.0320) (*)\n",
      "02:41 madminer.utils.ml.sc INFO      Epoch 18: train loss 0.0380 (mse_score: 0.0380)\n",
      "02:41 madminer.utils.ml.sc INFO                val. loss  0.0322 (mse_score: 0.0322)\n",
      "02:41 madminer.utils.ml.sc INFO      Epoch 19: train loss 0.0376 (mse_score: 0.0376)\n",
      "02:41 madminer.utils.ml.sc INFO                val. loss  0.0317 (mse_score: 0.0317) (*)\n",
      "02:41 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0371 (mse_score: 0.0371)\n",
      "02:41 madminer.utils.ml.sc INFO                val. loss  0.0316 (mse_score: 0.0316) (*)\n",
      "02:42 madminer.utils.ml.sc INFO      Epoch 21: train loss 0.0367 (mse_score: 0.0367)\n",
      "02:42 madminer.utils.ml.sc INFO                val. loss  0.0314 (mse_score: 0.0314) (*)\n",
      "02:42 madminer.utils.ml.sc INFO      Epoch 22: train loss 0.0363 (mse_score: 0.0363)\n",
      "02:42 madminer.utils.ml.sc INFO                val. loss  0.0317 (mse_score: 0.0317)\n",
      "02:43 madminer.utils.ml.sc INFO      Epoch 23: train loss 0.0360 (mse_score: 0.0360)\n",
      "02:43 madminer.utils.ml.sc INFO                val. loss  0.0308 (mse_score: 0.0308) (*)\n",
      "02:43 madminer.utils.ml.sc INFO      Epoch 24: train loss 0.0357 (mse_score: 0.0357)\n",
      "02:43 madminer.utils.ml.sc INFO                val. loss  0.0309 (mse_score: 0.0309)\n",
      "02:43 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0354 (mse_score: 0.0354)\n",
      "02:43 madminer.utils.ml.sc INFO                val. loss  0.0310 (mse_score: 0.0310)\n",
      "02:44 madminer.utils.ml.sc INFO      Epoch 26: train loss 0.0351 (mse_score: 0.0351)\n",
      "02:44 madminer.utils.ml.sc INFO                val. loss  0.0307 (mse_score: 0.0307) (*)\n",
      "02:44 madminer.utils.ml.sc INFO      Epoch 27: train loss 0.0349 (mse_score: 0.0349)\n",
      "02:44 madminer.utils.ml.sc INFO                val. loss  0.0305 (mse_score: 0.0305) (*)\n",
      "02:45 madminer.utils.ml.sc INFO      Epoch 28: train loss 0.0346 (mse_score: 0.0346)\n",
      "02:45 madminer.utils.ml.sc INFO                val. loss  0.0305 (mse_score: 0.0305) (*)\n",
      "02:45 madminer.utils.ml.sc INFO      Epoch 29: train loss 0.0344 (mse_score: 0.0344)\n",
      "02:45 madminer.utils.ml.sc INFO                val. loss  0.0305 (mse_score: 0.0305) (*)\n",
      "02:46 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0342 (mse_score: 0.0342)\n",
      "02:46 madminer.utils.ml.sc INFO                val. loss  0.0302 (mse_score: 0.0302) (*)\n",
      "02:46 madminer.utils.ml.sc INFO      Epoch 31: train loss 0.0340 (mse_score: 0.0340)\n",
      "02:46 madminer.utils.ml.sc INFO                val. loss  0.0302 (mse_score: 0.0302) (*)\n",
      "02:46 madminer.utils.ml.sc INFO      Epoch 32: train loss 0.0340 (mse_score: 0.0340)\n",
      "02:46 madminer.utils.ml.sc INFO                val. loss  0.0305 (mse_score: 0.0305)\n",
      "02:47 madminer.utils.ml.sc INFO      Epoch 33: train loss 0.0336 (mse_score: 0.0336)\n",
      "02:47 madminer.utils.ml.sc INFO                val. loss  0.0300 (mse_score: 0.0300) (*)\n",
      "02:47 madminer.utils.ml.sc INFO      Epoch 34: train loss 0.0334 (mse_score: 0.0334)\n",
      "02:47 madminer.utils.ml.sc INFO                val. loss  0.0298 (mse_score: 0.0298) (*)\n",
      "02:48 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0333 (mse_score: 0.0333)\n",
      "02:48 madminer.utils.ml.sc INFO                val. loss  0.0299 (mse_score: 0.0299)\n",
      "02:48 madminer.utils.ml.sc INFO      Epoch 36: train loss 0.0331 (mse_score: 0.0331)\n",
      "02:48 madminer.utils.ml.sc INFO                val. loss  0.0298 (mse_score: 0.0298) (*)\n",
      "02:48 madminer.utils.ml.sc INFO      Epoch 37: train loss 0.0330 (mse_score: 0.0330)\n",
      "02:48 madminer.utils.ml.sc INFO                val. loss  0.0297 (mse_score: 0.0297) (*)\n",
      "02:49 madminer.utils.ml.sc INFO      Epoch 38: train loss 0.0328 (mse_score: 0.0328)\n",
      "02:49 madminer.utils.ml.sc INFO                val. loss  0.0297 (mse_score: 0.0297) (*)\n",
      "02:49 madminer.utils.ml.sc INFO      Epoch 39: train loss 0.0327 (mse_score: 0.0327)\n",
      "02:49 madminer.utils.ml.sc INFO                val. loss  0.0297 (mse_score: 0.0297) (*)\n",
      "02:50 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0326 (mse_score: 0.0326)\n",
      "02:50 madminer.utils.ml.sc INFO                val. loss  0.0296 (mse_score: 0.0296) (*)\n",
      "02:50 madminer.utils.ml.sc INFO      Epoch 41: train loss 0.0325 (mse_score: 0.0325)\n",
      "02:50 madminer.utils.ml.sc INFO                val. loss  0.0295 (mse_score: 0.0295) (*)\n",
      "02:50 madminer.utils.ml.sc INFO      Epoch 42: train loss 0.0324 (mse_score: 0.0324)\n",
      "02:50 madminer.utils.ml.sc INFO                val. loss  0.0296 (mse_score: 0.0296)\n",
      "02:51 madminer.utils.ml.sc INFO      Epoch 43: train loss 0.0323 (mse_score: 0.0323)\n",
      "02:51 madminer.utils.ml.sc INFO                val. loss  0.0296 (mse_score: 0.0296)\n",
      "02:51 madminer.utils.ml.sc INFO      Epoch 44: train loss 0.0322 (mse_score: 0.0322)\n",
      "02:51 madminer.utils.ml.sc INFO                val. loss  0.0296 (mse_score: 0.0296)\n",
      "02:52 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0321 (mse_score: 0.0321)\n",
      "02:52 madminer.utils.ml.sc INFO                val. loss  0.0295 (mse_score: 0.0295)\n",
      "02:52 madminer.utils.ml.sc INFO      Epoch 46: train loss 0.0320 (mse_score: 0.0320)\n",
      "02:52 madminer.utils.ml.sc INFO                val. loss  0.0295 (mse_score: 0.0295) (*)\n",
      "02:52 madminer.utils.ml.sc INFO      Epoch 47: train loss 0.0320 (mse_score: 0.0320)\n",
      "02:52 madminer.utils.ml.sc INFO                val. loss  0.0295 (mse_score: 0.0295)\n",
      "02:53 madminer.utils.ml.sc INFO      Epoch 48: train loss 0.0318 (mse_score: 0.0318)\n",
      "02:53 madminer.utils.ml.sc INFO                val. loss  0.0296 (mse_score: 0.0296)\n",
      "02:53 madminer.utils.ml.sc INFO      Epoch 49: train loss 0.0318 (mse_score: 0.0318)\n",
      "02:53 madminer.utils.ml.sc INFO                val. loss  0.0296 (mse_score: 0.0296)\n",
      "02:54 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0317 (mse_score: 0.0317)\n",
      "02:54 madminer.utils.ml.sc INFO                val. loss  0.0294 (mse_score: 0.0294) (*)\n",
      "02:54 madminer.utils.ml.sc INFO    Early stopping did not improve performance\n",
      "02:54 madminer.utils.ml.sc INFO    Finished training\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'all_antitight',\n",
    "    use_tight_cuts=False,\n",
    "    use_antitight_cuts=True,\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal observable basis (no jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_obs = [0,1] + list(range(4,12)) + list(range(16,33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:52 madminer.ml          INFO    Training 10 estimators in ensemble\n",
      "21:52 madminer.ml          INFO    Training estimator 1 / 10 in ensemble\n",
      "21:52 madminer.ml          INFO    Starting training\n",
      "21:52 madminer.ml          INFO      Method:                 sally\n",
      "21:52 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_0.npy\n",
      "21:52 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_0.npy\n",
      "21:52 madminer.ml          INFO      Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "21:52 madminer.ml          INFO      Method:                 sally\n",
      "21:52 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "21:52 madminer.ml          INFO      Activation function:    tanh\n",
      "21:52 madminer.ml          INFO      Batch size:             128\n",
      "21:52 madminer.ml          INFO      Trainer:                amsgrad\n",
      "21:52 madminer.ml          INFO      Epochs:                 50\n",
      "21:52 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:52 madminer.ml          INFO      Validation split:       0.5\n",
      "21:52 madminer.ml          INFO      Early stopping:         True\n",
      "21:52 madminer.ml          INFO      Scale inputs:           True\n",
      "21:52 madminer.ml          INFO      Shuffle labels          False\n",
      "21:52 madminer.ml          INFO      Regularization:         None\n",
      "21:52 madminer.ml          INFO      Samples:                all\n",
      "21:52 madminer.ml          INFO    Loading training data\n",
      "21:52 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "21:52 madminer.ml          INFO    Rescaling inputs\n",
      "21:53 madminer.ml          INFO    Only using 27 of 33 observables\n",
      "21:53 madminer.ml          INFO    Creating model for method sally\n",
      "21:53 madminer.ml          INFO    Training model\n",
      "21:55 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.0928 (mse_score: 0.0928)\n",
      "21:55 madminer.utils.ml.sc INFO                val. loss  0.1476 (mse_score: 0.1476)\n",
      "21:58 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0756 (mse_score: 0.0756)\n",
      "21:58 madminer.utils.ml.sc INFO                val. loss  0.1246 (mse_score: 0.1246) (*)\n",
      "22:01 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0659 (mse_score: 0.0659)\n",
      "22:01 madminer.utils.ml.sc INFO                val. loss  0.1220 (mse_score: 0.1220)\n",
      "22:04 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0607 (mse_score: 0.0607)\n",
      "22:04 madminer.utils.ml.sc INFO                val. loss  0.1192 (mse_score: 0.1192)\n",
      "22:07 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0567 (mse_score: 0.0567)\n",
      "22:07 madminer.utils.ml.sc INFO                val. loss  0.1189 (mse_score: 0.1189)\n",
      "22:10 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0540 (mse_score: 0.0540)\n",
      "22:10 madminer.utils.ml.sc INFO                val. loss  0.1170 (mse_score: 0.1170) (*)\n",
      "22:13 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0520 (mse_score: 0.0520)\n",
      "22:13 madminer.utils.ml.sc INFO                val. loss  0.1178 (mse_score: 0.1178)\n",
      "22:15 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0502 (mse_score: 0.0502)\n",
      "22:15 madminer.utils.ml.sc INFO                val. loss  0.1173 (mse_score: 0.1173)\n",
      "22:19 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0489 (mse_score: 0.0489)\n",
      "22:19 madminer.utils.ml.sc INFO                val. loss  0.1175 (mse_score: 0.1175)\n",
      "22:25 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0479 (mse_score: 0.0479)\n",
      "22:25 madminer.utils.ml.sc INFO                val. loss  0.1171 (mse_score: 0.1171)\n",
      "22:25 madminer.utils.ml.sc INFO    Early stopping after epoch 30, with loss 0.12 compared to final loss 0.12\n",
      "22:25 madminer.utils.ml.sc INFO    Finished training\n",
      "22:25 madminer.ml          INFO    Training estimator 2 / 10 in ensemble\n",
      "22:25 madminer.ml          INFO    Starting training\n",
      "22:25 madminer.ml          INFO      Method:                 sally\n",
      "22:25 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_1.npy\n",
      "22:25 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_1.npy\n",
      "22:25 madminer.ml          INFO      Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "22:25 madminer.ml          INFO      Method:                 sally\n",
      "22:25 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "22:25 madminer.ml          INFO      Activation function:    tanh\n",
      "22:25 madminer.ml          INFO      Batch size:             128\n",
      "22:25 madminer.ml          INFO      Trainer:                amsgrad\n",
      "22:25 madminer.ml          INFO      Epochs:                 50\n",
      "22:25 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:25 madminer.ml          INFO      Validation split:       0.5\n",
      "22:25 madminer.ml          INFO      Early stopping:         True\n",
      "22:25 madminer.ml          INFO      Scale inputs:           True\n",
      "22:25 madminer.ml          INFO      Shuffle labels          False\n",
      "22:25 madminer.ml          INFO      Regularization:         None\n",
      "22:25 madminer.ml          INFO      Samples:                all\n",
      "22:25 madminer.ml          INFO    Loading training data\n",
      "22:25 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "22:25 madminer.ml          INFO    Rescaling inputs\n",
      "22:25 madminer.ml          INFO    Only using 27 of 33 observables\n",
      "22:25 madminer.ml          INFO    Creating model for method sally\n",
      "22:25 madminer.ml          INFO    Training model\n",
      "22:30 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1293 (mse_score: 0.1293)\n",
      "22:30 madminer.utils.ml.sc INFO                val. loss  0.1108 (mse_score: 0.1108) (*)\n",
      "22:32 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.1036 (mse_score: 0.1036)\n",
      "22:32 madminer.utils.ml.sc INFO                val. loss  0.0958 (mse_score: 0.0958) (*)\n",
      "23:09 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0920 (mse_score: 0.0920)\n",
      "23:09 madminer.utils.ml.sc INFO                val. loss  0.0918 (mse_score: 0.0918) (*)\n",
      "00:17 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0847 (mse_score: 0.0847)\n",
      "00:17 madminer.utils.ml.sc INFO                val. loss  0.0890 (mse_score: 0.0890) (*)\n",
      "00:59 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0791 (mse_score: 0.0791)\n",
      "00:59 madminer.utils.ml.sc INFO                val. loss  0.0886 (mse_score: 0.0886)\n",
      "02:38 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0751 (mse_score: 0.0751)\n",
      "02:38 madminer.utils.ml.sc INFO                val. loss  0.0875 (mse_score: 0.0875) (*)\n",
      "03:52 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0721 (mse_score: 0.0721)\n",
      "03:52 madminer.utils.ml.sc INFO                val. loss  0.0874 (mse_score: 0.0874)\n",
      "04:34 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0698 (mse_score: 0.0698)\n",
      "04:34 madminer.utils.ml.sc INFO                val. loss  0.0869 (mse_score: 0.0869)\n",
      "05:48 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0679 (mse_score: 0.0679)\n",
      "05:48 madminer.utils.ml.sc INFO                val. loss  0.0861 (mse_score: 0.0861) (*)\n",
      "07:04 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0665 (mse_score: 0.0665)\n",
      "07:04 madminer.utils.ml.sc INFO                val. loss  0.0860 (mse_score: 0.0860)\n",
      "07:04 madminer.utils.ml.sc INFO    Early stopping after epoch 49, with loss 0.09 compared to final loss 0.09\n",
      "07:04 madminer.utils.ml.sc INFO    Finished training\n",
      "07:04 madminer.ml          INFO    Training estimator 3 / 10 in ensemble\n",
      "07:04 madminer.ml          INFO    Starting training\n",
      "07:04 madminer.ml          INFO      Method:                 sally\n",
      "07:04 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_2.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:04 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_2.npy\n",
      "07:04 madminer.ml          INFO      Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "07:04 madminer.ml          INFO      Method:                 sally\n",
      "07:04 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "07:04 madminer.ml          INFO      Activation function:    tanh\n",
      "07:04 madminer.ml          INFO      Batch size:             128\n",
      "07:04 madminer.ml          INFO      Trainer:                amsgrad\n",
      "07:04 madminer.ml          INFO      Epochs:                 50\n",
      "07:04 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "07:04 madminer.ml          INFO      Validation split:       0.5\n",
      "07:04 madminer.ml          INFO      Early stopping:         True\n",
      "07:04 madminer.ml          INFO      Scale inputs:           True\n",
      "07:04 madminer.ml          INFO      Shuffle labels          False\n",
      "07:04 madminer.ml          INFO      Regularization:         None\n",
      "07:04 madminer.ml          INFO      Samples:                all\n",
      "07:04 madminer.ml          INFO    Loading training data\n",
      "07:04 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "07:04 madminer.ml          INFO    Rescaling inputs\n",
      "07:04 madminer.ml          INFO    Only using 27 of 33 observables\n",
      "07:04 madminer.ml          INFO    Creating model for method sally\n",
      "07:04 madminer.ml          INFO    Training model\n",
      "08:17 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1656 (mse_score: 0.1656)\n",
      "08:17 madminer.utils.ml.sc INFO                val. loss  0.1228 (mse_score: 0.1228) (*)\n",
      "09:04 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.1443 (mse_score: 0.1443)\n",
      "09:04 madminer.utils.ml.sc INFO                val. loss  0.1116 (mse_score: 0.1116) (*)\n",
      "10:29 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.1311 (mse_score: 0.1311)\n",
      "10:29 madminer.utils.ml.sc INFO                val. loss  0.1043 (mse_score: 0.1043) (*)\n",
      "10:31 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.1235 (mse_score: 0.1235)\n",
      "10:31 madminer.utils.ml.sc INFO                val. loss  0.1015 (mse_score: 0.1015) (*)\n",
      "10:34 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.1177 (mse_score: 0.1177)\n",
      "10:34 madminer.utils.ml.sc INFO                val. loss  0.1025 (mse_score: 0.1025)\n",
      "10:36 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.1134 (mse_score: 0.1134)\n",
      "10:36 madminer.utils.ml.sc INFO                val. loss  0.1008 (mse_score: 0.1008)\n",
      "10:38 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.1100 (mse_score: 0.1100)\n",
      "10:38 madminer.utils.ml.sc INFO                val. loss  0.0998 (mse_score: 0.0998) (*)\n",
      "10:41 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.1074 (mse_score: 0.1074)\n",
      "10:41 madminer.utils.ml.sc INFO                val. loss  0.0999 (mse_score: 0.0999)\n",
      "10:43 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.1056 (mse_score: 0.1056)\n",
      "10:43 madminer.utils.ml.sc INFO                val. loss  0.0996 (mse_score: 0.0996)\n",
      "10:46 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.1041 (mse_score: 0.1041)\n",
      "10:46 madminer.utils.ml.sc INFO                val. loss  0.0998 (mse_score: 0.0998)\n",
      "10:46 madminer.utils.ml.sc INFO    Early stopping after epoch 37, with loss 0.10 compared to final loss 0.10\n",
      "10:46 madminer.utils.ml.sc INFO    Finished training\n",
      "10:46 madminer.ml          INFO    Training estimator 4 / 10 in ensemble\n",
      "10:46 madminer.ml          INFO    Starting training\n",
      "10:46 madminer.ml          INFO      Method:                 sally\n",
      "10:46 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_3.npy\n",
      "10:46 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_3.npy\n",
      "10:46 madminer.ml          INFO      Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "10:46 madminer.ml          INFO      Method:                 sally\n",
      "10:46 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "10:46 madminer.ml          INFO      Activation function:    tanh\n",
      "10:46 madminer.ml          INFO      Batch size:             128\n",
      "10:46 madminer.ml          INFO      Trainer:                amsgrad\n",
      "10:46 madminer.ml          INFO      Epochs:                 50\n",
      "10:46 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:46 madminer.ml          INFO      Validation split:       0.5\n",
      "10:46 madminer.ml          INFO      Early stopping:         True\n",
      "10:46 madminer.ml          INFO      Scale inputs:           True\n",
      "10:46 madminer.ml          INFO      Shuffle labels          False\n",
      "10:46 madminer.ml          INFO      Regularization:         None\n",
      "10:46 madminer.ml          INFO      Samples:                all\n",
      "10:46 madminer.ml          INFO    Loading training data\n",
      "10:46 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "10:46 madminer.ml          INFO    Rescaling inputs\n",
      "10:46 madminer.ml          INFO    Only using 27 of 33 observables\n",
      "10:46 madminer.ml          INFO    Creating model for method sally\n",
      "10:46 madminer.ml          INFO    Training model\n",
      "10:48 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1197 (mse_score: 0.1197)\n",
      "10:48 madminer.utils.ml.sc INFO                val. loss  0.1482 (mse_score: 0.1482)\n",
      "10:51 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0959 (mse_score: 0.0959)\n",
      "10:51 madminer.utils.ml.sc INFO                val. loss  0.1280 (mse_score: 0.1280) (*)\n",
      "10:54 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0851 (mse_score: 0.0851)\n",
      "10:54 madminer.utils.ml.sc INFO                val. loss  0.1256 (mse_score: 0.1256) (*)\n",
      "10:56 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0772 (mse_score: 0.0772)\n",
      "10:56 madminer.utils.ml.sc INFO                val. loss  0.1225 (mse_score: 0.1225)\n",
      "11:00 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0716 (mse_score: 0.0716)\n",
      "11:00 madminer.utils.ml.sc INFO                val. loss  0.1224 (mse_score: 0.1224)\n",
      "11:04 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0677 (mse_score: 0.0677)\n",
      "11:04 madminer.utils.ml.sc INFO                val. loss  0.1227 (mse_score: 0.1227)\n",
      "11:06 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0644 (mse_score: 0.0644)\n",
      "11:06 madminer.utils.ml.sc INFO                val. loss  0.1211 (mse_score: 0.1211)\n",
      "11:10 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0621 (mse_score: 0.0621)\n",
      "11:10 madminer.utils.ml.sc INFO                val. loss  0.1211 (mse_score: 0.1211)\n",
      "11:13 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0603 (mse_score: 0.0603)\n",
      "11:13 madminer.utils.ml.sc INFO                val. loss  0.1216 (mse_score: 0.1216)\n",
      "11:16 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0589 (mse_score: 0.0589)\n",
      "11:16 madminer.utils.ml.sc INFO                val. loss  0.1211 (mse_score: 0.1211)\n",
      "11:16 madminer.utils.ml.sc INFO    Early stopping after epoch 32, with loss 0.12 compared to final loss 0.12\n",
      "11:16 madminer.utils.ml.sc INFO    Finished training\n",
      "11:16 madminer.ml          INFO    Training estimator 5 / 10 in ensemble\n",
      "11:16 madminer.ml          INFO    Starting training\n",
      "11:16 madminer.ml          INFO      Method:                 sally\n",
      "11:16 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_4.npy\n",
      "11:16 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_4.npy\n",
      "11:16 madminer.ml          INFO      Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "11:16 madminer.ml          INFO      Method:                 sally\n",
      "11:16 madminer.ml          INFO      Hidden layers:          (100, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:16 madminer.ml          INFO      Activation function:    tanh\n",
      "11:16 madminer.ml          INFO      Batch size:             128\n",
      "11:16 madminer.ml          INFO      Trainer:                amsgrad\n",
      "11:16 madminer.ml          INFO      Epochs:                 50\n",
      "11:16 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "11:16 madminer.ml          INFO      Validation split:       0.5\n",
      "11:16 madminer.ml          INFO      Early stopping:         True\n",
      "11:16 madminer.ml          INFO      Scale inputs:           True\n",
      "11:16 madminer.ml          INFO      Shuffle labels          False\n",
      "11:16 madminer.ml          INFO      Regularization:         None\n",
      "11:16 madminer.ml          INFO      Samples:                all\n",
      "11:16 madminer.ml          INFO    Loading training data\n",
      "11:16 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "11:16 madminer.ml          INFO    Rescaling inputs\n",
      "11:16 madminer.ml          INFO    Only using 27 of 33 observables\n",
      "11:16 madminer.ml          INFO    Creating model for method sally\n",
      "11:16 madminer.ml          INFO    Training model\n",
      "11:19 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1199 (mse_score: 0.1199)\n",
      "11:19 madminer.utils.ml.sc INFO                val. loss  0.1370 (mse_score: 0.1370) (*)\n",
      "11:21 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0972 (mse_score: 0.0972)\n",
      "11:21 madminer.utils.ml.sc INFO                val. loss  0.1258 (mse_score: 0.1258) (*)\n",
      "11:23 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0858 (mse_score: 0.0858)\n",
      "11:23 madminer.utils.ml.sc INFO                val. loss  0.1162 (mse_score: 0.1162)\n",
      "11:26 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0799 (mse_score: 0.0799)\n",
      "11:26 madminer.utils.ml.sc INFO                val. loss  0.1138 (mse_score: 0.1138) (*)\n",
      "11:28 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0740 (mse_score: 0.0740)\n",
      "11:28 madminer.utils.ml.sc INFO                val. loss  0.1124 (mse_score: 0.1124) (*)\n",
      "11:31 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0708 (mse_score: 0.0708)\n",
      "11:31 madminer.utils.ml.sc INFO                val. loss  0.1122 (mse_score: 0.1122)\n",
      "11:33 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0681 (mse_score: 0.0681)\n",
      "11:33 madminer.utils.ml.sc INFO                val. loss  0.1119 (mse_score: 0.1119)\n",
      "11:35 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0661 (mse_score: 0.0661)\n",
      "11:35 madminer.utils.ml.sc INFO                val. loss  0.1111 (mse_score: 0.1111)\n",
      "11:38 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0645 (mse_score: 0.0645)\n",
      "11:38 madminer.utils.ml.sc INFO                val. loss  0.1100 (mse_score: 0.1100) (*)\n",
      "11:41 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0632 (mse_score: 0.0632)\n",
      "11:41 madminer.utils.ml.sc INFO                val. loss  0.1106 (mse_score: 0.1106)\n",
      "11:41 madminer.utils.ml.sc INFO    Early stopping after epoch 45, with loss 0.11 compared to final loss 0.11\n",
      "11:41 madminer.utils.ml.sc INFO    Finished training\n",
      "11:41 madminer.ml          INFO    Training estimator 6 / 10 in ensemble\n",
      "11:41 madminer.ml          INFO    Starting training\n",
      "11:41 madminer.ml          INFO      Method:                 sally\n",
      "11:41 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_5.npy\n",
      "11:41 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_5.npy\n",
      "11:41 madminer.ml          INFO      Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "11:41 madminer.ml          INFO      Method:                 sally\n",
      "11:41 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "11:41 madminer.ml          INFO      Activation function:    tanh\n",
      "11:41 madminer.ml          INFO      Batch size:             128\n",
      "11:41 madminer.ml          INFO      Trainer:                amsgrad\n",
      "11:41 madminer.ml          INFO      Epochs:                 50\n",
      "11:41 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "11:41 madminer.ml          INFO      Validation split:       0.5\n",
      "11:41 madminer.ml          INFO      Early stopping:         True\n",
      "11:41 madminer.ml          INFO      Scale inputs:           True\n",
      "11:41 madminer.ml          INFO      Shuffle labels          False\n",
      "11:41 madminer.ml          INFO      Regularization:         None\n",
      "11:41 madminer.ml          INFO      Samples:                all\n",
      "11:41 madminer.ml          INFO    Loading training data\n",
      "11:41 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "11:41 madminer.ml          INFO    Rescaling inputs\n",
      "11:41 madminer.ml          INFO    Only using 27 of 33 observables\n",
      "11:41 madminer.ml          INFO    Creating model for method sally\n",
      "11:41 madminer.ml          INFO    Training model\n",
      "11:44 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1235 (mse_score: 0.1235)\n",
      "11:44 madminer.utils.ml.sc INFO                val. loss  0.1434 (mse_score: 0.1434) (*)\n",
      "11:46 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.1004 (mse_score: 0.1004)\n",
      "11:46 madminer.utils.ml.sc INFO                val. loss  0.1270 (mse_score: 0.1270) (*)\n",
      "11:50 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0891 (mse_score: 0.0891)\n",
      "11:50 madminer.utils.ml.sc INFO                val. loss  0.1198 (mse_score: 0.1198) (*)\n",
      "11:53 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0816 (mse_score: 0.0816)\n",
      "11:53 madminer.utils.ml.sc INFO                val. loss  0.1175 (mse_score: 0.1175) (*)\n",
      "11:56 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0761 (mse_score: 0.0761)\n",
      "11:56 madminer.utils.ml.sc INFO                val. loss  0.1152 (mse_score: 0.1152) (*)\n",
      "11:59 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0719 (mse_score: 0.0719)\n",
      "11:59 madminer.utils.ml.sc INFO                val. loss  0.1134 (mse_score: 0.1134) (*)\n",
      "12:02 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0692 (mse_score: 0.0692)\n",
      "12:02 madminer.utils.ml.sc INFO                val. loss  0.1135 (mse_score: 0.1135)\n",
      "12:05 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0670 (mse_score: 0.0670)\n",
      "12:05 madminer.utils.ml.sc INFO                val. loss  0.1134 (mse_score: 0.1134)\n",
      "12:08 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0653 (mse_score: 0.0653)\n",
      "12:08 madminer.utils.ml.sc INFO                val. loss  0.1124 (mse_score: 0.1124)\n",
      "12:11 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0639 (mse_score: 0.0639)\n",
      "12:11 madminer.utils.ml.sc INFO                val. loss  0.1126 (mse_score: 0.1126)\n",
      "12:11 madminer.utils.ml.sc INFO    Early stopping after epoch 49, with loss 0.11 compared to final loss 0.11\n",
      "12:11 madminer.utils.ml.sc INFO    Finished training\n",
      "12:11 madminer.ml          INFO    Training estimator 7 / 10 in ensemble\n",
      "12:11 madminer.ml          INFO    Starting training\n",
      "12:11 madminer.ml          INFO      Method:                 sally\n",
      "12:11 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_6.npy\n",
      "12:11 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_6.npy\n",
      "12:11 madminer.ml          INFO      Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "12:11 madminer.ml          INFO      Method:                 sally\n",
      "12:11 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "12:11 madminer.ml          INFO      Activation function:    tanh\n",
      "12:11 madminer.ml          INFO      Batch size:             128\n",
      "12:11 madminer.ml          INFO      Trainer:                amsgrad\n",
      "12:11 madminer.ml          INFO      Epochs:                 50\n",
      "12:11 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "12:11 madminer.ml          INFO      Validation split:       0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:11 madminer.ml          INFO      Early stopping:         True\n",
      "12:11 madminer.ml          INFO      Scale inputs:           True\n",
      "12:11 madminer.ml          INFO      Shuffle labels          False\n",
      "12:11 madminer.ml          INFO      Regularization:         None\n",
      "12:11 madminer.ml          INFO      Samples:                all\n",
      "12:11 madminer.ml          INFO    Loading training data\n",
      "12:11 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "12:11 madminer.ml          INFO    Rescaling inputs\n",
      "12:11 madminer.ml          INFO    Only using 27 of 33 observables\n",
      "12:11 madminer.ml          INFO    Creating model for method sally\n",
      "12:11 madminer.ml          INFO    Training model\n",
      "12:14 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.0887 (mse_score: 0.0887)\n",
      "12:14 madminer.utils.ml.sc INFO                val. loss  0.2115 (mse_score: 0.2115) (*)\n",
      "12:16 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0706 (mse_score: 0.0706)\n",
      "12:16 madminer.utils.ml.sc INFO                val. loss  0.1952 (mse_score: 0.1952) (*)\n",
      "12:19 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0620 (mse_score: 0.0620)\n",
      "12:19 madminer.utils.ml.sc INFO                val. loss  0.1890 (mse_score: 0.1890) (*)\n",
      "12:21 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0561 (mse_score: 0.0561)\n",
      "12:21 madminer.utils.ml.sc INFO                val. loss  0.1880 (mse_score: 0.1880)\n",
      "12:24 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0523 (mse_score: 0.0523)\n",
      "12:24 madminer.utils.ml.sc INFO                val. loss  0.1852 (mse_score: 0.1852) (*)\n",
      "12:27 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0496 (mse_score: 0.0496)\n",
      "12:27 madminer.utils.ml.sc INFO                val. loss  0.1856 (mse_score: 0.1856)\n",
      "12:29 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0478 (mse_score: 0.0478)\n",
      "12:29 madminer.utils.ml.sc INFO                val. loss  0.1837 (mse_score: 0.1837)\n",
      "12:32 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0463 (mse_score: 0.0463)\n",
      "12:32 madminer.utils.ml.sc INFO                val. loss  0.1831 (mse_score: 0.1831) (*)\n",
      "12:35 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0451 (mse_score: 0.0451)\n",
      "12:35 madminer.utils.ml.sc INFO                val. loss  0.1836 (mse_score: 0.1836)\n",
      "12:38 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0443 (mse_score: 0.0443)\n",
      "12:38 madminer.utils.ml.sc INFO                val. loss  0.1827 (mse_score: 0.1827)\n",
      "12:38 madminer.utils.ml.sc INFO    Early stopping after epoch 47, with loss 0.18 compared to final loss 0.18\n",
      "12:38 madminer.utils.ml.sc INFO    Finished training\n",
      "12:38 madminer.ml          INFO    Training estimator 8 / 10 in ensemble\n",
      "12:38 madminer.ml          INFO    Starting training\n",
      "12:38 madminer.ml          INFO      Method:                 sally\n",
      "12:38 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_7.npy\n",
      "12:38 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_7.npy\n",
      "12:38 madminer.ml          INFO      Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "12:38 madminer.ml          INFO      Method:                 sally\n",
      "12:38 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "12:38 madminer.ml          INFO      Activation function:    tanh\n",
      "12:38 madminer.ml          INFO      Batch size:             128\n",
      "12:38 madminer.ml          INFO      Trainer:                amsgrad\n",
      "12:38 madminer.ml          INFO      Epochs:                 50\n",
      "12:38 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "12:38 madminer.ml          INFO      Validation split:       0.5\n",
      "12:38 madminer.ml          INFO      Early stopping:         True\n",
      "12:38 madminer.ml          INFO      Scale inputs:           True\n",
      "12:38 madminer.ml          INFO      Shuffle labels          False\n",
      "12:38 madminer.ml          INFO      Regularization:         None\n",
      "12:38 madminer.ml          INFO      Samples:                all\n",
      "12:38 madminer.ml          INFO    Loading training data\n",
      "12:38 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "12:38 madminer.ml          INFO    Rescaling inputs\n",
      "12:38 madminer.ml          INFO    Only using 27 of 33 observables\n",
      "12:38 madminer.ml          INFO    Creating model for method sally\n",
      "12:38 madminer.ml          INFO    Training model\n",
      "12:41 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.2319 (mse_score: 0.2319)\n",
      "12:41 madminer.utils.ml.sc INFO                val. loss  0.1353 (mse_score: 0.1353) (*)\n",
      "12:44 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.2071 (mse_score: 0.2071)\n",
      "12:44 madminer.utils.ml.sc INFO                val. loss  0.1250 (mse_score: 0.1250)\n",
      "12:46 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.1905 (mse_score: 0.1905)\n",
      "12:46 madminer.utils.ml.sc INFO                val. loss  0.1167 (mse_score: 0.1167)\n",
      "12:49 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.1806 (mse_score: 0.1806)\n",
      "12:49 madminer.utils.ml.sc INFO                val. loss  0.1081 (mse_score: 0.1081) (*)\n",
      "12:52 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.1734 (mse_score: 0.1734)\n",
      "12:52 madminer.utils.ml.sc INFO                val. loss  0.1060 (mse_score: 0.1060)\n",
      "12:55 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.1687 (mse_score: 0.1687)\n",
      "12:55 madminer.utils.ml.sc INFO                val. loss  0.1042 (mse_score: 0.1042) (*)\n",
      "12:58 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.1651 (mse_score: 0.1651)\n",
      "12:58 madminer.utils.ml.sc INFO                val. loss  0.1043 (mse_score: 0.1043)\n",
      "13:00 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.1617 (mse_score: 0.1617)\n",
      "13:00 madminer.utils.ml.sc INFO                val. loss  0.1038 (mse_score: 0.1038)\n",
      "13:03 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.1594 (mse_score: 0.1594)\n",
      "13:03 madminer.utils.ml.sc INFO                val. loss  0.1024 (mse_score: 0.1024) (*)\n",
      "13:06 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.1574 (mse_score: 0.1574)\n",
      "13:06 madminer.utils.ml.sc INFO                val. loss  0.1020 (mse_score: 0.1020) (*)\n",
      "13:06 madminer.utils.ml.sc INFO    Early stopping did not improve performance\n",
      "13:06 madminer.utils.ml.sc INFO    Finished training\n",
      "13:06 madminer.ml          INFO    Training estimator 9 / 10 in ensemble\n",
      "13:06 madminer.ml          INFO    Starting training\n",
      "13:06 madminer.ml          INFO      Method:                 sally\n",
      "13:06 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_8.npy\n",
      "13:06 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_8.npy\n",
      "13:06 madminer.ml          INFO      Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "13:06 madminer.ml          INFO      Method:                 sally\n",
      "13:06 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "13:06 madminer.ml          INFO      Activation function:    tanh\n",
      "13:06 madminer.ml          INFO      Batch size:             128\n",
      "13:06 madminer.ml          INFO      Trainer:                amsgrad\n",
      "13:06 madminer.ml          INFO      Epochs:                 50\n",
      "13:06 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:06 madminer.ml          INFO      Validation split:       0.5\n",
      "13:06 madminer.ml          INFO      Early stopping:         True\n",
      "13:06 madminer.ml          INFO      Scale inputs:           True\n",
      "13:06 madminer.ml          INFO      Shuffle labels          False\n",
      "13:06 madminer.ml          INFO      Regularization:         None\n",
      "13:06 madminer.ml          INFO      Samples:                all\n",
      "13:06 madminer.ml          INFO    Loading training data\n",
      "13:06 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:06 madminer.ml          INFO    Rescaling inputs\n",
      "13:06 madminer.ml          INFO    Only using 27 of 33 observables\n",
      "13:06 madminer.ml          INFO    Creating model for method sally\n",
      "13:06 madminer.ml          INFO    Training model\n",
      "13:09 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.2005 (mse_score: 0.2005)\n",
      "13:09 madminer.utils.ml.sc INFO                val. loss  0.1568 (mse_score: 0.1568) (*)\n",
      "13:12 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.1790 (mse_score: 0.1790)\n",
      "13:12 madminer.utils.ml.sc INFO                val. loss  0.1456 (mse_score: 0.1456) (*)\n",
      "13:15 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.1658 (mse_score: 0.1658)\n",
      "13:15 madminer.utils.ml.sc INFO                val. loss  0.1389 (mse_score: 0.1389) (*)\n",
      "13:18 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.1580 (mse_score: 0.1580)\n",
      "13:18 madminer.utils.ml.sc INFO                val. loss  0.1367 (mse_score: 0.1367) (*)\n",
      "13:21 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.1525 (mse_score: 0.1525)\n",
      "13:21 madminer.utils.ml.sc INFO                val. loss  0.1361 (mse_score: 0.1361)\n",
      "13:24 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.1485 (mse_score: 0.1485)\n",
      "13:24 madminer.utils.ml.sc INFO                val. loss  0.1350 (mse_score: 0.1350)\n",
      "13:27 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.1455 (mse_score: 0.1455)\n",
      "13:27 madminer.utils.ml.sc INFO                val. loss  0.1335 (mse_score: 0.1335)\n",
      "13:30 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.1430 (mse_score: 0.1430)\n",
      "13:30 madminer.utils.ml.sc INFO                val. loss  0.1333 (mse_score: 0.1333) (*)\n",
      "13:33 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.1411 (mse_score: 0.1411)\n",
      "13:33 madminer.utils.ml.sc INFO                val. loss  0.1333 (mse_score: 0.1333)\n",
      "13:36 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.1397 (mse_score: 0.1397)\n",
      "13:36 madminer.utils.ml.sc INFO                val. loss  0.1327 (mse_score: 0.1327) (*)\n",
      "13:36 madminer.utils.ml.sc INFO    Early stopping did not improve performance\n",
      "13:36 madminer.utils.ml.sc INFO    Finished training\n",
      "13:36 madminer.ml          INFO    Training estimator 10 / 10 in ensemble\n",
      "13:36 madminer.ml          INFO    Starting training\n",
      "13:36 madminer.ml          INFO      Method:                 sally\n",
      "13:36 madminer.ml          INFO      Training data:          x at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/x_train_9.npy\n",
      "13:36 madminer.ml          INFO                              t_xz (theta0) at /Users/johannbrehmer/work/projects/madminer/diboson_mining/data/samples/wgamma_sys/train_local/t_xz_train_9.npy\n",
      "13:36 madminer.ml          INFO      Features:               [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "13:36 madminer.ml          INFO      Method:                 sally\n",
      "13:36 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "13:36 madminer.ml          INFO      Activation function:    tanh\n",
      "13:36 madminer.ml          INFO      Batch size:             128\n",
      "13:36 madminer.ml          INFO      Trainer:                amsgrad\n",
      "13:36 madminer.ml          INFO      Epochs:                 50\n",
      "13:36 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:36 madminer.ml          INFO      Validation split:       0.5\n",
      "13:36 madminer.ml          INFO      Early stopping:         True\n",
      "13:36 madminer.ml          INFO      Scale inputs:           True\n",
      "13:36 madminer.ml          INFO      Shuffle labels          False\n",
      "13:36 madminer.ml          INFO      Regularization:         None\n",
      "13:36 madminer.ml          INFO      Samples:                all\n",
      "13:36 madminer.ml          INFO    Loading training data\n",
      "13:36 madminer.ml          INFO    Found 1000000 samples with 34 parameters and 33 observables\n",
      "13:36 madminer.ml          INFO    Rescaling inputs\n",
      "13:36 madminer.ml          INFO    Only using 27 of 33 observables\n",
      "13:36 madminer.ml          INFO    Creating model for method sally\n",
      "13:36 madminer.ml          INFO    Training model\n",
      "13:39 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.1574 (mse_score: 0.1574)\n",
      "13:39 madminer.utils.ml.sc INFO                val. loss  0.1071 (mse_score: 0.1071)\n",
      "13:42 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.1298 (mse_score: 0.1298)\n",
      "13:42 madminer.utils.ml.sc INFO                val. loss  0.0916 (mse_score: 0.0916) (*)\n",
      "13:44 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.1176 (mse_score: 0.1176)\n",
      "13:44 madminer.utils.ml.sc INFO                val. loss  0.0866 (mse_score: 0.0866) (*)\n",
      "13:47 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.1089 (mse_score: 0.1089)\n",
      "13:47 madminer.utils.ml.sc INFO                val. loss  0.0846 (mse_score: 0.0846) (*)\n",
      "13:50 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.1025 (mse_score: 0.1025)\n",
      "13:50 madminer.utils.ml.sc INFO                val. loss  0.0833 (mse_score: 0.0833) (*)\n",
      "13:53 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0977 (mse_score: 0.0977)\n",
      "13:53 madminer.utils.ml.sc INFO                val. loss  0.0853 (mse_score: 0.0853)\n",
      "13:56 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0941 (mse_score: 0.0941)\n",
      "13:56 madminer.utils.ml.sc INFO                val. loss  0.0837 (mse_score: 0.0837)\n",
      "14:01 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0914 (mse_score: 0.0914)\n",
      "14:01 madminer.utils.ml.sc INFO                val. loss  0.0836 (mse_score: 0.0836)\n",
      "14:04 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0893 (mse_score: 0.0893)\n",
      "14:04 madminer.utils.ml.sc INFO                val. loss  0.0856 (mse_score: 0.0856)\n",
      "14:07 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0876 (mse_score: 0.0876)\n",
      "14:07 madminer.utils.ml.sc INFO                val. loss  0.0834 (mse_score: 0.0834)\n",
      "14:07 madminer.utils.ml.sc INFO    Early stopping after epoch 39, with loss 0.08 compared to final loss 0.08\n",
      "14:07 madminer.utils.ml.sc INFO    Finished training\n"
     ]
    }
   ],
   "source": [
    "train_ensemble(\n",
    "    'minimal',\n",
    "    use_tight_cuts=False,\n",
    "    features=[min_obs for _ in range(n_estimators)],\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'minimal_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[min_obs for _ in range(n_estimators)],\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just resurrection phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'phi_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[32] for _ in range(n_estimators)],\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble(\n",
    "    'pta_phi_tight',\n",
    "    use_tight_cuts=True,\n",
    "    features=[[9, 32] for _ in range(n_estimators)],\n",
    "    validation_split=0.5,\n",
    "    early_stopping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (higgs_inference)",
   "language": "python",
   "name": "higgs_inference"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
